- en: '*Chapter 7*: Model Deployment and Automation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 7 章*：模型部署与自动化'
- en: In the previous chapter, you saw how the platform enables you to build and register
    the model in an autonomous fashion. In this chapter, we will extend the **machine
    learning** (**ML**) engineering domain to model deployment, monitoring, and automation
    of deployment activities.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解了平台如何使你能够以自主管理的方式构建和注册模型。在本章中，我们将扩展 **机器学习** (**ML**) 工程领域，涵盖模型部署、监控和部署活动的自动化。
- en: You will learn how the platform provides the model packaging and deployment
    capabilities and how you can automate them. You will take the model from the registry,
    package it as a container, and deploy the model onto the platform to be consumed
    as an API. You will then automate all these steps using the workflow engine provided
    by the platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你将了解平台如何提供模型打包和部署功能，以及如何自动化这些过程。你将从注册表中获取模型，将其打包成容器，并将模型部署到平台上，作为 API 提供服务。接着，你将使用平台提供的工作流引擎自动化所有这些步骤。
- en: Once your model is deployed, it works well for the data it was trained upon.
    The real world, however, changes. You will see how the platform allows you to
    observe your model's performance. This chapter discusses the tools and techniques
    to monitor your model performance. The performance data could be used to decide
    whether the model needs retraining on the new dataset, or whether it is time to
    build a new model for the given problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型部署完成，它将在其训练数据上表现良好。然而，现实世界是不断变化的。你将看到平台如何让你观察模型的性能。本章将讨论监控模型性能的工具和技术。这些性能数据可以帮助决定模型是否需要基于新的数据集重新训练，或者是否该为给定问题构建一个新的模型。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下主题：
- en: Understanding model inferencing with Seldon Core
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解使用 Seldon Core 的模型推理
- en: Packaging, running, and monitoring a model using Seldon Core
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Seldon Core 打包、运行和监控模型
- en: Understanding Apache Airflow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Apache Airflow
- en: Automating ML model deployments in Airflow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Airflow 中自动化 ML 模型部署
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with **Operator Lifecycle Manager**. Building such
    a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and **Open Data Hub**
    (**ODH**) is installed on your Kubernetes cluster. Installing ODH is covered in
    [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含一些动手设置和练习。你将需要一个运行中的 Kubernetes 集群，并且已配置 **Operator Lifecycle Manager**。如何构建这样的
    Kubernetes 环境将在[*第 3 章*](B18332_03_ePub.xhtml#_idTextAnchor040)中讨论，*探索 Kubernetes*。在尝试本章的技术练习之前，请确保你已经有一个可用的
    Kubernetes 集群，并且 **Open Data Hub** (**ODH**) 已经安装在你的 Kubernetes 集群上。安装 ODH 的过程将在[*第
    4 章*](B18332_04_ePub.xhtml#_idTextAnchor055)中讨论，*机器学习平台的结构*。
- en: Understanding model inferencing with Seldon Core
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解使用 Seldon Core 的模型推理
- en: In the previous chapter, you built the model. These models are built by data
    science teams to be used in production and serve the prediction requests. There
    are many ways to use a model in production, such as embedding the model with your
    customer-facing program, but the most common way is to expose the model as a REST
    API. The REST API can then be used by any application. In general, running and
    serving a model in production is called **model serving**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你已经构建了模型。这些模型是由数据科学团队构建的，用于生产环境并处理预测请求。有多种方式可以将模型投入生产使用，比如将模型嵌入到面向客户的程序中，但最常见的方式是将模型作为
    REST API 公开。然后，任何应用程序都可以使用这个 REST API。通常，运行和提供生产中的模型被称为 **模型服务**。
- en: However, once the model is in production, it needs to be monitored for performance
    and needs updating to meet the expected criteria. A hosted model solution enables
    you to not only serve the model but monitor its performance and generate alerts
    that can be used to trigger retraining of the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦模型进入生产环境，它需要被监控其性能，并且需要更新以满足预期标准。托管的模型解决方案不仅可以让你提供模型服务，还能监控其性能并生成警报，用以触发模型的重新训练。
- en: 'Seldon is a UK-based firm that created a set of tools to manage the model''s
    life cycle. Seldon Core is an open source framework that helps expose ML models
    to be consumed as REST APIs. Seldon Core automatically exposes the monitoring
    statistics for the REST API, which can be consumed by **Prometheus**, the platform''s
    monitoring component. To expose your model as a REST API in the platform, the
    following steps are required:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon 是一家总部位于英国的公司，创建了一套用于管理模型生命周期的工具。Seldon Core 是一个开源框架，帮助将机器学习模型暴露为 REST
    API 以供使用。Seldon Core 会自动暴露 REST API 的监控统计信息，这些信息可以被平台的监控组件**Prometheus**使用。要在平台中将模型暴露为
    REST API，您需要完成以下步骤：
- en: Write a language-specific wrapper for your model to expose as a service.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您的模型编写特定语言的包装器，将其暴露为服务。
- en: Containerize your model.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的模型容器化。
- en: Define and deploy the model using the inference graph of your model using Seldon
    Deployment **custom resource** (**CR**) in Kubernetes
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Seldon 部署 **自定义资源**（**CR**）在 Kubernetes 中定义并部署模型，利用模型的推理图。
- en: Next, we will see these three steps in detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细了解这三个步骤。
- en: Wrapping the model using Python
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 封装模型
- en: Let's see how you can apply the preceding steps. In [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086),
    *Machine Learning Engineering*, you registered your experiment details and a model
    with the MLflow server. Recall that the model file was stored in the artifacts
    of MLflow and named `model.pkl`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何应用前面的步骤。在 [*第六章*](B18332_06_ePub.xhtml#_idTextAnchor086)，*机器学习工程* 中，您已将实验细节和模型注册到
    MLflow 服务器。回忆一下，模型文件被存储在 MLflow 的工件中，并命名为 `model.pkl`。
- en: 'Let''s take the model file and write a simple Python wrapper around it. The
    job of the wrapper is to use Seldon libraries to conveniently expose the model
    as a REST service. You can find the example of the wrapper in the code at `chapter7/model_deploy_pipeline/model_build_push/Predictor.py`.
    The key component of this wrapper is a function named `predict` that will be invoked
    from an HTTP endpoint created by the Seldon framework. *Figure 7.1* shows a simple
    Python wrapper using a `joblib` model:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将以模型文件为基础，写一个简单的 Python 包装器。包装器的作用是使用 Seldon 库将模型便捷地暴露为 REST 服务。您可以在 `chapter7/model_deploy_pipeline/model_build_push/Predictor.py`
    的代码中找到包装器的示例。这个包装器的关键组件是一个名为 `predict` 的函数，它会从 Seldon 框架创建的 HTTP 端点被调用。*图 7.1*
    显示了一个使用 `joblib` 模型的简单 Python 包装器：
- en: '![Figure 7.1 – A Python language wrapper for the model prediction'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1 – 用于模型预测的 Python 语言包装器'
- en: '](img/B18332_07_001.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_001.jpg)'
- en: Figure 7.1 – A Python language wrapper for the model prediction
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 用于模型预测的 Python 语言包装器
- en: The `predict` function receives a `numpy` array (`data_array`) and a set of
    column names (`column_names`), serialized from the HTTP request. The method returns
    the result of the prediction as either a `numpy` array or a list of values or
    bytes. There are many more methods available for the language wrapper and a full
    list is available at [https://docs.seldon.io/projects/seldon-core/en/v1.12.0/python/python_component.html#low-level-methods](https://docs.seldon.io/projects/seldon-core/en/v1.12.0/python/python_component.html#low-level-methods).
    Note that in later chapters of this book, you will see a more thorough inferencing
    example that will have additional wrappers for data transformation before prediction.
    But, for this chapter, we keep it as simple as possible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict` 函数接收一个 `numpy` 数组（`data_array`）和一组列名（`column_names`），这些都是从 HTTP 请求中序列化过来的。该方法返回预测结果，结果可以是一个
    `numpy` 数组，或者是一个值或字节的列表。对于语言包装器，还有许多其他方法可以使用，完整的方法列表可以在 [https://docs.seldon.io/projects/seldon-core/en/v1.12.0/python/python_component.html#low-level-methods](https://docs.seldon.io/projects/seldon-core/en/v1.12.0/python/python_component.html#low-level-methods)
    中找到。请注意，在本书的后续章节中，您将看到更为详细的推理示例，其中会有额外的包装器用于数据转换。但在本章中，我们尽量保持简单。'
- en: The language wrapper is ready, and the next stage is to containerize the model
    and the language wrapper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 语言包装器已准备好，下一步是将模型和语言包装器容器化。
- en: Containerizing the model
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器化模型
- en: What would you put in the container? Let's start with a list. You will need
    the model and the wrapper files. You will need the Seldon Python packages available
    in the container. Once you have all these packages, then you will use the Seldon
    services to expose the model. *Figure 7.2* shows a `Docker` file that is building
    one such container. This file is available in `Chapter 7/model_deployment_pipeline/model_build_push/Dockerfile.py`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你会把什么放入容器中？我们从一份清单开始。你需要模型和包装器文件。你还需要容器中可用的Seldon Python包。获取这些包后，你将使用Seldon服务来暴露模型。*图7.2*展示了一个构建此类容器的`Docker`文件。该文件位于`Chapter
    7/model_deployment_pipeline/model_build_push/Dockerfile.py`。
- en: '![Figure 7.2 – Docker file to package the model as a container'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.2 – 用于将模型打包为容器的Docker文件'
- en: '](img/B18332_07_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_002.jpg)'
- en: Figure 7.2 – Docker file to package the model as a container
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 用于将模型打包为容器的Docker文件
- en: 'Now, let''s understand the content of the Docker file:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解Docker文件的内容：
- en: '*Line 1* indicates the base container image for your model service. We have
    chosen the freely available image from Red Hat, but you can choose as per your
    convenience. This image could be your organization''s base image with the standard
    version of Python and related software.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第1行*指示了你模型服务的基础容器镜像。我们选择了来自Red Hat的免费镜像，但你可以根据自己的需要选择。这张镜像可以是你公司基础镜像，包含标准版本的Python及相关软件。'
- en: In *Line 3*, we have created a `microservice` directory to place all the related
    artifacts in our container.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第3行*，我们创建了一个`microservice`目录，将所有相关的文件放入我们的容器中。
- en: In *Line 4*, the first file we need to build the container is `base_requirements.txt`.
    This file contains the packages and dependencies for the Seldon Core system. You
    can find this file at `chapter7/model_deployment_pipeline/model_build_push/base_requirements.txt`.
    In this file, you will see that Seldon Core packages and `joblib` packages have
    been added.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第4行*，我们构建容器所需的第一个文件是`base_requirements.txt`。该文件包含Seldon Core系统的包和依赖项。你可以在`chapter7/model_deployment_pipeline/model_build_push/base_requirements.txt`中找到此文件。在该文件中，你会看到已添加了Seldon
    Core包和`joblib`包。
- en: '*Figure 7.3* shows the `base_requirements.txt` file:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.3*展示了`base_requirements.txt`文件：'
- en: '![Figure 7.3 – File adding Seldon and Joblib to the container'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3 – 向容器中添加Seldon和Joblib文件'
- en: '](img/B18332_07_003.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_003.jpg)'
- en: Figure 7.3 – File adding Seldon and Joblib to the container
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 向容器中添加Seldon和Joblib文件
- en: '*Line 5* is using the `base_requirements.txt` file to install the Python packages
    onto the container.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第5行*使用`base_requirements.txt`文件将Python包安装到容器中。'
- en: In *Lines 7* and *8*, when you are training the model, you may use different
    packages. During inferencing, some of the packages may be needed; for example,
    if you have done input data scaling before model training using a library, you
    may need the same library to apply the scaling at inference time.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*第7行*和*第8行*，当你训练模型时，可能会使用不同的包。在推理时，某些包可能会需要；例如，如果你在模型训练之前使用某个库进行了输入数据缩放，你可能需要相同的库来在推理时应用缩放。
- en: In [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning Engineering*,
    you registered your experiment details and a model with the MLflow server. Recall
    that the model file was stored in the artifacts along with a file containing packages
    used to train the model named `requirements.txt`. Using the `requirements.txt`
    file generated by MLflow, you can install the packages required to run your model,
    or you may choose to add these dependencies on your own to a custom file. *Figure
    7.4* shows the MLflow snapshot referred to in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086),
    *Machine Learning Engineering*. You can see the `requirements.txt` file here next
    to the `model.pkl` file.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B18332_06_ePub.xhtml#_idTextAnchor086)，*机器学习工程*中，你将实验详情和模型注册到MLflow服务器。回想一下，模型文件与包含训练模型所用包的文件`requirements.txt`一起存储在工件中。使用MLflow生成的`requirements.txt`文件，你可以安装运行模型所需的包，或者你也可以选择将这些依赖项添加到自己的自定义文件中。*图7.4*展示了在[*第6章*](B18332_06_ePub.xhtml#_idTextAnchor086)，*机器学习工程*中提到的MLflow快照。你可以在这里看到与`model.pkl`文件并排的`requirements.txt`文件。
- en: '![Figure 7.4 – MLflow run artifacts'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 – MLflow运行工件'
- en: '](img/B18332_07_004.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_004.jpg)'
- en: Figure 7.4 – MLflow run artifacts
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – MLflow运行工件
- en: '*Line 10*: You add the language wrapper files and the model files to the container.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*第10行*：你将语言包装器文件和模型文件添加到容器中。'
- en: '*Line 11*: Here, you are using the `seldon-core-microservice` server to start
    the inferencing server. Notice that the parameters have been passed here, and
    in the next section, you will see how we can pass these parameters:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 11 行*：在这里，您使用 `seldon-core-microservice` 服务器启动推理服务器。请注意，参数已经在这里传递，接下来您将看到如何传递这些参数：'
- en: '**MODEL_NAME**: This is the name of the Python class in the language wrapper
    containing the model.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MODEL_NAME**：这是包含模型的语言包装器中的 Python 类名称。'
- en: '`MODEL`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL`。'
- en: '**GRPC_PORT**: The port at which the **Google Remote Procedure Call** (**gRPC**)
    endpoint will listen for model inference.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRPC_PORT**：**Google 远程过程调用**（**gRPC**）端点将监听模型推理的端口。'
- en: '**METRICS_PORT**: The port at which the service performance data will be exposed.
    Note that this is the performance data for the service and not the model.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**METRICS_PORT**：暴露服务性能数据的端口。请注意，这里指的是服务的性能数据，而非模型的数据。'
- en: '**HTTP_NAME**: The HTTP port where will you serve the model over HTTP.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP_NAME**：您将通过 HTTP 提供模型的 HTTP 端口。'
- en: Now, we have a container specification in the form of the Docker file. Next,
    we will see how to deploy the container on the Kubernetes platform using the Seldon
    controller.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个以 Docker 文件形式的容器规格。接下来，我们将看到如何使用 Seldon 控制器在 Kubernetes 平台上部署容器。
- en: Deploying the model using the Seldon controller
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Seldon 控制器部署模型
- en: Our ML platform provides a Seldon controller, a piece of software that runs
    as a pod and assists in deploying the containers you built in the preceding section.
    Note that the controller in our platform is the extension of the existing Seldon
    operator. At the time of writing, the Seldon operator was not compatible with
    Kubernetes version 1.22, so we have extended the existing operator to work with
    the latest and future versions of the Kubernetes platform.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的机器学习平台提供了一个 Seldon 控制器，它作为一个 Pod 运行，并协助部署您在前一部分构建的容器。请注意，我们平台中的控制器是现有 Seldon
    操作符的扩展。在撰写本文时，Seldon 操作符不兼容 Kubernetes 版本 1.22，因此我们扩展了现有操作符，以便与最新及未来版本的 Kubernetes
    平台兼容。
- en: 'Refer to [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy
    of a Machine Learning Platform*, where you learned about installing ODH and how
    it works on the Kubernetes cluster. In an equivalent manner, the Seldon controller
    is also installed by the ODH operator. The `manifests/ml-platform.yaml` file has
    the configuration for installing the Seldon controller. *Figure 7.5* shows the
    settings:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 [*第 4 章*](B18332_04_ePub.xhtml#_idTextAnchor055)，*机器学习平台的构成*，在那里您学习了如何安装
    ODH 以及它如何在 Kubernetes 集群上工作。以同样的方式，Seldon 控制器也是由 ODH 操作符安装的。`manifests/ml-platform.yaml`
    文件包含了安装 Seldon 控制器的配置。*图 7.5* 展示了这些设置：
- en: '![Figure 7.5 – MLFlow section of the manifest file'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 清单文件的 MLFlow 部分'
- en: '](img/B18332_07_005.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_005.jpg)'
- en: Figure 7.5 – MLFlow section of the manifest file
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 清单文件的 MLFlow 部分
- en: 'Let''s verify whether the Seldon controller is running correctly in the cluster:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证 Seldon 控制器是否在集群中正确运行：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see the following response:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 7.6 – Seldon controller pod'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – Seldon 控制器 Pod'
- en: '](img/B18332_07_006.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_006.jpg)'
- en: Figure 7.6 – Seldon controller pod
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – Seldon 控制器 Pod
- en: 'The Seldon controller pod is installed by the ODH operators, which watch for
    the Seldon Deployment CR. This schema for this resource is defined by the Seldon
    Deployment `manifests/odhseldon/cluster/base/seldon-operator-crd-seldondeployments.yaml`.
    Once you create the Seldon Deployment CR, the controller deploys the pods associated
    with the CR. *Figure 7.7* shows this relationship:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon 控制器 Pod 是由 ODH 操作符安装的，这些操作符会监视 Seldon 部署 CR。该资源的架构由 Seldon 部署 `manifests/odhseldon/cluster/base/seldon-operator-crd-seldondeployments.yaml`
    定义。一旦创建 Seldon 部署 CR，控制器将部署与该 CR 关联的 Pod。*图 7.7* 展示了这种关系：
- en: '![Figure 7.7 – Components of the platform for deploying Seldon services'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – 用于部署 Seldon 服务的平台组件'
- en: '](img/B18332_07_007.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_007.jpg)'
- en: Figure 7.7 – Components of the platform for deploying Seldon services
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 用于部署 Seldon 服务的平台组件
- en: Let's see the different components of the Seldon Deployment CR. You can find
    one simple example in `chapter7/manual_model_deployment/SeldonDeploy.yaml`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Seldon 部署 CR 的不同组件。您可以在 `chapter7/manual_model_deployment/SeldonDeploy.yaml`
    中找到一个简单的示例。
- en: 'The Seldon Deployment CR contains all the information that is required by the
    Seldon controller to deploy your model on the Kubernetes cluster. There are three
    main sections in the Seldon Deployment CR:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon 部署 CR 包含了 Seldon 控制器部署模型到 Kubernetes 集群所需的所有信息。Seldon 部署 CR 主要有三个部分：
- en: '`apiVersion`, `kind`, and other Kubernetes-related information. You will define
    the labels and name of the Seldon Deployment as any other Kubernetes object. You
    can see in the following screenshot that it contains the labels and annotations
    for the object:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apiVersion`、`kind` 以及其他与 Kubernetes 相关的信息。你将像定义其他 Kubernetes 对象一样定义 Seldon
    部署的标签和名称。你可以在以下截图中看到它包含了对象的标签和注释：'
- en: '![Figure 7.8 – Seldon Deployment – Kubernetes-related information'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – Seldon 部署 – 与 Kubernetes 相关的信息'
- en: '](img/B18332_07_008.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_008.jpg)'
- en: Figure 7.8 – Seldon Deployment – Kubernetes-related information
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – Seldon 部署 – 与 Kubernetes 相关的信息
- en: '`chapter7/manual_model_deployment/SeldonDeploy.yaml` file that has this information.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含该信息的 `chapter7/manual_model_deployment/SeldonDeploy.yaml` 文件。
- en: Notice that `containers` take an array for the `image` object, so you can add
    more images to it. The `image` key will have the location of your container. The
    `env` array defines the environment variables that will be available for the pod.
    Recall that, in our Docker file in the previous section, these variables have
    been used. `MODEL_NAME` has a value of `Predictor`, which is the name of the class
    you have used as a wrapper. `SERVICE_TYPE` has a value of `MODEL`, which mentions
    the type of service this container provides.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`containers` 为 `image` 对象提供了一个数组，因此你可以向其中添加更多的镜像。`image` 键将包含你的容器位置。`env`
    数组定义了将可用于 pod 的环境变量。回想一下，在前一部分的 Docker 文件中，这些变量已被使用。`MODEL_NAME` 的值为 `Predictor`，即你用作封装器的类名。`SERVICE_TYPE`
    的值为 `MODEL`，表示此容器提供的服务类型。
- en: The last part has `hpaSpec`, which the Seldon controller will translate onto
    the `maxReplicas` is set to `1`, so there will not be any new pods, but you can
    control this value for each deployment. The scalability will kick in if the CPU
    utilization goes beyond 80% for the pods in the following example; however, because
    `maxReplica` is `1`, there will not be any new pods created.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分包含 `hpaSpec`，Seldon 控制器会将其转换为 `maxReplicas` 设置为 `1`，因此不会创建新的 pods，但你可以为每次部署控制此值。如果
    CPU 使用率超过 80%（对于以下示例中的 pods），可扩展性将启动；然而，由于 `maxReplica` 设置为 `1`，因此不会创建新的 pods。
- en: '![Figure 7.9 – Seldon Deployment – Seldon service containers'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9 – Seldon 部署 – Seldon 服务容器'
- en: '](img/B18332_07_009.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_009.jpg)'
- en: Figure 7.9 – Seldon Deployment – Seldon service containers
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – Seldon 部署 – Seldon 服务容器
- en: '`graph` key builds the inference graph for your service. An inference graph
    will have different nodes and you will define what container will be used at each
    node. You will see there is a `children` key that takes an array of objects through
    which you define your inference graph. For this example, `graph` has only one
    node and the `children` key has no information associated with it; however, in
    the later chapters, you will see how to build the inference graph with more nodes.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`graph` 键为你的服务构建推理图。推理图将有多个节点，你将定义每个节点使用的容器。你会看到有一个 `children` 键，它接受一个对象数组，你通过该数组定义推理图。在此示例中，`graph`
    只有一个节点，`children` 键没有与之关联的信息；然而，在后续章节中，你将看到如何构建具有更多节点的推理图。'
- en: The remaining fields under the graph define the first node of your inference
    graph. The `name` field has the value that corresponds to the name you have given
    in the `containers` section. Note that this is the key through which Seldon knows
    what container would be serving at this node of your inference graph.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图表下方的其余字段定义了推理图的第一个节点。`name` 字段的值与 `containers` 部分中你所定义的名称相对应。请注意，这是 Seldon
    知道在推理图的该节点处提供服务的容器的关键。
- en: The other important part is the `logger` section. Seldon can automatically forward
    the request and response to the URL mentioned under the `logger` section. The
    capability of forwarding the request and response can be used for a variety of
    scenarios, such as storing the payload for audit/legal reasons or applying data
    drift algorithms to trigger retraining or anything else. Note that Seldon can
    also forward to Kafka if needed, but this is outside the scope of this book.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要部分是 `logger` 部分。Seldon 可以自动将请求和响应转发到 `logger` 部分下提到的 URL。转发请求和响应的功能可以用于多种场景，例如出于审计/法律原因存储负载，或者应用数据漂移算法触发重新训练，或其他用途。请注意，Seldon
    也可以在需要时将请求转发到 Kafka，但这超出了本书的讨论范围。
- en: '![Figure 7.10 – Seldon Deployment – inference graph'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.10 – Seldon 部署 – 推理图'
- en: '](img/B18332_07_010.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_010.jpg)'
- en: Figure 7.10 – Seldon Deployment – inference graph
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – Seldon 部署 – 推理图
- en: Once you create the Seldon Deployment CR using the routine `kubectl` command,
    the Seldon controller will deploy the pods, and the model will be available for
    consumption as a service.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你使用常规的 `kubectl` 命令创建 Seldon 部署 CR，Seldon 控制器将部署 pods，并且模型将作为服务供消费。
- en: Next, we'll move on to packaging and deploying the basic model that you built
    in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning Engineering*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续打包并部署你在[*第 6 章*](B18332_06_ePub.xhtml#_idTextAnchor086)中构建的基础模型，*机器学习工程*。
- en: Packaging, running, and monitoring a model using Seldon Core
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Seldon Core 打包、运行和监控模型
- en: In this section, you will package and build the container from the model file
    you built in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning
    Engineering*. You will then use the Seldon Deployment to deploy and access the
    model. Later in this book, you will automate the process, but to do it manually,
    as you'll do in this section, we will further strengthen your understanding of
    the components and how they work.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将打包并构建容器，使用你在[*第 6 章*](B18332_06_ePub.xhtml#_idTextAnchor086)中构建的模型文件。然后，你将使用
    Seldon 部署来部署并访问该模型。稍后在本书中，你将自动化这一过程，但手动完成，如本节所做的那样，将进一步加深你对组件及其工作原理的理解。
- en: 'Before you start this exercise, please make sure that you have created an account
    with a public Docker registry. We will use the free `quay.io` as our registry,
    but you are free to use your preferred one:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始此练习之前，请确保你已经在公共 Docker 注册表上创建了一个帐户。我们将使用免费的 `quay.io` 作为我们的注册表，但你也可以使用你偏好的其他注册表：
- en: 'Let''s first verify that MLflow and Minio (our S3 server) are running in our
    cluster:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先验证 MLflow 和 Minio（我们的 S3 服务器）是否在集群中运行：
- en: '[PRE1]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see the following response:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.11 – MLflow and Minio are running on the platform'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.11 – MLflow 和 Minio 正在平台上运行'
- en: '](img/B18332_07_011.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_011.jpg)'
- en: Figure 7.11 – MLflow and Minio are running on the platform
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – MLflow 和 Minio 正在平台上运行
- en: 'Get the ingress list for MLflow, and log in to MLflow using the `mlflow` URL
    available from the following output:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 MLflow 的 ingress 列表，并通过以下输出中提供的 `mlflow` URL 登录 MLflow：
- en: '[PRE2]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see the following response:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.12 – ingress in your Kubernetes cluster'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.12 – 你的 Kubernetes 集群中的 ingress'
- en: '](img/B18332_07_012.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_012.jpg)'
- en: Figure 7.12 – ingress in your Kubernetes cluster
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 你的 Kubernetes 集群中的 ingress
- en: Once you are in the MLflow UI, navigate to the experiment that you recorded
    in [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning Engineering*.
    The name of the experiment is **HelloMIFlow**.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦进入 MLflow UI，导航到你在[*第 6 章*](B18332_06_ePub.xhtml#_idTextAnchor086)中记录的实验，*机器学习工程*。该实验的名称是**HelloMIFlow**。
- en: '![Figure 7.13 – MlFlow Experiment Tracking'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.13 – MlFlow 实验跟踪'
- en: '](img/B18332_07_013.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_013.jpg)'
- en: Figure 7.13 – MlFlow Experiment Tracking
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – MlFlow 实验跟踪
- en: Select the first run from the right-hand panel to get to the detail page of
    the run. From the `model.pkl` and you will see a little download arrow icon to
    the right. Use the icon to download the `requirements.txt` files from this screen.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从右侧面板选择第一个运行，进入该运行的详细页面。从`model.pkl`文件中，你会看到右侧有一个小的下载箭头图标。点击该图标下载`requirements.txt`文件。
- en: '![Figure 7.14 – MLflow experiment tracking – run details'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.14 – MLflow 实验跟踪 – 运行详情'
- en: '](img/B18332_07_014.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_014.jpg)'
- en: Figure 7.14 – MLflow experiment tracking – run details
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – MLflow 实验跟踪 – 运行详情
- en: Go to the folder where you have cloned the code repository that comes with this
    book. If you have not done so, please clone the [https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git)
    repository on your local machine.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入你克隆的代码仓库所在文件夹。如果还没克隆，请在本地机器上克隆[https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git](https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git)仓库。
- en: 'Then, go to the `chapter7/model_deploy_pipeline/model_build_push` folder and
    copy the two files downloaded in the previous step to this folder. In the end,
    this folder will have the following files:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，进入`chapter7/model_deploy_pipeline/model_build_push`文件夹，并将上一步骤中下载的两个文件复制到此文件夹。最终，这个文件夹将包含以下文件：
- en: '![Figure 7.15 – Sample files to package the model as a container'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.15 – 示例文件，用于将模型打包为容器'
- en: '](img/B18332_07_015.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_015.jpg)'
- en: Figure 7.15 – Sample files to package the model as a container
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 示例文件，用于将模型打包为容器
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The last two files are the ones that you have just copied. All other files are
    coming from the code repository that you have cloned.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个文件就是你刚才复制的文件。所有其他文件来自你克隆的代码仓库。
- en: Curious people will note that the `requirements.txt` file that you have downloaded
    from the MLFlow server contains the packages required while you run the notebook
    for model training. Not all of these packages (`mlflow`, for example) will be
    needed to execute the saved model. To keep things simple, we will add all of them
    to our container.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 好奇的人会注意到，你从 MLFlow 服务器下载的`requirements.txt`文件包含了在运行笔记本进行模型训练时所需的包。并不是所有这些包（例如`mlflow`）都需要用来执行保存的模型。为了简单起见，我们将它们全部添加到我们的容器中。
- en: 'Now, let''s build the container on the local machine:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在本地机器上构建容器：
- en: '[PRE3]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following response:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.16 – Packaging the model as a container'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.16 – 将模型打包为容器'
- en: '](img/B18332_07_016.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_016.jpg)'
- en: Figure 7.16 – Packaging the model as a container
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 将模型打包为容器
- en: 'The next step is to tag the container and push it to the repository of your
    choice. Before you push your image to a repository, you will need to have an account
    with an image registry. If you do not have one, you can create one at [https://hub.docker.com](https://hub.docker.com)
    or [https://quay.io](https://quay.io). Once you have created your registry, you
    can run the following commands to tag and push the image:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是标记容器并将其推送到你选择的仓库。在将镜像推送到仓库之前，你需要拥有一个镜像注册表账户。如果没有，可以在[https://hub.docker.com](https://hub.docker.com)
    或 [https://quay.io](https://quay.io) 创建一个账户。创建好注册表后，你可以运行以下命令来标记并推送镜像：
- en: '[PRE4]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see the following response. You will notice that, in the following
    screenshot, we refer to `quay.io/ml-on-k8s` as our registry:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应。你会注意到，在以下截图中，我们将`quay.io/ml-on-k8s`称为我们的注册表：
- en: '![Figure 7.17 – Pushing the model to a public repository'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.17 – 将模型推送到公共仓库'
- en: '](img/B18332_07_017.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_017.jpg)'
- en: Figure 7.17 – Pushing the model to a public repository
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 将模型推送到公共仓库
- en: Now that your container is available in a registry, you will need to use the
    Seldon Deployment CR to deploy it as a service. Open the `chapter7/manual_model_deployment/SeldonDeploy.yaml`
    file and adjust the location of the image.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你的容器已经在一个注册表中可用，你需要使用 Seldon 部署 CR 将其部署为服务。打开`chapter7/manual_model_deployment/SeldonDeploy.yaml`文件，并调整镜像的位置。
- en: 'You can see the file after I have modified *line 16* (as per my image location)
    as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我修改后的文件，它显示了*第16行*（根据我的镜像位置）如下：
- en: '![Figure 7.18 – Seldon Deployment CR with the image location'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.18 – 带有镜像位置的 Seldon 部署 CR'
- en: '](img/B18332_07_018.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_018.jpg)'
- en: Figure 7.18 – Seldon Deployment CR with the image location
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – 带有镜像位置的 Seldon 部署 CR
- en: 'Let''s deploy the model as a service by deploying the `chapter7/manual_model_deployment/SeldonDeploy.yaml`
    file. Run the following command:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过部署`chapter7/manual_model_deployment/SeldonDeploy.yaml`文件来将模型作为服务部署。运行以下命令：
- en: '[PRE5]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should see the following response:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.19 – Creating the Seldon Deployment CR'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.19 – 创建 Seldon 部署 CR'
- en: '](img/B18332_07_019.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_019.jpg)'
- en: Figure 7.19 – Creating the Seldon Deployment CR
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 创建 Seldon 部署 CR
- en: 'Validate that the container is in a running state. Run the following command:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证容器是否处于运行状态。运行以下命令：
- en: '[PRE6]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You will note that the name that you have put in the `graph` section of the
    `SeldonDeploy.yaml` file (`model-test-predictor`) is part of the container name.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，你在 `SeldonDeploy.yaml` 文件的 `graph` 部分中填写的名称（`model-test-predictor`）是容器名称的一部分。
- en: 'You should see the following response:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.20 – Validating the pod after the Seldon Deployment CR'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.20 – 在 Seldon 部署 CR 后验证 pod'
- en: '](img/B18332_07_020.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_020.jpg)'
- en: Figure 7.20 – Validating the pod after the Seldon Deployment CR
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – 在 Seldon 部署 CR 后验证 pod
- en: 'Great! You have a model running as a service. Now, let''s see what is in the
    pod created for us by the Seldon controller. Run the following command to get
    a list of containers inside our pod:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 很好！你已经有一个模型作为服务在运行。现在，让我们看看 Seldon 控制器为我们创建的 pod 中包含什么内容。运行以下命令来获取 pod 中容器的列表：
- en: '[PRE7]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should see the following response:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.21 – Containers inside the Seldon pod'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.21 – Seldon pod 中的容器'
- en: '](img/B18332_07_021.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_021.jpg)'
- en: Figure 7.21 – Containers inside the Seldon pod
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21 – Seldon pod 中的容器
- en: You will see that there are two containers. One is `model-test-predictor`, which
    is the image that we have built, and the second container is `seldon-container-engine`,
    which is the Seldon server.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到有两个容器。一个是 `model-test-predictor`，它是我们构建的镜像，另一个是 `seldon-container-engine`，它是
    Seldon 服务器。
- en: 'The `model-test-predictor` container has the model and is using the language
    wrapper to expose the model over HTTP and gRPC. You can use the following command
    to see the logs and what ports have been exposed from `model-test-predictor`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`model-test-predictor` 容器包含模型，并使用语言包装器通过 HTTP 和 gRPC 暴露模型。你可以使用以下命令查看日志以及从
    `model-test-predictor` 暴露了哪些端口：'
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should see the following response (among other logs):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应（以及其他日志）：
- en: '![Figure 7.22 – Containers log showing the ports'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.22 – 显示端口的容器日志'
- en: '](img/B18332_07_022.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_022.jpg)'
- en: Figure 7.22 – Containers log showing the ports
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.22 – 显示端口的容器日志
- en: 'You can see that the servers are ready to take the calls on `9000` for HTTP
    and on `6005` for the metrics server. This metrics server will have the Prometheus-based
    monitoring data exposed on the `/prometheus` endpoint. You can see this in the
    following portion of the log:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，服务器已准备好在 `9000` 端口接收 HTTP 请求，在 `6005` 端口接收指标服务器请求。这个指标服务器将通过 `/prometheus`
    端点暴露基于 Prometheus 的监控数据。你可以在以下日志部分看到这一点：
- en: '![Figure 7.23 – Containers log showing the Prometheus endpoint'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.23 – 显示 Prometheus 端点的容器日志'
- en: '](img/B18332_07_023.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_023.jpg)'
- en: Figure 7.23 – Containers log showing the Prometheus endpoint
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.23 – 显示 Prometheus 端点的容器日志
- en: The second container is `seldon-container-engine`, which does the orchestration
    for the inference graph and forwards the payloads to the service configured by
    you in the `logger` section of the Seldon Deployment CR.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个容器是 `seldon-container-engine`，它负责推理图的编排，并将负载转发到你在 Seldon 部署 CR 的 `logger`
    部分配置的服务。
- en: 'In this step, you will find out what Kubernetes objects your Seldon Deployment
    CR has created for you. A simple way to find out is by running the command as
    follows. This command depends on the Seldon controller labeling the objects it
    creates with the label key as `seldon-deployment-id`, and the value is the name
    of your Seldon Deployment CR, which is `model-test`:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，你将了解 Seldon 部署 CR 为你创建了哪些 Kubernetes 对象。查看这些对象的一种简单方法是运行以下命令。此命令依赖于 Seldon
    控制器使用标签键 `seldon-deployment-id` 对它所创建的对象进行标记，并且值是你的 Seldon 部署 CR 的名称，即 `model-test`：
- en: '[PRE9]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should see the following response:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.24 – Kubernetes objects created by the Seldon controller'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.24 – Seldon 控制器创建的 Kubernetes 对象'
- en: '](img/B18332_07_024.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_024.jpg)'
- en: Figure 7.24 – Kubernetes objects created by the Seldon controller
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24 – Seldon 控制器创建的 Kubernetes 对象
- en: You can see that there are Deployment objects, services, and **Horizontal Pod
    Autoscalers** (**HPA**) objects created for you for the Seldon controller using
    the configuration that you have provided in the Seldon Deployment CR. The deployment
    ends up creating pods and a replica set for your pods. The Seldon controller made
    it easy to deploy our model on the Kubernetes platform.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，Seldon 控制器根据你在 Seldon 部署 CR 中提供的配置，为你创建了 Deployment 对象、服务和**水平 Pod 自动扩展器**（**HPA**）对象。最终，部署会创建
    pod 和为你的 pod 创建副本集。Seldon 控制器使我们在 Kubernetes 平台上部署模型变得更加简单。
- en: 'You may have noticed that there is no ingress object created by the Seldon
    Deployment CR. Let''s create the ingress object so that we can call our model
    from outside the cluster by running the command as follows. The ingress object
    is created by the file in `chapter7/manual_model_deployment/Ingress.yaml`. Make
    sure to adjust the `host` value as per your configuration, as you have done in
    earlier chapters. You will also notice that the ingress is forwarding traffic
    to port `8000`. Seldon provides the listener to this port, which orchestrates
    the inference call. This service is available in the container named `seldon-container-engine`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能已经注意到，Seldon 部署 CR 没有创建入口对象。让我们创建入口对象，以便通过运行以下命令从集群外部调用我们的模型。入口对象是通过`chapter7/manual_model_deployment/Ingress.yaml`中的文件创建的。确保根据你的配置调整`host`值，正如你在前面的章节中所做的那样。你还会注意到，入口将流量转发到端口`8000`。Seldon
    提供了对该端口的监听器，负责协调推理调用。此服务可在名为`seldon-container-engine`的容器中使用：
- en: '[PRE10]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should see the following response:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.25 – Creating ingress objects for our service'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.25 – 为我们的服务创建入口对象'
- en: '](img/B18332_07_025.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_025.jpg)'
- en: Figure 7.25 – Creating ingress objects for our service
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25 – 为我们的服务创建入口对象
- en: 'Validate that the ingress has been created by issuing the following command:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下命令来验证入口是否已创建：
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should see the following response:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.26 – Validating the ingress for our service'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.26 – 验证我们服务的入口'
- en: '](img/B18332_07_026.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_026.jpg)'
- en: Figure 7.26 – Validating the ingress for our service
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.26 – 验证我们服务的入口
- en: 'Since our Seldon Deployment CR has referenced a logger URL, you will deploy
    a simple HTTP echo server that will just print the calls it received. This will
    assist us in validating whether the payloads have been forwarded to the configured
    URL in the `logger` section of the Seldon Deployment CR. A very simple echo server
    can be created via the following command:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的 Seldon 部署 CR 已引用了日志记录器 URL，你将部署一个简单的 HTTP 回显服务器，它只会打印收到的请求。这将帮助我们验证负载是否已转发到
    Seldon 部署 CR 中`logger`部分配置的 URL。可以通过以下命令创建一个非常简单的回显服务器：
- en: '[PRE12]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should see the following response:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.27 – Creating a simple HTTP echo server to validate payload logging'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.27 – 创建一个简单的 HTTP 回显服务器以验证负载日志记录'
- en: '](img/B18332_07_027.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_027.jpg)'
- en: Figure 7.27 – Creating a simple HTTP echo server to validate payload logging
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.27 – 创建一个简单的 HTTP 回显服务器以验证负载日志记录
- en: 'Validate that the pod has been created by issuing the following command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下命令来验证 Pod 是否已创建：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should see the following response:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.28 – Validating a simple HTTP echo server'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.28 – 验证一个简单的 HTTP 回显服务器'
- en: '](img/B18332_07_028.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_028.jpg)'
- en: Figure 7.28 – Validating a simple HTTP echo server
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.28 – 验证一个简单的 HTTP 回显服务器
- en: Let's make a call for our model to predict something. The model we developed
    in the previous chapter is not very useful, but it will help us understand and
    validate the overall process of packaging and deploying the model.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们调用模型进行预测。我们在上一章中开发的模型并不是非常有用，但它将帮助我们理解并验证整体的模型打包和部署过程。
- en: Recall from [*Chapter 6*](B18332_06_ePub.xhtml#_idTextAnchor086), *Machine Learning
    Engineering*, that the `hellomlflow` notebook has the input for the model with
    shape `(4,2)`, and the output shape is `(4,)`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下 [*第六章*](B18332_06_ePub.xhtml#_idTextAnchor086)，*机器学习工程*，`hellomlflow`笔记本的模型输入形状是
    `(4,2)`，输出形状是 `(4,)`。
- en: '![Figure 7.29 – Input and output for the model'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.29 – 模型的输入和输出'
- en: '](img/B18332_07_029.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_029.jpg)'
- en: Figure 7.29 – Input and output for the model
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29 – 模型的输入和输出
- en: 'So, if we want to send data to our model, it would be an array of integer pairs
    such as [`2,1`]. When you make a call to your model, the input data is required
    within an `ndarray` field under a key named `data`. The input would look as follows.
    This is the format the Seldon service expects for the data to be sent to it:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们要向模型发送数据，它将是一个整数对数组，例如[`2,1`]。当你调用模型时，输入数据必须位于一个名为`data`的字段下，并使用`ndarray`格式。输入数据将如下所示。这是
    Seldon 服务期望发送给它的数据格式：
- en: '![Figure 7.30 – Input for the model as an HTTP payload'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.30 – 作为 HTTP 负载的模型输入'
- en: '](img/B18332_07_030.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_030.jpg)'
- en: Figure 7.30 – Input for the model as an HTTP payload
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.30 – 作为 HTTP 负载的模型输入
- en: 'Next is the REST endpoint for the model. It will be the ingress that you created
    in *Step 13* and the standard Seldon URL. The final form would be as follows:
    http://<INGRESS_LOCATION>/api/v1.0/predictions.'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是模型的 REST 端点。它将是你在 *第 13 步* 中创建的 ingress 和标准的 Seldon URL。最终形式如下：http://<INGRESS_LOCATION>/api/v1.0/predictions。
- en: This would translate, in my case, to [http://model-test.192.168.61.72.nip.io/api/v1.0/predictions](http://model-test.192.168.61.72.nip.io/api/v1.0/predictions).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这将转换为我的情况下的[http://model-test.192.168.61.72.nip.io/api/v1.0/predictions](http://model-test.192.168.61.72.nip.io/api/v1.0/predictions)。
- en: Now, you have the payload and the URL to send this request to.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经有了负载和要发送此请求的 URL。
- en: In this step, you will make a call to your model. We are using a commonly used
    command-line option to make this call; however, you may choose to use other software,
    such as Postman, to make this HTTP call.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，你将调用你的模型。我们使用的是常用的命令行选项来进行此调用；不过，你也可以选择使用其他软件，如 Postman，来进行此 HTTP 调用。
- en: 'You will use the `POST` HTTP verb in the call and then provide the location
    of the service. You will have to pass the `Content-Type` header to mention JSON
    content and the body is passed using the `data-raw` flag of the curl program:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用 `POST` HTTP 动词进行调用，并提供服务的地址。你需要传递 `Content-Type` 头部来标明 JSON 内容，并使用 curl
    程序的 `data-raw` 标志来传递请求体：
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The final request should look as follows. Before making this call, make sure
    to change the URL as per your ingress location:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的请求应如下所示。在发出此请求之前，请确保根据你的 ingress 位置修改 URL：
- en: '[PRE15]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should see the following response. Note that the output of the command
    shows the array of the same shape as per our model, which is `(4,)`, and it is
    under the `ndarray` key in the following screenshot:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应。注意，命令的输出显示了与我们模型相同形状的数组，即`(4,)`，并且它位于以下截图中的`ndarray`键下：
- en: '![Figure 7.31 – Output payload for the model inference call'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.31 – 模型推理调用的输出负载](img/B18332_07_032.jpg)'
- en: '](img/B18332_07_031.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_031.jpg)'
- en: Figure 7.31 – Output payload for the model inference call
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.31 – 模型推理调用的输出负载
- en: 'Now, let''s verify that the model payload has been logged onto our echo server.
    You are validating the capability of Seldon to capture input and output and send
    it to the desired location for further processing, such as drift detection or
    audit logging:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们验证模型负载是否已记录到我们的 echo 服务器上。你正在验证 Seldon 捕获输入和输出并将其发送到目标位置以进行进一步处理（例如漂移检测或审计日志记录）的能力：
- en: '[PRE16]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You will see there is a separate record for the input and the output payload.
    You can use the `ce-requestid` key to correlate the two records in the logs. The
    following screenshot displays the main fields of the captured input payload of
    the inference call:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到输入和输出负载有单独的记录。你可以使用 `ce-requestid` 键来关联日志中的两个记录。以下截图显示了推理调用的捕获输入负载的主要字段：
- en: '![Figure 7.32 – Captured input payload forwarded to the echo pod'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.32 – 捕获的输入负载转发到 echo pod](img/B18332_07_031.jpg)'
- en: '](img/B18332_07_032.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_032.jpg)'
- en: Figure 7.32 – Captured input payload forwarded to the echo pod
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.32 – 捕获的输入负载转发到 echo pod
- en: 'The following screenshot displays the main fields of the output payload of
    the inference call:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了推理调用输出负载的主要字段：
- en: '![Figure 7.33 – Captured output payload forwarded to the echo pod'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.33 – 捕获的输出负载转发到 echo pod](img/B18332_07_033.jpg)'
- en: '](img/B18332_07_033.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_033.jpg)'
- en: Figure 7.33 – Captured output payload forwarded to the echo pod
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.33 – 捕获的输出负载转发到 echo pod
- en: Now, let's verify that service monitoring data is captured by the Seldon engine
    and is available for us to use and record. Note that the way Prometheus works
    is by scraping repetitively, so this data is in the current state and the Prometheus
    server is responsible for calling this URL and record in its database.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们验证 Seldon 引擎是否已捕获服务监控数据，并且我们可以使用并记录这些数据。请注意，Prometheus 的工作方式是通过重复抓取，因此这些数据处于当前状态，Prometheus
    服务器负责调用此 URL 并将其记录在其数据库中。
- en: 'The URL format for this information is as follows. The ingress is the same
    as you created in *Step 13*:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 该信息的 URL 格式如下。Ingress 是你在 *第 13 步* 中创建的相同：
- en: '[PRE17]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This would translate to the following for my ingress:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这将转换为我 ingress 的以下内容：
- en: '[PRE18]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Open a browser and access the URL in it. You should see the following response:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 打开浏览器并访问其中的 URL。你应该看到以下响应：
- en: '![Figure 7.34 – Accessing monitoring data in Prometheus format'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.34 – 以 Prometheus 格式访问监控数据](img/B18332_07_034.jpg)'
- en: '](img/B18332_07_034.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_034.jpg)'
- en: Figure 7.34 – Accessing monitoring data in Prometheus format
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.34 – 以 Prometheus 格式访问监控数据
- en: You will find that a lot of information is captured, including response times,
    the number of HTTP responses per status code (`200`, `400`, `500`, and so on),
    data capture, server performance, and exposing the Go runtime metrics. We encourage
    you to go through these parameters to develop an understanding of the data available.
    In the later chapters, you will see how to harvest and plot this data to visualize
    the performance of the model inferencing server.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现收集了很多信息，包括响应时间、每个状态码的 HTTP 响应数量（`200`、`400`、`500` 等）、数据捕获、服务器性能以及暴露 Go 运行时指标。我们鼓励你深入了解这些参数，以便理解可用的数据。在后续章节中，你将看到如何收集和绘制这些数据，以便可视化模型推理服务器的性能。
- en: You have done a great deal in this exercise. The aim of this section was to
    showcase the steps and components involved to deploy a model using Seldon Core.
    In the next section, you will be introduced to the workflow component of the platform,
    Airflow, and in the next couple of chapters, all of these steps will be automated
    using the components in the ML platform.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这个练习中做得非常棒。本节的目的是展示使用 Seldon Core 部署模型的步骤和组件。在接下来的章节中，你将介绍平台的工作流组件 Airflow，并且接下来的几章中，所有这些步骤将通过
    ML 平台中的组件进行自动化。
- en: Introducing Apache Airflow
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Apache Airflow
- en: Apache Airflow is an open source software designed for programmatically authoring,
    executing, scheduling, and monitoring workflows. A workflow is a sequence of tasks
    that can include data pipelines, ML workflows, deployment pipelines, and even
    infrastructure tasks. It was developed by Airbnb as a workflow management system
    and was later open sourced as a project in Apache Software Foundation's incubation
    program.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Airflow 是一款开源软件，旨在以编程方式创建、执行、调度和监控工作流。工作流是一个任务序列，可以包括数据管道、ML 工作流、部署管道，甚至基础设施任务。它由
    Airbnb 开发，作为一个工作流管理系统，后来在 Apache 软件基金会的孵化项目中开源。
- en: While most workflow engines use XML to define workflows, Airflow uses Python
    as the core language for defining workflows. The tasks within the workflow are
    also written in Python.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数工作流引擎使用 XML 来定义工作流，但 Airflow 使用 Python 作为定义工作流的核心语言。工作流中的任务也都是用 Python
    编写的。
- en: Airflow has many features, but we will cover only the fundamental bits of Airflow
    in this book. This section is by no means a detailed guide for Airflow. Our focus
    is to introduce you to the software components for the ML platform. Let's start
    with DAG.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 具有许多功能，但本书只介绍 Airflow 的基础部分。本节绝不是 Airflow 的详细指南。我们的重点是介绍 ML 平台的软件组件。让我们从
    DAG 开始。
- en: Understanding DAG
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 DAG
- en: A workflow can be simply defined as a sequence of **tasks**. In Airflow, the
    sequence of tasks follows a data structure called a **directed acyclic graph**
    (**DAG**). If you remember your computer science data structures, a DAG is composed
    of nodes and one-way vertices organized in a way to ensure that there are no cycles
    or loops. Hence, a workflow in Airflow is called a DAG.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流可以简单地定义为一系列**任务**。在 Airflow 中，任务序列遵循一种称为**有向无环图**（**DAG**）的数据结构。如果你还记得计算机科学中的数据结构，DAG
    由节点和单向顶点组成，这些节点和顶点按一定方式组织，以确保没有循环或环路。因此，Airflow 中的工作流被称为 DAG。
- en: '*Figure 7.35* shows a typical example of a data pipeline workflow:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.35* 显示了一个典型的数据管道工作流示例：'
- en: '![Figure 7.35 – Typical data pipeline workflow'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.35 – 典型的数据管道工作流'
- en: '](img/B18332_07_035.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_035.jpg)'
- en: Figure 7.35 – Typical data pipeline workflow
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.35 – 典型的数据管道工作流
- en: 'The example workflow in *Figure 7.36* is composed of tasks represented by boxes.
    The order of execution of these tasks is determined by the direction of the arrows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.36* 中的示例工作流由表示任务的框组成。任务的执行顺序由箭头的方向决定：'
- en: '![Figure 7.36 – Example workflow with parallel execution'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.36 – 带有并行执行的示例工作流'
- en: '](img/B18332_07_036.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_036.jpg)'
- en: Figure 7.36 – Example workflow with parallel execution
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.36 – 带有并行执行的示例工作流
- en: Another example of a workflow is shown in *Figure 7.36*. In this example, there
    are tasks that are executed in parallel. The **Generate Report** tasks will wait
    for both **Transform Data** tasks to complete. This is called **execution dependency**
    and it is one of the problems Airflow is solving. Tasks can only execute if the
    upstream tasks are completed.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个工作流示例显示在*图 7.36*中。在这个示例中，有些任务是并行执行的。**生成报告**任务将在两个**转换数据**任务完成后才会开始执行。这被称为**执行依赖性**，它是
    Airflow 解决的问题之一。任务只有在上游任务完成后才能执行。
- en: 'You can configure the workflow however you want as long as there are no cycles
    in the graph, as shown in *Figure 7.37*:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 只要图中没有循环，你可以根据需求配置工作流，如*图7.37*所示：
- en: '![Figure 7.37 – Example workflow with cycle'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.37 – 带有循环的示例工作流'
- en: '](img/B18332_07_037.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_037.jpg)'
- en: Figure 7.37 – Example workflow with cycle
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.37 – 带有循环的示例工作流
- en: In the example in *Figure 7.37*, the **Clean Data** task will never be executed
    because it is dependent on the **Store Data** task, which will also not be executed.
    Airflow only allows acyclic graphs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.37*中的例子中，**清理数据**任务永远不会执行，因为它依赖于**存储数据**任务，而后者也不会执行。Airflow只允许非循环图。
- en: 'As illustrated, a DAG is a series of tasks, and there are three common types
    of tasks in Airflow:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，DAG是一系列任务，Airflow中有三种常见的任务类型：
- en: '**Operators**: Predefined tasks that you can use to execute something, They
    can be strung together to form a pipeline or a workflow. Your DAG is composed
    mostly, if not entirely, of operators.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作符**：预定义的任务，你可以用来执行某些操作。它们可以串联起来形成一个管道或工作流。你的DAG主要由操作符组成，甚至完全由操作符组成。'
- en: '**Sensors**: Subtypes of operators that are used for a series of other operators
    based on an external event.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器**：操作符的子类型，用于基于外部事件触发一系列其他操作符。'
- en: '`@task`. This allows you to run regular Python functions as tasks.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@task`。这允许你将常规的Python函数作为任务运行。'
- en: Airflow operators are extendable, which means there are quite a lot of predefined
    operators created by the community that you can simply use. One of the operators
    that you will mostly use in the following exercises is the **Notebook Operator**.
    This operator allows you to run any Jupyter notebook as tasks in the DAG.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow操作符是可扩展的，这意味着社区已经创建了许多预定义的操作符，你可以直接使用。在接下来的练习中，你将主要使用的操作符之一是**Notebook操作符**。这个操作符允许你将任何Jupyter笔记本作为DAG中的任务来运行。
- en: So, what are the advantages of using DAGs to execute a sequence of tasks? Isn't
    it enough to just write a script that can execute other scripts sequentially?
    Well, the answer lies in the features that Airflow offers, which we will explore
    next.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，使用DAG执行任务序列的优势是什么呢？仅仅编写一个能够按顺序执行其他脚本的脚本就足够了吗？答案在于Airflow所提供的特性，我们将继续探讨这些特性。
- en: Exploring Airflow features
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索Airflow特性
- en: 'The advantages that Airflow brings when compared with `cron` jobs and scripts
    can be detailed by its features. Let''s start by looking at some of those features:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 与`cron`作业和脚本相比，Airflow带来的优势可以通过其特性详细说明。让我们首先来看一下这些特性：
- en: '**Failure and error management**: In the event of a task failure, Airflow handles
    errors and failures gracefully. Tasks can be configured to automatically retry
    when they fail. You can also configure how many times it retries.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障和错误管理**：在任务失败时，Airflow能够优雅地处理错误和失败。可以配置任务在失败时自动重试，并且可以配置重试次数。'
- en: In terms of execution sequence, there are two types of task dependencies in
    a typical workflow that can be managed in Airflow much easier than writing a script.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 就执行顺序而言，典型工作流中有两种任务依赖关系，它们在Airflow中比编写脚本更容易管理。
- en: '**Data dependencies**: Some tasks may require that the other tasks be processed
    first because they require data that is generated by other tasks. This can be
    managed in Airflow. Moreover, Airflow allows the passing of small amounts of metadata
    from the output of one task as an input to another task.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据依赖性**：某些任务可能需要先处理其他任务，因为它们需要其他任务生成的数据。这可以在Airflow中进行管理。此外，Airflow允许将一个任务的输出中的小量元数据作为输入传递给另一个任务。'
- en: '**Execution dependencies**: You may be able to script execution dependencies
    in a small workflow. However, imagine scripting a workflow in Bash with a hundred
    tasks, where some tasks can run concurrently while others can only run sequentially.
    I imagine this to be a pretty daunting task. Airflow helps simplify this by creating
    DAGs.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行依赖性**：你可能能够在小型工作流中编写执行依赖性脚本。然而，想象一下用Bash编写一个有一百个任务的工作流，其中一些任务可以并行运行，而其他任务只能顺序执行。我想这会是一个相当令人生畏的任务。Airflow通过创建DAG帮助简化了这一过程。'
- en: '**Scalability**: Airflow can horizontally scale to multiple machines or containers.
    The tasks in the workflow may be executed on different nodes while being orchestrated
    centrally by a common scheduler.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：Airflow可以水平扩展到多个机器或容器。工作流中的任务可以在不同节点上执行，同时由一个共同的调度器集中协调。'
- en: '`git` repository containing your DAGs. This allows you to implement the continuous
    integration of DAGs.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`git`仓库包含你的DAG。这使你能够实现DAG的持续集成。'
- en: The next step is to understand the different components of Airflow.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是了解 Airflow 的不同组件。
- en: Understanding Airflow components
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Airflow 组件
- en: 'Airflow comprises multiple components running as independent services. *Figure
    7.38* shows the components of Airflow and their interactions:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 包含多个作为独立服务运行的组件。*图 7.38* 显示了 Airflow 组件及其相互作用：
- en: '![Figure 7.38 – Airflow components'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.38 – Airflow 组件'
- en: '](img/B18332_07_038.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_038.jpg)'
- en: Figure 7.38 – Airflow components
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.38 – Airflow 组件
- en: There are three core services in Airflow. The **Airflow Web** serves the user
    interface where users can visually monitor and interact with DAGs and tasks. The
    **Airflow Scheduler** is a service responsible for scheduling tasks for the Airflow
    Worker. Scheduling does not only mean executing tasks according to their scheduled
    time. It's also about executing the tasks in a particular sequence, taking into
    account the execution dependencies and failure management. **Airflow Worker**
    is the service that executes the tasks. This is also the main scalability point
    of Airflow. The more Airflow Worker is running, the more tasks can be executed
    concurrently.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 中有三个核心服务。**Airflow Web** 提供用户界面，用户可以在其中直观地监控和与 DAG 及任务交互。**Airflow Scheduler**
    是负责调度 Airflow Worker 任务的服务。调度不仅仅是根据预定时间执行任务，还包括按照特定顺序执行任务，考虑到执行依赖关系和失败管理。**Airflow
    Worker** 是执行任务的服务。这也是 Airflow 的主要可扩展性点。运行的 Airflow Worker 越多，可以并行执行的任务就越多。
- en: The DAG repository is a directory in the filesystem where DAG files written
    in Python are stored and retrieved by the scheduler. The Airflow instance configured
    in our platform includes a sidecar container that synchronizes the DAG repository
    with a remote `git` repository. This simplifies the deployment of DAGs by simply
    pushing a Python file to Git.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 仓库是文件系统中的一个目录，存储并由调度器检索用 Python 编写的 DAG 文件。我们平台中配置的 Airflow 实例包括一个侧车容器，它将
    DAG 仓库与远程 `git` 仓库同步。这简化了 DAG 的部署，只需将 Python 文件推送到 Git。
- en: We will not dig too deep into Airflow in this book. The objective is for you
    to learn enough to a point where you are able to create pipelines in Airflow with
    minimal Python coding. You will use the Elyra notebooks pipeline builder feature
    to build Airflow pipelines graphically. If you want to learn more about Airflow
    and how to build pipelines programmatically in Python, we recommend that you start
    with Apache Airflow's very rich documentation at [https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html](https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们不会深入探讨 Airflow。目标是让你学到足够的内容，以便能够使用最少的 Python 编码在 Airflow 中创建管道。你将使用 Elyra
    笔记本管道构建器功能，以图形方式构建 Airflow 管道。如果你想了解更多关于 Airflow 的内容，以及如何在 Python 中以编程方式构建管道，建议你从
    Apache Airflow 的丰富文档开始，网址是 [https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html](https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html)。
- en: Now that you have a basic understanding of Airflow, it's time to take a look
    at it in action. In [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The
    Anatomy of a Machine Learning Platform*, you installed a fresh instance of ODH.
    This process also installed the Airflow services for you. Now, let's validate
    this installation.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对 Airflow 有了基本了解，是时候看看它的实际操作了。在 [*第 4 章*](B18332_04_ePub.xhtml#_idTextAnchor055)，《机器学习平台的架构》中，你安装了一个新的
    ODH 实例。这个过程也为你安装了 Airflow 服务。现在，让我们验证这个安装。
- en: Validating the Airflow installation
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证 Airflow 安装
- en: 'To validate that Airflow is running correctly in your cluster, you need to
    perform the following steps:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证 Airflow 是否在你的集群中正常运行，你需要执行以下步骤：
- en: 'Check whether all the Airflow pods are running by executing the following command:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令检查所有 Airflow pod 是否都在运行：
- en: '[PRE19]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should see the three Airflow services pods in running status, as shown
    in the following screenshot in *Figure 7.39*. Verify that all pods are in the
    `Running` state:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到三个 Airflow 服务 pod 处于运行状态，如 *图 7.39* 所示的截图。验证所有 pod 是否都处于 `Running` 状态：
- en: '![Figure 7.39 – Airflow pods in the Running state'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.39 – 运行状态下的 Airflow pod'
- en: '](img/B18332_07_039.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_039.jpg)'
- en: Figure 7.39 – Airflow pods in the Running state
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.39 – 运行状态下的 Airflow pod
- en: 'Get the URL of Airflow Web by looking at the ingress host of `ap-airflow2`.
    You can do this by executing the following command:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查看 `ap-airflow2` 的入口主机，获取 Airflow Web 的 URL。你可以通过执行以下命令来完成：
- en: '[PRE20]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see results similar to *Figure 7.39*. Take note of the host value
    of the `ap-airflow2` ingress. The IP address may be different in your environment:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似于 *图 7.39* 的结果。注意 `ap-airflow2` ingress 的主机值。你的环境中的 IP 地址可能不同：
- en: '![Figure 7.40 – Airflow ingress in the ml-workshop namespace'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.40 – ml-workshop 命名空间中的 Airflow ingress'
- en: '](img/B18332_07_040.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_040.jpg)'
- en: Figure 7.40 – Airflow ingress in the ml-workshop namespace
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.40 – ml-workshop 命名空间中的 Airflow ingress
- en: 'Navigate to [https://airflow.192.168.49.2.nip.io](https://airflow.192.168.49.2.nip.io).
    Note that the domain name is the host value of the `ap-airflow2` ingress. You
    should see the Airflow Web UI, as shown in *Figure 7.41*:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://airflow.192.168.49.2.nip.io](https://airflow.192.168.49.2.nip.io)。请注意，域名是
    `ap-airflow2` ingress 的主机值。你应该能看到 Airflow Web UI，如 *图 7.41* 所示：
- en: '![Figure 7.41 – Home screen of Apache Airflow'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.41 – Apache Airflow 的主页'
- en: '](img/B18332_07_041.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_041.jpg)'
- en: Figure 7.41 – Home screen of Apache Airflow
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.41 – Apache Airflow 的主页
- en: If you are able to load the Airflow landing page, it means that the Airflow
    installation is valid. You must have also noticed that in the table listing the
    DAGs, there are already existing DAGs currently in failing status. These are existing
    DAG files that are in [https://github.com/airflow-dags/dags/](https://github.com/airflow-dags/dags/),
    the default configured DAG repository. You will need to create your own DAG repository
    for your experiments. The next section will provide the details on how to do this.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够加载 Airflow 着陆页，则说明 Airflow 安装有效。你还应该注意到，在列出 DAG 的表格中，已经有现有的 DAG 正处于失败状态。这些是已经存在的
    DAG 文件，位于默认配置的 DAG 仓库 [https://github.com/airflow-dags/dags/](https://github.com/airflow-dags/dags/)
    中。你需要为你的实验创建自己的 DAG 仓库。下一部分将提供如何执行此操作的详细信息。
- en: Configuring the Airflow DAG repository
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Airflow DAG 仓库
- en: 'A DAG repository is a Git repository where Airflow picks up the DAG files that
    represent your pipelines or workflows. To configure Airflow to point to your own
    DAG repository, you need to create a Git repository and point the Airflow Scheduler
    and Airflow Web to this Git repository. You will use **GitHub** to create this
    repository. The following steps will guide you through the process:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 仓库是一个 Git 仓库，Airflow 从中获取表示管道或工作流的 DAG 文件。要配置 Airflow 指向你自己的 DAG 仓库，你需要创建一个
    Git 仓库，并将 Airflow 调度器和 Airflow Web 指向该 Git 仓库。你将使用 **GitHub** 创建这个仓库。以下步骤将指导你完成此过程：
- en: 'Create a GitHub repository by going to [https://github.com](https://github.com).
    This requires that you have an existing account with GitHub. For the purpose of
    this exercise, let''s call this repository `airflow-dags`. Take note of the URL
    of your new Git repository. It should look like this: [https://github.com/your-user-name/airflow-dags.git](https://github.com/your-user-name/airflow-dags.git).
    We assume that you already know how to create a new repository on GitHub.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问 [https://github.com](https://github.com) 创建一个 GitHub 仓库。这要求你已经有一个 GitHub
    帐号。为了本次练习，我们将这个仓库命名为 `airflow-dags`。请记下你新创建的 Git 仓库的 URL，它应类似于：[https://github.com/your-user-name/airflow-dags.git](https://github.com/your-user-name/airflow-dags.git)。我们假设你已经知道如何在
    GitHub 上创建新的仓库。
- en: 'Edit your instance of ODH by editing the `kfdef` (**Kubeflow definition**)
    object. You can do this by executing the following command:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编辑 `kfdef`（**Kubeflow 定义**）对象来编辑你的 ODH 实例。你可以通过执行以下命令来完成：
- en: '[PRE21]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You should be presented with a `vim` editor showing the `kfdef` manifest file
    as shown in *Figure 7.42*. Press *i* to start editing.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到一个 `vim` 编辑器，显示 `kfdef` 清单文件，如 *图 7.42* 所示。按 *i* 开始编辑。
- en: '![Figure 7.42 – vim editor showing the section defining the Airflow instance'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.42 – vim 编辑器显示定义 Airflow 实例的部分'
- en: '](img/B18332_07_042.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_042.jpg)'
- en: Figure 7.42 – vim editor showing the section defining the Airflow instance
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.42 – vim 编辑器显示定义 Airflow 实例的部分
- en: Replace the value of the `DAG_REPO` parameter with the URL of the Git repository
    you created in *Step 1*. The edited file should look like the screenshot in *Figure
    7.43*. Press *Esc*, then *:*, and type `wq` and press *Enter* to save the changes
    you made to the `kfdef` object.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替换 `DAG_REPO` 参数的值为你在 *步骤 1* 中创建的 Git 仓库的 URL。编辑后的文件应如 *图 7.43* 中的截图所示。按 *Esc*，然后
    *:*，输入 `wq` 并按 *Enter* 保存你对 `kfdef` 对象所做的更改。
- en: '![Figure 7.43 – Value of the DAG_REPO parameter after editing'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.43 – 编辑后 `DAG_REPO` 参数的值'
- en: '](img/B18332_07_043.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_043.jpg)'
- en: Figure 7.43 – Value of the DAG_REPO parameter after editing
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.43 – 编辑后 `DAG_REPO` 参数的值
- en: The changes will be picked up by the ODH operator and will be applied to the
    affected Kubernetes deployment objects, in this case, Airflow Web and Airflow
    Scheduler deployments. This process will take a couple of minutes to complete.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 更改将被 ODH 操作符拾取，并应用到受影响的 Kubernetes 部署对象，在这种情况下，是 Airflow Web 和 Airflow 调度器部署。这个过程需要几分钟才能完成。
- en: 'Validate the changes by inspecting the Airflow deployments. You can do this
    by running the following command to look into the applied manifest of the deployment
    object:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过检查 Airflow 部署来验证更改。你可以运行以下命令来查看部署对象的应用清单：
- en: '[PRE22]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This should return a line containing the URL of your GitHub repository.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回一行，包含你 GitHub 仓库的 URL。
- en: 'Because this repository is new and is empty, you should not see any DAG files
    when you open the Airflow Web UI. To validate the Airflow web application, navigate
    to your Airflow URL, or refresh your existing browser tab, and you should see
    an empty Airflow DAG list similar to the screenshot in *Figure 7.44*:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这个仓库是新的且为空，当你打开 Airflow Web UI 时，你应该看不到任何 DAG 文件。要验证 Airflow web 应用程序，导航到你的
    Airflow URL，或刷新现有的浏览器标签，你应该看到一个空的 Airflow DAG 列表，类似于*图 7.44*中的截图：
- en: '![Figure 7.44 – Empty Airflow DAG list'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.44 – 空的 Airflow DAG 列表'
- en: '](img/B18332_07_044.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_044.jpg)'
- en: Figure 7.44 – Empty Airflow DAG list
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.44 – 空的 Airflow DAG 列表
- en: Now that you have validated your Airflow installation and updated the DAG repository
    to your own `git` repository, it's time to put Airflow to good use.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经验证了 Airflow 安装并将 DAG 仓库更新为自己的 `git` 仓库，是时候充分利用 Airflow 了。
- en: Configuring Airflow runtime images
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Airflow 运行时镜像
- en: Airflow pipelines, or DAGs, can be authored by writing Python files using the
    Airflow libraries. However, it is also possible to create DAGs graphically from
    an Elyra notebook. In this section, you will create an Airflow DAG from Elyra,
    push it to the DAG repository, and execute it in Airflow.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 流水线，或 DAG，可以通过编写 Python 文件并使用 Airflow 库来创建。然而，也可以从 Elyra 笔记本中图形化创建 DAG。在本节中，你将从
    Elyra 创建一个 Airflow DAG，将其推送到 DAG 仓库，并在 Airflow 中执行它。
- en: 'To further validate the Airflow setup and test the configuration, you will
    need to run a simple `Hello world` pipeline. Follow the steps to create a two-task
    pipeline. You will create Python files, a pipeline, and configure runtime images
    to be used throughout the process:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步验证 Airflow 设置并测试配置，你需要运行一个简单的 `Hello world` 流水线。按照步骤创建一个包含两个任务的流水线。你将创建
    Python 文件、一个流水线，并配置整个过程使用的运行时镜像：
- en: If you do not have a running notebook environment, start a notebook environment
    by navigating to JupyterHub, clicking **Start My Server**, and selecting a notebook
    image to run, as shown in *Figure 7.45*. Let's use **Base Elyra Notebook Image**
    this time as we do not require any special libraries.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有运行的笔记本环境，请通过导航到 JupyterHub，点击**启动我的服务器**，并选择要运行的笔记本镜像，如*图 7.45*所示，这时我们使用**Base
    Elyra 笔记本镜像**，因为我们不需要任何特殊库。
- en: '![Figure 7.45 – JupyterHub landing page showing Base Elyra Notebook Image selected'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.45 – 显示已选择 Base Elyra 笔记本镜像的 JupyterHub 登陆页面'
- en: '](img/B18332_07_045.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_045.jpg)'
- en: Figure 7.45 – JupyterHub landing page showing Base Elyra Notebook Image selected
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.45 – 显示已选择 Base Elyra 笔记本镜像的 JupyterHub 登陆页面
- en: In your Elyra browser, navigate to the `Machine-Learning-on-Kubernetes/chapter7/model_deploy_pipeline/`
    directory.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 Elyra 浏览器中，导航到 `Machine-Learning-on-Kubernetes/chapter7/model_deploy_pipeline/`
    目录。
- en: Open a new pipeline editor. You can do this by selecting the menu item `untitled.pipeline`.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的流水线编辑器。你可以通过选择菜单项 `untitled.pipeline` 来实现。
- en: '![Figure 7.46 – Elyra notebook'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.46 – Elyra 笔记本'
- en: '](img/B18332_07_046.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_046.jpg)'
- en: Figure 7.46 – Elyra notebook
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.46 – Elyra 笔记本
- en: Right-click on the `untitled.pipeline` file and rename it to `hello_world.pipeline`.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键点击 `untitled.pipeline` 文件并将其重命名为 `hello_world.pipeline`。
- en: 'Create two Python files with the same contents containing the following line:
    `print(''Hello airflow!'')`. You can do this by selecting the menu items `hello.py`
    and `world.py`. Your directory structure should look like the screenshot in *Figure
    7.47*:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个内容相同的 Python 文件，包含以下行：`print('Hello airflow!')`。你可以通过选择菜单项 `hello.py` 和
    `world.py` 来实现。你的目录结构应类似于*图 7.47*中的截图：
- en: '![Figure 7.47 – Elyra directory structure showing the hello.pipeline file'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.47 – 显示 hello.pipeline 文件的 Elyra 目录结构'
- en: '](img/B18332_07_047.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_047.jpg)'
- en: Figure 7.47 – Elyra directory structure showing the hello.pipeline file
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.47 – 显示 hello.pipeline 文件的 Elyra 目录结构
- en: Create a pipeline with two tasks by dragging the `hello.py` file into the pipeline
    editor window. Do the same for `world.py`. Connect the tasks by dragging the tiny
    circle on the right of the task box to another box. The resulting pipeline topology
    should look like the illustration in *Figure 7.48*. Save the pipeline by clicking
    the **Save** icon in the top toolbar.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 `hello.py` 文件拖动到管道编辑器窗口中，创建一个包含两个任务的管道。对 `world.py` 执行相同操作。通过将任务框右侧的小圆圈拖动到另一个框中，连接这些任务。最终的管道拓扑应该类似于*图
    7.48*中的插图。通过点击顶部工具栏中的**保存**图标来保存管道。
- en: '![Figure 7.48 – Task topology'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.48 – 任务拓扑'
- en: '](img/B18332_07_048.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_048.jpg)'
- en: Figure 7.48 – Task topology
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.48 – 任务拓扑
- en: 'Before we can run this pipeline, we need to configure each of the tasks. Because
    each task will run as a container in Kubernetes, we need to tell which container
    image that task will use. Select the **Runtime Images** icon on the toolbar on
    the left. Then, click the **+** button to add a new runtime image, as shown in
    *Figure 7.49*:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们运行这个管道之前，需要配置每个任务。因为每个任务将作为容器在 Kubernetes 上运行，我们需要指定该任务将使用哪个容器镜像。选择左侧工具栏上的**运行时镜像**图标。然后，点击**+**按钮添加一个新的运行时镜像，如*图
    7.49*所示：
- en: '![Figure 7.49 – Adding a new runtime image in Elyra'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.49 – 在 Elyra 中添加新运行时镜像'
- en: '](img/B18332_07_049.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_049.jpg)'
- en: Figure 7.49 – Adding a new runtime image in Elyra
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.49 – 在 Elyra 中添加新运行时镜像
- en: In the **Add new Runtime Image** dialog, add the details of the **Kaniko Container
    Builder** image, as shown in *Figure 7.50*, and hit the **SAVE & CLOSE** button.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**添加新运行时镜像**对话框中，添加**Kaniko 容器构建器**镜像的详细信息，如*图 7.50*所示，然后点击**保存并关闭**按钮。
- en: This container image ([https://quay.io/repository/ml-on-k8s/kaniko-container-builder](https://quay.io/repository/ml-on-k8s/kaniko-container-builder))
    contains the tools required to build Docker files and push images to an image
    registry from within Kubernetes. This image can also pull ML models and metadata
    from the MLflow model registry. You will use this image to build containers that
    host your ML model in the next section. This container image was created for the
    purpose of this book. You can use any container image as a runtime image for your
    pipeline tasks as long as the image can run on Kubernetes.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这个容器镜像（[https://quay.io/repository/ml-on-k8s/kaniko-container-builder](https://quay.io/repository/ml-on-k8s/kaniko-container-builder)）包含了在
    Kubernetes 中构建 Docker 文件并将镜像推送到镜像仓库所需的工具。这个镜像也可以从 MLflow 模型仓库中拉取机器学习模型和元数据。你将在下一部分使用这个镜像构建容器，以托管你的机器学习模型。此容器镜像是为本书的目的创建的。你可以使用任何能够在
    Kubernetes 上运行的容器镜像作为管道任务的运行时镜像。
- en: '![Figure 7.50 – Add new Runtime Image dialog for Kaniko builder'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.50 – 为 Kaniko 构建器添加新的运行时镜像对话框'
- en: '](img/B18332_07_050.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_050.jpg)'
- en: Figure 7.50 – Add new Runtime Image dialog for Kaniko builder
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.50 – 为 Kaniko 构建器添加新的运行时镜像对话框
- en: 'Add another runtime image called **Airflow Python Runner**. The container image
    is located at [https://quay.io/repository/ml-on-k8s/airflow-python-runner](https://quay.io/repository/ml-on-k8s/airflow-python-runner).
    This image can run any Python 3.8 scripts, and interact with Kubernetes and Spark
    operators. You will use this image to deploy container images to Kubernetes in
    the next section. Refer to *Figure 7.51* for the **Add new Runtime Image** dialog
    field values, and then hit the **SAVE & CLOSE** button:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个名为**Airflow Python Runner**的运行时镜像。容器镜像位于 [https://quay.io/repository/ml-on-k8s/airflow-python-runner](https://quay.io/repository/ml-on-k8s/airflow-python-runner)。该镜像可以运行任何
    Python 3.8 脚本，并与 Kubernetes 和 Spark 操作符进行交互。你将在下一部分使用此镜像将容器镜像部署到 Kubernetes。请参阅*图
    7.51*中的**添加新运行时镜像**对话框字段值，然后点击**保存并关闭**按钮：
- en: '![Figure 7.51 – Add new Runtime Image dialog for Airflow Python Runner'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.51 – 为 Airflow Python Runner 添加新运行时镜像对话框'
- en: '](img/B18332_07_051.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_051.jpg)'
- en: Figure 7.51 – Add new Runtime Image dialog for Airflow Python Runner
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.51 – 为 Airflow Python Runner 添加新运行时镜像对话框
- en: Pull the images from the remote repository to the local Docker daemon of your
    Kubernetes cluster. This will help speed up the start up times of tasks in Airflow
    by using a runtime image that is already pulled into the local Docker instance.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从远程仓库将镜像拉取到你的 Kubernetes 集群的本地 Docker 守护进程中。这将有助于通过使用已经拉取到本地 Docker 实例的运行时镜像，加快
    Airflow 任务的启动速度。
- en: 'You can do this by running the following command on the same machine where
    your Minikube is running. This command allows you to connect your Docker client
    to the Docker daemon inside your Minikube **virtual machine** (**VM**):'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在运行 Minikube 的同一台机器上运行以下命令来实现这一点。此命令允许你将 Docker 客户端连接到 Minikube **虚拟机**（**VM**）内部的
    Docker 守护进程：
- en: '[PRE23]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Pull the **Kaniko Container Builder** image by running the following command
    in the same machine where your Minikube is running. This will pull the image from
    [quay.io](http://quay.io) to the Docker daemon inside your Minikube:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在与你的Minikube运行的同一台机器上运行以下命令来拉取**Kaniko Container Builder**镜像。这将从[quay.io](http://quay.io)拉取镜像到你Minikube内的Docker守护进程：
- en: '[PRE24]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Pull the **Airflow Python Runner** image by running the following command in
    the same machine where your Minikube is running:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在与你的Minikube运行的同一台机器上运行以下命令来拉取**Airflow Python Runner**镜像：
- en: '[PRE25]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Assign `hello.py` task. You can do this by right-clicking the task box and selecting
    the **Properties** context menu item. The properties of the task will be displayed
    in the right pane of the pipeline editor, as shown in *Figure 7.52*. Using the
    **Runtime Image** drop-down box, select **Kaniko Container Builder**.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配`hello.py`任务。你可以通过右键点击任务框并选择**属性**上下文菜单项来完成此操作。任务的属性将在管道编辑器的右侧窗格中显示，如*图 7.52*所示。使用**运行时镜像**下拉框，选择**Kaniko
    Container Builder**。
- en: '![Figure 7.52 – Setting the runtime image of a task in the pipeline editor'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.52 – 在管道编辑器中设置任务的运行时镜像'
- en: '](img/B18332_07_052.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_052.jpg)'
- en: Figure 7.52 – Setting the runtime image of a task in the pipeline editor
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.52 – 在管道编辑器中设置任务的运行时镜像
- en: Note
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you do not see the newly added runtime images in the drop-down list, you
    need to close and reopen the pipeline editor. This will refresh the list of runtime
    images.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在下拉列表中没有看到新添加的运行时镜像，你需要关闭并重新打开管道编辑器。这将刷新运行时镜像的列表。
- en: 'Assign the `world.py` task. This is similar to *Step 10*, but for the `world.py`
    task. Refer to *Figure 7.53* for the **Runtime Image** value:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配`world.py`任务。这与*步骤 10*相似，但针对的是`world.py`任务。参见*图 7.53*中的**运行时镜像**值：
- en: '![Figure 7.53 – Setting the runtime image of a task in the pipeline editor'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.53 – 在管道编辑器中设置任务的运行时镜像'
- en: '](img/B18332_07_053.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_053.jpg)'
- en: Figure 7.53 – Setting the runtime image of a task in the pipeline editor
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.53 – 在管道编辑器中设置任务的运行时镜像
- en: 'You have just created an Airflow pipeline that has two tasks, where each task
    uses a different runtime. But, before we can run this pipeline in Airflow, we
    need to tell Elyra where Airflow is. To do this, select the **Runtimes** icon
    on the left toolbar of Elyra, as shown in *Figure 7.54*:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你刚刚创建了一个Airflow管道，其中包含两个任务，每个任务使用不同的运行时。但在我们运行这个管道之前，我们需要告诉Elyra Airflow的位置。为此，请点击Elyra左侧工具栏中的**运行时**图标，如*图
    7.54*所示：
- en: '![Figure 7.54 – Runtimes toolbar'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.54 – 运行时工具栏'
- en: '](img/B18332_07_054.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_054.jpg)'
- en: Figure 7.54 – Runtimes toolbar
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.54 – 运行时工具栏
- en: Hit the `ml-workshop`. This is the namespace of all your ML platform workloads.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`ml-workshop`。这是你所有ML平台工作负载的命名空间。
- en: '`github-username/airflow-dags` format. Replace `github-username` with your
    GitHub username.'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`github-username/airflow-dags`格式。将`github-username`替换为你的GitHub用户名。'
- en: '`minio`.'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`minio`。'
- en: '`minio123`.'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`minio123`。'
- en: Once all the fields are filled correctly, hit the **SAVE & CLOSE** button.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 填写所有字段后，点击**保存并关闭**按钮。
- en: '![Figure 7.55 – Adding a new Apache Airflow runtime configuration'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.55 – 添加新的Apache Airflow运行时配置'
- en: '](img/B18332_07_055.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_055.jpg)'
- en: Figure 7.55 – Adding a new Apache Airflow runtime configuration
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.55 – 添加新的Apache Airflow运行时配置
- en: 'Run the pipeline in Airflow by clicking the **Play** button in the top toolbar
    of the pipeline editor. This will bring up a **Run pipeline** dialog. Select **Apache
    Airflow runtime** as the runtime platform and **MyAirflow** as the runtime configuration,
    and then hit **OK**. Refer to *Figure 7.56*:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击管道编辑器顶部工具栏中的**播放**按钮来运行Airflow管道。这将弹出**运行管道**对话框。选择**Apache Airflow运行时**作为运行时平台，并选择**MyAirflow**作为运行时配置，然后点击**确定**。参见*图
    7.56*：
- en: '![Figure 7.56 – Run pipeline dialog'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.56 – 运行管道对话框'
- en: '](img/B18332_07_056.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_056.jpg)'
- en: Figure 7.56 – Run pipeline dialog
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.56 – 运行管道对话框
- en: This action generates an Airflow DAG file and pushes the file to the GitHub
    repository configured as a DAG repository. You can verify this by checking your
    GitHub repository for newly pushed files.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作会生成一个Airflow DAG文件并将文件推送到配置为DAG仓库的GitHub仓库。你可以通过检查你的GitHub仓库中是否有新推送的文件来验证这一点。
- en: Open the Airflow website. You should see the newly create DAG, as shown in *Figure
    7.57*. If you do not see it, refresh the Airflow page a few times. Sometimes,
    it takes a few seconds before the DAGs appear in the UI.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Airflow网站。你应该能看到新创建的DAG，如*图 7.57*所示。如果看不到，刷新Airflow页面几次。有时候DAG在UI中出现需要几秒钟。
- en: '![Figure 7.57 – Airflow showing a running DAG'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.57 – Airflow显示正在运行的DAG'
- en: '](img/B18332_07_057.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_057.jpg)'
- en: Figure 7.57 – Airflow showing a running DAG
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.57 – Airflow 显示正在运行的 DAG
- en: The DAG should succeed in a few minutes. If it does fail, you need to review
    the steps to make sure you set the correct values and that you did not miss any
    steps.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 应该会在几分钟内成功运行。如果失败，你需要检查步骤，确保设置了正确的值，并且没有遗漏任何步骤。
- en: You have just created a basic Airflow DAG using Elyra's graphical pipeline editor.
    The generated DAG is, by default, configured to only run once, indicated by the
    `@once` annotation. In the real world, you may not want to run your DAGs directly
    from Elyra. You may want to add additional customizations to the DAG file. In
    this case, instead of running the DAG by clicking the play button, use the export
    feature. This will export the pipeline into a DAG file that you can further customize,
    such as setting the schedule. You can then push the customized DAG file to the
    DAG repository to submit it to Airflow.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚使用 Elyra 的图形管道编辑器创建了一个基本的 Airflow DAG。生成的 DAG 默认配置为仅运行一次，通过`@once`注解表示。在实际应用中，你可能不希望直接从
    Elyra 运行 DAG。你可能想要对 DAG 文件进行额外的自定义。在这种情况下，使用导出功能而不是点击播放按钮来运行 DAG。这将把管道导出为一个 DAG
    文件，你可以进一步自定义，比如设置计划任务。然后，你可以将自定义的 DAG 文件推送到 DAG 仓库并提交给 Airflow。
- en: You have just validated your Airflow setup, added Airflow runtime configuration,
    and integrated Elyra with Airflow. Now it is time to build a real deployment pipeline!
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚验证了 Airflow 设置，添加了 Airflow 运行时配置，并将 Elyra 与 Airflow 集成。现在是时候构建一个真正的部署管道了！
- en: Automating ML model deployments in Airflow
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Airflow 中自动化 ML 模型部署
- en: You have seen in the preceding sections how to manually package an ML model
    into a running HTTP service on Kubernetes. You have also seen how to create and
    run basic pipelines in Airflow. In this section, you will put this new knowledge
    together by creating an Airflow DAG to automate the model deployment process.
    You will create a simple Airflow pipeline for packaging and deploying an ML model
    from the MLflow model registry to Kubernetes.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在前面的章节中了解了如何手动将 ML 模型打包成运行在 Kubernetes 上的 HTTP 服务。你还了解了如何在 Airflow 中创建和运行基础管道。在本节中，你将通过创建一个
    Airflow DAG 来自动化模型部署过程，从而将这些新知识结合起来。你将创建一个简单的 Airflow 管道，用于将 ML 模型从 MLflow 模型注册中心打包并部署到
    Kubernetes。
- en: Creating the pipeline by using the pipeline editor
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用管道编辑器创建管道
- en: 'Similar to the previous section, you will use Elyra''s pipeline editor to create
    the model build and deployment DAG:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于上一节，你将使用 Elyra 的管道编辑器来创建模型构建和部署的 DAG：
- en: If you do not have a running Elyra environment, start a notebook environment
    by navigating to JupyterHub, clicking **Start My Server**, and selecting a notebook
    image to run, as shown in *Figure 7.45*. Let's use **Base Elyra Notebook Image**
    because this time, we do not require any special libraries.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有运行中的 Elyra 环境，可以通过导航到 JupyterHub，点击**启动我的服务器**，并选择一个笔记本镜像来启动笔记本环境，如*图 7.45*所示。我们这次使用**基础
    Elyra 笔记本镜像**，因为这次我们不需要任何特殊的库。
- en: In your Elyra browser, navigate to the `Machine-Learning-on-Kubernetes/chapter7/model_deploy_pipeline/`
    directory.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Elyra 浏览器中，导航到`Machine-Learning-on-Kubernetes/chapter7/model_deploy_pipeline/`目录。
- en: Open a new pipeline editor. You can do this by selecting the menu item `untitled.pipeline`.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的管道编辑器。你可以通过选择菜单项`untitled.pipeline`来完成此操作。
- en: 'Right-click on the `untitled.pipeline` file and rename it `model_deploy.pipeline`.
    Your directory structure should look like the screenshot in *Figure 7.58*:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键点击`untitled.pipeline`文件，将其重命名为`model_deploy.pipeline`。你的目录结构应当像*图 7.58*中截图所示：
- en: '![Figure 7.58 – Elyra showing empty pipeline editor'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.58 – Elyra 显示空的管道编辑器'
- en: '](img/B18332_07_058.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_058.jpg)'
- en: Figure 7.58 – Elyra showing empty pipeline editor
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.58 – Elyra 显示空的管道编辑器
- en: 'You will build a pipeline with two tasks in it. The first task will pull the
    model artifacts from the MLflow model registry, package the model as a container
    using Seldon core, and then push the container image to an image repository. To
    create the first task, drag and drop the `build_push_image.py` file from the `model_build_push`
    directory to the pipeline editor''s workspace. This action will create a new task
    in the pipeline editor window, as shown in *Figure 7.59*:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将构建一个包含两个任务的管道。第一个任务将从 MLflow 模型注册中心拉取模型工件，使用 Seldon Core 将模型打包成容器，并将容器镜像推送到镜像仓库。要创建第一个任务，拖动并将`build_push_image.py`文件从`model_build_push`目录拖放到管道编辑器的工作区。此操作将在管道编辑器窗口中创建一个新任务，如*图
    7.59*所示：
- en: '![Figure 7.59 – Elyra pipeline editor showing the build_push_image task'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.59 – Elyra 管道编辑器显示 build_push_image 任务'
- en: '](img/B18332_07_059.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_059.jpg)'
- en: Figure 7.59 – Elyra pipeline editor showing the build_push_image task
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.59 – Elyra 管道编辑器显示 build_push_image 任务
- en: 'The second task will pull the container image from the image repository and
    deploy it to Kubernetes. Create the second task by dragging the `deploy_model.py`
    file from `model_deploy directory` and dropping it into the pipeline editor workspace.
    This action will create a second task in the pipeline editor, as shown in *Figure
    7.60*:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个任务将从镜像仓库拉取容器镜像，并将其部署到 Kubernetes。通过将 `model_deploy directory` 中的 `deploy_model.py`
    文件拖放到管道编辑器工作区来创建第二个任务。这将会在管道编辑器中创建一个第二个任务，如 *图 7.60* 所示：
- en: '![Figure 7.60 – Elyra pipeline editor showing the deploy_model task'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.60 – Elyra 管道编辑器显示 deploy_model 任务'
- en: '](img/B18332_07_060.jpg)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_060.jpg)'
- en: Figure 7.60 – Elyra pipeline editor showing the deploy_model task
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.60 – Elyra 管道编辑器显示 deploy_model 任务
- en: Connect the two tasks by dragging the tiny circle at the right-hand side of
    the `build_push_image.py` task to the `deploy_model.py` task box. The task topology
    should look like the illustration in *Figure 7.61*. Take note of the direction
    of the arrow highlighted in the red box.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拖动 `build_push_image.py` 任务右侧的小圆圈到 `deploy_model.py` 任务框，连接这两个任务。任务拓扑应如下图所示
    *图 7.61*。注意红框中箭头的方向。
- en: '![Figure 7.61 – Task topology of the DAG'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.61 – DAG 任务拓扑'
- en: '](img/B18332_07_061.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_061.jpg)'
- en: Figure 7.61 – Task topology of the DAG
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.61 – DAG 任务拓扑
- en: Configure the `build_push_image.py` task by right-clicking the box and selecting
    **Properties**. A property panel will appear on the right side of the editor,
    as shown in *Figure 7.62*. Select **Kaniko Container Builder** as the runtime
    image for this task.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过右击框并选择 **属性** 来配置 `build_push_image.py` 任务。一个属性面板将出现在编辑器的右侧，如 *图 7.62* 所示。选择
    **Kaniko Container Builder** 作为此任务的运行时镜像。
- en: '![Figure 7.62 – Pipeline editor with the property panel displayed showing the
    Kaniko Builder runtime'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.62 – 显示属性面板的管道编辑器，展示 Kaniko Builder 运行时'
- en: '](img/B18332_07_062.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_062.jpg)'
- en: Figure 7.62 – Pipeline editor with the property panel displayed showing the
    Kaniko Builder runtime
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.62 – 显示属性面板的管道编辑器，展示 Kaniko Builder 运行时
- en: Add file dependencies to `build_push_image.py` by clicking the `Dockerfile`
    – This is the Docker file that will be built to produce the container image that
    contains the ML model and the Predictor Python file.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 `Dockerfile` 来向 `build_push_image.py` 添加文件依赖 – 这是将要构建的 Docker 文件，用于生成包含
    ML 模型和预测器 Python 文件的容器镜像。
- en: '`Predictor.py` – This is the Python file used by Seldon to define the inference
    graph. You have seen this file in the preceding section.'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Predictor.py` – 这是 Seldon 用来定义推理图的 Python 文件。你在前面的章节中已经看到过这个文件。'
- en: '`Base_requirements.txt` – This is a regular text file that contains a list
    of Python packages required to run this model. This is used by the `pip install`
    command inside the Docker file.'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Base_requirements.txt` – 这是一个常规文本文件，包含运行此模型所需的 Python 包列表。它由 Docker 文件中的 `pip
    install` 命令使用。'
- en: At this point, you should have an idea of what the entire pipeline does. Because
    the pipeline needs to push a container image to a registry, you will need a container
    registry to hold your ML model containers. Create a new repository in a container
    registry of your choice. For the exercises in this book, we will use `mlflowdemo`.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对整个管道的工作原理有一个大致的了解。因为管道需要将容器镜像推送到注册中心，你将需要一个容器注册中心来存放你的 ML 模型容器。在你选择的容器注册中心中创建一个新的仓库。在本书的练习中，我们将使用
    `mlflowdemo`。
- en: 'Once you have the image repository created, set the `build_push_image.py` task,
    as shown in *Figure 7.63*. The following are the six variables you need to set:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你创建了镜像仓库，设置 `build_push_image.py` 任务，如 *图 7.63* 所示。以下是你需要设置的六个变量：
- en: '`MODEL_NAME` is the name of the ML model registered in MLflow. You used the
    name `mlflowdemo` in the previous sections. Set the value of this variable to
    `mlflowdemo`.'
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_NAME` 是在 MLflow 中注册的 ML 模型名称。在前面的章节中，你使用了名称 `mlflowdemo`。将此变量的值设置为 `mlflowdemo`。'
- en: '`MODEL_VERSION` is the version number of the ML model registered in MLflow.
    Set the value of this variable to `1`.'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_VERSION` 是在 MLflow 中注册的 ML 模型的版本号。将此变量的值设置为 `1`。'
- en: '`CONTAINER_REGISTRY` is the container registry API endpoint. For Docker Hub,
    this is available at [https://index.docker.io/v1](https://index.docker.io/v1).
    Set the value of this variable to `https://index.docker.io/v1/`.'
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTAINER_REGISTRY` 是容器注册表的 API 端点。对于 Docker Hub，地址为 [https://index.docker.io/v1](https://index.docker.io/v1)。将此变量的值设置为
    `https://index.docker.io/v1/`。'
- en: '`CONTAINER_REGISTRY_USER` is the username of the user who will push images
    to the image registry. Set this to your Docker Hub username.'
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTAINER_REGISTRY_USER` 是将镜像推送到镜像注册表的用户的用户名。将其设置为你的 Docker Hub 用户名。'
- en: '`CONTAINER_REGISTRY_PASSWORD` is the password of your Docker Hub user. In production,
    you do not want to do this. You may use secret management tools to serve your
    Docker Hub password. However, to keep things simple for this exercise, you will
    put your Docker Hub password as an environment variable.'
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTAINER_REGISTRY_PASSWORD` 是你 Docker Hub 用户的密码。在生产环境中，你不应该这样做。你可以使用密钥管理工具来存储你的
    Docker Hub 密码。然而，为了简单起见，在本练习中，你将把 Docker Hub 密码作为环境变量。'
- en: '`CONTAINER_DETAILS` is the name of the repository where the image will be pushed,
    along with the name and tag of the image. This includes the Docker Hub username
    in the `your-username/mlflowdemo:latestv` format.'
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTAINER_DETAILS` 是镜像将被推送到的仓库名称，包括镜像名称和标签。这包括 Docker Hub 用户名，格式为 `your-username/mlflowdemo:latestv`。'
- en: 'Save the changes by clicking the **Save** icon from the top toolbar of the
    pipeline editor:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击管道编辑器顶部工具栏的**保存**图标来保存更改：
- en: '![Figure 7.63 – Example environment variables of the build_push_image.py task'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.63 – build_push_image.py 任务的示例环境变量'
- en: '](img/B18332_07_063.jpg)'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_063.jpg)'
- en: Figure 7.63 – Example environment variables of the build_push_image.py task
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.63 – build_push_image.py 任务的示例环境变量
- en: 'Configure the `deploy_model.py` task by setting the runtime image, the file
    dependencies, and the environment variables, as shown in *Figure 7.64*. There
    are four environment variables you need to set, as detailed in the following list:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 `deploy_model.py` 任务，设置运行时镜像、文件依赖和环境变量，如*图 7.64*所示。你需要设置四个环境变量，具体如下：
- en: '`MODEL_NAME` is the name of the ML model registered in MLflow. You used the
    name `mlflowdemo` in the previous sections. Set the value of this variable to
    `mlflowdemo`.'
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MODEL_NAME` 是在 MLflow 中注册的 ML 模型名称。你在前面的章节中使用了 `mlflowdemo` 这个名称。将此变量的值设置为
    `mlflowdemo`。'
- en: '`MODEL_VERSION` is the version number of the ML model registered in MLflow.
    Set the value of this variable to `1`.'
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MODEL_VERSION` 是在 MLflow 中注册的 ML 模型的版本号。将此变量的值设置为 `1`。'
- en: '`CONTAINER_DETAILS` is the name of the repository to where the image will be
    pushed and the image name and tag. This includes the Docker Hub username in the
    `your-username/mlflowdemo:latest` format.'
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`CONTAINER_DETAILS` 是镜像将被推送到的仓库名称，包括镜像名称和标签。这包括 Docker Hub 用户名，格式为 `your-username/mlflowdemo:latest`。'
- en: '`CLUSTER_DOMAIN_NAME` is the DNS name of your Kubernetes cluster, in this case,
    the IP address of Minikube, which is `<Minikube IP>.nip.io`. For example, if the
    response of the `minikube ip` command is `192.168.49.2`, then the cluster domain
    name is `192.168.49.2.nip.io`. This is used to configure the ingress of the ML
    model HTTP service so that it is accessible outside the Kubernetes cluster.'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`CLUSTER_DOMAIN_NAME` 是你 Kubernetes 集群的 DNS 名称，在本例中是 Minikube 的 IP 地址，即 `<Minikube
    IP>.nip.io`。例如，如果 `minikube ip` 命令的响应是 `192.168.49.2`，那么集群域名就是 `192.168.49.2.nip.io`。此项用于配置
    ML 模型 HTTP 服务的入口，以便它可以在 Kubernetes 集群外部访问。'
- en: Save the changes by clicking the **Save** icon from the top toolbar of the pipeline
    editor.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击管道编辑器顶部工具栏的**保存**图标来保存更改。
- en: '![Figure 7.64 – Properties of the deploy_model.py task'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.64 – deploy_model.py 任务的属性'
- en: '](img/B18332_07_064.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_064.jpg)'
- en: Figure 7.64 – Properties of the deploy_model.py task
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.64 – deploy_model.py 任务的属性
- en: You are now ready to run the pipeline. Hit the **Play** button from the top
    toolbar of the pipeline editor. This will bring up the **Run** pipeline dialog,
    as shown in *Figure 7.65*. Select **Apache Airflow runtime** under **Runtime Platform**,
    and **MyAirflow** under **Runtime Configuration**. Click the **OK** button. This
    will generate the Airflow DAG Python file and push it to the Git repository.
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经准备好运行管道了。点击管道编辑器顶部工具栏的**播放**按钮。这将弹出**运行管道**对话框，如*图 7.65*所示。在**运行平台**下选择**Apache
    Airflow 运行时**，在**运行配置**下选择**MyAirflow**。点击**确定**按钮。这将生成 Airflow DAG Python 文件并将其推送到
    Git 仓库。
- en: '![Figure 7.65 – Run pipeline dialog'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.65 – 运行管道对话框'
- en: '](img/B18332_07_065.jpg)'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_065.jpg)'
- en: Figure 7.65 – Run pipeline dialog
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.65 – 运行管道对话框
- en: Once the DAG is successfully generated and pushed to the `git` repository, you
    should see a dialog as shown in *Figure 7.66*. Click **OK**.
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 DAG 成功生成并推送到 `git` 仓库，你应该会看到一个对话框，如 *图 7.66* 所示。点击 **确定**。
- en: '![Figure 7.66 – DAG submission confirmation dialog'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.66 – DAG 提交确认对话框'
- en: '](img/B18332_07_066.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_066.jpg)'
- en: Figure 7.66 – DAG submission confirmation dialog
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.66 – DAG 提交确认对话框
- en: Navigate to Airflow's GUI. You should see a new DAG, labeled **model_deploy-some-number**,
    appear in the DAGs table, and it should start running shortly, as shown in *Figure
    7.67*. The mint green color of the job indicates that it is currently running.
    Dark green indicates that it is successful.
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 Airflow 的 GUI。你应该能在 DAG 表格中看到一个新的 DAG，标签为 **model_deploy-some-number**，它应该会很快开始运行，如
    *图 7.67* 所示。任务的薄荷绿色表示它正在运行，深绿色表示它已经成功。
- en: Note
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: If you do not see the new DAG, refresh the page until you see it. It may take
    a few seconds for the Airflow to sync with the Git repository.
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你没有看到新的 DAG，请刷新页面直到看到它。Airflow 可能需要几秒钟的时间与 Git 仓库同步。
- en: '![Figure 7.67 – Airflow GUI showing the model_deploy DAG'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.67 – Airflow GUI 显示 model_deploy DAG'
- en: '](img/B18332_07_067.jpg)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_067.jpg)'
- en: Figure 7.67 – Airflow GUI showing the model_deploy DAG
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.67 – Airflow GUI 显示 model_deploy DAG
- en: Meanwhile, you can explore the DAG by clicking the DAG name and selecting the
    **Graph View** tab. It should display the topology of tasks as you designed it
    in Elyra's pipeline editor, as shown in *Figure 7.68*. You may explore the DAG
    further by selecting the **<> Code** tab. This will display the generated source
    code of the DAG.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，你可以通过点击 DAG 名称并选择 **图形视图** 标签来探索 DAG。它应该会显示你在 Elyra 流水线编辑器中设计的任务拓扑，如 *图 7.68*
    所示。你还可以通过选择 **<> 代码** 标签进一步探索 DAG。这将显示生成的 DAG 源代码。
- en: '![Figure 7.68 – Graph view of the model_deploy DAG in Airflow'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.68 – Airflow 中 model_deploy DAG 的图形视图'
- en: '](img/B18332_07_068.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_068.jpg)'
- en: Figure 7.68 – Graph view of the model_deploy DAG in Airflow
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.68 – Airflow 中 model_deploy DAG 的图形视图
- en: 'After a few minutes, the job should succeed and you should see the outline
    of all the tasks in **Graph View** turn to dark green. You can also explore the
    tasks by looking at the pods in Kubernetes. Run the following command and you
    should see two pods with the **Completed** status, as shown in *Figure 7.69*.
    These pods are the two tasks in the pipeline that have been executed successfully:'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几分钟后，任务应该会成功，你应该能看到 **图形视图** 中所有任务的轮廓变成深绿色。你还可以通过查看 Kubernetes 中的 pod 来探索这些任务。运行以下命令，你应该能看到两个
    **完成** 状态的 pod，如 *图 7.69* 所示。这些 pod 是流水线中两个已经成功执行的任务：
- en: '[PRE26]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should see the following response:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 7.69 – Kubernetes pods with a Completed status'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.69 – Kubernetes pod 完成状态'
- en: '](img/B18332_07_069.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_07_069.jpg)'
- en: Figure 7.69 – Kubernetes pods with a Completed status
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.69 – Kubernetes pod 完成状态
- en: You have just created a complete ML model build and deployment pipeline using
    Seldon Core, Elyra's pipeline editor, orchestrated by Airflow, and deployed to
    Kubernetes.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚使用 Seldon Core、Elyra 的流水线编辑器、Airflow 编排并部署到 Kubernetes，成功创建了一个完整的 ML 模型构建和部署流水线。
- en: Seldon Core and Airflow are big tools that have a lot more features that we
    have not covered and will not be entirely covered in this book. We have given
    you the essential knowledge and skills to start exploring these tools further
    as part of your ML platform.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon Core 和 Airflow 是功能强大的工具，它们有更多的功能，我们在本书中没有涉及，也不会完全覆盖。我们已经为你提供了开始进一步探索这些工具的基本知识和技能，作为你
    ML 平台的一部分。
- en: Summary
  id: totrans-479
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Congratulations! You made it this far!
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经走到了这一步！
- en: As of this point, you have already seen and used JupyterHub, Elyra, Apache Spark,
    MLflow, Apache Airflow, Seldon Core, and Kubernetes. You have learned how these
    tools can solve the problems that MLOps is trying to solve. And, you have seen
    all these tools running well on Kubernetes.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经使用过 JupyterHub、Elyra、Apache Spark、MLflow、Apache Airflow、Seldon Core
    和 Kubernetes。你已经学会了这些工具如何解决 MLOps 所要解决的问题。而且，你已经看到所有这些工具都在 Kubernetes 上良好运作。
- en: There are a lot more things that we want to show you on the platform. However,
    we can only write so much, as the features of each of those tools that you have
    seen are enough to fill an entire book.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想在平台上向你展示更多的内容。然而，我们能写的内容有限，因为你已经看到的每个工具的功能足以填满一本书。
- en: In the next chapter, we will take a step back to look at the big picture of
    what has been built so far. Then, you will start using the platform end-to-end
    on an example use case. You will be wearing different hats, such as data scientist,
    ML engineer, data engineer, and a DevOps person in the succeeding chapters.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将退一步，从整体上回顾到目前为止所构建的内容。接下来，你将开始在一个示例用例中全面使用平台。在接下来的章节中，你将扮演不同的角色，例如数据科学家、机器学习工程师、数据工程师以及运维工程师。
