- en: '*Chapter 10*: Monitoring and Logging'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第十章*：监控与日志'
- en: The previous chapters covered cluster configuration, backup, and recovery. This
    chapter will cover Rancher monitoring and how Rancher uses Prometheus and Grafana
    to collect metrics for a cluster and then appoint them. Then, we will cover Rancher
    logging and how Rancher uses the Banzai Cloud Logging operator and Fluent Bit
    to collect the logs from the Kubernetes components and collect application logs,
    including filtering logs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节介绍了集群配置、备份和恢复。本章将介绍 Rancher 监控，以及 Rancher 如何使用 Prometheus 和 Grafana 来收集集群的指标并进行展示。接下来，我们将讨论
    Rancher 日志，以及 Rancher 如何使用 Banzai Cloud Logging 操作符和 Fluent Bit 来收集 Kubernetes
    组件的日志和应用日志，包括日志过滤。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: What is Prometheus and Grafana?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Prometheus 和 Grafana？
- en: Deploying Rancher's monitoring stack
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署 Rancher 的监控栈
- en: Adding custom application metrics to Prometheus
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向 Prometheus 添加自定义应用指标
- en: Creating alert rules in Prometheus
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Prometheus 中创建告警规则
- en: Creating a Grafana dashboard
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 Grafana 仪表盘
- en: What is the Banzai Cloud Logging operator?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Banzai Cloud Logging 操作符？
- en: What is Fluent Bit?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Fluent Bit？
- en: Deploying Rancher logging
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署 Rancher 日志
- en: Filtering application logs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤应用日志
- en: Writing logs to multiple log servers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志写入多个日志服务器
- en: What is Prometheus and Grafana?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Prometheus 和 Grafana？
- en: In this section, we'll be covering the most popular monitoring solution for
    Kubernetes clusters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 Kubernetes 集群最流行的监控解决方案。
- en: '**Prometheus** is an open source monitoring and alerting framework that the
    Kubernetes community has widely adopted. Prometheus was initially created by SoundCloud
    back in 2012 before it was accepted by the **Cloud Native Computing Foundation**
    (**CNCF**) as its second incubated project after Kubernetes. Prometheus was built
    from the ground up to work with Kubernetes, the core idea being that everything
    should be discoverable via the Kubernetes API. At this point, Prometheus will
    pull the metrics and store them as time-series key-value pairs.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**Prometheus** 是一个开源的监控和告警框架，Kubernetes 社区广泛采用它。Prometheus 最初由 SoundCloud 于
    2012 年创建，后来被 **云原生计算基金会**（**CNCF**）接受，成为继 Kubernetes 之后的第二个孵化项目。Prometheus 从零开始构建，旨在与
    Kubernetes 一起使用，核心思想是所有内容都应该通过 Kubernetes API 可发现。此时，Prometheus 会拉取指标并将它们存储为时间序列键值对。'
- en: Of course, the first question that always comes up is, *what are metrics?* In
    the simplest terms, it's a numerical measurement of a resource. For example, it
    can be the current memory usage of a pod or the current number of connections
    to a database server. It is important to note that Prometheus doesn't support
    anything but an integer or floating-point number for the values. You can't set
    a value to something such as the words *up* or *down* for the value of a metric.
    For example, if you want to check whether a job was successful or failed, you
    might output that value as `0` for a successful status and `1` for a failed status.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，首先总是会有一个问题，*什么是指标？* 简单来说，指标是对某个资源的数值度量。例如，它可以是某个 Pod 当前的内存使用量，或者是当前数据库服务器的连接数。需要注意的是，Prometheus
    只支持整数或浮点数作为指标的值。你不能将值设置为像 *up* 或 *down* 这样的词语。例如，如果你想检查某个任务是否成功或失败，你可能会将成功的状态输出为
    `0`，将失败的状态输出为 `1`。
- en: The other central point with metrics is they should be a point-in-time value.
    For example, you might want the average number of connections at a given time.
    So, you would define a metrics endpoint at the pod level. One of the traps for
    new players is to add the metric endpoint to a service record. This might be easier
    but is not recommended for the long term because you might want a different rule
    in the future. For example, you start with just finding an average number of connections
    over the last 5 minutes, and then you want to change that to 15 minutes. Do you
    change the value of the current metric, which might affect historical reporting,
    or do you add another metric, which then means you are collecting duplicate data?
    The best approach is to output the raw data as a metric and then process it inside
    Prometheus and Grafana.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与度量指标相关的另一个核心问题是，它们应该是某一时刻的值。例如，你可能希望知道某一时刻的平均连接数。因此，你需要在Pod级别定义一个度量指标端点。新手常见的陷阱之一是将度量指标端点添加到服务记录中。这可能会更容易，但从长远来看并不推荐这样做，因为将来你可能会希望有不同的规则。例如，你一开始只想知道过去5分钟内的平均连接数，之后又想把时间范围改为15分钟。那么，你是改变当前度量指标的值，这可能会影响历史报告，还是添加一个新的度量指标，这样就意味着你在收集重复的数据呢？最佳的方法是将原始数据输出为度量指标，然后在Prometheus和Grafana中处理这些数据。
- en: The next question that comes up is, *how does Prometheus get its data?* Because
    Prometheus uses a `pull` instead of a `push` model, this is done by running a
    web server that exports the metrics as a simple text output of key-value pairs.
    This is commonly called an exporter in Prometheus. These exporters can be built
    directly into your application, as in the case of most of the core components
    of Kubernetes. For example, etcd has a built-in metrics exporter that runs on
    a different port, `2379`. It is common to run metrics on a different port than
    the main application because Prometheus, by default, will try making a `GET` request
    without authentication. Prometheus can query an endpoint that requires authentication,
    but setting up the tokens or credentials requires additional work and maintenance.
    So, most users will avoid it and use the fact that metrics are only exposed internally
    to the cluster and not to the public as *good enough* security.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的问题是，*Prometheus是如何获取数据的？* 由于Prometheus使用的是`pull`模型而不是`push`模型，这通过运行一个Web服务器来实现，该服务器将度量指标作为简单的键值对文本输出。这通常在Prometheus中被称为导出器（exporter）。这些导出器可以直接嵌入到应用程序中，就像Kubernetes的核心组件大多数情况一样。例如，etcd有一个内置的度量指标导出器，运行在不同的端口`2379`上。将度量指标运行在与主应用程序不同的端口上是常见的做法，因为Prometheus默认会尝试发起没有认证的`GET`请求。Prometheus可以查询需要认证的端点，但设置令牌或凭证需要额外的工作和维护。因此，大多数用户会避免这种情况，并利用度量指标仅对集群内部公开而不对外部开放的事实，认为这就足够了的安全性。
- en: Of course, Prometheus uses exporters to collect metrics, so the question of
    what exporters are available comes up. And luckily, because of the open source
    community, there are a significant number of third-party exporters for most standard
    applications. For example, almost all major open source databases have an exporter
    such as MySQL, CouchDB, MongoDB, MSSQL, Oracle DB, and PostgreSQL. You can find
    the official list at https://prometheus.io/docs/instrumenting/exporters/#databases.
    It's the same with standard web servers such as Apache and NGINX, with the complete
    list available at https://prometheus.io/docs/instrumenting/exporters/#http. And,
    of course, almost all Kubernetes native applications such as CoreDNS, Longhorn,
    Linkerd, and OPA Gatekeeper have Prometheus exporters built right into the application.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Prometheus使用导出器来收集度量指标，因此就会出现关于有哪些导出器可用的问题。幸运的是，得益于开源社区，几乎所有标准应用程序都有相当多的第三方导出器。例如，几乎所有主要的开源数据库都有对应的导出器，如MySQL、CouchDB、MongoDB、MSSQL、Oracle
    DB和PostgreSQL。你可以在https://prometheus.io/docs/instrumenting/exporters/#databases找到官方列表。标准的Web服务器，如Apache和NGINX也是如此，完整的列表可以在https://prometheus.io/docs/instrumenting/exporters/#http找到。当然，几乎所有Kubernetes原生应用程序，如CoreDNS、Longhorn、Linkerd和OPA
    Gatekeeper，也都将Prometheus导出器直接嵌入到应用程序中。
- en: For application developers, there are several libraries available for Go, Java/JVM,
    Python, and Node.js that allow even custom applications to add built-in support
    for Prometheus. Of course, if you can't find a premade exporter for your application,
    upstream Prometheus provides excellent resources for writing your exporter, including
    naming standards, example code, and different technical aspects for handling use
    cases. All this can be found at [https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应用开发人员，Go、Java/JVM、Python 和 Node.js 等语言有多个库可供使用，即使是自定义应用程序也可以内置支持 Prometheus。当然，如果您找不到适用于您的应用程序的现成导出程序，Prometheus
    上游提供了出色的资源，帮助您编写导出程序，包括命名标准、示例代码以及处理用例的不同技术方面。所有这些内容都可以在 [https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/)
    找到。
- en: Finally, one of the newer features added to Prometheus is alerting. Because
    Prometheus is already collecting your environment and applications data, it makes
    sense to add alerting into Prometheus using AlertManager. The core concept is
    that you define a set of queries that will run inside the Prometheus server that,
    if violated, will trigger an alert, which will be sent to Alertmanager, which
    will forward that alert to several external services such as email, Slack, and
    PagerDuty. Later in this chapter, we'll cover creating alerts in Prometheus along
    with some examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Prometheus 新增的一个功能是告警。由于 Prometheus 已经在收集您的环境和应用数据，因此通过使用 AlertManager 将告警集成到
    Prometheus 中是合乎逻辑的。其核心概念是，您定义一组查询，这些查询将在 Prometheus 服务器内部运行，如果条件被触发，将激活一个告警，并将告警发送到
    AlertManager，后者会将告警转发到多个外部服务，如电子邮件、Slack 和 PagerDuty。在本章稍后的内容中，我们将介绍如何在 Prometheus
    中创建告警，并提供一些示例。
- en: The one main feature that Prometheus is missing is a way to visualize your data.
    This is where Grafana comes into the picture. Grafana allows you to visualize
    data stored in Prometheus and other data sources such as MySQL, Loki, and InfluxDB.
    The main idea behind Grafana is that you create a dashboard that will query a
    data source (Prometheus, in this case) and then use this data to develop a range
    of graphs, charts, gauges, and so on. It is important to note that Gradana doesn't
    store any data outside of caching query results. Grafana also supports exploring
    logs from sources such as Loki and Elasticsearch. It also has a notification system
    that can trigger alerts based on queries, as Prometheus does. This can be helpful
    for application teams to create custom alerts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 缺少的一个主要功能是可视化数据的方法。这正是 Grafana 发挥作用的地方。Grafana 允许您可视化存储在 Prometheus
    及其他数据源（如 MySQL、Loki 和 InfluxDB）中的数据。Grafana 的主要思想是，您创建一个仪表盘，它会查询一个数据源（在本例中为 Prometheus），然后使用这些数据来开发各种图表、图形、仪表等。需要注意的是，Grafana
    不会存储任何数据，除了缓存查询结果外。Grafana 还支持从 Loki 和 Elasticsearch 等来源中探索日志。它还具有通知系统，可以基于查询触发告警，正如
    Prometheus 所做的那样。这对于应用程序团队创建自定义告警非常有帮助。
- en: Deploying Rancher's monitoring stack
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Rancher 的监控栈
- en: With Rancher, there are two main versions of monitoring, v1 and v2\. The original
    v1 monitoring that came with Rancher 2.0 to 2.4 is based on Prometheus and Grafana.
    But with the Rancher server and UI managing the deployment and configuration of
    the monitoring stack, the basic idea is to deploy Prometheus at the cluster level
    and additional Prometheus servers for each Rancher project. This approach was
    fine if you had a small number of projects that didn't need to be controlled via
    automation. This was mainly done because, initially, all the configurations of
    the Prometheus server were done by changing configmap. This required a great deal
    of work to manage the monitoring settings as clusters and applications grew in
    size and complexity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Rancher，监控有两个主要版本，v1 和 v2。最初的 v1 监控版本是与 Rancher 2.0 到 2.4 一起发布的，基于 Prometheus
    和 Grafana。但由于 Rancher 服务器和 UI 管理监控栈的部署和配置，基本思路是将 Prometheus 部署到集群级别，并为每个 Rancher
    项目额外部署 Prometheus 服务器。如果您的项目数量较少，并且不需要通过自动化进行控制，这种方法是可行的。最初，所有 Prometheus 服务器的配置都是通过更改
    configmap 完成的。随着集群和应用的规模和复杂性增长，这种方法需要大量工作来管理监控设置。
- en: With the creation of the Prometheus operator, it all changed. The core idea
    is that the Prometheus operator monitors a set of **Custom Resource Definitions**
    (**CRDs**). This includes the description of the Prometheus server and its related
    services such as node-exporter and Alertmanager. It is important to note that
    monitoring v1 and v2 has built-in rules and dashboards but, most importantly,
    the configuration of probes, alerts, and other related Prometheus settings, with
    Prometheus operator handling, creating, and updating the configuration files used
    by Prometheus.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Prometheus 操作符的创建，一切发生了变化。核心思想是，Prometheus 操作符监控一组 **自定义资源定义** (**CRDs**)。这包括
    Prometheus 服务器及其相关服务（如 node-exporter 和 Alertmanager）的描述。需要注意的是，v1 和 v2 监控都有内置的规则和仪表板，但最重要的是，探针、警报和其他相关的
    Prometheus 设置的配置由 Prometheus 操作符处理，操作符会创建并更新 Prometheus 使用的配置文件。
- en: In October 2020, Rancher 2.5 migrated from monitoring v1 to v2, with v2 being
    built on the operator model. It is important to note that both Prometheus and
    Grafana moved to this new model. This also included Rancher using the standard
    upstream Prometheus and Grafana image instead of the Rancher customized images.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年10月，Rancher 2.5 从 v1 监控迁移到 v2，v2 是基于操作符模型构建的。需要注意的是，Prometheus 和 Grafana
    都迁移到了这个新模型。这也包括 Rancher 使用标准的上游 Prometheus 和 Grafana 镜像，而非 Rancher 自定义的镜像。
- en: 'If you are currently running the old v1 monitoring, migrating to the new v2
    monitoring is recommended. The official process can be found at [https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/](https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/),
    but the process can be summarized as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你当前正在使用旧的 v1 监控，建议迁移到新的 v2 监控。官方过程可以在 [https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/](https://rancher.com/docs/rancher/v2.5/en/monitoring-alerting/guides/migrating/)
    中找到，但过程可以总结如下：
- en: You need to delete all the current settings and configurations.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要删除所有当前的设置和配置。
- en: Then, uninstall the old Prometheus server and its components.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，卸载旧的 Prometheus 服务器及其组件。
- en: At this point, you can install v2 monitoring and reconfigure all the settings.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，你可以安装 v2 监控并重新配置所有设置。
- en: You must make sure that nothing is left behind from v1 monitoring before installing
    v2\. Luckily, one of the engineers at Rancher named Bastian Hofmann created a
    script that handles the process of collecting all the alerts and dashboards and
    migrating them over to v2 ([https://github.com/bashofmann/rancher-monitoring-v1-to-v2](https://github.com/bashofmann/rancher-monitoring-v1-to-v2)).
    It is important to note that it is not an official script, and you should take
    an etcd backup before starting this process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 v2 之前，你必须确保 v1 监控的所有内容都已删除。幸运的是，Rancher 的一位工程师 Bastian Hofmann 创建了一个脚本，处理收集所有警报和仪表板并将它们迁移到
    v2 的过程（[https://github.com/bashofmann/rancher-monitoring-v1-to-v2](https://github.com/bashofmann/rancher-monitoring-v1-to-v2)）。需要注意的是，这不是一个官方脚本，在开始此过程之前，你应该先备份
    etcd。
- en: For deploying monitoring v1, log into the Rancher UI, go to **Tools** | **Monitoring**,
    and click the **Enable** button. At this point, the Rancher server will take over
    deploying the Prometheus server and node exporters. Then, all the monitoring configuration
    will be done via the Rancher UI. For example, if you wanted to view the CPU usage
    of a pod, you would browse to the pod in the Rancher UI, and Grafana graphs will
    be displayed right inside the UI. For additional details about the workload metrics
    that can be collected, please see the official Rancher documentation at [https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/](https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署 v1 监控，请登录到 Rancher UI，进入 **工具** | **监控**，并点击 **启用** 按钮。此时，Rancher 服务器将负责部署
    Prometheus 服务器和节点出口程序。然后，所有监控配置将通过 Rancher UI 完成。例如，如果你想查看某个 Pod 的 CPU 使用情况，你可以在
    Rancher UI 中浏览到该 Pod，并且 Grafana 图表将直接显示在 UI 中。有关可以收集的工作负载指标的更多详细信息，请参见官方 Rancher
    文档：[https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/](https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/cluster-monitoring/cluster-metrics/)。
- en: 'It is important to note that cluster monitoring is only designed to be used
    by users that have full view access to the cluster. If you want to scope monitoring
    to a single project, you''ll need to enable project monitoring by going to the
    project and selecting **Monitoring** from the **Tools** menu. This will cause
    the Rancher server to deploy an additional Prometheus server with its own namespace
    inside the project. This Prometheus server is scoped to the project and its namespace:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，集群监控仅供具有集群完全查看权限的用户使用。如果你希望将监控范围限定为单个项目，你需要启用项目监控，方法是进入项目并从 **工具** 菜单中选择
    **监控**。这将导致 Rancher 服务器在项目内部署一个具有自己命名空间的 Prometheus 服务器。这个 Prometheus 服务器的作用范围仅限于该项目及其命名空间：
- en: '![Figure 10.1 – Rancher monitoring v1'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.1 – Rancher 监控 v1](img/B18053_10_02.jpg)'
- en: '](img/B18053_10_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_10_01.jpg)'
- en: Figure 10.1 – Rancher monitoring v1
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – Rancher 监控 v1
- en: 'For deploying monitoring v2, you have a couple of different options. The first
    one is to go to **Cluster explorer** | **Cluster Tools** and click **Install**
    next to **Monitoring**:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署监控 v2，你有几种不同的选择。第一种是进入 **集群资源管理器** | **集群工具**，然后点击 **监控** 旁边的 **安装**：
- en: '![Figure 10.2 – Rancher monitoring v2'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.2 – Rancher 监控 v2](img/B18053_10_01.jpg)'
- en: '](img/B18053_10_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_10_02.jpg)'
- en: Figure 10.2 – Rancher monitoring v2
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – Rancher 监控 v2
- en: 'This will deploy Rancher''s monitoring chart via the app catalog. This chart
    is just a repackage of upstream images with no code changes. The only real difference
    is to use Rancher''s Docker Hub repositories in place of upstreams. Also, the
    default namespace is set to `cattle-monitoring-system`, but this can be customized
    if you so choose. Because monitoring v2 is a Helm chart, you can choose to deploy
    it directly via the `helm` command, which can be very helpful when managing clusters
    at scale, using tools such as Rancher''s fleet. The following is an example command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过应用目录部署 Rancher 的监控 chart。这个 chart 只是对上游镜像的重新打包，没有代码更改。唯一的真正区别是使用 Rancher
    的 Docker Hub 仓库来替代上游的镜像。同时，默认命名空间被设置为 `cattle-monitoring-system`，但如果需要，可以自定义该命名空间。由于监控
    v2 是一个 Helm chart，你可以选择直接通过 `helm` 命令进行部署，这对于使用像 Rancher 的 Fleet 这样的工具来管理大规模集群非常有用。以下是一个示例命令：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can find the complete command and `values.yaml` for installing Rancher monitoring
    via the `helm` command at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `helm` 命令在 [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-monitoring-v2)
    找到安装 Rancher 监控的完整命令和 `values.yaml` 文件。
- en: 'The second option is to deploy the upstream Helm chart, commonly called `kube-prometheus-stack`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是部署上游的 Helm chart，通常称为`kube-prometheus-stack`：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It is important to note that, at the time of writing, this chart is still in
    beta and is subject to change, and Rancher doesn't support all the versions available
    in the upstream charts. So, it recommends reviewing Rancher's support matrix at
    [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
    You can also find a complete list of the configuration options by viewing the
    chart value at https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在写作时，这个 chart 仍处于测试阶段，并且可能会有所更改，并且 Rancher 并不支持上游 chart 中的所有版本。因此，建议查看
    Rancher 的支持矩阵，网址为 [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)。你还可以通过查看
    chart 值来找到完整的配置选项，网址为 https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration。
- en: At this point, you should have Prometheus and Grafana installed on your cluster.
    It is important to note that it can take approximately 5–10 minutes for all the
    pods and services that Prometheus needs to start entirely. It is also important
    to note that, at the time of writing, Rancher does not fully support Prometheus
    federation – the idea being that you can have a central Prometheus server that
    scans all other Prometheus servers across your other clusters. If you would like
    to learn more about this, I recommend looking at the official documentation at
    [https://prometheus.io/docs/prometheus/latest/federation/](https://prometheus.io/docs/prometheus/latest/federation/),
    but I would note that this is still a new feature and still evolving.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你应该已经在集群上安装了 Prometheus 和 Grafana。需要注意的是，Prometheus 启动所需的所有 pod 和服务大约需要 5–10
    分钟才能完全启动。还需要注意的是，在写本文时，Rancher 并不完全支持 Prometheus 联邦——也就是你可以拥有一个中央 Prometheus 服务器，扫描你其他集群中的所有
    Prometheus 服务器。如果你想了解更多信息，我推荐查看官方文档[https://prometheus.io/docs/prometheus/latest/federation/](https://prometheus.io/docs/prometheus/latest/federation/)，但需要注意的是，这仍然是一个新特性，仍在发展中。
- en: Adding custom application metrics to Prometheus
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将自定义应用程序指标添加到 Prometheus
- en: Of course, now that you have Prometheus and Grafana all installed and working,
    the question becomes, *how do you get metrics from our applications into Prometheus?*
    In this section, we will cover two main ways of doing this.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现在你已经安装并使 Prometheus 和 Grafana 正常工作，接下来问题就变成了，*我们如何将应用程序的指标导入 Prometheus？*
    在这一部分，我们将介绍两种主要的做法。
- en: The easiest way is to use a community-created chart such as Bitnami's MariaDB
    chart, including the `metrics.enabled=true` option. This option enables a sidecar
    that adds `mysqld-exporter` to the deployment, with many community-created charts
    using this model of having the exporter be a sidecar container to the main container.
    It is important to note that you should read the documentation for the Helm chart
    to see if any additional steps need to be taken when enabling metrics in your
    chart, as some applications will require a service account or permissions to be
    set for the exporter to work correctly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是使用社区创建的图表，比如 Bitnami 的 MariaDB 图表，并包含`metrics.enabled=true`选项。这个选项启用一个
    sidecar，它将`mysqld-exporter`添加到部署中，许多社区创建的图表使用这种将导出器作为 sidecar 容器与主容器一起使用的模型。需要注意的是，你应该阅读
    Helm 图表的文档，以查看启用指标时是否需要采取任何额外步骤，因为有些应用程序需要为导出器设置服务帐户或权限才能正常工作。
- en: Besides enabling the metrics, you'll also see an annotation section with the
    `prometheus.io/scrape=true` and `prometheus.io/port=9000` key pair. The port might
    be different, but it is a standard practice to set it to something in this range.
    These two annotations are significant, as they are what Prometheus uses when discovering
    all the different pods that should be scraped.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了启用指标外，你还会看到一个带有`prometheus.io/scrape=true`和`prometheus.io/port=9000`键值对的注释部分。端口可能不同，但通常设置为这个范围内的某个值是标准做法。这两个注释非常重要，因为它们是
    Prometheus 在发现所有应该被抓取的不同 pod 时使用的。
- en: But let's assume that you are using a custom-made application and want to capture
    metrics from this application. The following are a couple of examples of different
    applications where the metrics exporter is installed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在使用一个定制的应用程序，并希望从这个应用程序捕获指标。以下是安装了指标导出器的不同应用程序的几个示例。
- en: With GoLang, Prometheus provides an official library located at [https://github.com/prometheus/client_golang/](https://github.com/prometheus/client_golang/).
    This library handles most of the heavy lifting when generating the metrics output.
    You can find an example Go application and deployment at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go).
    You need to run the `kubectl apply -f deploy.yaml` command in order to deploy
    the example application. If you curl the pod IP address with the path/metrics,
    you'll find that the application returns a list of different metrics (for example,
    curl `10.42.7.23:8080/metrics`). Once the application is up and running, you can
    send a `GET` request to `/ping`, which will return the word `pong`. Then, inside
    the application, it will increase a counter called `ping_request_count`, which
    is a custom metric that is being exposed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GoLang 时，Prometheus 提供了一个官方库，位于[https://github.com/prometheus/client_golang/](https://github.com/prometheus/client_golang/)。这个库处理了生成度量输出时的大部分繁重工作。你可以在[https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/go)找到示例
    Go 应用程序和部署文件。你需要运行 `kubectl apply -f deploy.yaml` 命令来部署示例应用程序。如果你使用 curl 命令访问
    pod 的 IP 地址并附带路径 `/metrics`，你会发现应用程序返回了不同度量的列表（例如，curl `10.42.7.23:8080/metrics`）。一旦应用程序启动并运行，你可以发送一个
    `GET` 请求到 `/ping`，它将返回 `pong` 字样。然后，应用程序内部会增加一个名为 `ping_request_count` 的计数器，这是一个正在暴露的自定义度量。
- en: 'Here is an example of the metrics output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是度量输出的一个示例：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The full output can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt.](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt.)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的输出可以在[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/go/output.txt)找到。
- en: With Apache2, we need to take the sidecar option to add the exporter to the
    deployment. In our example, we are going to use a popular third-party exporter
    called `apache_exporter`. You can read more about this project at [https://github.com/Lusitaniae/apache_exporter](https://github.com/Lusitaniae/apache_exporter).
    The basic idea behind this project is to act as a translation layer between the
    Apache `mod_status` module and Prometheus. We need to install/enable the `mod_status`
    module to the primary web server container in the example deployment. Then, we
    need to expose the `server-status` page to the sidecar container that hosts the
    exporter. You can find the example and deployment file at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache).
    You need to run the `kubectl apply -f deploy.yaml` command to deploy the example
    application.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Apache2 时，我们需要使用 sidecar 选项将导出器添加到部署中。在我们的示例中，我们将使用一个流行的第三方导出器，叫做`apache_exporter`。你可以在[https://github.com/Lusitaniae/apache_exporter](https://github.com/Lusitaniae/apache_exporter)了解更多关于这个项目的信息。这个项目的基本思路是充当
    Apache `mod_status` 模块与 Prometheus 之间的翻译层。我们需要在示例部署中的主 Web 服务器容器上安装/启用 `mod_status`
    模块。然后，我们需要将 `server-status` 页面暴露给托管导出器的 sidecar 容器。你可以在[https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/apache)找到示例和部署文件。你需要运行
    `kubectl apply -f deploy.yaml` 命令来部署示例应用程序。
- en: 'Here is an example of the metrics output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是度量输出的一个示例：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The full output can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的输出可以在[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/apache/output.txt)找到。
- en: With NGINX, we will use a similar process as we did with Apache, but this time,
    we will use an exporter provided by NGINX. You can find the example and deployment
    file at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx).
    You simply need to run the `kubectl apply -f deploy.yaml` command.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NGINX 时，我们将采用与使用 Apache 时类似的过程，但这次我们将使用 NGINX 提供的导出器。你可以在[https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/examples/nginx)找到示例和部署文件。你只需运行
    `kubectl apply -f deploy.yaml` 命令。
- en: 'Here is an example of the metrics output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是度量输出的一个示例：
- en: '[PRE34]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The full output can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的输出可以在 [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch10/examples/nginx/output.txt.)
    找到。
- en: At this point, we can monitor the metrics of our different applications, but
    we're missing the ability to create alerts based on these metrics. In the next
    section, we'll dive into alert rules for Prometheus.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以监控不同应用程序的指标，但我们还缺少根据这些指标创建告警的能力。在接下来的部分，我们将深入探讨 Prometheus 的告警规则。
- en: Creating alert rules in Prometheus
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Prometheus 中创建告警规则
- en: The Prometheus operator defines alert rules via the CRD PrometheusRule. At its
    core, all an alert is is an expression with a trigger. Let's look at the following
    example alert. This alert is from Longhorn, which we'll cover in the next chapter.
    As you can see, the expression is denoted by the `expr` field, which has a formula
    to take the actual size of the volume, divided by the capacity, and convert it
    to a percentage. Then, if that value is greater than 90%, the expression is `true`,
    which will trigger an alert. The `description` section is mainly for the end user.
    Still, it's important to note that you can have variables inside the description
    because the alert will contain the same explanation as the summary, typically
    used for the subject line. For example, when sending an email alert, the email's
    subject will be set to the subject of the alert, with the body of the email being
    the description.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 运算符通过 CRD PrometheusRule 定义告警规则。从本质上讲，告警只是一个带有触发器的表达式。让我们看一下下面的示例告警。这个告警来自
    Longhorn，我们将在下一章中讨论。正如你所看到的，表达式由 `expr` 字段表示，它包含一个公式，将卷的实际大小除以容量并转换为百分比。然后，如果该值大于
    90%，表达式为 `true`，就会触发告警。`description` 部分主要是面向最终用户的。值得注意的是，你可以在描述中使用变量，因为告警将包含与摘要相同的解释，通常用于主题行。例如，当发送邮件告警时，邮件的主题将设置为告警的主题，而邮件正文则是描述内容。
- en: 'Here is an example of an alert:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个告警的示例：
- en: '[PRE58]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: It is important to note that Prometheus will only find rules located in the
    same namespace as the server by default. This can cause issues, as application
    teams might need access to the namespace to add/edit/remove their alerts. To work
    around this issue, you'll need to add the following settings in your `values.yaml`
    file.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，默认情况下 Prometheus 只会查找与服务器位于同一命名空间中的规则。这可能会导致问题，因为应用团队可能需要访问该命名空间以添加/编辑/删除他们的告警。为了解决这个问题，你需要在
    `values.yaml` 文件中添加以下设置。
- en: 'Here is an example of `values.yaml`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `values.yaml` 的示例：
- en: '[PRE80]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: At this point, we have Prometheus up and running. It's collecting all the data
    about our cluster, but the built-in UI doesn't give you a way to visualize this
    data in a useful way. In the next section, we'll be diving into Grafana to bring
    dashboards to our data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经启动了 Prometheus，并且它正在收集集群的所有数据，但内置的 UI 并没有提供一个有用的方式来可视化这些数据。在接下来的部分，我们将深入探讨
    Grafana，以便为我们的数据创建仪表板。
- en: Creating a Grafana dashboard
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Grafana 仪表板
- en: 'At this point, we should have Prometheus and Grafana up and running, with the
    server collecting all the data about our cluster. Still, you can''t see most of
    the data unless you use Rancher''s monitoring charts, including some prebuilt
    dashboards that are mostly related to the cluster and its core services such as
    etcd, kube-apiserver, and CoreDNS. But, of course, a question comes up: *How do
    I create my own?*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该已经启动了 Prometheus 和 Grafana，服务器正在收集有关集群的所有数据。然而，除非你使用 Rancher 的监控图表，否则大部分数据是不可见的，这些图表包括一些预构建的仪表板，主要与集群及其核心服务（如
    etcd、kube-apiserver 和 CoreDNS）相关。当然，问题也随之而来：*我如何创建自己的仪表板？*
- en: The most straightforward answer is to find a premade dashboard and let someone
    else do all the hard work for you. The Grafana Labs dashboard repository is the
    most extensive resource, located at [https://grafana.com/grafana/dashboards/](https://grafana.com/grafana/dashboards/).
    Their search tool lets you filter results by applications, data sources, and so
    on. But the coolest part is their dashboard ID system. All dashboards on the official
    site have an ID number – for example, the NGINX Ingress controller dashboard has
    an ID of `9614`, and all you need to do to use this dashboard is copy that ID
    number and go to the Grafana UI. Browse to `admin/prom-operator`. Then, paste
    the ID number in, and you're done.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的答案是找到预制仪表板，并让其他人为您完成所有繁重的工作。Grafana Labs仪表板存储库是最全面的资源，位于[https://grafana.com/grafana/dashboards/](https://grafana.com/grafana/dashboards/)。它们的搜索工具允许您按应用程序、数据源等筛选结果。但最酷的部分是它们的仪表板ID系统。官方网站上的所有仪表板都有一个ID号码，例如NGINX
    Ingress控制器仪表板的ID是`9614`，要使用此仪表板，您只需复制该ID号码并转到Grafana UI。浏览到`admin/prom-operator`，然后粘贴ID号码，完成。
- en: Of course, Rancher monitoring provides some example dashboards bundled into
    the rancher-monitoring chart. You can find the raw JSON files at [https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards](https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards).
    Additionally, you can add baseboards by ID too, with some of the most important
    ones being the component and etcd dashboards, which can be used to provide a great
    deal of insight into cluster performance issues.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Rancher监控提供了一些示例仪表板，捆绑在rancher-monitoring图表中。您可以在[https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards](https://github.com/rancher/system-charts/tree/dev-v2.6/charts/rancher-monitoring/v0.3.1/charts/grafana/dashboards)找到原始JSON文件。此外，您还可以通过ID添加基础仪表板，其中一些最重要的仪表板包括组件和etcd仪表板，可用于深入了解集群性能问题。
- en: But let's say the application you are deploying is a community-created application
    that doesn't have a dashboard on the official site. Most repositories will have
    the dashboard defined as a JSON file. You'll copy and paste it into the UI and
    import it with this file. But let's say you are deploying a custom in-house application
    and want to start from zero. I recommend watching the *Getting started with Grafana
    dashboard design* video at [https://grafana.com/go/webinar/guide-to-dashboard-design/](https://grafana.com/go/webinar/guide-to-dashboard-design/).
    I use a lot of community-created dashboards and tune them to my needs. You can
    click the share button at the top and export a dashboard as a JSON file, at which
    point you can copy and paste the parts you like. It is also imperative that you
    save your work when making changes to your dashboard by clicking the save icon
    at the top-right corner. If you close the page without clicking that icon, all
    your changes will be lost.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设您要部署的应用程序是社区创建的，而官方网站上没有仪表板。大多数存储库将仪表板定义为JSON文件。您可以将其复制并粘贴到UI中，并使用此文件导入它。但是假设您要部署的是自定义内部应用程序，并希望从零开始。我建议观看[https://grafana.com/go/webinar/guide-to-dashboard-design/](https://grafana.com/go/webinar/guide-to-dashboard-design/)的《Getting
    started with Grafana dashboard design》视频。我使用了很多社区创建的仪表板，并根据需要进行调整。您可以在顶部单击共享按钮，并将仪表板导出为JSON文件，然后复制和粘贴您喜欢的部分。在更改仪表板时，通过单击右上角的保存图标保存您的工作非常重要。如果关闭页面而未单击该图标，则所有更改将丢失。
- en: So far in this chapter, we have been talking about monitoring and alerting.
    In the next section, we will shift gears and focus on the other half of the equation,
    logging.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们一直在讨论监控和警报。在接下来的部分中，我们将转变关注点，专注于等式的另一半，即日志记录。
- en: What is the Banzai Cloud Logging operator?
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Banzai Cloud Logging操作员？
- en: Along with migrating to monitoring v2 in Rancher 2.5, Rancher also migrated
    to logging v2 for mostly the same reasons. In v1, logging is built on Fluentd
    and uses plugins to ship logs to different logging services such as Elasticsearch,
    Splunk, Kafka, and Syslog. With v1, the Rancher server was in complete control
    of the logging deployments, which made customizing and tuning the logging solution
    complicated. Most of the settings were hard coded inside Rancher. This is where
    Banzai's Logging operator comes into the picture.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Rancher 2.5迁移到监控v2，Rancher也因大致相同的原因迁移到了日志记录v2。在v1中，日志记录是基于Fluentd构建的，并使用插件将日志发送到不同的日志服务，如Elasticsearch、Splunk、Kafka和Syslog。在v1中，Rancher服务器完全控制日志部署，这使得定制和调整日志解决方案变得复杂。大多数设置是硬编码在Rancher中的。这就是Banzai的Logging操作符进入的地方。
- en: The Logging operator uses the CRD model, just like the Prometheus operator,
    wherein you'll define your Fluent Bit deployment and its setting via a CRD. The
    operator takes over pushing out your changes. Because everything is a CRD, including
    your settings, you can let your application teams define their logging settings.
    For example, one team might want their logs sent to a cloud log service such as
    Splunk, while another team might have the legal requirement for everything to
    stay running on RKE or another K8s cluster hosted on-premises, and you can do
    this because of the Logging operator. The idea is that you have logging flows
    that are a set of application pods going to an output, which can be any number
    of logging servers/services.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Logging操作符使用与Prometheus操作符相同的CRD模型，在该模型中，你通过CRD定义Fluent Bit的部署及其设置。操作符会接管推送你的更改。因为一切都是CRD，包括设置，你可以让应用程序团队定义他们的日志设置。例如，一个团队可能希望将他们的日志发送到云日志服务，如Splunk，而另一个团队可能有法律要求必须保持所有日志在RKE或其他托管在本地的K8s集群上运行，你可以通过Logging操作符做到这一点。其理念是，你有一组应用程序Pod流，它们将日志发送到一个输出，输出可以是任何数量的日志服务器/服务。
- en: What is Fluent Bit and Fluentd?
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Fluent Bit和Fluentd？
- en: When talking about logging, two of the questions are, *what is Fluent Bit?*
    and *what is Fluentd?*
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈到日志记录时，两个问题是，*什么是Fluent Bit？* 和 *什么是Fluentd？*
- en: Before diving into Fluent Bit, let's talk about Fluentd, which came first. Fluentd
    is an open source project written in Ruby by the Treasure Data team back in 2011\.
    Its core idea was that all logs should be JSON objects. With Fluentd and Docker,
    the basic process used to collect logs from the containers is to use the default
    Docker log driver that writes the container logs to a file on the disk. Then,
    Fluentd will read the whole log file and bring forward the events onto the server,
    at which point Fluentd will open a tail file handler that will hold the log file
    open and read all writes to it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入Fluent Bit之前，我们先来谈谈先出现的Fluentd。Fluentd是一个开源项目，由Treasure Data团队于2011年使用Ruby编写。它的核心思想是所有日志都应该是JSON对象。使用Fluentd和Docker时，收集容器日志的基本过程是使用默认的Docker日志驱动程序，将容器日志写入磁盘上的文件。然后，Fluentd将读取整个日志文件并将事件转发到服务器，此时Fluentd将打开一个尾文件处理程序，保持日志文件打开并读取所有写入的内容。
- en: It is important to note that Fluentd has a process for handling log rotation,
    so it is recommended to enable log rotation in Docker Engine. The following is
    an example configuration. You can find the complete documentation at [https://docs.docker.com/config/containers/logging/configure/](https://docs.docker.com/config/containers/logging/configure/).
    Docker will wait until the `logs` file is 100 MB before rotating the file in the
    following example. This is done to prevent the loss of events for applications
    that create large amounts of events. Fluentd needs to read all the events and
    forward them to the log server before the rotation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Fluentd有一个处理日志轮换的流程，因此建议在Docker Engine中启用日志轮换。以下是一个配置示例。你可以在[https://docs.docker.com/config/containers/logging/configure/](https://docs.docker.com/config/containers/logging/configure/)找到完整的文档。Docker会等到`logs`文件达到100
    MB后再进行轮换，如以下示例所示。这么做是为了防止丢失生成大量事件的应用程序的事件。Fluentd需要在轮换前读取所有事件并将其转发到日志服务器。
- en: In the following example, we are defining the log options for the `json-file`
    log driver, which is built into Docker Engine by default.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们正在定义`json-file`日志驱动程序的日志选项，该驱动程序是Docker Engine默认内置的。
- en: 'Here is an example at `/etc/docker/daemon.json`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`/etc/docker/daemon.json`的一个示例：
- en: '[PRE84]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: In this case, we are allowing the log file for all containers to reach a maximum
    size of 100 MB before rotating the file. Then, we'll only be keeping the last
    three rotated files.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们允许所有容器的日志文件在达到最大 100 MB 大小时才开始轮换文件。然后，我们将只保留最后三个轮换的文件。
- en: Now, let's go over Fluent Bit. Basically, Fluentd was designed to be a simple
    tool that is fast and lightweight. Fluent Bit is built on top of Fluentd and is
    intended to provide the additional filtering and routing that Banzai logging needs.
    You can find more about the differences at [https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit](https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来了解 Fluent Bit。Fluentd 本质上是一个设计简洁、快速且轻量级的工具，而 Fluent Bit 是建立在 Fluentd
    基础之上的，旨在提供 Banzai 日志所需的额外过滤和路由功能。你可以在 [https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit](https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit)
    了解更多关于两者之间差异的信息。
- en: Deploying Rancher logging
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Rancher 日志
- en: With Rancher logging, it is recommended to deploy via the Apps and Marketplace
    in the Rancher UI by going to `cattle-logging-system` namespace. It is important
    to note that you'll see two applications, `rancher-logging` and `rancher-logging-crd`,
    in the **Installed** section after the installation is complete. Also, depending
    on the size of the cluster, it might take 5 to 15 minutes for all the pods to
    start up and go into the *ready* state. Once Rancher logging is installed on the
    cluster, we will be able to configure filtering and log flows, which we'll cover
    in the next two sections.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Rancher 日志时，建议通过 Rancher UI 中的“应用程序和市场”进行部署，方法是进入 `cattle-logging-system`
    命名空间。需要注意的是，安装完成后，你将在**已安装**部分看到两个应用程序，分别是 `rancher-logging` 和 `rancher-logging-crd`。另外，根据集群的大小，所有
    pod 启动并进入*准备就绪*状态可能需要 5 到 15 分钟。一旦 Rancher 日志安装完成，我们将能够配置过滤和日志流，接下来的两节内容将详细介绍这一点。
- en: 'Because logging v2 is a Helm chart, you can choose to deploy it directly via
    the `helm` command, which can be very helpful when managing clusters at scale,
    using tools such as Rancher''s Fleet. The following is an example command:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于日志 v2 是一个 Helm 图表，你可以选择通过 `helm` 命令直接部署它，这在使用 Rancher 的 Fleet 等工具管理大规模集群时非常有用。以下是一个示例命令：
- en: '[PRE91]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: You can find the full commands and `values.yaml` to install Rancher monitoring
    via the `helm` command at [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2](https://github.com/PacktPublishing/Rancher-Deep-Dive/tree/main/ch10/rancher-logging-v2)
    找到通过 `helm` 命令安装 Rancher 监控所需的完整命令和 `values.yaml` 文件。
- en: Filtering application logs
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过滤应用程序日志
- en: The first setting that most people configure with Rancher logging is **ClusterFlows**
    and **ClusterOutput**. ClusterFlow is designed to be scoped to all namespaces
    and can set the default logging policy for the cluster as a whole. To configure
    this setting, you'll go to the Rancher UI and browse to **Logging** and then **ClusterFlows**.
    From there, you'll fill out the form. Then, once that is done, you'll want to
    define ClusterOutput, where you define the target location for your logs, ElasticSearch,
    Splunk, Syslog, and so on. For an example of each of the different logging providers,
    please see Rancher's official documentation at [https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/](https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人配置 Rancher 日志时的第一个设置是 **ClusterFlows** 和 **ClusterOutput**。ClusterFlow 设计为作用于所有命名空间，并可以为整个集群设置默认日志策略。要配置此设置，您需要进入
    Rancher UI，浏览到**日志**，然后选择**ClusterFlows**。接下来，您需要填写表单。完成后，您将定义 ClusterOutput，在这里定义日志的目标位置，如
    ElasticSearch、Splunk、Syslog 等。关于不同日志提供程序的示例，请参见 Rancher 官方文档：[https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/](https://rancher.com/docs/rancher/v2.5/en/logging/custom-resource-config/outputs/)。
- en: 'Once you have ClusterFlows and ClusterOutput configured, you can call it done.
    But if you want to customize the logging for an application, you need to repeat
    the process. Still, this time, you''ll be configuring the Flows and Outputs, with
    the main difference being setting the selector rules or what the documentation
    calls `include` or `exclude` labels that you can use to limit the scope of the
    Flows and Outputs. The following is an example YAML for a NGINX application in
    the default namespace. It is important to note that Flows are namespace-scoped:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你配置了 ClusterFlows 和 ClusterOutput，就可以认为设置完成了。但是，如果你想为一个应用程序定制日志记录，你需要重复这个过程。不过这一次，你将配置
    Flows 和 Outputs，主要的区别是设置选择器规则，或者文档中所称的 `include` 或 `exclude` 标签，你可以使用这些标签来限制 Flows
    和 Outputs 的范围。以下是一个 NGINX 应用程序在默认命名空间中的 YAML 示例。需要注意的是，Flows 是命名空间范围的：
- en: 'Here is a Flow example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 Flow 示例：
- en: '[PRE94]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: At this point, we should have logging configured on our cluster and be forwarding
    the logs to a log server. In the next section, we'll cover a more advanced setup
    that a number of users use in their environments to log to multiple servers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该已经在集群中配置了日志记录并将日志转发到日志服务器。在下一节中，我们将介绍一些更高级的设置，许多用户在他们的环境中使用这种设置来将日志记录到多个服务器。
- en: Writing logs to multiple log servers
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向多个日志服务器写入日志
- en: Because you can define as many outputs as you would like, suppose you wanted
    to send logs to multiple log servers, such as sending to a local Syslog and Splunk
    server. It is imperative to note that this will duplicate your logs, so this is
    not recommended as an `failed to send` logs, so you can run into memory pressure
    issues if you have a misconfigured log server or the log server is offline for
    long periods.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你可以定义任意数量的输出，比如如果你想将日志发送到多个日志服务器，例如发送到本地的 Syslog 和 Splunk 服务器。需要特别注意的是，这会导致日志重复，因此不推荐这样做，因为
    `failed to send` 的日志可能会导致内存压力问题，尤其是在日志服务器配置错误或日志服务器长时间离线的情况下。
- en: You can find YAML examples in Rancher's official documentation, located at [https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/](https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/).
    It's important to note that, as of Rancher 2.6.3, the logging settings in the
    Rancher UI are still buggy (for example, [https://github.com/rancher/rancher/issues/36516](https://github.com/rancher/rancher/issues/36516),
    where the **ClusterOutput** field is failing to update in the UI), so it's recommended
    to use YAML files as much as possible.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Rancher 官方文档中找到 YAML 示例，文档地址为 [https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/](https://rancher.com/docs/rancher/v2.6/en/logging/custom-resource-config/outputs/)。需要注意的是，截至
    Rancher 2.6.3，Rancher UI 中的日志设置仍然存在一些问题（例如，[https://github.com/rancher/rancher/issues/36516](https://github.com/rancher/rancher/issues/36516)，其中
    **ClusterOutput** 字段未能在 UI 中更新），因此建议尽可能使用 YAML 文件。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about Rancher monitoring and logging. This includes
    how Prometheus, Grafana, Fluentd, and Fluent Bit work. We learned how to install
    Rancher monitoring and logging. We finally went into detail about some example
    dashboards. We ended the chapter by talking about customizing application logging
    and its flows.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了 Rancher 的监控和日志记录。这包括了 Prometheus、Grafana、Fluentd 和 Fluent Bit 的工作原理。我们学习了如何安装
    Rancher 监控和日志记录。最后，我们详细介绍了一些示例仪表板，并通过讨论定制应用程序日志记录及其流结束了本章内容。
- en: The next chapter will cover Rancher's storage project to provide storage to
    Kubernetes clusters.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍 Rancher 的存储项目，以为 Kubernetes 集群提供存储。
