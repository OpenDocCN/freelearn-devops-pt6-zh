- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Generative AI Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能基础
- en: '**Generative AI** (**GenAI**) has revolutionized our world and has grabbed
    everyone’s attention since the introduction of ChatGPT in November of 2022 by
    OpenAI ([https://openai.com/index/chatgpt/](https://openai.com/index/chatgpt/)).
    However, the foundational concepts of this technology have been around for quite
    some time. In this chapter, we will introduce the key concepts of GenAI and how
    it has evolved over time. We will then discuss how to think about a GenAI project
    and align it with the business objectives, covering the entire process for developing
    and deploying GenAI workloads, along with potential use cases across different
    industries.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成式人工智能** (**GenAI**) 自从 2022 年 11 月 OpenAI 推出 ChatGPT 后，已经彻底改变了我们的世界，并吸引了每个人的关注（[https://openai.com/index/chatgpt/](https://openai.com/index/chatgpt/)）。然而，这项技术的基础概念已经存在了相当长一段时间。在本章中，我们将介绍
    GenAI 的关键概念及其如何随着时间的推移而发展。然后，我们将讨论如何思考一个 GenAI 项目，并将其与业务目标对齐，涵盖整个开发和部署 GenAI 工作负载的过程，以及在不同行业中的潜在应用场景。'
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Artificial intelligence versus GenAI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能与 GenAI 的区别
- en: The evolution of machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的演进
- en: Transformer architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: The GenAI project life cycle
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI 项目生命周期
- en: The GenAI deployment stack
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI 部署堆栈
- en: GenAI project use cases
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI 项目应用场景
- en: Artificial Intelligence versus GenAI
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能与 GenAI 的区别
- en: Before we dive deeper into GenAI concepts, let’s discuss the differences between
    **Artificial Intelligence** (**AI**), **Machine Learning** (**ML**), **Deep Learning**
    (**DL**), and GenAI, as these terms are often used interchangeably.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解 GenAI 概念之前，让我们讨论一下 **人工智能** (**AI**)、**机器学习** (**ML**)、**深度学习** (**DL**)、和
    GenAI 之间的区别，因为这些术语经常被交替使用。
- en: '*Figure 1**.1* shows the relationships between these concepts.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.1* 显示了这些概念之间的关系。'
- en: '![Figure 1.1 – Relationships between AI, ML, DL, and GenAI](img/B31108_01_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – AI、ML、DL 与 GenAI 之间的关系](img/B31108_01_01.jpg)'
- en: Figure 1.1 – Relationships between AI, ML, DL, and GenAI
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – AI、ML、DL 与 GenAI 之间的关系
- en: 'Let’s learn more about these relationships:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更多地了解这些关系：
- en: '**AI**: AI refers to a system or algorithm that is capable of performing tasks
    that would otherwise typically require human intelligence. These tasks include
    reasoning, learning, problem-solving, perception, and language understanding.
    AI is a broad category and can include rule-based systems, expert systems, neural
    networks, and GenAI algorithms. The evolution of AI algorithms has provided machines
    with human-like senses and capabilities, such as vision to analyze the world around
    them, listening and speaking to understand natural language and respond verbally,
    and using sensor data to understand the external environment and respond accordingly.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI**：AI 是指能够执行通常需要人类智能的任务的系统或算法。这些任务包括推理、学习、问题解决、感知和语言理解。AI 是一个广泛的类别，可能包括基于规则的系统、专家系统、神经网络和
    GenAI 算法。AI 算法的发展使机器具备了类人感官和能力，如通过视觉分析周围世界，通过听觉和语言理解与人类交流，并利用传感器数据理解外部环境并作出响应。'
- en: '**ML**: ML is a subset of AI that involves algorithms and models that enable
    machines to learn from data and make predictions without requiring explicit coding.
    In traditional programming, developers write explicit instructions for a computer
    to execute, whereas in ML, algorithms learn from the patterns and relationships
    in data and make predictions. ML can further be divided into the following sub-categories:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML**：ML 是 AI 的一个子集，涉及使机器能够从数据中学习并进行预测的算法和模型，而不需要显式编程。在传统编程中，开发人员为计算机编写明确的指令来执行任务，而在
    ML 中，算法从数据中的模式和关系中学习并进行预测。ML 还可以进一步细分为以下子类别：'
- en: '**Supervised learning**: This uses labeled datasets to train the models. It
    can further be subdivided into classification and regression problems:'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：这是使用带标签的数据集来训练模型。它可以进一步细分为分类问题和回归问题：'
- en: '**Classification problems** use labeled data, such as labeled pictures of dogs
    and cats, to train the model. Once the model is trained, it can classify a user-provided
    picture using the classes it has been trained on.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类问题** 使用带标签的数据，例如带标签的狗猫图片，用来训练模型。模型训练完成后，可以使用其训练过的类别来分类用户提供的图片。'
- en: '**Regression problems**, on the other hand, use numerical data to understand
    the relationship between dependent and independent variables, such as house pricing
    based on different attributes. Once a model establishes a relationship, it can
    then forecast the pricing for different sets of attributes, even if the model
    has not been trained on these specific attributes. Some popular regression algorithms
    are linear regression, logistic regression, and polynomial regression.'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归问题**，另一方面，使用数值数据来理解依赖变量和自变量之间的关系，例如根据不同属性预测房价。一旦模型建立了这种关系，它就可以预测不同属性集的价格，即使该模型没有在这些特定属性上进行过训练。一些流行的回归算法包括线性回归、逻辑回归和多项式回归。'
- en: '**Unsupervised learning**: This uses ML algorithms to analyze and cluster unlabeled
    datasets to discover hidden patterns in data. Unsupervised learning can further
    be divided into the following two sub-categories:'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：这类算法利用机器学习分析和聚类未标注的数据集，以发现数据中的潜在模式。无监督学习可以进一步细分为以下两类：'
- en: '**Clustering algorithms** group data based on similarities or differences.
    A popular clustering algorithm is the **k-means clustering algorithm**, which
    uses Euclidian distances between data points to measure the similarity between
    data points and assign them in *k* distinct, non-overlapping clusters. It iterates
    to refine the clusters to minimize the variance within each cluster. A typical
    use case is segmenting customers based on purchasing behavior, demographics, or
    preferences to target marketing strategies effectively.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法**根据相似性或差异性对数据进行分组。一种流行的聚类算法是**k均值聚类算法**，该算法使用数据点之间的欧几里得距离来衡量数据点之间的相似性，并将它们分配到*k*个不同且不重叠的聚类中。它通过迭代来精细化聚类，最小化每个聚类内部的方差。一个典型的应用场景是根据购买行为、人口统计特征或偏好对客户进行细分，从而有效地制定营销策略。'
- en: '**Dimensionality reduction** is another form of unsupervised learning, which
    is used to reduce the number of features/dimensions in a given dataset. It aims
    to simplify models, reduce computational costs, and improve overall model performance.
    **Principal Component Analysis** (**PCA**) ([https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c))
    is a popular algorithm used for dimensionality reduction. It achieves this by
    finding a new set of features called components, which are composites of the original
    features that are uncorrelated with one another.'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**是另一种无监督学习方法，用于减少给定数据集中的特征/维度数量。它旨在简化模型，降低计算成本，并提高整体模型性能。**主成分分析**（**PCA**）([https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c))
    是一种常用的降维算法。它通过寻找一组新的特征，称为主成分，这些主成分是原始特征的组合，并且相互之间不相关，从而实现降维。'
- en: '**Semi-supervised learning**: This is a type of ML that combines supervised
    and unsupervised learning by leveraging both labeled and unlabeled data for training.
    This is particularly useful when obtaining labeled data is time-consuming and
    expensive because you can use small amounts of labeled data for training and then
    iteratively apply it to the large amounts of unlabeled data. This can be applied
    in both classification and regression use cases, such as spam/image/object detection,
    speech recognition, and forecasting.'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督学习**：这是一种结合监督学习和无监督学习的机器学习方法，通过利用标注数据和未标注数据进行训练。当获取标注数据既费时又昂贵时，这种方法尤为有用，因为你可以使用少量标注数据进行训练，然后反复将其应用于大量未标注数据。这可以应用于分类和回归问题，如垃圾邮件/图像/物体检测、语音识别和预测等。'
- en: '**Reinforcement learning**: In reinforcement learning, there is an agent and
    reward system, and algorithms learn by trial and error to maximize the reward
    for the agent. An **agent** is an autonomous system, like a computer program or
    robot, that can make decisions and act in response to its environment without
    direct human instructions. **Rewards** are given from the environment when agent
    actions lead to a positive outcome. For example, if we want to train a robot to
    walk without falling over, positive rewards are given for actions that help the
    robot to remain upright, and negative rewards are given for actions that cause
    it to fall over. The robot begins by trying different actions randomly, such as
    leaning forward, moving its legs, or shifting its weight. As it performs these
    actions, it observes the resulting changes in its state. The robot uses feedback
    (rewards) to update its understanding of which actions are beneficial and thus
    learns to walk over time.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：在强化学习中，有一个代理和奖励系统，算法通过反复试验来学习最大化代理的奖励。**代理**是一个自主系统，比如计算机程序或机器人，能够做出决策并根据其环境采取行动，而无需直接的人工指令。**奖励**是当代理的行为带来积极结果时，环境给予的反馈。例如，如果我们想训练一个机器人走路而不摔倒，当机器人做出有助于保持直立的动作时，会给予正向奖励，而做出导致摔倒的动作时，会给予负向奖励。机器人首先随机尝试不同的动作，比如向前倾、移动腿或改变体重。随着它执行这些动作，它观察到其状态的变化。机器人利用反馈（奖励）来更新其对哪些动作有益的理解，从而随着时间推移学会走路。'
- en: 'We have summarized the different categories of ML in *Figure 1**.2*:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在*图1.2*中总结了机器学习的不同类别：
- en: '![Figure 1.2 – Different categories of ML](img/B31108_01_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 机器学习的不同类别](img/B31108_01_02.jpg)'
- en: Figure 1.2 – Different categories of ML
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 机器学习的不同类别
- en: '**DL**: DL is a subset of ML that involves deep neural networks with many layers.
    Conceptually, it is inspired by the human brain, which has billions of deeply
    connected neurons and provides humans with very advanced cognition. Some popular
    examples of deep neural nets are **Convolutional Neural Networks** (**CNNs**),
    used for image processing, and **Recurrent Neural Networks** (**RNNs**), which
    are used for analyzing time series data or natural language processing.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习（DL）**：深度学习是机器学习（ML）的一个子集，涉及具有多层的深度神经网络。从概念上讲，它的灵感来源于人类大脑，大脑拥有数十亿个深度连接的神经元，赋予人类非常先进的认知能力。一些流行的深度神经网络例子包括**卷积神经网络**（**CNNs**），用于图像处理，以及**递归神经网络**（**RNNs**），用于分析时间序列数据或自然语言处理。'
- en: '**GenAI**: GenAI is a further subset of DL and focuses on creating new data,
    such as text, images, music, and other forms of content. Lots of generative applications
    are based on **Foundational Models** (**FMs**), which are large-scale AI models
    trained on vast amounts of diverse data, serving as a base for a wide range of
    downstream tasks. They are pre-trained on broad datasets and can be fine-tuned
    for specific applications. **Large Language Models** (**LLMs**) are a subset of
    FMs specifically designed for understanding and generating human language. GenAI
    is the primary focus of this book; we will be diving into its details later in
    the book.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成式人工智能（GenAI）**：生成式人工智能是深度学习（DL）的一个进一步子集，专注于创建新数据，如文本、图像、音乐和其他形式的内容。许多生成应用基于**基础模型**（**FMs**），即在大量多样化数据上训练的大规模AI模型，作为广泛下游任务的基础。它们在广泛的数据集上预训练，并可以针对特定应用进行微调。**大规模语言模型**（**LLMs**）是FMs的一个子集，专门设计用于理解和生成自然语言。生成式人工智能是本书的主要重点；我们将在本书后续部分深入探讨其细节。'
- en: Now that we understand the key differences between AI, ML, DL, and GenAI, let’s
    explore the evolution of ML and how transformer architecture has revolutionized
    the ML landscape, particularly in the field of **Natural Language** **Processing**
    (**NLP**).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了AI、ML、DL和生成式人工智能之间的关键区别，让我们来探讨一下机器学习的演变，以及变压器架构如何彻底改变了机器学习的格局，特别是在**自然语言处理**（**NLP**）领域。
- en: Evolution of machine learning
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的演变
- en: Since this book is about GenAI, what could be a better way to start it than
    asking ChatGPT to summarize the evolution of AI and ML over the last decade?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书是关于生成式人工智能的，那开始时请ChatGPT总结过去十年AI和机器学习（ML）发展的演变岂不更好？
- en: '`"Why did the chicken cross the road?" Describe how that question''s answer
    evolved using AI/ML over the` `last decade.`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`"为什么鸡要过马路？"` 描述过去十年间，AI/ML是如何演化出该问题的答案的。'
- en: '**ChatGPT Response (ChatGPT-4o, June** **16th, 2024)**:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**ChatGPT回应（ChatGPT-4o，2024年6月16日）**：'
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As can be seen from the preceding example, transformers revolutionized NLP
    by enabling parallel processing and improving scalability and efficiency; however,
    prior to transformers, deep neural networks, such as CNNs and RNNs, dominated
    the DL field since being introduced in the 1980s. Here are some brief descriptions
    of these neural networks:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的例子所示，transformers 通过实现并行处理和提高可扩展性与效率，彻底改变了自然语言处理（NLP）；然而，在 transformers
    之前，深度神经网络（如 CNN 和 RNN）自1980年代被引入以来，主导了深度学习（DL）领域。以下是这些神经网络的简要描述：
- en: '**CNNs** are similar in functionality to how our visual cortex functions. Our
    brain processes images from the retina by using specialized neurons that handle
    specific types of information or features. Similarly, the different filters in
    a CNN can detect various sets of features in an image or dataset. To learn more,
    refer to the paper by Yann LeCun et al. *Backpropagation Applied to Handwritten
    Zip Code Recognition* ([https://ieeexplore.ieee.org/document/6795724](https://ieeexplore.ieee.org/document/6795724)),
    presented in 1989\. CNNs are still commonly used for image analysis.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNN** 的功能类似于我们视觉皮层的工作原理。我们的脑部通过使用处理特定类型信息或特征的专门神经元来处理来自视网膜的图像。同样，CNN 中的不同滤波器能够检测图像或数据集中各种特征。欲了解更多信息，请参考
    Yann LeCun 等人于1989年发布的论文 *《反向传播应用于手写邮政编码识别》*（[https://ieeexplore.ieee.org/document/6795724](https://ieeexplore.ieee.org/document/6795724)）。CNN
    目前仍广泛用于图像分析。'
- en: '**RNNs** are commonly used for sequences of data, or time series data, to analyze
    patterns and potentially forecast future events, such as analyzing historical
    stock market data to predict future trade options. RNNs are also frequently used
    in NLP, as natural language is a sequence of words where the order matters and
    can significantly impact the meaning.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RNN** 常用于数据序列或时间序列数据，以分析模式并可能预测未来事件，如分析历史股市数据以预测未来交易选项。RNN 也常用于 NLP，因为自然语言是一个单词序列，其中单词顺序很重要，并且可能对意义产生重大影响。'
- en: The concept of RNNs was introduced in the 1986 paper by *David Rumelhart, Geoffrey
    Hinton, et al., Learning representations by back-propagating errors* *(*[https://www.nature.com/articles/323533a0](https://www.nature.com/articles/323533a0)*)*.
    This seminal paper introduced the concept of the backpropagation algorithm, based
    on the gradient descent concept, which is an essential technology for training
    neural networks and has revolutionized the entire AI field. In *Appendix 1A*,
    we have included a brief mathematical introduction to RNNs and their popular variants,
    such as **Long Short-Term Memory** (**LSTM**) networks and **Gated Recurrent**
    **Units** (**GRUs**).
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RNN 的概念最早出现在 *David Rumelhart, Geoffrey Hinton 等人于1986年发表的论文《通过反向传播误差学习表示》*（*
    [https://www.nature.com/articles/323533a0](https://www.nature.com/articles/323533a0)
    *）中。这篇具有开创性的论文提出了反向传播算法的概念，该算法基于梯度下降原理，是训练神经网络的核心技术，已经彻底改变了整个 AI 领域。在 *附录1A* 中，我们包括了
    RNN 及其流行变体（如**长短期记忆**（**LSTM**）网络和**门控循环单元**（**GRU**））的简要数学介绍。
- en: In 2007, Fei-Fei Li, then a professor of Computer Science at Stanford University,
    started the ImageNet competition ([https://www.image-net.org/](https://www.image-net.org/)),
    which included a massive dataset of images available on the internet that were
    labeled for training and testing. Every year, different AI/ML teams try to automate
    the prediction and improve the accuracy of their models using this training dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2007年，时任斯坦福大学计算机科学教授的李飞飞启动了 ImageNet 竞赛（[https://www.image-net.org/](https://www.image-net.org/)），该竞赛包括一个庞大的图像数据集，这些图像可以在互联网上获得，并且已标注用于训练和测试。每年，不同的
    AI/ML 团队都会尝试使用这个训练数据集自动化预测，并提高其模型的准确性。
- en: Until 2011, the state-of-the-art technologies in the ImageNet competition were
    based on classical ML approaches, such as **Support Vector Machines** (**SVMs**),
    which tried to create an isolation plane between two different categories with
    the maximum margin in between. A breakthrough came with the introduction of AlexNet
    in 2012, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. This
    paper ([https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html))
    won the ImageNet competition using deep CNNs and brought GPU programming to the
    forefront of AIML development.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 直到 2011 年，ImageNet 竞赛中的最先进技术仍然基于经典的机器学习方法，如 **支持向量机** (**SVM**)，该方法试图在两个不同类别之间创建一个最大间隔的隔离平面。2012
    年，AlexNet 的推出突破了这一局限，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 开发了这一模型。这篇论文
    ([https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html))
    使用深度卷积神经网络赢得了 ImageNet 竞赛，并使 GPU 编程成为人工智能与机器学习发展的重要前沿。
- en: '**Transformers**: In 2017, the transformer architecture was introduced by Vaswani
    et al. in their seminal paper *Attention Is All You Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    It revolutionized NLP by enabling parallel processing and improving the scalability
    and efficiency of NLP. **Bidirectional Encoder Representations from Transformers**
    (**BERT**), developed by Google, significantly improved the understanding of context
    in language models. Since then, there have been lots of LLMs introduced by different
    companies, such as the GPT series by OpenAI and Claude by Anthropic.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer**：2017 年，Vaswani 等人在他们的开创性论文 *Attention Is All You Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))
    中介绍了 Transformer 架构。它通过实现并行处理并提升了 NLP 的可扩展性和效率，彻底改变了自然语言处理（NLP）。Google 开发的 **双向编码器表示的
    Transformer** (**BERT**) 大大改善了语言模型中上下文理解的能力。从那时起，不同公司陆续推出了大量的 LLM，例如 OpenAI 的
    GPT 系列和 Anthropic 的 Claude。'
- en: Over the last two decades, ML has evolved from basic algorithmic models that
    were rule-based and dependent on manually curated features to advanced, context-aware
    models using deep learning frameworks such as neural networks and transformers.
    Let’s take a closer look at the transformer architecture now.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去二十年中，机器学习从依赖手动挑选特征并基于规则的基本算法模型发展到了使用深度学习框架（如神经网络和 Transformer）的高级上下文感知模型。现在让我们更详细地了解
    Transformer 架构。
- en: Transformer architecture
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: 'A **transformer model** uses an **encoder-decoder** architecture, where the
    encoder maps the input sequences/tokens through a self-attention mechanism. This
    mapped data is used by the decoder to generate the output sequence. The mapping
    of input tokens retains not only their intrinsic values but also their context
    and weight in the original sequence. Let’s go through some key aspects of the
    encoder architecture in the following figure:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer 模型**采用 **编码器-解码器** 架构，其中编码器通过自注意力机制将输入序列/标记映射。解码器使用这些映射数据生成输出序列。输入标记的映射不仅保留了它们的内在值，还保留了它们在原始序列中的上下文和权重。接下来，我们将通过下图介绍编码器架构的一些关键方面：'
- en: '![Figure 1.3 – Transformer architecture from the Attention Is All You Need
    paper](img/B31108_01_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 来自《Attention Is All You Need》论文的 Transformer 架构](img/B31108_01_03.jpg)'
- en: Figure 1.3 – Transformer architecture from the Attention Is All You Need paper
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 来自《Attention Is All You Need》论文的 Transformer 架构
- en: 'Here is a breakdown of the concepts highlighted in *Figure 1**.3*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 *图 1.3* 中突出显示的概念：
- en: '**Input embeddings**: Marked as **1** in the figure, this is a key part of
    the transformer model, which converts input sequences/tokens into high-dimensional
    vector embeddings. In real-world applications, output embeddings from a trained
    model may be stored in high-dimensional vector databases, such as Elasticsearch,
    Milvus, or PineCone. Vector databases help to find similar searches in high-dimensional
    space using either Euclidian distance or cosine similarity, and similar objects
    are assigned closer to each other in this high-dimensional vector space, as shown
    in the following figure.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入嵌入**：如图中标记为 **1**，这是 transformer 模型的关键部分，它将输入序列/标记转换为高维向量嵌入。在实际应用中，训练模型的输出嵌入可能会存储在高维向量数据库中，如
    Elasticsearch、Milvus 或 PineCone。向量数据库帮助通过欧几里得距离或余弦相似度在高维空间中找到相似的搜索，类似的对象会被分配到该高维向量空间中更接近的位置，如下图所示。'
- en: '![Figure 1.4 – Assignment of similar objects in high-dimensional space](img/B31108_01_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 高维空间中相似对象的分配](img/B31108_01_04.jpg)'
- en: Figure 1.4 – Assignment of similar objects in high-dimensional space
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 高维空间中相似对象的分配
- en: '**Positional encoding**: Positional encoding, marked as **2** in *Figure 1**.2*,
    provides information about the order of tokens in an input sequence. Unlike RNNs,
    which have the knowledge of time step *t* or the notion of the sequence, transformer
    models rely on self-attention mechanisms and lack awareness of intrinsic token
    order. Positional encoding injects sequential information into the token embeddings.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码**：位置编码，在*图 1.2*中标记为**2**，提供了输入序列中标记顺序的信息。与 RNN（循环神经网络）不同，RNN 具有时间步长
    *t* 或序列的概念，而 Transformer 模型依赖自注意力机制，并且缺乏对内在标记顺序的感知。位置编码将顺序信息注入标记嵌入中。'
- en: 'For example, if the input sequence is `The Brown hat`, here is how the mechanism
    would work:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果输入序列是 `The Brown hat`，以下是机制的工作原理：
- en: '`The` -> [0.1, 0.2]'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`The` -> [0.1, 0.2]'
- en: '`Brown` -> [0.3, 0.4]'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Brown` -> [0.3, 0.4]'
- en: '`hat` -> [0.5, 0.6]'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hat` -> [0.5, 0.6]'
- en: '**Positional encoding vectors**: We generate positional encodings for each
    position in the sentence. Transformer models typically use sinusoidal functions
    for positional encoding, however for illustration purposes, let’s use the following
    example:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码向量**：我们为句子中的每个位置生成位置编码。Transformer 模型通常使用正弦函数进行位置编码，但为了说明，我们采用以下示例：'
- en: '**Position 0**: [0.01, 0.02]'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置 0**: [0.01, 0.02]'
- en: '**Position 1**: [0.03, 0.04]'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置 1**: [0.03, 0.04]'
- en: '**Position 2**: [0.05, 0.06]*   `The` + Position 0: [0.1, 0.2] + [0.01, 0.02]
    = [0.11, 0.22]*   `Brown` + Position 1: [0.3, 0.4] + [0.03, 0.04] = [0.33, 0.44]*   `hat`
    + Position 2: [0.5, 0.6] + [0.05, 0.06] = [0.55, 0.66]'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置 2**: [0.05, 0.06]*   `The` + 位置 0: [0.1, 0.2] + [0.01, 0.02] = [0.11,
    0.22]*   `Brown` + 位置 1: [0.3, 0.4] + [0.03, 0.04] = [0.33, 0.44]*   `hat` + 位置
    2: [0.5, 0.6] + [0.05, 0.06] = [0.55, 0.66]'
- en: Now, each word has a unique vector that includes both the word’s meaning and
    its position in the sentence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个单词都有一个独特的向量，其中包括该单词的意义和它在句子中的位置。
- en: '**Multi-head attention mechanism**: In a multi-head attention mechanism, marked
    as **3** in *Figure 1**.2*, the model calculates the query, key, and value vectors
    for every I/P sequence and each attention head. A possible analogy for thinking
    about the multi-head attention mechanism is a team of detectives solving a very
    complex case, where each detective is an attention head. Every attention head
    has its own query vectors, which is an aspect of the case that a detective is
    focused upon, such as motive for the crime, weapon used, and so on. Finally, the
    key is the clues or pieces of evidence related to that given motive. So, if the
    query is the weapon used, the detective would investigate the crime scene for
    anything that could be used as a weapon, and the value would be the insights gained
    from each bit of evidence (e.g., fingerprints on a weapon). By having multiple
    attention heads, a model can focus on different parts of the input simultaneously,
    capturing various patterns or dependencies in the data, such as word meanings,
    words’ relative positions, and sentence structures, similar to our analogy, where
    different detectives are focusing on different aspects of the case. Besides that,
    multi-head attention also allows the parallel processing of inputs, making it
    highly efficient and speeding up both training and inference by distributing them
    across different devices.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力机制**：在多头注意力机制中，在*图 1.2*中标记为**3**，模型为每个输入序列和每个注意力头计算查询、键和值向量。一个可能的类比是将多头注意力机制比作一组侦探解决一个非常复杂的案件，其中每个侦探就是一个注意力头。每个注意力头都有自己的查询向量，专注于案件的某一方面，比如犯罪动机、使用的武器等。最后，键是与该动机相关的线索或证据。所以，如果查询是使用的武器，侦探会调查犯罪现场，寻找任何可以作为武器的物品，而值则是从每一条证据中获得的见解（例如，武器上的指纹）。通过拥有多个注意力头，模型可以同时关注输入的不同部分，捕捉数据中的各种模式或依赖关系，如单词的意义、单词的相对位置和句子结构，类似于我们的类比，不同的侦探关注案件的不同方面。此外，多头注意力机制还允许并行处理输入，使其高效，能够加速训练和推理过程，将任务分配到不同的设备上。'
- en: In *Appendix 1B*, we cover the mathematical model and complexities of key-value
    pairs, as well as feed-forward networks and the concept of temperature in transformer
    models.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*附录 1B*中，我们涵盖了键值对的数学模型和复杂性，以及前馈网络和 Transformer 模型中的温度概念。
- en: Now that we understand how transformer architecture has revolutionized DL by
    using an encoder-decoder framework that relies on self-attention mechanisms to
    enable the parallel processing of input data, let’s explore a typical GenAI project
    life cycle.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经理解了变换器架构如何通过使用依赖自注意力机制的编码器-解码器框架，使深度学习实现了革命性的突破，进而支持输入数据的并行处理，接下来我们将探索一个典型的生成式人工智能（GenAI）项目生命周期。
- en: GenAI project life cycle
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI项目生命周期
- en: Enterprise spending on GenAI projects has been growing exponentially since 2023,
    with c-suite executives planning to spend even more on GenAI projects ([https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026](https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026)).
    However, there is growing concern about how to quantify the **Return on Investment**
    (**ROI**) of these efforts, such as revenue impact, efficiency, and accuracy gains.
    Moving forward, ROI will become a critical part of the conversation, as enterprises
    look for new GenAI projects. So, before starting a new GenAI project, it is recommended
    to think about the entire project life cycle. In this section, we will be covering
    the project life cycle.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自2023年以来，企业在GenAI项目上的支出呈指数级增长，C-suite高管计划在GenAI项目上投入更多资金（[https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026](https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026)）。然而，如何量化这些努力的**投资回报率**（**ROI**）正成为日益关注的问题，比如收入影响、效率和准确性的提升。未来，ROI将成为讨论的关键部分，因为企业在寻找新的GenAI项目时将重点考虑。因此，在启动新GenAI项目之前，建议全面思考项目的整个生命周期。本节将详细介绍项目生命周期。
- en: Let’s first look at the following figure, which outlines the end-to-end GenAI
    project life cycle, starting from defining business objectives, or KPIs. This
    is followed by selecting or training FMs and optimizing them using techniques
    such as fine tuning and prompt tuning. Then, the model is evaluated, deployed,
    and continuously monitored to ensure that business goals are met.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下以下图表，该图展示了端到端的GenAI项目生命周期，从定义业务目标或KPI开始。接下来是选择或训练FM，并通过微调和提示调优等技术进行优化。然后，模型被评估、部署并持续监控，以确保业务目标得以实现。
- en: '![Figure1.5 – GenAI project life cycle](img/B31108_01_05.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – GenAI项目生命周期](img/B31108_01_05.jpg)'
- en: Figure1.5 – GenAI project life cycle
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – GenAI项目生命周期
- en: 'Let’s take a closer look at each stage of the GenAI project life cycle:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看GenAI项目生命周期的每个阶段：
- en: '**Business objectives and FM selection**: The GenAI project life cycle starts
    with the business objective/problem statement we are trying to solve with GenAI.
    Business objectives could be increasing customer conversion or retention, creating
    personalized marketing campaigns, or creating a customer chatbox using enterprise
    internal data:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务目标和FM选择**：GenAI项目生命周期从我们试图通过GenAI解决的业务目标/问题陈述开始。业务目标可能包括提高客户转化率或留存率、创建个性化的营销活动，或者利用企业内部数据创建客户聊天机器人：'
- en: '**Critical KPIs**: After the use case, the next consideration is business-critical
    KPIs, such as cost per inference. For example, if we are creating a personalized
    marketing campaign, we should ensure that the cost per customization is less than
    the product of the **Lifetime Value** (**LTV**) of the customer and the probability
    of customer conversion. If the cost of inference is more than the business value
    the project is expected to deliver, the project might not have long-term sustainability.
    Other KPIs to think about could include latency, as sub-second response times
    might be needed for certain use cases or throughput, if a certain number of tokens
    is expected per second.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键KPI**：在确定使用案例后，下一个需要考虑的是业务关键绩效指标（KPI），例如每次推理的成本。例如，如果我们在创建个性化营销活动，我们应确保每次定制的成本低于客户**生命周期价值**（**LTV**）与客户转化概率的乘积。如果推理成本高于项目预计带来的业务价值，项目可能无法保持长期的可持续性。其他需要考虑的KPI可能包括延迟，因为某些使用案例可能需要亚秒级的响应时间，或者吞吐量，如果期望每秒处理一定数量的令牌。'
- en: '**Selecting and training a FM**: The next step is to either pick an existing
    FM or train a new one. Training a new FM could be extremely resource intensive
    and cost billions of dollars and significant time, so in the majority of applications,
    it is better to pick an existing FM and do domain-specific optimization. When
    picking an existing model, you can select either an open source model or a proprietary
    model that can be accessed through web interfaces or over APIs, such as Claude
    or ChatGPT. It is always a good idea to check the licensing terms of these models
    to ensure that they meet application requirements and can scale with your enterprise
    needs.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择和训练FM**：下一步是选择现有的FM或训练一个新的FM。训练一个新的FM可能非常消耗资源，成本高达数十亿美元且需要大量时间，因此在大多数应用中，选择现有的FM并进行特定领域的优化更为可取。选择现有模型时，可以选择开源模型或通过网页接口或API访问的专有模型，如Claude或ChatGPT。检查这些模型的许可条款，以确保它们符合应用需求，并能随着企业需求的增长进行扩展，始终是一个好主意。'
- en: '**Model optimization**: Once the FM is selected, the next step is to optimize
    the model for the business use case using techniques such as fine tuning, prompt
    tuning, Reinforcement Learning from Human Feedback (RLHF), and DPO:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型优化**：一旦选择了FM，下一步是使用微调、提示调优、人类反馈强化学习（RLHF）和DPO等技术来优化模型，以适应业务用例。'
- en: '**Fine tuning**: In fine tuning, a domain-specific labeled dataset with prompt
    and completion data is used to train the model for a specific domain or set of
    domains. Since all model weights can be updated in fine tuning, the compute requirements
    for fine tuning are similar to those for full training. However, the training
    time is smaller, as we are training the model on a smaller dataset. To reduce
    the training resources, there are options such as **Performance-Efficient Fine
    Tuning** (**PEFT**), which only updates a small set of weights and thus reduces
    the compute requirements. **Low Rank Adoption** (**LoRA**) is a very popular form
    of PEFT, where the original model matrix is reparametrized using a low-rank representation
    to significantly reduce the number of model parameters to be updated. As an example,
    in Vaswani’s paper, each attention head has the dimensionality [512, 64], that
    is, 32,768 parameters to train per head. With LoRA, we train much smaller weight
    matrices to offer 80% or higher weight reduction. We will cover this topic in
    detail in [*Chapter 4*](B31108_04.xhtml#_idTextAnchor049). Another version of
    LoRA is **QLoRA** (**Quantized Lower Rank Adoption**). In QLoRA, we use quantization
    to compress the model weights from 32-bit precision to 8-bit or 4-bit precision,
    which dramatically reduces the model size and makes it easier to run on GPUs with
    less memory.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：在微调中，使用一个包含提示和完成数据的特定领域标注数据集来训练模型，以适应特定领域或一组领域。由于在微调过程中所有模型权重都可以更新，因此微调的计算需求与完整训练类似。然而，训练时间较短，因为我们是在一个较小的数据集上训练模型。为了减少训练资源，可以选择例如**性能高效微调**（**PEFT**）的方法，它只更新少量的权重，从而降低计算需求。**低秩适配**（**LoRA**）是PEFT中一种非常流行的方法，其中原始模型矩阵通过低秩表示重新参数化，从而显著减少需要更新的模型参数数量。例如，在Vaswani的论文中，每个注意力头的维度是[512,
    64]，也就是每个头需要训练32,768个参数。通过LoRA，我们训练更小的权重矩阵，从而实现80%或更高的权重减少。我们将在[*第4章*](B31108_04.xhtml#_idTextAnchor049)中详细讨论这一主题。LoRA的另一版本是**QLoRA**（**量化低秩适配**）。在QLoRA中，我们使用量化技术将模型权重从32位精度压缩到8位或4位精度，这大大减少了模型大小，并使其在内存较小的GPU上运行更加高效。'
- en: '**Prompt tuning** is a low-cost technique where soft prompt tokens are added
    to the input query to optimize model performance for domain-specific tasks. These
    tokens are optimized as the model is retrained on the labeled domain-specific
    data. Unlike traditional fine tuning, which adjusts the model’s parameters across
    numerous training iterations, prompt tuning focuses on refining the prompts to
    guide the model more effectively.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示调优**是一种低成本的技术，其中向输入查询中添加软提示令牌，以优化模型在特定领域任务中的表现。随着模型在标注的领域特定数据上重新训练，这些令牌会被优化。与传统的微调不同，后者会在多个训练迭代中调整模型的参数，提示调优则专注于精炼提示，以更有效地引导模型。'
- en: In **RLHF** ([https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)),
    human feedback is used to train LLMs to align with human preferences using RL
    techniques. In this approach, the LLM generates multiple responses to a variety
    of prompts, which humans evaluate and rank based on criteria such as accuracy,
    relevance, and ethical standards. Using the ranked responses, a reward model is
    trained to predict the human preference score for any given LLM response, and
    the LLM is fine-tuned using RL algorithms guided by the developed reward model.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**RLHF** ([https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf))
    中，使用人类反馈训练LLM，使其与人类偏好对齐，并使用强化学习（RL）技术。在这种方法中，LLM生成对各种提示的多个响应，人类根据准确性、相关性和道德标准等标准对其进行评估和排名。使用排名后的响应，训练一个奖励模型来预测任何给定LLM响应的人类偏好得分，并通过RL算法对LLM进行微调，奖励模型为其指导。
- en: In **Direct Preference Optimization** (**DPO**) ([https://huggingface.co/papers/2305.18290](https://huggingface.co/papers/2305.18290)),
    a policy is trained with a simple classification objective to best align with
    human preferences without using RL. Similar to RLHF, multiple outputs are generated
    by the LLM for a set of input prompts. Human evaluators compare these outputs
    in pairs and indicate which output they prefer. This creates a dataset of preference
    pairs, such as (Ap, Anp), where Ap indicates a preferred answer and Anp indicates
    a not-preferred answer. A loss function is then designed to maximize the probability
    that the preferred output is ranked higher than the less-preferred output as the
    LLM output. The model parameters are optimized to minimize this loss function
    over all collected preference pairs. This directly tunes the model to produce
    outputs that are more aligned with human preferences.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接偏好优化** (**DPO**) ([https://huggingface.co/papers/2305.18290](https://huggingface.co/papers/2305.18290))
    中，使用简单的分类目标训练一个策略，以最佳方式与人类偏好对齐，而无需使用强化学习（RL）。与RLHF类似，LLM会为一组输入提示生成多个输出。人类评估者将这些输出成对比较，并指出他们偏好的输出。这将生成一组偏好对数据集，例如（Ap,
    Anp），其中Ap表示偏好的答案，Anp表示不偏好的答案。然后设计一个损失函数，最大化偏好输出在LLM输出中高于不偏好输出的概率。模型参数经过优化，以最小化所有收集的偏好对上的损失函数。这直接调优模型，以生成更符合人类偏好的输出。'
- en: '**Evaluation**: After the models are trained, you need to evaluate them for
    accuracy using metrics such as *ROUGE* ([https://huggingface.co/spaces/evaluate-metric/rouge](https://huggingface.co/spaces/evaluate-metric/rouge))
    or *BLEU* ([https://huggingface.co/spaces/evaluate-metric/bleu](https://huggingface.co/spaces/evaluate-metric/bleu)),
    based on the use case:'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：在模型训练后，需要使用如*ROUGE* ([https://huggingface.co/spaces/evaluate-metric/rouge](https://huggingface.co/spaces/evaluate-metric/rouge))
    或 *BLEU* ([https://huggingface.co/spaces/evaluate-metric/bleu](https://huggingface.co/spaces/evaluate-metric/bleu))
    等指标根据使用场景评估模型的准确性：'
- en: '**Bilingual Evaluation Understudy** (**BLEU**) measures how closely machine-generated
    text matches a set of reference texts. This metric was originally designed for
    machine translation and is also commonly used to evaluate text generation tasks,
    such as **n-grams** (contiguous sequences of *n* words) in generated text against
    reference texts.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双语评估替代** (**BLEU**) 衡量机器生成文本与一组参考文本的匹配程度。此指标最初是为机器翻译设计的，也常用于评估文本生成任务，例如生成文本中与参考文本的**n-grams**（连续的*n*个单词）对比。'
- en: '**Recall-Oriented Understudy for Gisting Evaluation** (**ROUGE**) is a set
    of metrics used to evaluate the quality of summaries and translations generated
    by models. It compares the overlap between the generated text and a set of reference
    texts. ROUGE metrics are particularly popular in evaluating the performance of
    summarization systems.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回导向的摘要评估** (**ROUGE**) 是一套用于评估模型生成的摘要和翻译质量的指标。它比较生成文本与一组参考文本之间的重叠程度。ROUGE指标在评估摘要系统的性能时尤其受欢迎。'
- en: Besides those metrics, developers should also evaluate generated responses against
    the **Honesty, Harmlessness, and Helpfulness** (**3H**) metric and ensure that
    training data is free of biases and harmful content.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些指标外，开发人员还应根据**诚实性、无害性和有用性** (**3H**) 指标评估生成的响应，并确保训练数据没有偏见和有害内容。
- en: '**Deployment optimization**: Once the model training/fine tuning is complete,
    the next step is to explore options to reduce the model size and optimize it for
    latency and cost. Some possible options are quantization, distillation, and pruning:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署优化**：一旦模型训练/微调完成，下一步是探索减少模型大小并优化其延迟和成本的选项。一些可能的选项包括量化、蒸馏和剪枝：'
- en: '**Quantization**: In quantization, we can explore model precision tradeoffs,
    such as changing model weights from FP32 (floating point, 32 bit), which requires
    4 bytes of memory per parameter, to FP16 or Bfloat16, which requires 2 bytes of
    memory per parameter, or even Int 8, which requires only 1 byte per parameter.
    So, by moving from FP32 to Int8, we could reduce memory requirements by 4X; however,
    that could impact model accuracy. So, we need to evaluate model performance/accuracy
    to see whether these trade-offs are acceptable.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化**：在量化过程中，我们可以探索模型精度的权衡，例如将模型权重从 FP32（浮动点，32位）转换为 FP16 或 Bfloat16（每个参数需要
    2 字节内存），甚至是 Int 8（每个参数只需要 1 字节内存）。因此，从 FP32 转换到 Int8，我们可以将内存需求减少 4 倍；然而，这可能会影响模型的准确性。因此，我们需要评估模型的性能/准确性，以判断这些权衡是否可以接受。'
- en: '**Distillation**: In distillation, we can train a student model with a smaller
    size than the original model by minimizing the distillation loss. The goal of
    distillation is to create a student model that approximates the performance of
    the teacher model, while being more efficient in terms of computational resources,
    such as memory and inference time.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蒸馏**：在蒸馏过程中，我们通过最小化蒸馏损失来训练一个比原始模型更小的学生模型。蒸馏的目标是创建一个在计算资源（如内存和推理时间）上更高效的学生模型，同时尽可能接近教师模型的性能。'
- en: '**Pruning**: In pruning, we try to reduce the size and complexity of a model
    by removing less important parameters, such as weights close to zero, while maintaining/improving
    the model’s performance. The main goal of pruning is to create a more efficient
    model that requires fewer computational resources for inference and training without
    significantly affecting accuracy.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**：在剪枝过程中，我们通过去除不太重要的参数（如接近零的权重）来减少模型的大小和复杂性，同时保持或提高模型的性能。剪枝的主要目标是创建一个更加高效的模型，在推理和训练时需要更少的计算资源，同时不显著影响准确性。'
- en: '**Deployment options**: Once the model is ready, the next choice is to pick
    the deployment options, such as cloud, on-premises, or hybrid approach. The selection
    depends on criteria such as hardware availability, cost, CapEx versus OpEx, and
    data residency requirements. Usually, cloud deployment offers the most simplicity
    due to the managed services and scalability; however, there could be data residency
    requirements, requiring either on-premises or hybrid deployments.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署选项**：一旦模型准备好，接下来的选择是选择部署选项，如云部署、本地部署或混合部署。选择取决于硬件可用性、成本、资本支出与运营支出的对比以及数据驻留要求等标准。通常，云部署因其托管服务和可扩展性而提供最简便的选项；然而，可能存在数据驻留要求，迫使我们选择本地或混合部署。'
- en: '`summarize this text`. In few-shot learning, the user includes a few examples
    within the input prompt, which helps the model learn the desired output format
    and style.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summarize this text`。在少样本学习中，用户在输入提示中包含几个示例，帮助模型学习期望的输出格式和风格。'
- en: Once the model is deployed, we need to keep monitoring it to ensure that the
    model’s results don’t drift or get stale over time. We will explore model monitoring,
    performance, and drift in detail in [*Chapter 11*](B31108_11.xhtml#_idTextAnchor145).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型部署完成，我们需要持续监控它，以确保模型的结果不会随着时间的推移发生漂移或过时。我们将在[*第11章*](B31108_11.xhtml#_idTextAnchor145)中详细探讨模型监控、性能和漂移。
- en: In this section, we looked at the various stages of the GenAI project life cycle.
    It starts with defining the business objectives and KPIs, which is followed by
    model selection and various fine-tuning techniques. We evaluate model accuracy
    using metrics such as BLEU and ROGUE, and finally we deploy and continuously optimize
    the model. Now, let’s look at the various layers of the GenAI deployment stack
    for deploying models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们回顾了 GenAI 项目生命周期的各个阶段。它从定义业务目标和关键绩效指标（KPI）开始，然后是模型选择和各种微调技术。我们使用 BLEU
    和 ROGUE 等指标评估模型的准确性，最后部署并持续优化模型。接下来，让我们来看一下用于部署模型的 GenAI 部署堆栈的各个层次。
- en: GenAI deployment stack
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI 部署堆栈
- en: As we discuss GenAI application development and deployment over Kubernetes,
    it is a good idea to understand the entire deployment stack, which can help us
    to think about the right infrastructure, orchestration platform, and libraries.
    The following figure shows the various layers of the GenAI deployment stack, from
    the foundational infrastructure layer comprising compute, storage, and networking
    through to the orchestration, tools, and deployment layers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论基于Kubernetes的GenAI应用开发和部署时，了解整个部署堆栈是个好主意，这可以帮助我们思考合适的基础设施、编排平台和库。下图展示了GenAI部署堆栈的各个层次，从包括计算、存储和网络的基础设施层，到编排、工具和部署层。
- en: '![](img/B31108_01_06.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31108_01_06.jpg)'
- en: Figure1.6 – Deployment stack for GenAI applications
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – GenAI应用的部署堆栈
- en: 'Let’s a closer look at each of these layers:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些层次：
- en: '**Infrastructure layer**: We will start from the foundation layer of the stack
    and move upward. The foundation of this stack is the infrastructure layer, which
    covers compute, networking, and storage options:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施层**：我们将从堆栈的基础层开始，向上移动。这个堆栈的基础是基础设施层，涵盖了计算、网络和存储选项：'
- en: '**Compute**: For compute, we can use options such as CPUs, GPUs, custom accelerators,
    or a combination of these. As explained previously, LLMs are very computationally
    intensive. GPUs offer massively parallel matrix multiplication capabilities and
    are mostly favored for training workloads. For inference, both CPUs and GPUs are
    used, but for LLMs with billions of model parameters, GPUs are often needed for
    inference as well. Besides CPUs and GPUs, there are custom accelerators, such
    as AWS Inferentia and Trainium, which are custom silicon chips specially designed
    for ML and highly optimized for mathematical operations.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**：对于计算，我们可以选择CPU、GPU、定制加速器，或这些的组合。如前所述，LLM非常计算密集型。GPU提供大规模的并行矩阵乘法能力，通常在训练工作负载中受到青睐。对于推理，CPU和GPU都可以使用，但对于具有数十亿参数的LLM，推理时通常也需要GPU。除了CPU和GPU，还有定制加速器，如AWS
    Inferentia和Trainium，它们是专为ML设计的定制硅芯片，并且对数学运算进行了高度优化。'
- en: '**Networking**: Networking is the next critical infrastructure component. For
    language models that are very large, both training and inference could become
    a distributed system problem. To explain this, let’s look at recent LLM model
    trends:'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：网络是下一个关键的基础设施组件。对于非常大的语言模型，训练和推理可能会成为分布式系统问题。为了说明这一点，我们来看一下近期LLM模型的趋势：'
- en: '| **Year** | **Model** | **Model Size (in** **billion parameters)** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **年份** | **模型** | **模型大小（以** **十亿参数为单位）** |'
- en: '| --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2018 | BERT-L | 0.34 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | BERT-L | 0.34 |'
- en: '| 2019 | T5-L | 0.77 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | T5-L | 0.77 |'
- en: '| 2019 | GPT2 | 1.5 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | GPT2 | 1.5 |'
- en: '| 2020 | GPT3 | 175 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | GPT3 | 175 |'
- en: '| 2023 | GPT4 | Trillions |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | GPT4 | 万亿 |'
- en: Table 1.1 – Evolution of GenAI model sizes
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1 – GenAI模型规模的演变
- en: It’s evident here that GenAI models are growing exponentially, and more parameters
    generally mean a more complex model that can capture more intricate patterns in
    data, thus requiring more computational resources for training and inference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，GenAI模型正在呈指数级增长，更多的参数通常意味着更复杂的模型，能够捕捉数据中更复杂的模式，因此在训练和推理过程中需要更多的计算资源。
- en: To clarify, if we say a model has 1 billion parameters, it usually refers to
    model weights after training has been completed. However, for training these models,
    we need the model weights, gradients, Adam Optimizers, activations terms, and
    some temporary variables throughout the training epochs ([https://huggingface.co/docs/transformers/v4.33.3/perf_train_gpu_one#batch-size-choice](https://huggingface.co/docs/transformers/v4.33.3/perf_train_gpu_one#batch-size-choice)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，如果我们说一个模型有10亿个参数，通常是指训练完成后模型的权重。然而，训练这些模型时，我们需要模型权重、梯度、Adam优化器、激活项和一些在训练时期使用的临时变量（[https://huggingface.co/docs/transformers/v4.33.3/perf_train_gpu_one#batch-size-choice](https://huggingface.co/docs/transformers/v4.33.3/perf_train_gpu_one#batch-size-choice)）。
- en: So, during training, we might need to store up to *six parameters* per training
    weight, which could take up to 24 bytes at FP32 precision, 12 bytes at FP16 (or
    Bfloat16) precision, or 6 bytes at FP8 or Int8\. Actual precision depends on the
    accuracy requirements versus the training cost.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练过程中，我们可能需要为每个训练权重存储最多*六个参数*，这可能需要24字节（FP32精度）、12字节（FP16或Bfloat16精度）或6字节（FP8或Int8）。实际的精度取决于准确性要求与训练成本之间的权衡。
- en: Now, let’s say we are training or fine tuning a 70B-parameter model, such as
    Llama3\. It would require 840 GB (70B*12 bytes) of memory to store all the model
    weights and temporary variables with Bfloat16 or FP16 for precision. If we are
    using the latest NVIDIA H200 GPUs, which were introduced in 2024 and offer up
    to 141 GB of memory, we would still need about six GPUs to store these model parameters
    during training or full fine tuning, assuming the model is fully sharded.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在训练或微调一个70B参数的模型，比如Llama3。为了存储所有的模型权重和临时变量，使用Bfloat16或FP16进行精度存储，可能需要840GB的内存（70B*12字节）。如果我们使用的是2024年推出的最新NVIDIA
    H200 GPU，最大支持141GB内存，那么我们仍然需要大约六个GPU来存储这些模型参数，在训练或完全微调期间，假设模型是完全分片的。
- en: In reality, the actual GPU count could be higher, if we would like to train
    these models in a reasonable time. This explains that East-West traffic, that
    is, the traffic flowing within the data center nodes or GPUs, could become a performance
    bottleneck for large model training or fine tuning. For this reason, non-blocking
    networking technologies such as **memory coherence** and **Remote Direct Memory
    Access** (**RDMA**) could help scale performance across nodes. RDMA is a technology
    that lets nodes in a distributed system access the memory of other nodes without
    involving either the core processor or operating system. Similarly, memory coherence
    technology ensures that all the caches in the system have up-to-date memory information
    and that write operations by one node to memory are visible to all the nodes/caches
    that are connected coherently. These two technologies result in lower latency
    and increased throughput for distributed training and fine-tuning problems.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果我们希望在合理的时间内训练这些模型，实际的GPU数量可能会更高。这就解释了东西向流量，也就是在数据中心节点或GPU之间流动的流量，可能会成为大规模模型训练或微调的性能瓶颈。因此，像**内存一致性**和**远程直接内存访问**（**RDMA**）这样的非阻塞网络技术，可以帮助在节点间扩展性能。RDMA是一项技术，允许分布式系统中的节点在不涉及核心处理器或操作系统的情况下，访问其他节点的内存。同样，内存一致性技术确保系统中所有的缓存都拥有最新的内存信息，并且一个节点对内存的写操作能够被所有连接一致的节点/缓存看到。这两项技术能够降低延迟并提高分布式训练和微调任务的吞吐量。
- en: '**Storage**: After networking, the next infrastructure choice is storage, with
    choices such as block storage, file storage, or Lustre. In a **block storage system**,
    such as Amazon S3, data is stored in data blocks. These block sizes range from
    512 bytes to 64 KB, and multiple blocks can be accessed in parallel, which could
    provide higher I/O bandwidth. In a **file storage** system, data is stored in
    files and directories, making it easier to manage and structure the data. However,
    a file storage system creates an additional overhead of managing the file hierarchy
    and metadata. **Lustre** is a popular storage system that is now gaining traction
    for GenAI applications and has been in use for quite some time in high-performance
    computing. Lustre provides a massively parallel file storage system and can be
    scaled horizontally by adding more resources.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**：在网络之后，下一步的基础设施选择是存储，选项包括块存储、文件存储或Lustre。在**块存储系统**中，如亚马逊S3，数据存储在数据块中。这些数据块的大小范围从512字节到64KB，多个块可以并行访问，从而提供更高的I/O带宽。在**文件存储**系统中，数据存储在文件和目录中，这使得数据的管理和结构化变得更加容易。然而，文件存储系统增加了管理文件层次结构和元数据的额外开销。**Lustre**是一种流行的存储系统，近年来在生成式AI（GenAI）应用中获得了越来越多的关注，并且在高性能计算中已使用了相当长的时间。Lustre提供了一个大规模并行的文件存储系统，可以通过增加更多资源进行水平扩展。'
- en: If you are planning to store data in databases, you might choose SQL for fixed
    schema data or NoSQL databases for unstructured data, such as images and videos.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你计划将数据存储在数据库中，可能会选择SQL用于固定模式的数据，或者选择NoSQL数据库来存储非结构化数据，如图片和视频。
- en: '**Compute unit**: After infrastructure, the next thing is to select the compute
    unit. The possible options are **bare-metal machines**, **Virtual Machines** (**VMs**),
    or **containers**. Containers pack all software dependencies, such as language
    runtimes and libraries, into an image, and multiple containers can share the same
    node/kernel. This allows much tighter bin packing or resource utilization as compared
    to VMs. For VMs, each VM requires a separate OS and runtime before the application
    can be deployed. Bare-metal machines are physical servers that are dedicated to
    a single tenant or customer, unlike VMs, which run on shared physical servers
    through a hypervisor.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算单元**：在选择基础设施之后，下一步是选择计算单元。可能的选项包括**裸金属机器**、**虚拟机**（**VMs**）或**容器**。容器将所有软件依赖项，如语言运行时和库，打包成镜像，多个容器可以共享同一个节点/内核。与虚拟机相比，这允许更紧密的资源利用或资源打包。对于虚拟机，每个虚拟机在部署应用程序之前都需要单独的操作系统和运行时。裸金属机器是专门为单一租户或客户提供的物理服务器，不像虚拟机那样通过虚拟机监控程序在共享物理服务器上运行。'
- en: '**Orchestration platform**: After compute unit selection, we choose the orchestration
    platform to manage the underlying infrastructure life cycle. This orchestration
    platform needs to be capable of scaling up or down based on workload demand and
    should be able to withstand network outages or hardware failures. **Kubernetes**
    (**K8s**) has emerged as a leading orchestration platform for containers and will
    be the primary focus in this book, as lots of leading companies, such as OpenAI
    ([https://openai.com/research/scaling-kubernetes-to-7500-nodes](https://openai.com/research/scaling-kubernetes-to-7500-nodes))
    and Anthropic, are using it for their GenAI workload orchestration. OpenStack
    is an open source orchestration platform for VMs.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编排平台**：在选择计算单元后，我们选择编排平台来管理底层基础设施的生命周期。这个编排平台需要能够根据工作负载需求进行扩展或缩减，并且应该能够承受网络中断或硬件故障。**Kubernetes**（**K8s**）已成为容器编排的领先平台，本书将重点讨论它，因为许多领先公司，如
    OpenAI ([https://openai.com/research/scaling-kubernetes-to-7500-nodes](https://openai.com/research/scaling-kubernetes-to-7500-nodes))
    和 Anthropic，都在使用它来进行 GenAI 工作负载的编排。OpenStack 是一个开源虚拟机编排平台。'
- en: '**Frameworks**: After the orchestration platform comes the AI framework, such
    as TensorFlow or PyTorch. PyTorch **Fully Sharded Data Parallel** ( **FSDP** )
    and TensorFlow distributed libraries allow model parameters and training data
    to be distributed across multiple GPUs and help the system to scale for large
    model sizes.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架**：在选择编排平台之后，接下来是选择 AI 框架，例如 TensorFlow 或 PyTorch。PyTorch 的**完全分片数据并行**（**FSDP**）和
    TensorFlow 分布式库允许将模型参数和训练数据分布到多个 GPU 上，并帮助系统扩展以适应大型模型。'
- en: '**IDEs**: After orchestration, the next selection is what **Integrated Development
    Environment** (**IDE**) to use, such as JupyterHub, as well as what libraries
    to use, such as cuDNN, NumPy, or pandas, based on the infrastructure selection
    made earlier.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成开发环境**（**IDE**）：在完成基础设施选择后，接下来的选择是使用什么**集成开发环境**（**IDE**），例如 JupyterHub，以及选择哪些库，如
    cuDNN、NumPy 或 pandas，这些都取决于之前做出的基础设施选择。'
- en: '**Endpoints**: Finally, the user can select the final deployment endpoint,
    such as offering models as an API, a platform, or a workload. Considering this
    stack for scalability, cost, latency, and disaster recovery could help users avoid
    very expensive rearchitecting once application use starts to scale. In our experience,
    data science teams often pick a simple architecture for proofs of concept, but
    these implementations sometimes don’t scale well and are very expensive during
    deployment.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**终端节点**：最后，用户可以选择最终的部署终端节点，例如将模型提供为 API、平台或工作负载。考虑到可扩展性、成本、延迟和灾难恢复等因素，可以帮助用户在应用开始扩展时避免昂贵的重构。根据我们的经验，数据科学团队通常会为概念验证选择简单的架构，但这些实现有时无法很好地扩展，而且在部署过程中非常昂贵。'
- en: In this section, we discussed various layers of the GenAI deployment stack,
    from the foundational infrastructure layer to the high-level abstraction layer.
    We also looked at the challenges these models present as model sizes grow and
    the different ways to solve them. Let’s take a look at GenAI use cases across
    industries.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了 GenAI 部署堆栈的各个层次，从基础设施层到高级抽象层。我们还考察了随着模型规模增长，这些模型所带来的挑战及其不同的解决方案。接下来，让我们看看各行业中的
    GenAI 使用案例。
- en: GenAI use cases
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI 使用案例
- en: GenAI is transforming all industries. As per McKinsey ([https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights)),
    it is expected to add trillions of dollars to the economy by 2030\. The following
    are some of the industry verticals affected and use cases. It is by no stretch
    a comprehensive list, as the list of applications is growing very rapidly.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI 正在改变所有行业。根据麦肯锡的研究（[https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights)），预计到
    2030 年，GenAI 将为经济增加数万亿美元。以下是一些受影响的行业领域及其应用案例。这不是一个全面的列表，因为应用场景的数量正在迅速增长。
- en: 'However, learning about some of these will give you an idea of the potential
    GenAI carries:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，了解其中一些将帮助你了解 GenAI 的潜力：
- en: '`the best running shoes under $100 with red stripes`. GenAI can comprehend
    the user’s request and provide recommendations accordingly.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`最好的跑步鞋，价格低于 100 美元，带红色条纹`。GenAI 能理解用户的需求并相应地提供推荐。'
- en: '**Review summarization**: GenAI can help summarize user reviews and overall
    sentiments, so users don’t have to read through all the different reviews.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评论总结**：GenAI 可以帮助总结用户评论和整体情感，使用户无需浏览所有不同的评论。'
- en: '**Finance**:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融**：'
- en: '**Financial reporting/analysis**: GenAI can help create financial reports and
    summaries based on the data and trend analysis.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**财务报告/分析**：GenAI 可以根据数据和趋势分析，帮助创建财务报告和总结。'
- en: '**Customer service**: GenAI can help with personalized marketing and chatbots
    to address user questions based on their data.*   **Healthcare**:'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户服务**：GenAI 可以帮助进行个性化营销，并使用聊天机器人根据用户数据回答问题。*   **医疗保健**：'
- en: '**Drug discovery**: GenAI models can predict how different drugs interact with
    various biological targets (proteins, enzymes, etc.) by analyzing existing interaction
    data. This can help in identifying promising drug candidates more efficiently.
    They can also propose novel molecular structures that have the potential to be
    effective drugs. These models can be further optimized for properties such as
    binding affinity, bioavailability, and toxicity.'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**药物发现**：GenAI 模型可以通过分析现有的相互作用数据，预测不同药物如何与各种生物靶点（蛋白质、酶等）相互作用。这有助于更高效地识别有前景的药物候选物。它们还可以提出可能有效的药物的新分子结构。这些模型还可以进一步优化，例如在结合亲和力、生物利用度和毒性等属性方面。'
- en: '**Personalized medicine**: LLMs can integrate and analyze diverse types of
    patient data, such as medical records, lab results, imaging data, and genetic
    information, to create comprehensive patient profiles and identify genetic biomarkers
    that predict a patient’s response to particular treatments.*   **Education**:'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化医疗**：大型语言模型（LLM）可以整合和分析多种患者数据，如病历、实验室结果、影像数据和遗传信息，以创建全面的患者档案，并识别预测患者对特定治疗反应的遗传标记物。*   **教育**：'
- en: '**Personalized learning**: GenAI can help develop personalized learning paths,
    based on user journeys and responses.'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化学习**：GenAI 可以根据用户的旅程和反馈，帮助开发个性化的学习路径。'
- en: '**New language learning**: GenAI can help create personalized content and conversation
    examples.*   **Legal**:'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新语言学习**：GenAI 可以帮助创建个性化的内容和对话示例。*   **法律**：'
- en: '**Document review and summarization**: GenAI can help automate the review and
    summarization of legal documents and prior similar cases for legal precedents.'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档审阅和总结**：GenAI 可以帮助自动化审阅和总结法律文件以及类似案件的前例。'
- en: '**Contract generation**: GenAI can help create legal contracts based on the
    parameters provided.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合同生成**：GenAI 可以根据提供的参数帮助生成法律合同。'
- en: '**Customer interaction**: GenAI can help in developing chatbots for client
    inquiries and support.*   **Entertainment**:'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户互动**：GenAI 可以帮助开发聊天机器人，处理客户查询和支持。*   **娱乐**：'
- en: '**Content creation**: GenAI can help create scripts, music, artwork, and characters.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容创作**：GenAI 可以帮助创作剧本、音乐、艺术作品和角色。'
- en: '**Virtual reality**: GenAI can help create immersive VR environments and experiences.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟现实**：GenAI 可以帮助创建沉浸式的 VR 环境和体验。'
- en: '**Personalized content**: GenAI can help in tailoring entertainment content
    and recommendations to individual preferences.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化内容**：GenAI 可以根据个人偏好帮助定制娱乐内容和推荐。'
- en: In this section, we looked at various GenAI use cases across different industries,
    from enhancing customer experience in retail to drug discovery and personalized
    medicine in healthcare. This is not an exhaustive list and it continues to grow
    as technology and models evolve.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们考察了各个行业中 GenAI 的不同应用案例，从零售行业提升客户体验到医疗保健中的药物发现和个性化医疗。这只是一个非详尽的列表，随着技术和模型的进化，应用场景还在不断增长。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the differences between AI and GenAI. AI is a very
    broad term that refers to technologies that enable machines to emulate human intelligence
    and encompasses a broad range of applications, including GenAI, whereas GenAI
    specifically focuses on creating new content, such as text, images, and videos.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了 AI 和 GenAI 之间的区别。AI 是一个非常广泛的术语，指的是使机器能够模拟人类智能的技术，涵盖了广泛的应用领域，包括 GenAI，而
    GenAI 特别专注于创建新内容，如文本、图像和视频。
- en: We then looked at the evolution of ML, understanding its progression from CNNs/RNNs
    to the transformer architecture introduced in 2017\. Transformers have revolutionized
    AI with their ability to process sequences of data efficiently, making them fundamental
    to many GenAI applications, particularly in NLP.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们探讨了机器学习（ML）的演变，理解了从卷积神经网络（CNN）/循环神经网络（RNN）到 2017 年推出的 Transformer 架构的进展。Transformer
    以其高效处理数据序列的能力，革新了 AI，成为许多 GenAI 应用的基础，尤其是在自然语言处理（NLP）领域。
- en: The chapter also outlined the life cycle of a GenAI project, which includes
    business objectives and KPIs, foundational model selection, model training, evaluation,
    and deployment. Each stage is critical, with continuous iterations based on performance
    feedback.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还概述了一个 GenAI 项目的生命周期，包括商业目标和关键绩效指标（KPI）、基础模型选择、模型训练、评估和部署。每个阶段都至关重要，并且会根据性能反馈进行持续迭代。
- en: Finally, the chapter covered various use cases of GenAI across different sectors,
    including retail/e-commerce, finance, healthcare, and legal, that can leverage
    GenAI for summarization, recommendation, and personalization. This exploration
    underscores the versatile and transformative potential of GenAI in augmenting
    human creativity and transforming our day-to-day lives. In the next chapter, we
    will introduce the concepts of containers, K8s, and cover how K8s can manage the
    deployment, scaling, and operations of containerized workloads. We will also cover
    the specific advantages of using K8s in GenAI projects and what makes it attractive
    for GenAI applications.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章讨论了 GenAI 在不同行业中的各种应用案例，包括零售/电子商务、金融、医疗保健和法律行业，这些行业可以利用 GenAI 进行摘要、推荐和个性化。本次探索强调了
    GenAI 在增强人类创造力和转变我们日常生活中的多样性和变革潜力。在下一章中，我们将介绍容器、K8s 的概念，并讨论 K8s 如何管理容器化工作负载的部署、扩展和运维。我们还将探讨在
    GenAI 项目中使用 K8s 的具体优势，以及它为何对 GenAI 应用具有吸引力。
- en: Appendix 1A – RNNs
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 1A – 循环神经网络（RNN）
- en: In this section, we will provide a basic overview of how RNNs work, including
    a mathematical explanation of their functionality. RNNs handle sequential data
    by maintaining a hidden state (or memory cell) that can capture information from
    previous time steps. The following is a very simplistic and mathematical representation
    of an RNN.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将提供 RNN 工作原理的基本概述，包括其功能的数学解释。RNN 通过保持一个隐藏状态（或记忆单元）来处理序列数据，能够捕捉来自前一个时间步的信息。以下是
    RNN 的一个非常简单的数学表示。
- en: '![Figure 1.7 – Simple representation of an RNN](img/B31108_01_07.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – RNN 的简单表示](img/B31108_01_07.jpg)'
- en: Figure 1.7 – Simple representation of an RNN
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – RNN 的简单表示
- en: 'In this figure, **ht** represents the hidden state at a given time step **t**
    and can be presented as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，**ht** 表示给定时间步 **t** 的隐藏状态，可以表示为：
- en: '**ht = f(wh * ht-1 +wx *****Xt )**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**ht = f(wh * ht-1 + wx ***** Xt )**'
- en: Where **wh** is the weight for the hidden stage and **ht-1** is the output of
    this hidden stage at step **t-1**. **Xt** is the input, **wx** is the weight of
    the input stage, and **f** is the activation function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，**wh** 是隐藏阶段的权重，**ht-1** 是在时间步 **t-1** 上此隐藏阶段的输出。**Xt** 是输入，**wx** 是输入阶段的权重，**f**
    是激活函数。
- en: '**Output Yt = wy * ht +** **by**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出 Yt = wy * ht +** **by**'
- en: In RNNs, the output at time **t** depends upon the hidden stage, which includes
    the weighted output of the prior steps, such as **ht-1**, **ht-2**, and so on.
    This architecture can process inputs of any length, and the model size does not
    increase with the size of the input. However, this implementation is sequential
    in nature and can’t be accelerated beyond a point through parallel processing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RNN 中，时间 **t** 的输出依赖于隐藏状态，其中包括先前步骤的加权输出，如 **ht-1**、**ht-2** 等等。该架构可以处理任意长度的输入，并且随着输入的增加，模型大小不会增加。然而，这种实现是顺序的，且通过并行处理无法进一步加速。
- en: 'There are four different types of RNN topologies:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种不同类型的 RNN 拓扑结构：
- en: '**Sequence-to-sequence RNN**: For this RNN topology, both input and output
    are a sequence, such as stock market analysis'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列到序列 RNN**：对于这种 RNN 拓扑结构，输入和输出都是序列，例如股票市场分析。'
- en: '**Sequence-to-vector RNN**: This involves examples such as sentiment analysis
    by analyzing a statement or text'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列到向量 RNN**：这包括通过分析语句或文本进行情感分析等示例。'
- en: '**Vector-to-sequence analysis**: This includes practical scenarios, such as
    creating a caption from an image'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量到序列分析**：这包括实际场景，例如从图像生成说明文字。'
- en: '**Encoder-to-decoder**: This can be used for machine translation from one language
    to another'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器到解码器**：这可以用于将一种语言翻译成另一种语言。'
- en: Significant advancements in RNNs came with the introduction of LSTM networks.
    LSTM networks addressed the problem of vanishing and exploding gradients in standard
    RNNs, making it possible to learn long-range dependencies in sequences more effectively.
    LSTM cells maintain separate short-term and long-term states. GRU is another optimization
    over LSTM and gives better training performance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的显著进展源于 LSTM 网络的引入。LSTM 网络解决了标准 RNN 中梯度消失和爆炸的问题，使得学习序列中的长距离依赖关系变得更加有效。LSTM
    单元保持独立的短期和长期状态。GRU 是对 LSTM 的另一种优化，并且在训练性能上表现更好。
- en: Appendix 1B – Transformer mathematical models for the self-attention mechanism
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 1B – Transformer 自注意力机制的数学模型
- en: 'In this section, we will provide a basic overview of how the transformer model
    works, including a mathematical explanation of its functionality. We discussed
    the concepts of queues, keys, and values as part of transformer analysis earlier
    in this chapter. For a given attention head **i**, the following are the query,
    key, and value vectors:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供 Transformer 模型如何工作的基本概述，包括其功能的数学解释。我们在本章早些时候讨论了队列、键和值的概念，作为 Transformer
    分析的一部分。对于给定的注意力头 **i**，以下是查询、键和值向量：
- en: '**Q=** **X* Wi**Q'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q=** **X* Wi**Q'
- en: '**K=** **X* Wi**K'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**K=** **X* Wi**K'
- en: '**V=** **X* Wi**V'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**V=** **X* Wi**V'
- en: Where WiQ, WiK, and WiV are the weight vectors for the attention head **i**
    for the query, key, and values. These weights are the parameters that we optimize
    as we train the model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，WiQ、WiK 和 WiV 是注意力头 **i** 对查询、键和值的权重向量。这些权重是我们在训练模型时优化的参数。
- en: 'To understand the computational complexity of these calculations, let’s look
    over the dimensionality of these vectors:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些计算的计算复杂度，我们来看看这些向量的维度：
- en: X= [n, dmodel], where *n* is the number of tokens in the input sequence and
    dmodel is the dimensionality of the multi-dimensional space.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X= [n, dmodel]，其中 *n* 是输入序列中标记的数量，dmodel 是多维空间的维度。
- en: '**Weight vectors**: WiQ Wik Wiv = [dmodel ,dk ], where dk =dmodel / # of attention
    heads'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重向量**：WiQ Wik Wiv = [dmodel ,dk]，其中 dk = dmodel / 注意力头的数量'
- en: In the *Attention Is All You Need* paper, dmodel was 512 and the number of attention
    heads was 8, so dk =512/8 =64.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Attention Is All You Need* 论文中，dmodel 是 512，注意力头的数量是 8，因此 dk = 512/8 = 64。
- en: During a forward pass, each of the Q, K, and V vector calculations would require
    ~262,144 multiplications (8*64*512) and 261,632 additions (8*64*511). During training,
    model goes through multiple forward and backward passes for each head across multiple
    sets of training data. This explains the complexity and compute resource requirements
    for these transformer models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播过程中，每个 Q、K 和 V 向量的计算大约需要 262,144 次乘法运算（8*64*512）和 261,632 次加法运算（8*64*511）。在训练过程中，模型会对每个头部在多组训练数据上进行多次前向和反向传播。这解释了这些
    Transformer 模型的复杂性和计算资源需求。
- en: 'For each attention head, the attention score is calculated with the following
    equation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个注意力头，注意力得分是通过以下公式计算的：
- en: '![](img/B31108_01_08.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31108_01_08.jpg)'
- en: 'After computing the attention outputs for each head, they are concatenated
    to create multi-head attention, where Wo is the weight matrix for the output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个头部的注意力输出后，它们会被连接起来形成多头注意力，其中 Wo 是输出的权重矩阵：
- en: '![](img/B31108_01_09.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31108_01_09.jpg)'
- en: This final output is then used in subsequent layers of the transformer model
    to perform various tasks, such as translation, text generation, or classification.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出会在后续的 Transformer 模型层中被使用，用于执行各种任务，如翻译、文本生成或分类。
- en: Understanding the temperature parameter for GenAI use cases
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 GenAI 用例中的温度参数
- en: 'In the *softmax* function used as a part of the attention score, **temperature**
    refers to a parameter that controls the smoothness or sharpness of the probability
    distribution produced by the softmax function. Adjusting the temperature can influence
    how confident the model is about its prediction of the output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在作为注意力得分一部分的*softmax*函数中，**温度**指的是一个控制softmax函数生成的概率分布平滑度或尖锐度的参数。调整温度可以影响模型对输出预测的自信度：
- en: '![](img/B31108_01_10.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31108_01_10.jpg)'
- en: Lower temperature settings make the softmax distribution sharper (more confident
    predictions), while higher temperatures make the distribution smoother (less confident
    predictions) and could lead to more creative responses.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的温度设置使得 softmax 分布更加尖锐（预测更有信心），而较高的温度则使分布更平滑（预测信心较低），并可能导致更具创意的响应。
- en: The following is a sample response from ChatGPT for two temperature settings
    with the input prompt of `What is the purpose` `of Life?`
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 ChatGPT 在两种温度设置下的示例响应，输入提示为 `生命的目的是什么？`
- en: '**Temperature =** **2.0**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**温度 =** **2.0**：'
- en: '[PRE1]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Temperature =** **0.75**:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**温度 =** **0.75**：'
- en: '[PRE2]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
