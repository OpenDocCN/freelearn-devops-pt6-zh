- en: '*Chapter 8*: Building a Complete ML Project Using the Platform'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 8 章*：使用平台构建完整的机器学习项目'
- en: Until now, you have seen a few components of the platform and how it works.
    You will start this chapter by understanding the platform at a macro level. The
    holistic view will help you see how the components weave a complete solution for
    your **machine learning** (**ML**) needs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了平台的一些组件及其工作原理。本章将从宏观层面理解平台，全面的视角将帮助你看到这些组件如何为你的**机器学习**（**ML**）需求编织出完整的解决方案。
- en: In the later part of this chapter, you will see how you can start an ML project
    by using a simple example and how the teams and platform will help achieve your
    required goal.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，你将看到如何通过一个简单的示例启动机器学习项目，以及团队和平台如何帮助实现你的目标。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: Reviewing the complete picture of the ML platform
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审视机器学习平台的完整图景
- en: Understanding the business problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解业务问题
- en: Data collection, processing, and cleaning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集、处理和清洗
- en: Performing exploratory data analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行探索性数据分析
- en: Understanding feature engineering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解特征工程
- en: Building and evaluating the ML model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和评估机器学习模型
- en: Reproducibility
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可复现性
- en: Reviewing the complete picture of the ML platform
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审视机器学习平台的完整图景
- en: 'In the preceding chapters, you have built a complete ML platform on top of
    Kubernetes. You have installed, configured, and explored the different components
    of the platform. Before you start using the platform, let''s take a step back
    and look at the platform you have built from the tooling perspective. *Figure
    8.1* shows the complete logical architecture of the platform:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你已经在 Kubernetes 上构建了一个完整的机器学习平台。你安装、配置并探索了平台的不同组件。在开始使用平台之前，让我们退后一步，从工具的角度来看你构建的平台。*图
    8.1* 展示了平台的完整逻辑架构：
- en: '![Figure 8.1 – Logical platform architecture'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 逻辑平台架构'
- en: '](img/B18332_08_001.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_08_001.jpg)'
- en: Figure 8.1 – Logical platform architecture
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 逻辑平台架构
- en: The diagram in *Figure 8.1* also shows the interaction of each platform component.
    The entire platform runs inside Kubernetes and is managed entirely by the `Kfdef`
    file. It is also important to note that the ODH operator allows you to add or
    remove tools or swap one tool for another. For example, you could use Argo CD
    for model deployments instead of Airflow. Keycloak is also not part of the ODH
    project. However, the components must be secured by a single sign-on mechanism,
    and Keycloak is one of the best open source tools that can be used to add a single
    sign-on capability to the platform.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.1* 中的图示也展示了每个平台组件的交互。整个平台运行在 Kubernetes 内，并完全由 `Kfdef` 文件进行管理。还需要注意的是，ODH
    操作符允许你添加或删除工具，或将一种工具替换为另一种工具。例如，你可以使用 Argo CD 来进行模型部署，而不是使用 Airflow。Keycloak 也不属于
    ODH 项目的一部分。然而，组件必须通过单点登录机制进行保护，而 Keycloak 是一种非常适合为平台添加单点登录功能的开源工具。'
- en: Starting at the top of the diagram, you can see that end users interact with
    Jupyter notebooks, and the Spark, Airflow, and MLflow UIs. You have seen and experienced
    these interactions in the preceding chapters. The deployed ML model can then be
    used for inferencing by applications through REST API calls.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表的顶部开始，你可以看到最终用户与 Jupyter notebooks、Spark、Airflow 和 MLflow 的用户界面进行交互。你在前面的章节中已经见识并体验了这些交互。部署的机器学习模型随后可以通过
    REST API 调用供应用进行推理使用。
- en: In the middle of the diagram, you can see the interactions between the components
    and the kind of interactions they perform with each other. Jupyter servers and
    Airflow jobs can submit Spark applications to the managed Spark clusters. Airflow
    interacts with the MLflow model registry, while Jupyter notebooks can interact
    with MLflow to record experiment runs. Airflow also creates Seldon deployment
    objects that the Seldon controller then converts into running Pods with ML models
    exposed as REST services. There is no limit to how one component can interact
    with other platform components.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的中间，你可以看到各个组件之间的交互以及它们之间执行的交互类型。Jupyter 服务器和 Airflow 作业可以向托管的 Spark 集群提交
    Spark 应用程序。Airflow 与 MLflow 模型注册表进行交互，而 Jupyter notebooks 可以与 MLflow 进行交互，记录实验运行。Airflow
    还会创建 Seldon 部署对象，Seldon 控制器将其转换为运行的 Pods，并将机器学习模型暴露为 REST 服务。一个组件与其他平台组件的交互方式没有限制。
- en: At the bottom of the diagram, the ODH operator manages and operates the platform
    components. The ODH operator handles the installation and updates of these components.
    Spark, JupyterHub, and the Seldon controller are also Kubernetes operators that
    manage instances of Spark clusters, Jupyter notebook servers, and Seldon deployments,
    respectively.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的底部，ODH 操作员管理和操作平台组件。ODH 操作员负责这些组件的安装和更新。Spark、JupyterHub 和 Seldon 控制器也是
    Kubernetes 操作员，分别管理 Spark 集群、Jupyter notebook 服务器和 Seldon 部署实例。
- en: Lastly, the ODH operator also manages the Prometheus and Grafana instances.
    Prometheus is used to collect metrics from each of the components, including the
    statistics of Seldon deployments. Grafana can then visualize those metrics and
    can be configured to raise alerts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，ODH 操作员还管理 Prometheus 和 Grafana 实例。Prometheus 用于收集每个组件的指标，包括 Seldon 部署的统计数据。Grafana
    可以可视化这些指标，并可以配置为触发警报。
- en: The ODH project is still evolving. There may be changes as to what components
    will be included or excluded in the project in the future. Some of the officially
    supported components may get replaced with another over time. Therefore, it is
    important to understand the architecture and how the ODH operator works so that
    you keep it up to date.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ODH 项目仍在发展中。未来可能会有一些组件的增减变化。一些官方支持的组件可能会随着时间的推移被其他组件替代。因此，理解架构和 ODH 操作员的工作原理是非常重要的，这样你才能保持它的最新状态。
- en: In the next sections, we will take a step back and understand ML projects a
    bit more, starting with identifying opportunities where an ML solution fits. You
    will be taken through a scenario that will lead to the creation of a complete
    ML project.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将退后一步，深入了解机器学习项目，从识别适合机器学习解决方案的机会开始。你将会跟随一个场景，逐步创建一个完整的机器学习项目。
- en: Understanding the business problem
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解业务问题
- en: As with any software project, the first thing is to agree on the business problem
    you are trying to solve. We have chosen a fictitious scenario for this book to
    keep it simple while focusing on the process. You can apply the same approach
    to more complex projects.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 和任何软件项目一样，首先要做的是明确你试图解决的业务问题。我们为本书选择了一个虚构的场景，以便简化内容，集中关注过程。你也可以将这种方法应用于更复杂的项目。
- en: Let's assume that you work for an airline booking company as a lead data analyst.
    The business team of your company has reported that lots of customers complain
    about flights being delayed. It is causing the company to have bad customer experiences,
    and the phone staff spend lots of time explaining the details to customers. The
    business is looking at you to provide a solution to identify which airlines and
    which flights and times have a lower probability of delays so that the website
    can prioritize those airlines and, therefore, customers end up with fewer delays.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一家航空公司预订公司工作，担任首席数据分析师。公司业务团队报告称，许多客户抱怨航班延误，导致公司客户体验差，电话客服花费大量时间向客户解释细节。业务方希望你提供一个解决方案，识别哪些航空公司、航班和时段的延误概率较低，以便网站可以优先展示这些航空公司，从而减少客户的延误。
- en: Let's take a breather here and analyze how we can solve this problem. Do we
    need ML here? If we take the historical data and place the airlines into two buckets
    of *delayed* and *on time*, with each bucket placing the airlines into the right
    category, this attribute can then be used while the customer searches for airlines
    with better on-time performance. A team of data analysts will analyze the data
    and assign the ratings. Job done!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍作休息，分析一下如何解决这个问题。我们需要在这里使用机器学习吗？如果我们拿历史数据，将航空公司分为 *延误* 和 *准时* 两类，并将每家航空公司放入正确的类别，那么在客户搜索航空公司时，这个属性就可以作为参考，帮助他们找到准时性更好的航空公司。一组数据分析师将分析数据并给出评级。工作完成！
- en: While exploring this set, the business has mentioned that one bucket per airline
    may not provide the granularity that the solution requires. They would like to
    assess the performance, not at the airline level, but using other factors such
    as origin and destination airport, and time of day. So, airline A, with flights
    from Sydney to Melbourne, may go into the *on time* bucket, while the same airline
    may go into the *delayed* bucket when flying from Tokyo to Osaka. This suddenly
    expands the scope of the problem. If you need to analyze data at this granularity,
    it will take a lot of time to process and assign the correct category, and you
    may need to analyze this data very frequently.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索这组数据时，业务方提到每个航空公司一个桶的数据可能无法提供解决方案所需的粒度。他们希望从航司级别之外的其他因素进行性能评估，例如起点和终点机场，以及一天中的时间。因此，航空公司A的悉尼至墨尔本航班可能会进入*准时*桶，而同一航空公司从东京飞往大阪的航班则可能进入*延误*桶。这一下就扩大了问题的范围。如果你需要在这种粒度上分析数据，处理和分配正确的类别会花费大量时间，而且你可能需要频繁地分析这些数据。
- en: Have you started to think about how you can automate this? The business then
    mentions that the weather plays a vital role in this problem, and the forecast
    data from the weather bureau will need to be fetched and preprocessed to perform
    the analysis. You realize that performing this job with human teams will be slow
    and complicated and does not provide the solution that the business is looking
    for. You then mention to the business that you will need to investigate the existing
    data, which can be used to predict the correct category for a particular flight.
    You and the business agree that the aim is to predict the flight delay 10 days
    before the scheduled time with at least 75% accuracy, to improve the customer
    experience. You will also discuss the response time requirements for the model
    and understand how the model will be used in the overall business process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否已经开始考虑如何自动化这个过程？业务方接着提到天气在这个问题中起着至关重要的作用，气象局的预报数据需要被提取并预处理，以进行分析。你意识到，如果依赖人工团队来完成这个任务，会非常缓慢且复杂，且无法提供业务方所需的解决方案。于是你向业务方提到，你需要调查现有的数据，这些数据可以用来预测某个航班的正确类别。你和业务方一致认为，目标是在航班计划时间前10天预测延误，并且至少达到75%的准确率，以改善客户体验。你还将讨论模型的响应时间要求，并了解该模型如何在整体业务流程中使用。
- en: You have just defined the success criteria of this project. You have conveyed
    to the business that your team will analyze available data to assess its suitability
    for the project and then plan for the next steps. You have asked the business
    to associate a **subject matter expert** (**SME**) who can assist in data exploration
    at this stage.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚定义了这个项目的成功标准。你已将信息传达给业务方，说明你的团队将分析可用数据以评估其是否适用于项目，然后再规划下一步的工作。你已要求业务方指派一位**主题专家**(**SME**)来协助此阶段的数据探索。
- en: To summarize, you have outlined the business objectives and the scope of the
    project. You have also defined the evaluation criteria through which the success
    of the project would be measured. It is critical that you keep a note of the business
    value through each stage of the ML life cycle.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，你已经概述了业务目标和项目范围。你还定义了评估标准，用以衡量项目的成功。至关重要的是，你需要在机器学习生命周期的每个阶段都记录业务价值。
- en: Once you have defined the criteria, the next step is to start looking at the
    available data. For this use case, the data is available at [https://www.kaggle.com/usdot/flight-delays?select=flights.csv](https://www.kaggle.com/usdot/flight-delays?select=flights.csv).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了评估标准，下一步就是开始查看可用的数据。对于此用例，数据可以在[https://www.kaggle.com/usdot/flight-delays?select=flights.csv](https://www.kaggle.com/usdot/flight-delays?select=flights.csv)找到。
- en: Data collection, processing, and cleaning
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集、处理和清洗
- en: In this stage, you will begin with gathering raw data from the identified sources.
    You will write data pipelines to prepare and clean the raw data for analysis.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你将开始从已识别的数据源收集原始数据。你将编写数据管道，以准备和清理原始数据，供分析使用。
- en: Understanding data sources, location, and the format
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据来源、位置和格式
- en: You have started working with the SME to access a subset of the flight data.
    You will understand the data format and the integration process required to access
    this data. The data could be in CSV format, or it may be available in some **relational
    database management system** (**RDBMS**). It is vital to understand how this data
    would be available for your project and how this data is being maintained eventually.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你已开始与领域专家合作，访问一部分航班数据。你将了解数据格式以及访问这些数据所需的集成过程。数据可能是 CSV 格式，或者它可能存储在某些**关系型数据库管理系统**（**RDBMS**）中。了解这些数据如何为你的项目所用，以及这些数据最终是如何被维护的至关重要。
- en: Start this process by identifying what data is easily available. The SME has
    mentioned that the flight records data that covered the flight information, the
    scheduled and actual departure times, and the scheduled and actual arrival times
    is readily available. This information is available in the object store of your
    organization. This could be a good starting point.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从识别易于获取的数据开始。领域专家提到，包含航班信息、计划和实际出发时间、计划和实际到达时间的航班记录数据可以轻松获得。这些信息存储在你们组织的对象存储中。这可以作为一个良好的起点。
- en: Understanding data processing and cleaning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据处理与清理
- en: The data collected from the raw data sources may have many problems. The collected
    data may have duplication, missing values, and/or invalid records. For example,
    you may find that a column of the `string` type may have numerical data in it.
    You will then work with the SME to find out ways to handle the anomalies.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据源收集的数据可能存在许多问题。收集的数据可能存在重复、缺失值和/或无效记录。例如，你可能会发现某一列的`string`类型数据中竟然包含了数字。你将与领域专家合作，找出处理这些异常数据的方法。
- en: How would you handle the missing data? Choose an estimated value of the missing
    data from the existing set. Or you may decide to drop the column altogether if
    there are many missing values and you can not find any way to impute the missing
    values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你将如何处理缺失数据？从现有的数据集中选择一个估算值来填充缺失数据。或者，如果缺失值很多且你无法找到任何方式来填补这些缺失值，你可能决定完全删除该列。
- en: Implement data validation checks to make sure that the cleaned dataset has consistency
    and that the data quality problems described here are properly handled. Imagine
    that the age column has a value of `250`. Although we would all like to live this
    long or beyond, clearly this data is not valid. During this stage, you will find
    the discrepancy in the data and work out how to handle it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实现数据验证检查，确保清理后的数据集具有一致性，并且适当处理此处描述的数据质量问题。假设年龄列的值为`250`。虽然我们都希望能活这么长或者更长，但显然这个数据是无效的。在这个阶段，你将发现数据中的不一致性，并思考如何处理它。
- en: You may find that the flight arrival and departure times are in the local time
    zones, and you may choose to add a new column with the times represented in UTC
    format for easier comparisons.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现航班的到达和出发时间是以当地时区表示的，你可能选择新增一个列，将这些时间转换为 UTC 格式，以便于进行比较。
- en: Data cleaning can happen in both the data engineering stage and the model development
    stage. Data anomalies that are related to the domain or business logic may be
    found and handled in the data engineering stage, while data augmentation and data
    encoding are done at the model development stage. This is because it is the data
    scientist or the ML engineer who knows best what data formats the model training
    requires, while the data engineers work closer to the business domain experts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理可以发生在数据工程阶段和模型开发阶段。与领域或业务逻辑相关的数据异常可以在数据工程阶段发现并处理，而数据增强和数据编码则是在模型开发阶段完成的。这是因为数据科学家或机器学习工程师最了解模型训练所需的数据格式，而数据工程师则与业务领域专家的合作更为紧密。
- en: 'One way to implement such data validation in the data engineering phase is
    through Apache Spark. Spark has a set of built-in functions that you can use for
    data cleaning. The following code shows an example of how to filter out invalid
    rows or rows that contain malformed data while reading from a data source:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实现数据验证的一种方法是在数据工程阶段通过 Apache Spark。Spark 提供了一组内置函数，可以用于数据清理。以下代码示例展示了如何在从数据源读取数据时，过滤掉无效行或包含格式错误数据的行：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Another example is the `fillna()` function. It is used to replace null values
    with any other values. The following example shows how to replace all null values
    in the data frame with zeros:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个示例是 `fillna()` 函数。它用于用其他值替换空值。以下示例展示了如何用零替换数据框中的所有空值：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: On the model development side, there are several techniques to perform the same
    operations using pandas to manipulate data frames. You will see this in action
    in the following chapters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发方面，有几种技术可以使用pandas来执行相同的操作，以处理数据框架。你将在接下来的章节中看到这些技术的实际应用。
- en: Once you have executed the data cleaning pipeline and created an intermediary
    dataset that can be used for the next stage, the next step is to see whether the
    available data helps you in achieving the business goal.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你执行了数据清理流程并创建了一个可以用于下一阶段的中间数据集，接下来的步骤是看看现有数据是否有助于实现业务目标。
- en: Performing exploratory data analysis
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行探索性数据分析
- en: At this stage, you analyze the data to assess its suitability for the given
    problem. Data analysis is essential for building ML models. Before you create
    an ML model, you need to understand the context of the data. Analyzing vast amounts
    of company data and converting it into a useful result is extremely difficult,
    and there is no single answer on how to do it. Figuring out what data is meaningful
    and what data is vital for business is the foundation for your ML model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，你需要分析数据，以评估其是否适合给定的问题。数据分析对于构建机器学习模型至关重要。在创建机器学习模型之前，你需要理解数据的背景。分析大量的公司数据并将其转化为有用的结果是极其困难的，而且没有单一的方法来实现这一点。找出哪些数据是有意义的，哪些数据对业务至关重要，是构建机器学习模型的基础。
- en: This is a preliminary analysis, and it does not guarantee that the model will
    bring the expected results. However, it provides an opportunity to understand
    the data at a higher level and pivot if required.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项初步分析，不能保证模型会带来预期的结果。然而，它提供了一个机会，可以在更高层次上理解数据，并在需要时进行调整。
- en: Understanding sample data
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解样本数据
- en: When you get a set of data, you first try to understand it by merely looking
    at it. You then go through the business problem and try to determine what set
    of patterns would be helpful for the given situation. A lot of the time, you will
    need to collaborate with SMEs who have relevant domain knowledge.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当你获得一组数据时，你首先通过简单地查看数据来理解它。接着，你会深入了解业务问题，尝试确定哪些模式对当前情况有帮助。很多时候，你将需要与具有相关领域知识的专家进行合作。
- en: At this stage, you may choose to convert the data into a tabular form to better
    understand it. Classify the columns according to the data values. Understand each
    variable in the dataset and find out whether the values are continuous, or whether
    it represents a category. You will then summarize the columns using descriptive
    statistics to understand the values your columns contain. These statistics could
    be mean or median or anything that helps you understand the data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，你可以选择将数据转换为表格形式，以便更好地理解它。根据数据值对列进行分类。理解数据集中的每个变量，并找出这些值是连续的，还是表示某一类别。然后，你将使用描述性统计来总结列，以了解列中包含的值。这些统计数据可以是均值、中央値，或任何有助于你理解数据的指标。
- en: Understand the **data variance**. For example, your data has only 5% records
    of delayed flights and the remaining flights are on time. Would this dataset be
    good for your desired outcomes? You need to get a better dataset that represents
    a more balanced distribution. You may choose to downsample the dataset, if it
    is highly imbalanced, by reducing the examples from the majority class.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 了解**数据方差**。例如，你的数据中只有5%的航班记录是延误的，剩下的航班都是准时的。这个数据集对于你期望的结果会有帮助吗？你需要获取一个更好的数据集，代表更平衡的分布。如果数据集严重不平衡，你可以选择通过减少多数类别的样本来对数据集进行下采样。
- en: Humans are good at visualizing data so, to better understand the data, you will
    need to visualize your columns using charts. There is a series of different charts
    that can help you visualize your data. The platform we present here will assist
    you in writing code to visualize the data using popular libraries such as Matplotlib
    or Seaborn. Before you choose to visualize your data using a chart, think about
    what kind of information you are expected to get from the chart and how it can
    assist you in understanding the data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 人类擅长可视化数据，因此，为了更好地理解数据，你需要使用图表将列可视化。有一系列不同的图表可以帮助你可视化数据。我们在这里展示的平台将帮助你编写代码，使用流行的库，如Matplotlib或Seaborn，来可视化数据。在你选择使用图表可视化数据之前，思考一下你希望从图表中获得什么信息，以及它如何帮助你理解数据。
- en: As an example, we define three basic charts and their characteristics given
    in the following subsections.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们定义了以下小节中给出的三种基本图表及其特性。
- en: Box plots
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 箱型图
- en: 'A box plot ([https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review))
    is an excellent way to visualize and understand data variance. Box plots show
    results in quartiles, each containing 25% of the values in the dataset; the values
    are plotted to show how the data is distributed. *Figure 8.2* shows a sample box
    plot. Note the black dot is an **outlier**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图（[https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review)）是一个可视化并理解数据方差的极好方法。箱线图将结果以四分位数的形式展示，每个四分位数包含数据集的25%的值；这些值会被绘制出来，显示数据的分布情况。*图
    8.2* 显示了一个示例箱线图。注意黑点是一个**离群值**：
- en: '![Figure 8.2 – Box plot'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – 箱线图'
- en: '](img/B18332_08_002.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_08_002.jpg)'
- en: Figure 8.2 – Box plot
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 箱线图
- en: The first component of the box plot is the minimum value of the dataset. Then
    there is the lower quartile, or the minimum 25% values. After that, we have the
    median value at 50% of the dataset. Then, we have the upper quartile, the maximum
    25% value. At the top, we have the maximum value based on the range of the dataset.
    Finally, we have the outliers. Outliers are the extreme data points—on either
    the high or low side—that could potentially impact the analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图的第一个组成部分是数据集的最小值。接下来是下四分位数，即最小的25%的值。之后，我们得到的是数据集的中位数，即50%的位置。接下来是上四分位数，即最大的25%的值。最上面是数据集的最大值。最后，我们有离群值。离群值是极端的数据点——无论是高端还是低端——它们可能会对分析结果产生影响。
- en: Histograms
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直方图
- en: A **histogram** represents the numerical data distribution. To create a histogram,
    you first split the range of values into intervals called **bins**. Once you have
    defined the number of bins to hold your data, the data is then put into predefined
    ranges in the appropriate bin. The histogram chart shows the distribution of the
    data as per the predefined bins. *Figure 8.3* shows a sample histogram. Note that
    the bins are on the *x* axis of the plot. The following plot shows the distribution
    in just two bins. You can see that the distribution is biased toward the first
    bin.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**直方图**表示数值数据的分布情况。要创建直方图，首先将数值范围划分为多个区间，这些区间称为**箱子（bins）**。在确定了用于存储数据的箱子数量后，数据会被放入适当的箱子中。直方图图表显示了按预定义的箱子进行分布的数据。*图
    8.3* 显示了一个示例直方图。注意箱子位于图表的*x*轴上。以下图表显示了仅在两个箱子中的分布。你可以看到，分布偏向第一个箱子。'
- en: '![Figure 8.3 – Histogram'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – 直方图'
- en: '](img/B18332_08_003.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_08_003.jpg)'
- en: Figure 8.3 – Histogram
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 直方图
- en: Density plots
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密度图
- en: 'One of the drawbacks of histograms is that they are sensitive to bin margins
    and the number of bins. The distribution shape is affected by how the bins are
    defined. A histogram may be a better fit if your data contains more discrete values
    (such as gender or postcodes). Otherwise, an alternative is to use a **density
    plot**, which is a smoother version of a histogram. *Figure 8.4* shows a sample
    density plot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图的一个缺点是它们对箱子边界和箱子数量比较敏感。箱子的定义会影响分布的形状。如果你的数据包含更多离散的值（如性别或邮政编码），直方图可能会更适合。否则，可以考虑使用**密度图**，它是直方图的平滑版本。*图
    8.4* 显示了一个示例密度图：
- en: '![Figure 8.4 – Density plot'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.4 – 密度图'
- en: '](img/B18332_08_004.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_08_004.jpg)'
- en: Figure 8.4 – Density plot
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 密度图
- en: Once you have performed the exploratory data analysis, you may choose to go
    back and collect more data from existing sources or find new sources of data.
    If you are confident during this stage that the data you have captured can help
    you achieve the business goal, then you go to the next stage, feature engineering.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了探索性数据分析，你可以选择回去从现有的资源中收集更多数据，或者寻找新的数据源。如果你在此阶段确信你收集到的数据能够帮助你实现业务目标，那么你就可以进入下一阶段——特征工程。
- en: Understanding feature engineering
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解特征工程
- en: ML is all about data. No matter how advanced our algorithm is, if the data is
    not correct or not enough, our model will not be able to perform as desired. Feature
    engineering transforms input data into features that are closely aligned with
    the model's objectives and converts data into a format that assists in model training.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的核心是数据。无论我们的算法有多先进，如果数据不准确或不足，我们的模型就无法达到预期效果。特征工程将输入数据转换为与模型目标紧密对接的特征，并将数据转化为有助于模型训练的格式。
- en: Sometimes, there is data that may not be useful for a given training problem.
    How do we make sure that the algorithm is using only the right set of information?
    What about fields that are not individually useful, but when we apply a function
    to a group of fields, the data becomes particularly useful?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，某些数据对于给定的训练问题可能无用。我们如何确保算法仅使用正确的信息集？那些单独不实用的字段如何在我们将函数应用于一组字段时变得特别有用？
- en: The act of making your data useful for the algorithm is called feature engineering.
    Most of the time, a data scientist's job is to find the right set of data for
    a given problem. Feature engineering requires knowledge of domain-specific techniques,
    and you will collaborate with business SMEs to better understand the data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使数据对算法有用的行为称为特征工程。大多数情况下，数据科学家的工作是为给定问题找到正确的数据集。特征工程需要掌握特定领域的技术，您将与业务SME合作，以更好地理解数据。
- en: Feature engineering is not only about finding the right features from existing
    data, but you may need to create new features from existing data. These features
    are known as **engineered features**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程不仅仅是从现有数据中找到正确的特征，而且您可能需要从现有数据中创建新特征。这些特征被称为**工程特征**。
- en: 'Imagine that in your flight dataset, there are fields mentioning `scheduled_departure_time`
    and `departure_time`. Both of these fields will tell you whether the flights are
    late. However, your business is looking to classify whether the flights are late.
    You and the business agree to classify the delay into three categories, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在您的航班数据集中，有提到`scheduled_departure_time`和`departure_time`的字段。这两个字段将告诉您航班是否晚点。然而，您的业务希望对航班是否晚点进行分类。您和业务部门同意将延误分类为以下三个类别：
- en: On time
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准时
- en: Short-delayed
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短延误
- en: Long-delayed
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长延误
- en: A short delay captures the flights that departed with a maximum delay of 30
    minutes. All other delayed flights are classified by the long delay value in the
    delayed column. You will need to add this column or feature to your dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 短延误捕捉最多30分钟延误起飞的航班。所有其他延误航班将根据延误列中的长延误值进行分类。您需要将此列或特征添加到数据集中。
- en: You may end up dropping a column that may not be useful for the given problem.
    Do you think the `Cancellation Reason` column may be useful for predicting the
    flight delay? If not, you may choose to drop this column.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能最终会放弃对给定问题无用的列。您认为`Cancellation Reason`列对预测航班延误有用吗？如果不是，您可以选择放弃此列。
- en: You will also represent your data that can be easily digestible by the ML algorithms.
    A lot of ML algorithms operate on numerical values; however, not all data will
    be in the numerical format. You will apply techniques such as **one-hot encoding**
    to convert the columns into a numerical format.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您还将表示您的数据，使其易于ML算法消化。许多ML算法操作的是数值值；然而，并非所有数据都以数值格式存在。您将应用诸如**独热编码**之类的技术，将列转换为数值格式。
- en: Often, the ML algorithm works well with the value range between `–1` and `1`
    because it is faster to converge and results in better training time. Even if
    you have numerical data, it could be beneficial to convert it into the range,
    and the process of doing this is called **scaling**. During this stage, you may
    write code to scale the dataset.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，ML算法在值范围在`–1`和`1`之间的情况下表现良好，因为它收敛更快，训练时间更短。即使您有数值数据，将其转换为这个范围可能也会有益，这个过程称为**缩放**。在此阶段，您可以编写代码来缩放数据集。
- en: Data augmentation
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: In some cases, you may want to create additional records in your datasets for
    a couple of reasons. One reason is when you do not have enough data to train a
    meaningful model, while another is when you deliberately want to influence the
    behavior of the model to favor one answer over the other, such as correcting **overfitting**.
    This process of creating synthetic data is called **data augmentation**.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望出于几个原因在数据集中创建额外的记录。一个原因是当您没有足够的数据来训练有意义的模型时，另一个原因是当您有意地想影响模型的行为，以支持一个答案而不是另一个答案，如纠正**过拟合**。这种创建合成数据的过程称为**数据增强**。
- en: All activities related to data collection, processing, cleaning, data analysis,
    feature engineering, and data augmentation can be done in the platform by using
    Jupyter notebooks and, potentially, Apache Spark.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与数据收集、处理、清理、数据分析、特征工程和数据增强相关的活动都可以通过使用Jupyter笔记本和可能的Apache Spark平台来完成。
- en: Once you have the datasets cleaned, analyzed, and transformed, the next stage
    is to build and train an ML model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你清理、分析并转换了数据，下一阶段就是构建和训练机器学习模型。
- en: Building and evaluating the ML model
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和评估机器学习模型
- en: Congratulations! You are now ready to train your model. You will first evaluate
    what set of algorithms will be a good fit for the given problem. Is it a regression
    or classification problem? How do you evaluate to see whether the model is achieving
    75% correct predictability as described by the business?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在准备好训练你的模型了。你将首先评估哪些算法集适合当前问题。是回归问题还是分类问题？你如何评估模型是否达到了业务描述中的75%正确预测率？
- en: Selecting evaluation criteria
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择评估标准
- en: Let's start with accuracy as the model evaluation criteria. This records how
    many times the predicted values are the same as the labels in the test dataset.
    However, if the dataset does not have the right variance, the model may guess
    the majority class for each example, which is effectively not learning anything
    about the minority class.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从准确度作为模型评估标准开始。这记录了预测值与测试数据集中标签相同的次数。然而，如果数据集的方差不够，模型可能会对每个样本猜测大多数类别，这实际上没有学习到关于少数类的任何信息。
- en: 'You decided to use the confusion matrix to see the accuracy for each class.
    Let''s say you have 1,000 records in your data, out of which 50 are labeled as
    *delayed*. So, there are 950 examples with the *on time* label. Now, if the model
    correctly predicts `920` out of 950 for *on time* and `12` out of 50 for the *delayed*
    label, the matrix will look like the table in *Figure 8.5*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定使用混淆矩阵来查看每个类别的准确度。假设你有1,000条数据记录，其中50条被标记为*延迟*，其余950条标记为*准时*。现在，如果模型正确预测了950条数据中有`920`条是*准时*，并且50条数据中有`12`条是*延迟*，那么混淆矩阵将如下所示，如*图
    8.5*所示：
- en: '![Figure 8.5 – Confusion matrix'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.5 – 混淆矩阵'
- en: '](img/B18332_08_005.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_08_005.jpg)'
- en: Figure 8.5 – Confusion matrix
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 混淆矩阵
- en: For the imbalanced dataset, it is recommended to choose the metrics such as
    **recall** and **precision** or F-score to get a full picture. In this case, the
    precision is 31% (12/38) and the recall is 24% (12/50), compared to the accuracy,
    which is 93.2% (932/1000), and which could be misleading in your scenario.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不平衡的数据集，建议选择**召回率**、**精确率**或F-score等指标，以全面了解情况。在这种情况下，精确率为31%（12/38），召回率为24%（12/50），而准确度为93.2%（932/1000），但在你的场景中，准确度可能会导致误导。
- en: Building the model
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型
- en: You will start with splitting your data into training, validation, and test
    sets. Consider a scenario where you split your data into these sets and train
    a model; let's call this *experiment 1*. Now, you want to retrain the model using
    different hyperparameters and you split the data again for this new iteration
    and train the model; let's call it *experiment 2*. Can you compare the results
    of the two experiments if the data splits across the two experiments are not consistent?
    It is critical that your data splits are repeatable to compare different runs
    of your training exercise.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你将开始将数据拆分为训练集、验证集和测试集。考虑一种情景，你将数据拆分为这些集合并训练一个模型；我们称之为*实验 1*。现在，你想使用不同的超参数重新训练模型，并再次拆分数据用于这一新的迭代并训练模型；我们称之为*实验
    2*。如果两个实验中的数据拆分不一致，你能比较两个实验的结果吗？确保数据拆分是可重复的，这对于比较训练过程中的不同运行至关重要。
- en: You will try different algorithms or an ensemble of algorithms to assess the
    performance of the data validation set and review the quality of the predictions.
    During this stage, every time you try a new adjustment to the model (for example,
    hyperparameter or different algorithms), you will measure and record the evaluation
    metrics that were set with the SME during the *Understanding the business problem*
    stage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你将尝试不同的算法或算法的集成，来评估数据验证集的表现，并检查预测的质量。在这个阶段，每次你对模型进行新的调整（例如，超参数或不同的算法）时，你都将衡量并记录与业务专家在*理解业务问题*阶段共同设定的评估指标。
- en: Most of the steps of the modeling stage are iterative. Depending on the result
    of your experiments, you might realize that the model performance is not as expected.
    In this case, you may want to go back to the previous steps of the life cycle,
    such as feature engineering. Or, you may want to redo your data analysis to make
    sure you understand the data correctly. During training, you will revisit the
    business objectives and data to find the right balance. You may decide that additional
    data points from new sources are needed to enhance the training data. It is highly
    recommended that you present the results to the business stakeholders during this
    stage. This communication will share the value of the model to the business in
    the initial stages, collect early feedback, and give the team a chance to course-correct
    if required.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 模型阶段的大多数步骤是迭代的。根据实验的结果，你可能会意识到模型的表现没有达到预期。在这种情况下，你可能想回到生命周期的前面步骤，比如特征工程。或者，你可能希望重新做一次数据分析，以确保你正确理解数据。在训练过程中，你将重新审视商业目标和数据，以找到正确的平衡。你可能会决定需要来自新源的额外数据点来增强训练数据。在这个阶段，强烈建议你将结果展示给业务相关方。这种沟通会在初期阶段向业务展示模型的价值，收集早期反馈，并为团队提供必要时调整方向的机会。
- en: The next stage is to deploy your model for inferencing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段是部署模型以进行推理。
- en: Deploying the model
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型
- en: Once you have trained your model, the next stage is to version the model in
    MLflow and deploy it into an environment where the model can be used to make predictions
    for the incoming requests. The versioning of the models will allow you to keep
    track of models and roll back to an older version if the need arises.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好了模型，下一阶段就是在MLflow中为模型版本控制，并将其部署到可以用来对传入请求进行预测的环境中。模型的版本控制将使你能够跟踪模型，并在需要时回滚到旧版本。
- en: In this book, we will use the on-line model inference approach. The model has
    been containerized using the platform's Seldon component and exposed as a REST
    API. Each call to this REST API will result in one prediction. The stateless container
    running on Kubernetes will scale hundreds of thousands of requests because of
    the inherent ability of containers to scale.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们将使用在线模型推理的方法。模型已经通过平台的Seldon组件进行了容器化，并作为REST API暴露。每次调用这个REST API都会产生一个预测。运行在Kubernetes上的无状态容器将能够处理数十万次请求，因为容器本身具备扩展能力。
- en: The other way is to serve the incoming requests in batches. Imagine a scenario
    where you have hundreds of thousands of records of labeled data, and you want
    to test that model behavior for all these records. Making individual REST API
    calls may not be the right approach in this scenario. Instead, batch inferencing
    provides an asynchronous approach to making predictions for millions of records.
    Seldon has the capability to infer batches of data, but it is out of scope for
    this book.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方式是将传入的请求按批处理。假设有数十万条标记数据记录，并且你想测试这些记录上模型的表现。在这种情况下，单独进行REST API调用可能并不是正确的做法。相反，批量推理提供了一种异步的方法，可以为数百万条记录进行预测。Seldon具备对数据批量推理的能力，但这超出了本书的范围。
- en: The REST API you expose for your flight delay prediction could be utilized by
    the web application to further enhance the customer experience.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你为航班延误预测所暴露的REST API可以被网页应用程序利用，从而进一步提升客户体验。
- en: Reproducibility
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可重现性
- en: Now, you know what an ML life cycle would look like and how the platform assists
    you in every step of your journey. As an individual, you may be able to write
    every step of the data pipelines and model training and tuning in a single notebook.
    However, this may cause a problem in teams where different people are working
    on different parts of the life cycle. Let's say someone wants to run the model
    training part but the entire process is tied up with one another. Your team may
    not be able to scale with this approach.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经知道了一个机器学习生命周期的样子，以及平台如何在你每一步旅程中提供帮助。作为个人，你可能能够在一个单一的笔记本中编写数据管道和模型训练与调优的每个步骤。然而，在团队中，可能会有人负责生命周期的不同部分，这样做可能会造成问题。假设某人想运行模型训练部分，但整个过程彼此相互关联。使用这种方法，团队可能无法扩展。
- en: A better and more scalable approach is to write different notebooks for various
    stages (such as data processing and model training) in your project life cycle
    and use a workflow engine to tie them up. Using the Kubernetes platform, all the
    stages will be executed using containers and provide a consistent environment
    for your project between different runs. The platform provides Airflow, an engine
    that could be used for creating and executing workflows.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好且更具可扩展性的方法是，为项目生命周期的各个阶段（如数据处理和模型训练）编写不同的笔记本，并使用工作流引擎将它们关联起来。使用 Kubernetes
    平台，所有阶段将通过容器执行，并为您的项目在不同运行之间提供一致的环境。该平台提供了 Airflow，这是一个可以用于创建和执行工作流的引擎。
- en: Summary
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this short chapter, we wanted to step back and show you the big picture of
    the platform and the model life cycle. We encourage you to refer to [*Chapter
    2*](B18332_02_ePub.xhtml#_idTextAnchor027), *Understanding MLOps*, where we presented
    a typical ML life cycle, for a more detailed discussion. Recall the importance
    of collaborations across multiple teams and how investing more time in understanding
    the available data will result in a model that delivers the expected business
    value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章简短的内容中，我们希望回顾并向您展示平台和模型生命周期的整体图景。我们鼓励您参考[*第二章*](B18332_02_ePub.xhtml#_idTextAnchor027)，*理解
    MLOps*，在该章中我们展示了一个典型的机器学习生命周期，进行更为详细的讨论。请回想一下跨多个团队合作的重要性，以及在理解可用数据上投入更多时间将如何导致一个能够提供预期商业价值的模型。
- en: Now you know what the various stages of your project will look like. In the
    next two chapters, you will implement the flight delay prediction service using
    the ML platform that we have presented in this book and you will perform each
    of the stages we have described in this chapter. The idea is to show you how the
    platform caters to every stage of your project and how you can implement this
    platform in your organization.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了项目的各个阶段将如何进行。在接下来的两章中，您将使用本书中展示的机器学习平台实现航班延误预测服务，并执行我们在本章中描述的每一个阶段。我们的目的是向您展示平台如何满足项目的每个阶段，以及您如何在组织中实施该平台。
