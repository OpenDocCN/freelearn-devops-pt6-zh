- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Deploying the Big Data Stack on Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上部署大数据堆栈
- en: In this chapter, we will cover the deployment of key big data technologies –
    Spark, Airflow, and Kafka – on Kubernetes. As container orchestration and management
    have become critical for running data workloads efficiently, Kubernetes has emerged
    as the de facto standard. By the end of this chapter, you will be able to successfully
    deploy and manage big data stacks on Kubernetes for building robust data pipelines
    and applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖在 Kubernetes 上部署关键的大数据技术 —— Spark、Airflow 和 Kafka。随着容器编排和管理变得对于高效运行数据工作负载至关重要，Kubernetes
    已成为事实上的标准。在本章结束时，你将能够成功在 Kubernetes 上部署和管理大数据堆栈，构建强大的数据管道和应用程序。
- en: We will start by deploying Apache Spark on Kubernetes using the Spark operator.
    You will learn how to configure and monitor Spark jobs running as Spark applications
    on your Kubernetes cluster. Being able to run Spark workloads on Kubernetes brings
    important benefits such as dynamic scaling, versioning, and unified resource management.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用 Spark 操作员在 Kubernetes 上部署 Apache Spark。你将学习如何配置和监控在 Kubernetes 集群上运行的
    Spark 应用程序。能够在 Kubernetes 上运行 Spark 工作负载带来了重要的好处，如动态扩展、版本控制和统一的资源管理。
- en: Next, we will deploy Apache Airflow on Kubernetes. You will configure Airflow
    on Kubernetes, link its logs to S3 for easier debugging and monitoring, and set
    it up to orchestrate data pipelines built using tools such as Spark. Running Airflow
    on Kubernetes improves reliability, scaling, and resource utilization.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在 Kubernetes 上部署 Apache Airflow。你将配置 Kubernetes 上的 Airflow，将其日志链接到 S3，以便于调试和监控，并设置它来编排使用
    Spark 等工具构建的数据管道。在 Kubernetes 上运行 Airflow 可以提高可靠性、扩展性和资源利用率。
- en: Finally, we will deploy Apache Kafka on Kubernetes, which is critical for streaming
    data pipelines. Running Kafka on Kubernetes simplifies operations, scaling, and
    cluster management.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在 Kubernetes 上部署 Apache Kafka，这对流数据管道至关重要。在 Kubernetes 上运行 Kafka 简化了操作、扩展和集群管理。
- en: By the end of this chapter, you will have hands-on experience with deploying
    and managing big data stacks on Kubernetes. This will enable you to build robust,
    reliable data applications leveraging Kubernetes as your container orchestration
    platform.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将拥有在 Kubernetes 上部署和管理大数据堆栈的实践经验。这将使你能够利用 Kubernetes 作为容器编排平台，构建强大、可靠的数据应用程序。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Deploying Spark on Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上部署 Spark
- en: Deploying Airflow on Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上部署 Airflow
- en: Deploying Kafka on Kubernetes
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上部署 Kafka
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the activities in this chapter, you should have an AWS account and `kubectl`,
    `eksctl`, and `helm` installed. For instructions on how to set up an AWS account
    and `kubectl` and `eksctl` installation, refer to [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015).
    For `helm` installation instructions, access [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的活动中，你应该拥有一个 AWS 账户，并已安装 `kubectl`、`eksctl` 和 `helm`。关于如何设置 AWS 账户以及安装 `kubectl`
    和 `eksctl` 的说明，请参考 [*第 1 章*](B21927_01.xhtml#_idTextAnchor015)。有关 `helm` 安装的说明，请访问
    [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)。
- en: We will also be using the Titanic dataset for our exercises. You can find the
    version we will use at [https://github.com/neylsoncrepalde/titanic_data_with_semicolon](https://github.com/neylsoncrepalde/titanic_data_with_semicolon).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用 Titanic 数据集进行练习。你可以在 [https://github.com/neylsoncrepalde/titanic_data_with_semicolon](https://github.com/neylsoncrepalde/titanic_data_with_semicolon)
    找到我们将使用的版本。
- en: All code in this chapter is available in the GitHub repository at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes),
    in the `Chapter08` folder.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码都可以在 GitHub 仓库 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)
    的 `Chapter08` 文件夹中找到。
- en: Deploying Spark on Kubernetes
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上部署 Spark
- en: To help us deploy resources on Kubernetes, we are going to use **Helm**. Helm
    is a package manager for Kubernetes that helps install applications and services.
    Helm uses templates called **Charts**, which package up installation configuration,
    default settings, dependencies, and more, into an easy-to-deploy bundle.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们在 Kubernetes 上部署资源，我们将使用**Helm**。Helm 是 Kubernetes 的包管理器，帮助安装应用程序和服务。Helm
    使用名为**Charts**的模板，将安装配置、默认设置、依赖关系等打包成一个易于部署的包。
- en: On the other hand, we have **Operators**. Operators are custom controllers that
    extend the Kubernetes API to manage applications and their components. They provide
    a declarative way to create, configure, and manage complex stateful applications
    on Kubernetes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们有**操作符**。操作符是自定义控制器，扩展了 Kubernetes API，用于管理应用程序及其组件。它们提供了一种声明性方式来创建、配置和管理
    Kubernetes 上的复杂状态应用程序。
- en: 'Some key benefits of using operators include the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用操作符的一些关键优势包括以下几点：
- en: '**Simplified application deployment and lifecycle management**: Operators abstract
    away low-level details and provide high-level abstractions for deploying applications
    without needing to understand the intricacies of Kubernetes'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化的应用程序部署和生命周期管理**：操作符抽象了底层细节，为应用程序部署提供了高层次的抽象，而无需了解 Kubernetes 的复杂性。'
- en: '**Integration with monitoring tools**: Operators expose custom metrics and
    logs, enabling integration with monitoring stacks such as Prometheus and Grafana'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与监控工具的集成**：操作符暴露自定义指标和日志，支持与 Prometheus 和 Grafana 等监控堆栈的集成。'
- en: '**Kubernetes native**: Operators leverage Kubernetes’ extensibility and are
    written specifically for Kubernetes, allowing them to be cloud-agnostic.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 原生**：操作符利用 Kubernetes 的可扩展性，并专门为 Kubernetes 编写，使其能够保持云无关性。'
- en: Operators extend Kubernetes by creating **Custom Resource Definitions** (**CRDs**)
    and controllers. A CRD allows you to define a new resource type in Kubernetes.
    For example, the SparkOperator defines a SparkApplication resource.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符通过创建**自定义资源定义**（**CRDs**）和控制器来扩展 Kubernetes。CRD 允许你在 Kubernetes 中定义一个新的资源类型。例如，SparkOperator
    定义了一个 SparkApplication 资源。
- en: The operator then creates a controller that watches for these custom resources
    and performs actions based on the resource `spec`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符接着创建一个控制器，监视这些自定义资源并根据资源的`spec`执行操作。
- en: 'For example, when a SparkApplication resource is created, the SparkOperator
    controller will do the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当创建 SparkApplication 资源时，SparkOperator 控制器会执行以下操作：
- en: Create the driver and executor Pods based on the spec
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据规范创建驱动程序和执行器 Pods
- en: Mount storage volumes
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挂载存储卷
- en: Monitor the status of the application
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控应用程序的状态
- en: Perform logging and monitoring
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行日志记录和监控
- en: 'Now, let’s get to it:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始吧：
- en: 'To start, let’s create an AWS EKS cluster using `eksctl`:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用 `eksctl` 创建一个 AWS EKS 集群：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Remember that this line of code takes several minutes to complete. Now, there
    are some important configurations to give our Kubernetes cluster permission to
    create volumes on our behalf. For this, we need to install the AWS EBS CSI driver.
    This is not required for deploying Spark applications, but it will be very important
    for Airflow deployment.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，这行代码需要几分钟才能完成。现在，我们需要进行一些重要的配置，以便允许我们的 Kubernetes 集群代表我们创建卷。为此，我们需要安装 AWS
    EBS CSI 驱动程序。这对于部署 Spark 应用程序不是必须的，但对于 Airflow 部署来说非常重要。
- en: 'First, we need to associate the IAM OIDC provider with the EKS cluster, which
    allows IAM roles and users to authenticate to the Kubernetes API. To do that,
    in the terminal, type the following:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将 IAM OIDC 提供者与 EKS 集群关联，这样 IAM 角色和用户就能通过 Kubernetes API 进行身份验证。为此，请在终端中输入以下命令：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will create an IAM service account called `ebs-csi-controller-sa`
    in the `kube-system` namespace, with the specified IAM role and policy attached.
    This service account will be used by the EBS CSI driver:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在 `kube-system` 命名空间中创建一个名为 `ebs-csi-controller-sa` 的 IAM 服务账户，并附加指定的
    IAM 角色和策略。这个服务账户将由 EBS CSI 驱动程序使用：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we will enable the EBS CSI driver in the cluster and link it to the
    service account and role created earlier. Remember to change `<YOUR_ACCOUNT_NUMBER>`
    for the real value:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将在集群中启用 EBS CSI 驱动程序，并将其链接到之前创建的服务账户和角色。记得将 `<YOUR_ACCOUNT_NUMBER>` 替换为真实值：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let’s start the actual Spark operator deployment. We will create a namespace
    to organize our resources next:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始实际的 Spark 操作符部署。接下来我们将创建一个命名空间来组织我们的资源：
- en: '[PRE4]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will use the SparkOperator Helm chart available online to deploy the
    operator:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用网上提供的 SparkOperator Helm chart 来部署操作符：
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Check whether the operator was correctly deployed:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查操作符是否正确部署：
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should see output like this:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到类似这样的输出：
- en: '[PRE7]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we need to register our AWS credentials as a Kubernetes Secret to make
    them available for Spark. This will allow our Spark applications to access resources
    in AWS:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将 AWS 凭证注册为 Kubernetes Secret，以使其可供 Spark 使用。这将允许我们的 Spark 应用程序访问 AWS
    中的资源：
- en: '[PRE8]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, it’s time to develop our Spark code. By now, you should have the Titanic
    dataset stored on Amazon S3\. At [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py),
    you will find simple code that reads the Titanic dataset from the S3 bucket and
    writes it into another bucket (this second S3 bucket must have been created previously
    – you can do it in the AWS console).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，到了开发我们的Spark代码的时候了。到目前为止，你应该已经将泰坦尼克号数据集存储在Amazon S3上。在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.py)上，你可以找到读取S3桶中的泰坦尼克号数据集并将其写入另一个桶的简单代码（这个第二个S3桶必须是事先创建的——你可以在AWS控制台中创建）。
- en: 'Save this file as `spark_job.py` and upload it to a different S3 bucket. This
    is where the SparkOperator is going to look for the code to run the application.
    Note that this PySpark code is slightly different from what we saw earlier, in
    [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092). Here, we are setting Spark configurations
    separately from the Spark session. We will go through those configurations in
    detail:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此文件保存为`spark_job.py`并上传到另一个S3桶中。这是SparkOperator将要查找代码以运行应用程序的地方。请注意，这段PySpark代码与我们之前在[*第5章*](B21927_05.xhtml#_idTextAnchor092)中看到的有所不同。在这里，我们将Spark配置与Spark会话分开设置。我们将详细讨论这些配置：
- en: '`.set("spark.cores.max", "2")`: This limits the maximum number of cores this
    Spark application will use to two. This prevents the overallocation of resources.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.set("spark.cores.max", "2")`：这限制了此Spark应用程序最多使用两个核心。这可以防止资源的过度分配。'
- en: '`.set("spark.executor.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")`
    and `.set("spark.driver.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")`:
    These enable support for reading and writing to S3 using Signature Version 4 authentication,
    which is more secure.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.set("spark.executor.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")`
    和 `.set("spark.driver.extraJavaOptions", "-Dcom.amazonaws.services.s3.enableV4=true")`：这些配置启用了使用签名版本4认证对S3进行读写操作的支持，这种认证方式更为安全。'
- en: '`.set("spark.hadoop.fs.s3a.fast.upload", True)`: This property enables the
    fast upload feature of the S3A connector which improves performance when saving
    data to S3.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.set("spark.hadoop.fs.s3a.fast.upload", True)`：此属性启用了S3A连接器的快速上传功能，这可以提高将数据保存到S3时的性能。'
- en: '`.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")`:
    This configuration sets the S3 FileSystem implementation to use the newer, optimized
    `s3a` instead of the older `s3` connector.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")`：此配置将S3文件系统的实现设置为使用更新的、优化的`s3a`，而不是旧版的`s3`连接器。'
- en: '`.set("spark.hadoop.fs.s3a.aws.crendentials.provider", "com.amazonaws.auth.EnvironmentVariablesCredentials")`:
    This configures Spark to obtain AWS credentials from the environment variables,
    rather than needing to specify them directly in code.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.set("spark.hadoop.fs.s3a.aws.crendentials.provider", "com.amazonaws.auth.EnvironmentVariablesCredentials")`：这将Spark配置为从环境变量中获取AWS凭证，而无需直接在代码中指定。'
- en: '`.set("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.3")`: This adds
    a dependency on the Hadoop AWS module so Spark can access S3.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.set("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.3")`：这会添加对Hadoop
    AWS模块的依赖，以便Spark能够访问S3。'
- en: Also note that, by default, Spark uses log-level `INFO`. In this code, we set
    it to `WARN` to reduce logging and improve logs’ readability. Remember to change
    `<YOUR_BUCKET>` for your own S3 buckets.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还需要注意的是，默认情况下，Spark使用`INFO`日志级别。在此代码中，我们将其设置为`WARN`，以减少日志记录并提高日志的可读性。记得将`<YOUR_BUCKET>`替换为你自己的S3桶。
- en: After uploading this code to S3, it’s time to create a YAML file with the SparkApplication
    definitions. The content for the code is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传此代码到S3后，接下来就是创建一个包含SparkApplication定义的YAML文件。代码的内容可以在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/spark/spark_job.yaml)找到。
- en: The code defines a new SparkApplication resource. This is only possible because
    the SparkOperator created the SparkApplication custom resource. Let’s take a closer
    look at what this YAML definition is doing.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码定义了一个新的SparkApplication资源。这只有在SparkOperator创建了SparkApplication自定义资源后才有可能。让我们仔细看看这个YAML定义在做什么。
- en: The first block of the YAML file specifies the apiVersion and the kind of resource
    as Spark application. It also sets a name and namespace for the application.
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: YAML文件的第一个块指定了apiVersion和资源类型为Spark应用程序。它还为应用程序设置了名称和命名空间。
- en: The second block defines a volume mount called “ivy” that will be used to cache
    dependencies and avoid fetching them for each job run. It mounts to `/tmp` in
    the driver and executors.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个块定义了一个名为“ivy”的卷挂载，用于缓存依赖项，以避免每次作业运行时重新获取它们。它挂载到驱动程序和执行器的`/tmp`目录。
- en: The third block configures Spark properties, enabling the Ivy cache directory
    and setting the resource allocation batch size for Kubernetes.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个块配置了Spark属性，启用了Ivy缓存目录，并设置了Kubernetes的资源分配批量大小。
- en: The fourth block configures Hadoop properties to use the S3A file system implementation.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四个块配置了Hadoop属性，以使用S3A文件系统实现。
- en: The fifth block sets this Spark application as a Python one, the Python version
    to use, running in cluster mode, and the Docker image to use – in this case, a
    previously prepared Spark image that integrates with AWS and Kafka. It also defines
    that the image will always be pulled from Docker Hub, even if it is already present
    in the cluster.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五个块将此Spark应用程序设置为Python类型，指定要使用的Python版本，运行模式为集群模式，并设置要使用的Docker镜像——在此情况下是一个之前准备好的与AWS和Kafka集成的Spark镜像。它还定义了即使该镜像已经存在于集群中，也会始终从Docker
    Hub拉取镜像。
- en: The sixth block specifies the main Python application file location in S3 and
    the Spark version – in this case, 3.1.1.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第六个块指定了S3中主Python应用程序文件的位置以及Spark版本——在这种情况下是3.1.1。
- en: The seventh block sets `restartPolicy` to `Never`, so the application runs only
    once.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第七个块将`restartPolicy`设置为`Never`，这样应用程序只会运行一次。
- en: The remaining blocks set configuration for the driver and executor Pods. Here,
    we set up AWS access key secrets for accessing S3, we request one core and 1 GB
    memory for the driver and the same resources for the executors, we mount an `emptyDir`
    volume called “ivy” for caching dependencies, and we set Spark and driver Pod
    labels for tracking.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其余的块设置了驱动程序和执行器Pod的配置。在这里，我们设置了用于访问S3的AWS访问密钥密文，要求为驱动程序和执行器分别分配1个核心和1GB内存，并为它们提供相同的资源，我们挂载了一个名为“ivy”的`emptyDir`卷用于缓存依赖项，并设置了Spark和驱动程序Pod标签用于跟踪。
- en: 'Once this file is saved on your computer and you already have the `.py` file
    on S3, it’s time to run the Spark Application. In the terminal, type the following:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦这个文件保存在你的计算机上，并且你已经有了存储在S3中的`.py`文件，就可以运行Spark应用程序了。在终端中，输入以下内容：
- en: '[PRE9]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can get a few more details on the application using the following:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式获取应用程序的更多细节：
- en: '[PRE10]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To see the logs of our Spark Application, type this:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要查看Spark应用程序的日志，请输入以下命令：
- en: '[PRE11]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And that’s it! You just ran your first Spark application on Kubernetes! Kubernetes
    won’t let you deploy another job with the same name, so, to test again, you should
    delete the application:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你刚刚在Kubernetes上运行了你的第一个Spark应用程序！Kubernetes不会让你用相同的名称部署另一个作业，因此，要重新测试，你应该删除该应用程序：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, let’s see how to deploy Airflow on Kubernetes using the official Helm chart.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用官方Helm图表在Kubernetes上部署Airflow。
- en: Deploying Airflow on Kubernetes
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署Airflow
- en: Airflow deployment on Kubernetes is very straightforward. Nevertheless, there
    are some important details in the Helm chart configuration that we need to pay
    attention to.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署Airflow非常简单。然而，在Helm图表配置中有一些重要的细节，我们需要注意。
- en: 'First, we will download the most recent Helm chart to our local environment:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将下载最新的Helm图表到本地环境：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we need to configure a `custom_values.yaml` file to change the default
    deployment configurations for the chart. An example of this YAML file is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml).
    We will not go through the entire file but just the most important configurations
    that are needed for this deployment:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要配置一个`custom_values.yaml`文件，以更改图表的默认部署配置。此YAML文件的示例可以在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/custom_values.yaml)找到。我们将不逐行查看整个文件，而是只关注部署所需的最重要配置：
- en: In the `defaultAirflowTag` and `airflowVersion` parameters, make sure to set
    `2.8.3`. This is the latest Airflow version available for the 1.13.1 Helm chart
    version.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`defaultAirflowTag`和`airflowVersion`参数中，确保设置为`2.8.3`。这是1.13.1 Helm图表版本可用的最新Airflow版本。
- en: The `executor` parameter should be set to `KubernetesExecutor`. This guarantees
    that Airflow will use Kubernetes infrastructure to launch tasks dynamically.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`executor` 参数应设置为`KubernetesExecutor`。这样可以确保Airflow使用Kubernetes基础设施来动态启动任务。'
- en: 'In the `env` section, we will configure “remote logging” to allow Airflow to
    save logs in S3\. This is best practice for auditing and saving Kubernetes storage
    resources. Here, we configure three environment variables for Airflow. The first
    one sets remote logging to `"True"`; the second one defines in which S3 bucket
    and folder Airflow will write logs, and the last one defines a “connection” that
    Airflow will use to authenticate in AWS. We will have to set this in the Airflow
    UI later. This is an example of what this block should look like:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`env`部分，我们将配置“远程日志记录”以允许Airflow将日志保存到S3中。这是审计和节省Kubernetes存储资源的最佳实践。在这里，我们为Airflow配置了三个环境变量。第一个设置远程日志记录为`"True"`；第二个定义Airflow将日志写入的S3桶和文件夹；最后一个定义Airflow将在AWS中进行身份验证时使用的“连接”。稍后我们将在Airflow
    UI中设置此项。这是这个块应该是什么样的例子：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the webserver block, we must configure the first user credentials and the
    type of service. The `service` parameter should be set to “LoadBalancer” so we
    can access the Airflow UI from a browser. The `defaultUser` block should look
    like this:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在webserver块中，我们必须配置第一个用户凭据和服务类型。`service` 参数应该设置为“LoadBalancer”，这样我们就可以通过浏览器访问Airflow的UI界面。`defaultUser`
    块应该如下所示：
- en: '[PRE15]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It is important to have a simple password in the values file and change it in
    the UI as soon as the deployment is ready. This way, your credentials do not get
    stored in plain text. This would be a major security incident.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在值文件中设置一个简单的密码，并在部署完成后尽快在UI中更改它，这一点非常重要。这样，你的凭据就不会以明文形式存储。这将避免发生重大安全事件。
- en: The `redis.enabled` parameter should be set to `false`. As we are using the
    Kubernetes executor, Airlfow will not need Redis to manage tasks. If we don’t
    set this parameter to `false`, Helm will deploy a Redis Pod anyway.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`redis.enabled` 参数应设置为`false`。因为我们使用的是Kubernetes执行器，所以Airflow不需要Redis来管理任务。如果我们没有将此参数设置为`false`，Helm仍然会部署一个Redis
    Pod。'
- en: 'Lastly, in the `dags` block, we will configure `gitSync`. This is the easiest
    way to send our DAG files to Airflow and keep them updated in GitHub (or any other
    Git repository you prefer). First, you should create a GitHub repository with
    a folder named `"dags"` to store Python DAG files. Then, you should configure
    the `gitSync` block as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在`dags`块中，我们将配置`gitSync`。这是将我们的DAG文件发送到Airflow并保持它们在GitHub（或你喜欢的任何其他Git仓库）中更新的最简单方法。首先，你应该创建一个名为“dags”的GitHub仓库，用来存储Python
    DAG文件。然后，你应该按如下方式配置`gitSync`块：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that we omitted several comments in the original file for readability.
    The `custom_values.yaml` file is ready for deployment. We can now proceed with
    this command in the terminal:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们在原始文件中省略了几个注释以提高可读性。`custom_values.yaml` 文件已准备好部署。现在我们可以在终端中执行以下命令：
- en: '[PRE17]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This deployment can take a few minutes to finish because Airflow will do a database
    migration job before making the UI available.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署可能需要几分钟才能完成，因为Airflow在使UI可用之前会执行数据库迁移任务。
- en: 'Next, we need to get the URL for the UI’s LoadBalancer. In the terminal, type
    this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要获取UI的LoadBalancer的URL。在终端中，输入以下命令：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the columns `EXTERNAL-IP`, you will notice one `not empty` value for the
    `airflow-webserver` service. Copy this URL and paste it into your browser, adding
    `":8080"` to access Airflow’s correct port.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在`EXTERNAL-IP`列中，你会注意到`airflow-webserver`服务有一个`非空`值。复制这个URL并粘贴到浏览器中，添加`:8080`以访问Airflow的正确端口。
- en: After logging in to the UI, click on `aws_conn` (as we stated in the values
    file), choose **Amazon Web Services** for the connection type, and enter your
    access key ID and the secret access key. (At this point, you should have your
    AWS credentials stored locally – if you don’t, in the AWS console, go to IAM and
    generate new credentials for your user. You will not be able to see the old credentials
    onscreen.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到UI后，点击`aws_conn`（正如我们在值文件中所述），选择**Amazon Web Services**作为连接类型，并输入你的访问密钥ID和密钥访问密钥。（此时，你应该已经将AWS凭据存储在本地——如果没有，在AWS控制台中，进入IAM并为你的用户生成新的凭据。你将无法在屏幕上看到旧的凭据。）
- en: Finally, we will use DAG code adapted from [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092)
    that will run smoothly on Kubernetes. This DAG will download the Titanic dataset
    automatically from the internet, perform simple calculations, and print the results,
    which will be accessed on the “logs” page. The content for the code is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用从 [*第 5 章*](B21927_05.xhtml#_idTextAnchor092) 中改编的 DAG 代码，该代码将在 Kubernetes
    上顺利运行。此 DAG 将自动从互联网下载 Titanic 数据集，执行简单的计算，并打印结果，这些结果将在“日志”页面上查看。代码内容可以通过以下链接访问：[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter08/airflow/dags/titanic_dag.py)。
- en: Upload a copy of this Python code to your GitHub repository and, in a few seconds,
    it will show on the Airflow UI. Now, activate your DAG (click the `switch` button)
    and follow the execution (*Figure 8**.1*).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将此 Python 代码的副本上传到你的 GitHub 仓库，并在几秒钟内它将出现在 Airflow UI 中。现在，激活你的 DAG（点击 `switch`
    按钮）并跟踪执行 (*图 8.1*)。
- en: '![Figure 8.1 – DAG successfully executed](img/B21927_08.1.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – DAG 成功执行](img/B21927_08.1.jpg)'
- en: Figure 8.1 – DAG successfully executed
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – DAG 成功执行
- en: Then click on any task to select it and click on **Logs**. You will see Airflow
    logs being read directly from S3 (*Figure 8**.2*).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后点击任何任务以选择它，点击 **日志**。你将看到 Airflow 日志直接从 S3 读取 (*图 8.2*)。
- en: '![Figure 8.2 – Airflow logs in S3](img/B21927_08_02.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – S3 中的 Airflow 日志](img/B21927_08_02.jpg)'
- en: Figure 8.2 – Airflow logs in S3
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – S3 中的 Airflow 日志
- en: Congratulations! You now have Airflow up and running on Kubernetes. In [*Chapter
    10*](B21927_10.xhtml#_idTextAnchor154), we will put all these pieces together
    to build a fully automated data pipeline.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你已经在 Kubernetes 上成功启动并运行 Airflow。在 [*第 10 章*](B21927_10.xhtml#_idTextAnchor154)
    中，我们将把所有这些部分组合在一起，构建一个完全自动化的数据管道。
- en: Next, we will deploy a Kafka cluster on Kubernetes using the Strimzi operator.
    Let’s get to it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在 Kubernetes 上使用 Strimzi 操作符部署一个 Kafka 集群。让我们开始吧。
- en: Deploying Kafka on Kubernetes
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上部署 Kafka
- en: 'Strimzi is an open source operator that facilitates the deployment and management
    of Kafka clusters on Kubernetes, creating new CRDs. It is developed and maintained
    by the Strimzi project, which is part of the **Cloud Native Computing Foundation**
    (**CNCF**). The Strimzi operator provides a declarative approach to managing Kafka
    clusters on Kubernetes. Instead of manually creating and configuring Kafka components,
    you define the desired state of your Kafka cluster using Kubernetes custom resources.
    The operator then takes care of deploying and managing the Kafka components according
    to the specified configuration:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Strimzi 是一个开源操作符，简化了在 Kubernetes 上部署和管理 Kafka 集群的过程，通过创建新的 CRD。它由 Strimzi 项目开发和维护，Strimzi
    项目是 **云原生计算基金会** (**CNCF**) 的一部分。Strimzi 操作符提供了一种声明式的方法来管理 Kubernetes 上的 Kafka
    集群。用户不再需要手动创建和配置 Kafka 组件，而是通过 Kubernetes 自定义资源定义所需的 Kafka 集群状态。然后，操作符会根据指定的配置自动部署和管理
    Kafka 组件：
- en: 'To deploy Strimzi in Kubernetes, first, we need to install its Helm chart:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 中部署 Strimzi，首先，我们需要安装其 Helm 图表：
- en: '[PRE19]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we install the operator with the following command:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下命令安装操作符：
- en: '[PRE20]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can check whether the deployment was successful with the following:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式检查部署是否成功：
- en: '[PRE21]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, it’s time to configure the deployment of our Kafka cluster. Here is a
    YAML file to configure a Kafka cluster using the new CRDs. Let’s break it into
    pieces for better understanding:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候配置我们 Kafka 集群的部署了。以下是一个 YAML 文件，用于使用新的 CRD 配置 Kafka 集群。为了更好地理解，我们将它分解成几个部分：
- en: '**kafka_jbod.yaml**'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kafka_jbod.yaml**'
- en: '[PRE22]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The first part of the code specifies the API version and the kind of resource
    being defined. In this case, it’s a Kafka resource managed by the Strimzi operator.
    Then, we define metadata for the Kafka resource, specifically its name, which
    is set to `kafka-cluster`. The next block specifies the configuration for the
    Kafka brokers. We’re setting the Kafka version to 3.7.0 and specifying that we
    want three replicas (Kafka broker instances) in the cluster:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码的第一部分指定了 API 版本和正在定义的资源类型。在本例中，这是一个由 Strimzi 操作符管理的 Kafka 资源。然后，我们为 Kafka
    资源定义了元数据，特别是其名称，设置为`kafka-cluster`。接下来的代码块指定了 Kafka 经纪人的配置。我们将 Kafka 版本设置为 3.7.0，并指定希望集群中有三个副本（Kafka
    经纪人实例）：
- en: '[PRE23]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we define the listeners for the Kafka brokers. We’re configuring three
    listeners:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们定义 Kafka 经纪人的监听器。我们将配置三个监听器：
- en: '`plain`: An internal listener on port `9092` without TLS encryption'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plain`: 一个没有 TLS 加密的内部监听器，端口为 `9092`'
- en: '`tls`: An internal listener on port `9093` with TLS encryption enabled'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tls`: 一个启用了 TLS 加密的内部监听器，端口为 `9093`'
- en: '`external`: An external listener on port `9094` exposed as a LoadBalancer service,
    without TLS encryption'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`external`: 一个暴露为 LoadBalancer 服务的外部监听器，端口为 `9094`，不使用 TLS 加密'
- en: '[PRE24]'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The next block configures the readiness and liveness probes for the Kafka brokers.
    The readiness probe checks whether the broker is ready to accept traffic, while
    the liveness probe checks whether the broker is still running. The `initialDelaySeconds`
    parameter specifies the number of seconds to wait before performing the first
    probe, and `timeoutSeconds` specifies the number of seconds after which the probe
    is considered failed:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一个块配置了 Kafka brokers 的就绪和存活探针。就绪探针检查 broker 是否准备好接受流量，而存活探针检查 broker 是否仍在运行。`initialDelaySeconds`
    参数指定了在执行第一次探针之前等待的秒数，而 `timeoutSeconds` 参数指定了探针被认为失败后的秒数：
- en: '[PRE25]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This `kafka.config` block specifies various configuration options for the Kafka
    brokers, such as the default replication factor, the number of partitions for
    new topics, the replication factor for the offsets and transaction state log topics,
    the log message format version, and the log retention period (in hours). The default
    log retention for Kafka is 7 days (168 hours), but we can change this parameter
    according to our needs. It is important to remember that longer retention periods
    imply more disk storage usage, so be careful:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个 `kafka.config` 块指定了 Kafka brokers 的各种配置选项，例如默认的复制因子、新主题的分区数量、偏移量和事务状态日志主题的复制因子、日志消息格式版本，以及日志保留期（以小时为单位）。Kafka
    默认的日志保留期是 7 天（168 小时），但我们可以根据需要修改这个参数。需要记住的是，更长的保留期意味着更多的磁盘存储使用，因此要小心：
- en: '[PRE26]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `kafka.storage` block configures the storage for the Kafka brokers. We’re
    using the `deleteClaim` set to `false` to prevent the persistent volume claims
    from being deleted when the Kafka cluster is deleted:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`kafka.storage` 块配置了 Kafka brokers 的存储。我们使用 `deleteClaim` 设置为 `false`，以防止在删除
    Kafka 集群时删除持久化存储卷声明：'
- en: '[PRE27]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, the `kafka.resources` block specifies the resource requests and limits
    for the Kafka brokers. We’re requesting 512 MiB of memory and 500 millicpu, and
    setting the memory limit to 1 GiB and the CPU limit to 1 `cpu`:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，`kafka.resources` 块指定了 Kafka brokers 的资源请求和限制。我们请求 512 MiB 的内存和 500 毫 CPU，并将内存限制设置为
    1 GiB，CPU 限制设置为 1 `cpu`：
- en: '[PRE28]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Finally, we have a `zookeeper` block that configures the ZooKeeper ensemble
    used by the Kafka cluster. We’re specifying three replicas for ZooKeeper, using
    a 10 GiB persistent volume claim for storage, and setting resource requests and
    limits similar to the Kafka brokers.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们有一个 `zookeeper` 块，它配置了 Kafka 集群使用的 ZooKeeper 集群。我们为 ZooKeeper 指定了三个副本，使用
    10 GiB 的持久化存储卷声明，并设置了类似于 Kafka brokers 的资源请求和限制。
- en: 'Once the configuration file is ready on your machine, type the following command
    to deploy the cluster to Kubernetes:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置文件准备好后，在你的机器上输入以下命令来将集群部署到 Kubernetes：
- en: '[PRE29]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This yields the following output:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE30]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, check the Pods:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，检查 Pods：
- en: '[PRE31]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output shows the three Kafka brokers and the ZooKeeper instances:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了三个 Kafka broker 和 ZooKeeper 实例：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Congrats! You have a fully operational Kafka cluster running on Kubernetes.
    Now, the next step is to deploy a Kafka Connect cluster and make everything ready
    for a real-time data pipeline. We will not do that right now for cloud cost efficiency,
    but we will get back to this configuration in [*Chapter 10*](B21927_10.xhtml#_idTextAnchor154).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经在 Kubernetes 上成功运行了一个完全操作的 Kafka 集群。接下来的步骤是部署一个 Kafka Connect 集群，并为实时数据管道做好一切准备。由于云端成本效率问题，我们暂时不会进行此操作，但我们将在
    [*第 10 章*](B21927_10.xhtml#_idTextAnchor154) 中回到这个配置。
- en: Summary
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to deploy and manage key big data technologies
    such as Apache Spark, Apache Airflow, and Apache Kafka on Kubernetes. Deploying
    these tools on Kubernetes provides several benefits, including simplified operations,
    better resource utilization, scaling, high availability, and unified cluster management.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何在 Kubernetes 上部署和管理 Apache Spark、Apache Airflow 和 Apache Kafka 等关键大数据技术。将这些工具部署到
    Kubernetes 上提供了多个好处，包括简化操作、更好的资源利用、扩展性、高可用性和统一的集群管理。
- en: You started by deploying the Spark operator on Kubernetes and running a Spark
    application to process data from Amazon S3\. This allows you to leverage Kubernetes
    for running Spark jobs in a cloud-native way, taking advantage of dynamic resource
    allocation and scaling.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先在 Kubernetes 上部署了 Spark 操作符，并运行了一个 Spark 应用程序来处理来自 Amazon S3 的数据。这使你能够利用
    Kubernetes 以云原生的方式运行 Spark 作业，利用动态资源分配和扩展的优势。
- en: Next, you deployed Apache Airflow on Kubernetes using the official Helm chart.
    You configured Airflow to run with the Kubernetes executor, enabling it to dynamically
    launch tasks on Kubernetes. You also set up remote logging to Amazon S3 for easier
    monitoring and debugging. Running Airflow on Kubernetes improves reliability,
    scalability, and resource utilization for orchestrating data pipelines.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你使用官方 Helm 图表在 Kubernetes 上部署了 Apache Airflow。你配置了 Airflow 使用 Kubernetes
    执行器运行，从而使其能够动态地在 Kubernetes 上启动任务。你还设置了远程日志记录到 Amazon S3，以便于监控和调试。在 Kubernetes
    上运行 Airflow 提高了可靠性、可扩展性和资源利用率，从而更好地协调数据管道。
- en: Finally, you deployed Apache Kafka on Kubernetes using the Strimzi operator.
    You configured a Kafka cluster with brokers, a ZooKeeper ensemble, persistent
    storage volumes, and internal/external listeners. Deploying Kafka on Kubernetes
    simplifies operations, scaling, high availability, and cluster management for
    building streaming data pipelines.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你使用 Strimzi 操作符在 Kubernetes 上部署了 Apache Kafka。你配置了一个包含代理、ZooKeeper 集群、持久化存储卷以及内部/外部监听器的
    Kafka 集群。在 Kubernetes 上部署 Kafka 简化了操作、扩展、高可用性和集群管理，从而帮助构建流数据管道。
- en: Overall, you now have hands-on experience with deploying and managing the key
    components of a big data stack on Kubernetes. This will enable you to build robust,
    scalable data applications and pipelines leveraging the power of container orchestration
    with Kubernetes. The skills learned in this chapter are crucial for running big
    data workloads efficiently in cloud-native environments.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，你现在已经具备了在 Kubernetes 上部署和管理大数据栈关键组件的实战经验。这将使你能够构建可靠、可扩展的数据应用和管道，利用 Kubernetes
    容器编排的强大功能。 本章中学习的技能对于在云原生环境中高效运行大数据工作负载至关重要。
- en: In the next chapter, we will see how to build a data consumption layer on top
    of Kubernetes and how to connect those layers with tools to visualize and use
    data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到如何在 Kubernetes 上构建数据消费层，并了解如何通过工具连接这些层来可视化和使用数据。
