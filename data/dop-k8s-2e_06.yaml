- en: Kubernetes Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: In [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started
    with Kubernetes*, we learned how to deploy containers with different resources
    and also looked at how to use volumes to persist data, dynamic provisioning, different
    storage classes, and advanced administration in Kubernetes. In this chapter, we'll
    learn how Kubernetes routes traffic to make all of this possible. Networking always
    plays an important role in the software world. We'll learn about Kubernetes networking
    step by step, looking at the communication between containers on a single host,
    multiple hosts, and inside a cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 3 章](a5cf080a-372a-406e-bb48-019af313c676.xhtml)，*Kubernetes 入门*中，我们学习了如何部署具有不同资源的容器，还了解了如何使用卷来持久化数据、动态配置、不同的存储类和
    Kubernetes 中的高级管理。在本章中，我们将学习 Kubernetes 如何路由流量以实现这一切。网络在软件世界中始终扮演着重要角色。我们将一步一步地了解
    Kubernetes 网络，探讨容器在单一主机、多主机和集群内的通信。
- en: 'The following are the topics we''ll cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Kubernetes networking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: Docker networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 网络
- en: Ingress
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口流量
- en: Network policy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略
- en: Service mesh
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务网格
- en: Kubernetes networking
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: 'There are plenty of options when it comes to implementing networking in Kubernetes.
    Kubernetes itself doesn''t care about how you implement it, but you must meet
    its three fundamental requirements:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中实现网络有很多选择。Kubernetes 本身并不关心你如何实现网络，但你必须满足其三个基本要求：
- en: All containers should be accessible to each other without NAT, regardless of
    which nodes they are on
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有容器应该能够相互访问，而无需进行 NAT，无论它们位于哪个节点上。
- en: All nodes should communicate with all containers
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有节点应该能够与所有容器通信
- en: The IP container should see itself in the same way as others see it
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP 容器应该像外部看到它一样看待自己。
- en: Before getting any further into this, we'll first examine how default container
    networking works.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解 Kubernetes 网络之前，我们首先检查一下默认容器网络是如何工作的。
- en: Docker networking
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 网络
- en: 'Let''s now review how docker networking works before getting into Kubernetes
    networking. For container networking, there are different modes: bridge, none,
    overlay, macvlan, and host. We''ve learned about the major modes in [Chapter 2](05e2d0b4-0e70-4480-b5a0-f3860ddb24f2.xhtml), *DevOps
    with Containers. *Bridge is the default networking model. Docker creates and attaches
    a virtual Ethernet device (also known as `veth`) and assigns a network namespace
    to each container.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下 Docker 网络是如何工作的，再进入 Kubernetes 网络。对于容器网络，有不同的模式：bridge、none、overlay、macvlan
    和 host。我们在[第 2 章](05e2d0b4-0e70-4480-b5a0-f3860ddb24f2.xhtml)，*DevOps 与容器*中了解了主要的网络模式。Bridge
    是默认的网络模式。Docker 会创建并附加一个虚拟以太网设备（也叫做 `veth`），并为每个容器分配一个网络命名空间。
- en: The **network namespace** is a feature in Linux that is logically another copy
    of a network stack. It has its own routing tables, ARP tables, and network devices.
    This is a fundamental concept of container networking.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络命名空间** 是 Linux 中的一个特性，它逻辑上是另一个网络栈的副本。它有自己的路由表、ARP 表和网络设备。这是容器网络的基本概念。'
- en: '`Veth` always comes in pairs; one is in the network namespace and the other
    is in the bridge. When the traffic comes into the host network, it will be routed
    into the bridge. The packet will be dispatched to its `veth`, and will go into
    the namespace inside the container, as shown in the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`Veth` 总是成对出现；一个在网络命名空间中，另一个在桥接中。当流量进入主机网络时，它会被路由到桥接中。数据包会被转发到它的 `veth`，然后进入容器内部的命名空间，如下图所示：'
- en: '![](img/35ce4055-4fd3-410d-b57e-e795da30a0c9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35ce4055-4fd3-410d-b57e-e795da30a0c9.png)'
- en: 'Let''s take a closer look at this. In the following example, we''ll use a minikube
    node as the docker host. First, we''ll have to use `minikube ssh` to ssh into
    the node because we''re not using Kubernetes yet. After we get into the minikube
    node, we''ll launch a container to interact with us:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这个问题。在以下示例中，我们将使用 minikube 节点作为 docker 主机。首先，我们必须使用 `minikube ssh` 登录到节点，因为我们还没有使用
    Kubernetes。进入 minikube 节点后，我们将启动一个容器来与我们互动：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s see the implementation of outbound traffic within a container. `docker
    exec <container_name or container_id>` can run a command in a running container.
    Let''s use `ip link list` to list all the interfaces:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下容器内的出站流量实现。`docker exec <container_name 或 container_id>` 可以在运行中的容器中执行命令。我们使用
    `ip link list` 来列出所有的网络接口：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can see that we have three interfaces inside the `busybox` container. One
    has an ID of `53` with the name `eth0@if54`. The number after `if` is the other
    interface ID in the pair. In this case, the pair ID is `54`. If we run the same
    command on the host, we can see that the `veth` in the host is pointing to `eth0`
    inside the container:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在 `busybox` 容器内有三个接口。其中一个接口的 ID 为 `53`，名称为 `eth0@if54`。`if` 后面的数字是配对接口的
    ID，在这个例子中，配对 ID 为 `54`。如果我们在主机上运行相同的命令，可以看到主机上的 `veth` 正指向容器内的 `eth0`：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have a `veth` on the host named `vethfeec36a@if53`. This pairs with `eth0@if54`
    in the container network namespace. `veth` 54 is attached to the `docker0` bridge,
    and eventually accesses the internet via `eth0`. If we take a look at the `iptables`
    rules, we can find a masquerading rule (also known as SNAT) on the host that docker
    creates for outbound traffic, which will make internet access available for containers:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在主机上有一个名为 `vethfeec36a@if53` 的 `veth`。它与容器网络命名空间中的 `eth0@if54` 配对。`veth` 54
    被附加到 `docker0` 桥接网络，最终通过 `eth0` 访问互联网。如果我们查看 `iptables` 规则，可以找到 docker 为出站流量创建的伪装规则（也称为
    SNAT），该规则将使容器能够访问互联网：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'On the other hand, as regards the inbound traffic, docker creates a custom
    filter chain on prerouting and dynamically creates forwarding rules in the `DOCKER`
    filter chain. If we expose a container port, `8080`, and map it to a host port, `8000`,
    we can see that we''re listening to port `8000` on any IP address (`0.0.0.0/0`),
    which will then be routed to container port `8080`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，关于入站流量，docker 在 prerouting 上创建了一个自定义过滤链，并在 `DOCKER` 过滤链中动态创建转发规则。如果我们暴露一个容器端口
    `8080`，并将其映射到主机端口 `8000`，我们可以看到我们正在监听任何 IP 地址（`0.0.0.0/0`）上的端口 `8000`，该流量将被路由到容器端口
    `8080`：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we know how a packet goes in/out of containers, let's look at how containers
    in a pod communicate with each other.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了数据包是如何进出容器的，接下来我们来看看 Pod 内的容器如何相互通信。
- en: 'User-defined custom bridges:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用户自定义的自定义桥接网络：
- en: As well as the default bridge network, docker also supports user-defined bridges.
    Users can create the custom bridge on the fly. This provides better network isolation,
    supports DNS resolution through embedded DNS server, and can be attached and detached
    from the container at runtime. For more information, please refer to the following
    documentation: [https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge](https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge).
    [](https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了默认的桥接网络外，docker 还支持用户自定义的桥接网络。用户可以动态创建自定义桥接网络。这提供了更好的网络隔离，支持通过内置 DNS 服务器进行
    DNS 解析，并且可以在运行时附加或分离容器。更多信息，请参考以下文档：[https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge](https://docs.docker.com/network/bridge/#manage-a-user-defined-bridge)。
- en: Container-to-container communications
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器间通信
- en: 'Pods in Kubernetes have their own real IP addresses. Containers within a pod
    share network namespace, so they see each other as *localhost*. This is implemented
    by the **network container** by default, which acts as a bridge to dispatch traffic
    for every container in a pod. Let''s see how this works in the following example.
    Let''s use the first example from [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml),
    *Getting Started with Kubernetes*, which includes two containers, `nginx` and
    `centos`, inside one pod:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的 Pod 拥有各自的真实 IP 地址。Pod 内的容器共享网络命名空间，因此它们相互视为 *localhost*。默认情况下，这是通过
    **网络容器** 实现的，网络容器充当每个容器的流量分发桥梁。让我们通过以下示例来看看这是如何工作的。我们使用 [第 3 章](a5cf080a-372a-406e-bb48-019af313c676.xhtml)
    中的第一个示例，*Kubernetes 入门*，其中包含一个 Pod 内的两个容器 `nginx` 和 `centos`：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we will describe the pod and look at its `Container ID`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将描述该 Pod 并查看其 `Container ID`：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this example, `web` has the container ID `d9bd923572ab`, and `centos` has
    the container ID `f4c019d289d4`. If we go into the `minikube/192.168.99.100` node
    using `docker ps`, we can check how many containers Kubernetes actually launches
    since we''re in minikube, which launches lots of other cluster containers. Check
    out the latest launch time by using the `CREATED` column, where we will find that
    there are three containers that have just been launched:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`web` 的容器 ID 是 `d9bd923572ab`，而 `centos` 的容器 ID 是 `f4c019d289d4`。如果我们进入
    `minikube/192.168.99.100` 节点并使用 `docker ps`，我们可以检查 Kubernetes 实际启动了多少个容器，因为我们处于
    minikube 环境，它启动了许多其他集群容器。通过查看 `CREATED` 列，我们可以看到有三个刚刚启动的容器：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is an additional container, `4ddd3221cc47`, that was launched. Before
    digging into which container it is, let''s check the network mode of our `web`
    container. We will find that the containers in our example pod are running in
    containers with a mapped container mode:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个额外的容器，`4ddd3221cc47`，它已被启动。在深入了解这个容器之前，让我们先查看 `web` 容器的网络模式。我们会发现，示例 Pod
    中的容器都在映射容器模式下运行：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `4ddd3221cc47` container is the so-called network container in this case.
    This holds the network namespace to let the `web` and `centos` containers join.
    Containers in the same network namespace share the same IP address and network
    configuration. This is the default implementation in Kubernetes for achieving
    container-to-container communications, which is mapped to the first requirement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`4ddd3221cc47` 容器在本例中是所谓的网络容器。它持有网络命名空间，使得 `web` 和 `centos` 容器可以连接。处于同一网络命名空间中的容器共享相同的
    IP 地址和网络配置。这是 Kubernetes 实现容器间通信的默认方式，映射到第一个要求。'
- en: Pod-to-pod communications
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 到 Pod 的通信
- en: Pod IP addresses are accessible from other pods, no matter which nodes they're
    on. This fits the second requirement. We'll describe the pods' communication within
    the same node and across nodes in the upcoming section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的 IP 地址可以从其他 Pod 访问，无论它们位于哪个节点上。这符合第二个要求。我们将在接下来的章节中描述同一节点内以及跨节点的 Pod 通信。
- en: Pod communication within the same node
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同一节点内的 Pod 通信
- en: 'Pod-to-pod communication within the same node goes through the bridge by default.
    Let''s say we have two pods that have their own network namespaces. When pod 1
    wants to talk to pod 2, the packet passes through pod 1''s namespace to the corresponding
    `veth` pair, `**vethXXXX**`, and eventually goes to the bridge. The bridge then
    broadcasts the destination IP to help the packet find its way. `vethYYYY` responds with
    the broadcasts. The packet then arrives at pod 2:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 同一节点内的 Pod 通信默认通过桥接进行。假设我们有两个 Pod，各自拥有自己的网络命名空间。当 Pod 1 想要与 Pod 2 通信时，数据包首先通过
    Pod 1 的命名空间，传递到相应的 `veth` 对 `**vethXXXX**`，最终到达桥接。然后，桥接会广播目标 IP，帮助数据包找到路径。`vethYYYY`
    响应广播。数据包随后到达 Pod 2：
- en: '![](img/766d9c4b-d680-429b-af61-4f0f3e5325f9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/766d9c4b-d680-429b-af61-4f0f3e5325f9.png)'
- en: Now that we know how the packet travels in a single node, we will move on and
    talk about how traffic gets routed when the pods are in different nodes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了数据包在单个节点中的传输方式，接下来我们将讨论当 Pods 在不同节点上时，流量是如何路由的。
- en: Pod communication across nodes
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨节点的 Pod 通信
- en: 'According to the second requirement, all nodes must communicate with all containers.
    Kubernetes delegates implementation to the **container network interface** (**CNI**).
    Users can choose different implementations, by L2, L3, or overlay. Overlay networking,
    also known as packet encapsulation, is one of the most common solutions. This
    wraps a message before leaving the source, delivers it, and unwraps the message
    at its destination. This leads to a situation in which the overlay increases the
    network latency and complexity. As long as all the containers can access each
    other across nodes, you''re free to use any technology, such as L2 adjacency or
    the L3 gateway. For more information about CNI, refer to its spec ([https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md)):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第二个要求，所有节点必须能够与所有容器通信。Kubernetes 将实现委托给**容器网络接口**（**CNI**）。用户可以选择不同的实现方式，如
    L2、L3 或覆盖网络。覆盖网络，也称为数据包封装，是最常见的解决方案之一。它在数据包离开源节点之前进行封装，传送后在目标节点解封。这样会导致覆盖网络增加网络延迟和复杂性。只要所有容器能够在不同节点之间互相访问，你可以自由选择任何技术，比如
    L2 邻接或 L3 网关。关于 CNI 的更多信息，请参考其规范（[https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md)）：
- en: '![](img/f8a72fe0-d350-4899-916f-39d07a47b72f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8a72fe0-d350-4899-916f-39d07a47b72f.png)'
- en: 'Let''s say we have a packet moving from pod 1 to pod 4\. The packet leaves
    the container interface and reaches the `veth` pair, and then passes through the
    bridge and the node''s network interface. Network implementation comes into play
    in step 4\. As long as the packet can be routed to the target node, you are free
    to use any options. In the following example, we''ll launch minikube with the
    `--network-plugin=cni` option. With CNI enabled, the parameters will be passed
    through kubelet in the node. Kubelet has a default network plugin, but you can
    probe any supported plugin when it starts up. Before starting `minikube`, you
    can use `minikube stop` first if it has been started or `minikube delete` to delete
    the whole cluster thoroughly before doing anything else. Although `minikube` is
    a single node environment that might not completely represent the production scenario
    we''ll encounter, this just gives you a basic idea of how all of this works. We
    will learn about the deployment of networking options in the real world in [Chapter
    9](acaa9855-1a87-4fd4-ad40-0955f5d12f28.xhtml), *Continuous Delivery*, and [Chapter
    10](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml), *Kubernetes on AWS*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据包从Pod 1发送到Pod 4。数据包离开容器接口，经过`veth`对，然后通过桥接器和节点的网络接口。在第4步中，网络实现开始发挥作用。只要数据包能够被路由到目标节点，你可以自由选择任何选项。在以下的例子中，我们将启动minikube并使用`--network-plugin=cni`选项。启用CNI后，参数将通过节点中的kubelet传递。Kubelet有一个默认的网络插件，但你可以在启动时探测任何支持的插件。在启动`minikube`之前，如果它已经启动，你可以先使用`minikube
    stop`停止，或者使用`minikube delete`完全删除整个集群，然后再进行其他操作。虽然`minikube`是一个单节点环境，可能无法完全代表我们将遇到的生产环境，但它可以帮助你大致了解这些工作的原理。我们将在[第9章](acaa9855-1a87-4fd4-ad40-0955f5d12f28.xhtml)《持续交付》和[第10章](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml)《AWS上的Kubernetes》中了解网络选项在实际部署中的应用：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When we specify the `network-plugin` option, `minikube` will use the directory
    specified in `--network-plugin-dir` for plugins on startup. In the CNI plugin,
    the default plugin directory is `/opt/cni/net.d`. After the cluster comes up,
    we can log into the node and look at the network interface configuration inside
    the minikube via `minikube ssh`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们指定`network-plugin`选项时，`minikube`会在启动时使用`--network-plugin-dir`中指定的目录来加载插件。在CNI插件中，默认的插件目录是`/opt/cni/net.d`。集群启动后，我们可以通过`minikube
    ssh`登录到节点，查看minikube中的网络接口配置：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will find that there is one new bridge in the node, and if we create the
    example pod again by using `5-1-1_pod.yml`, we will find that the IP address of
    the pod becomes `10.1.0.x`, which is attaching to `mybridge` instead of `docker0`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会发现节点中新增了一个桥接器，如果我们再次使用`5-1-1_pod.yml`创建示例Pod，我们会发现Pod的IP地址变成了`10.1.0.x`，并附加到`mybridge`上，而不是`docker0`：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is because we have specified that we''ll use CNI as the network plugin,
    and `docker0` will not be used (also known as the **container network model**,
    or **libnetwork**). The CNI creates a virtual interface, attaches it to the underlay
    network, sets the IP address, and eventually routes and maps it to the pods''
    namespace. Let''s take a look at the configuration that''s located at `/etc/cni/net.d/k8s.conf`
    in minikube:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们已经指定使用CNI作为网络插件，并且`docker0`将不会被使用（也叫做**容器网络模型**，或**libnetwork**）。CNI会创建一个虚拟接口，将其连接到基础网络，设置IP地址，并最终将其路由并映射到Pod的命名空间。我们来看看位于`/etc/cni/net.d/k8s.conf`的配置文件，这在minikube中：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this example, we are using the bridge CNI plugin to reuse the L2 bridge for
    pod containers. If the packet is from `10.1.0.0/16`, and its destination is to
    anywhere, it'll go through this gateway. Just like the diagram we saw earlier,
    we could have another node with CNI enabled with the `10.1.2.0/16` subnet so that
    ARP packets can go out to the physical interface on the node where the target
    pod is located. This then achieves pod-to-pod communication across nodes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了bridge CNI插件来重用L2桥接器用于Pod容器。如果数据包来自`10.1.0.0/16`，并且其目的地是任何地方，它将通过这个网关。就像我们之前看到的图示一样，我们可以有另一个启用了CNI的节点，使用`10.1.2.0/16`子网，这样ARP数据包就可以发送到目标Pod所在节点的物理接口。这样就实现了跨节点的Pod对Pod通信。
- en: 'Let''s check the rules in `iptables`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来检查一下`iptables`中的规则：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: All the related rules have been switched to `10.1.0.0/16` CIDR.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所有相关的规则已经切换为`10.1.0.0/16`的CIDR。
- en: Pod-to-service communications
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod与服务的通信
- en: Kubernetes is dynamic. Pods are created and deleted all the time. The Kubernetes
    service is an abstraction to define a set of pods by label selectors. We normally
    use the service to access pods instead of specifying a pod explicitly. When we
    create a service, an `endpoint` object will be created, which describes a set
    of pod IPs that the label selector in that service has selected.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是动态的，pod会不断创建和删除。Kubernetes服务是一种抽象，用于通过标签选择器定义一组pod。我们通常使用服务来访问pod，而不是明确指定一个pod。当我们创建服务时，会创建一个`endpoint`对象，描述该服务中标签选择器所选择的一组pod
    IP。
- en: In some cases, the `endpoint` object will not be created upon creation of the
    service. For example, services without selectors will not create a corresponding
    `endpoint` object. For more information, refer to the *Service without selectors*
    section in [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started
    with Kubernetes*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，`endpoint`对象在服务创建时不会自动创建。例如，缺少选择器的服务将不会创建相应的`endpoint`对象。有关更多信息，请参考[第3章](a5cf080a-372a-406e-bb48-019af313c676.xhtml)中的*没有选择器的服务*部分，*Kubernetes入门*。
- en: 'So, how does traffic get from pod to the pod behind the service? By default,
    Kubernetes uses `iptables` to perform this magic, and does so by using `kube-proxy`.
    This is explained in the following diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，流量是如何从pod传输到服务后面的pod的呢？默认情况下，Kubernetes使用`iptables`来实现这个过程，并通过`kube-proxy`来执行。下面的图示解释了这一点：
- en: '![](img/db7f1486-dc97-4352-8dd9-05253454ae78.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db7f1486-dc97-4352-8dd9-05253454ae78.png)'
- en: 'Let''s reuse the `3-2-3_rc1.yaml` and `3-2-3_nodeport.yaml` examples from [Chapter
    3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started with Kubernetes*,
    to observe the default behavior:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用来自[第3章](a5cf080a-372a-406e-bb48-019af313c676.xhtml)的`3-2-3_rc1.yaml`和`3-2-3_nodeport.yaml`示例，*Kubernetes入门*，来观察默认行为：
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s observe the `iptable` rules and see how this works. As you can see in
    the following code, our service IP is `10.0.0.167`. The two pods'' IP addresses
    underneath are `10.1.0.4` and `10.1.0.5`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们观察一下`iptables`规则，看看它是如何工作的。正如下面的代码所示，我们的服务IP是`10.0.0.167`，底下两个pod的IP地址分别是`10.1.0.4`和`10.1.0.5`：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s get into the minikube node by using `minikube ssh` and check its `iptables`
    rules:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`minikube ssh`进入minikube节点，并检查其`iptables`规则：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The key point here is that the service exposes the cluster IP to outside traffic
    from `KUBE-SVC-37ROJ3MK6RKFMQ2B`, which links to two custom chains, `KUBE-SEP-SVVBOHTYP7PAP3J5`
    and `KUBE-SEP-AYS7I6ZPYFC6YNNF`, with a statistic mode random probability of 0.5\.
    This means that `iptables` will generate a random number and tune it based on
    the probability distribution of 0.5 to the destination. These two custom chains
    have the `DNAT` target set to the corresponding pod IP. The `DNAT` target is responsible
    for changing the packets' destination IP address. By default, conntrack is enabled
    to track the destination and source of connection when the traffic comes in. All
    of this results in a routing behavior. When the traffic comes to the service,
    `iptables` will randomly pick one of the pods to route and modify the destination
    IP from the service IP to the real pod IP. It then gets the response and un-DNAT
    on the reply packets and sends them back to the requester.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点在于，服务将集群IP暴露给来自`KUBE-SVC-37ROJ3MK6RKFMQ2B`的外部流量，后者链接到两个自定义链，`KUBE-SEP-SVVBOHTYP7PAP3J5`和`KUBE-SEP-AYS7I6ZPYFC6YNNF`，并且其统计模式随机概率为0.5。意味着`iptables`会生成一个随机数，并根据0.5的概率分布调节该数值到目标。这两个自定义链的`DNAT`目标被设置为相应的pod
    IP。`DNAT`目标负责改变数据包的目标IP地址。默认情况下，conntrack会启用，跟踪连接的目标和源地址。当流量到达服务时，`iptables`会随机选择一个pod进行路由，并将目标IP从服务IP修改为真实的pod
    IP。然后，它接收响应，并对回复数据包进行反`DNAT`操作，发送回请求者。
- en: 'IPVS-based kube-proxy:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于IPVS的kube-proxy：
- en: In Kubernetes 1.11, the IPVS-based `kube-proxy` feature graduated to GA. This
    could deal with the scaling problem of `iptables` to tens of thousands of services.
    The **IP Virtual Server** (**IPVS**) is part of the Linux kernel, which can direct
    TCP or UDP requests to real servers. `ipvs` proxier is a good fit if your application
    contains a huge number of services. However, it will fall back on `iptables` in
    some specific cases. For more information, please refer to [https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/](https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 1.11 中，基于 IPVS 的 `kube-proxy` 功能正式发布。这可以解决 `iptables` 在处理数万个服务时的扩展问题。**IP
    虚拟服务器**（**IPVS**）是 Linux 内核的一部分，可以将 TCP 或 UDP 请求引导到实际的服务器上。如果你的应用包含大量服务，`ipvs`
    代理非常适合。然而，在某些特定情况下，它会回退到 `iptables`。有关更多信息，请参考 [https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/](https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/)。
- en: External-to-service communications
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部到服务的通信
- en: 'The ability to serve external traffic to Kubernetes is critical. Kubernetes
    provides two API objects to achieve this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将外部流量引入 Kubernetes 是至关重要的。Kubernetes 提供了两个 API 对象来实现这一目标：
- en: '**Service**: External network LoadBalancer or NodePort (L4)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Service**：外部网络 LoadBalancer 或 NodePort（L4）'
- en: '**Ingress**: HTTP(S) LoadBalancer (L7)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ingress**：HTTP(S) LoadBalancer（L7）'
- en: 'We''ll learn more about ingress in the next section. For now, we''ll focus
    on the L4 service. Based on what we''ve learned about pod-to-pod communication
    across nodes, the packet goes in and out between the service and pod. The following
    diagram is an illustration of this process. Let''s say we have two services: service
    A has three pods (pod a, pod b, and pod c) and service B gets only one pod (pod
    d). When the traffic comes in from the LoadBalancer, the packet will be dispatched
    to one of the nodes. Most of the LoadBalancer cloud itself is not aware of pods
    or containers; it only knows about the node. If the node passes the health check,
    then it will be the candidate for the destination.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中深入学习 ingress。现在，我们将重点讨论 L4 服务。基于我们对跨节点 pod 之间通信的理解，数据包在服务和 pod 之间进出。以下图示展示了这一过程。假设我们有两个服务：服务
    A 有三个 pod（pod a、pod b 和 pod c），而服务 B 只有一个 pod（pod d）。当流量从 LoadBalancer 进入时，数据包会被调度到某个节点。大多数
    LoadBalancer 本身并不了解 pod 或容器，它只知道节点。如果节点通过健康检查，那么它就会成为目标节点的候选节点。
- en: 'Let''s assume that we want to access service B; this currently only has one
    pod running on one node. However, LoadBalancer sends the packet to another node
    that doesn''t have any of our desired pods running. In this case, the traffic
    route will look like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要访问服务 B；目前它只有一个 pod 在一个节点上运行。然而，LoadBalancer 将数据包发送到另一个没有运行我们需要的 pod 的节点。在这种情况下，流量路由将如下所示：
- en: '![](img/baef9510-6fb1-4435-9ae0-862dc2d234bf.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/baef9510-6fb1-4435-9ae0-862dc2d234bf.png)'
- en: 'The packet routing journey will be as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包的路由过程如下：
- en: LoadBalancer will choose one of the nodes to forward to the packet. In GCE,
    it selects the instance based on a hash of the source IP and port, destination
    IP and port, and protocol. In AWS, load balancing is based on a round-robin algorithm.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LoadBalancer 会选择一个节点将数据包转发出去。在 GCE 中，它基于源 IP 和端口、目标 IP 和端口、协议的哈希值选择实例；在 AWS
    中，负载均衡基于轮询算法。
- en: Here, the routing destination will be changed to pod d (DNAT) and will forward
    the packet to the other node, similar to pod-to-pod communication across nodes.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，路由目标将被更改为 pod d（DNAT），并将数据包转发到另一个节点，类似于跨节点的 pod 到 pod 通信。
- en: Then comes service-to-pod communication. The packet arrives at pod d and pod
    d returns the response.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后是服务到 pod 的通信。数据包到达 pod d 后，pod d 会返回响应。
- en: Pod-to-service communication is manipulated by `iptables` as well.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 到服务的通信也通过 `iptables` 来操作。
- en: The packet will be forwarded to the original node.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包将被转发回原节点。
- en: The source and destination will be un-DNAT to the LoadBalancer and client, and
    will be sent all the way back to the requester.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源和目标将被取消 DNAT，返回给 LoadBalancer 和客户端，并一路发送回请求者。
- en: 'From Kubernetes 1.7, there is a new attribute in this service called `externalTrafficPolicy`.
    Here, you can set its value to local, and then, after the traffic goes into a
    node, Kubernetes will route the pods on that node if possible, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Kubernetes 1.7 开始，这项服务中新增了一个属性 `externalTrafficPolicy`。在这里，你可以将其值设置为 local，然后，流量进入节点后，Kubernetes
    会尽可能地将流量路由到该节点上的 pod，如下所示：
- en: '**`kubectl patch $service_name nodeport -p ''{"spec":{"externalTrafficPolicy":"Local"}}''`**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**`kubectl patch $service_name nodeport -p ''{"spec":{"externalTrafficPolicy":"Local"}}''`**'
- en: Ingress
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入口
- en: Pods and services in Kubernetes have their own IPs. However, this is normally
    not the interface you'd provide to the external internet. Though there is a service
    with a node IP configured, the port in the node IP can't be duplicated among the
    services. It is cumbersome to decide which port to manage with which service.
    Furthermore, the node comes and goes; it wouldn't be clever to provide a static
    node IP to an external service.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的Pods和服务有自己的IP地址。然而，这通常不是你提供给外部互联网的接口。尽管有一个配置了节点IP的服务，但节点IP中的端口在各个服务之间不能重复。决定哪个端口与哪个服务对应是很麻烦的。此外，节点是动态变化的；向外部服务提供静态的节点IP并不是一个明智的选择。
- en: 'Ingress defines a set of rules that allows the inbound connection to access
    Kubernetes cluster services. This brings the traffic into the cluster at L7, and
    allocates and forwards a port on each VM to the service port. This is shown in
    the following diagram. We define a set of rules and post them as source type ingress
    to the API server. When the traffic comes in, the ingress controller will then
    fulfill and route the ingress according to the ingress rules. As shown in the
    following diagram, ingress is used to route external traffic to the kubernetes
    endpoints by different URLs:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 入口定义了一组规则，允许入站连接访问Kubernetes集群服务。它在L7层将流量引入集群，并在每个虚拟机上分配并转发一个端口到服务端口。如下图所示，我们定义一组规则并将其作为源类型的入口发送到API服务器。当流量进入时，入口控制器将根据入口规则来执行和路由入口。如下图所示，入口用于通过不同的URL将外部流量路由到Kubernetes端点：
- en: '![](img/48b60b52-c612-47c2-9f77-169aa3f41111.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48b60b52-c612-47c2-9f77-169aa3f41111.png)'
- en: 'Now, we will go through an example and see how this works. In this example,
    we''ll create two services named `nginx` and `echoserver`, with the ingress paths `/welcome`
    and `/echoserver` configured. We can run this in `minikube`. The old version of
    `minikube` doesn''t enable ingress by default; we''ll have to enable it first:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个示例来看看这个如何工作。在这个示例中，我们将创建两个名为`nginx`和`echoserver`的服务，并配置入口路径`/welcome`和`/echoserver`。我们可以在`minikube`中运行这个。旧版本的`minikube`默认没有启用入口；我们需要先启用它：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Enabling ingress in `minikube` will create an `nginx` ingress controller ([https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx))
    and a `ConfigMap` to store `nginx` configuration, as well as a `Deployment` and
    service as default HTTP backends for handling unmapped requests. We can observe
    these by adding `--namespace=kube-system` in the `kubectl` command. Next, let''s
    create our backend resources. Here is our `nginx` `Deployment` and `Service`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在`minikube`中启用入口将创建一个`nginx`入口控制器（[https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx)）以及一个`ConfigMap`来存储`nginx`配置，还有一个`Deployment`和服务作为默认的HTTP后端，用于处理未映射的请求。我们可以通过在`kubectl`命令中添加`--namespace=kube-system`来观察这些资源。接下来，让我们创建后端资源。以下是我们的`nginx`
    `Deployment`和`Service`：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we''ll create another service with `Deployment`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建另一个带有`Deployment`的服务：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we''ll create the ingress resource. There is an annotation named `nginx.ingress.kubernetes.io/rewrite-target`.
    This is required if the `service` requests are coming from the root URL. Without
    a rewrite annotation, we''ll get 404 as a response. Refer to [https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/rewrite](https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/rewrite) for
    more annotation that''s supported in the `nginx` ingress controller:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建入口资源。有一个名为`nginx.ingress.kubernetes.io/rewrite-target`的注解。如果`service`请求来自根URL，则这是必需的。如果没有重写注解，我们将收到404响应。有关更多支持的注解，请参见[https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/rewrite](https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/rewrite)：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In some cloud providers, the service LoadBalancer controller is supported. This
    can be integrated with ingress via the `status.loadBalancer.ingress` syntax in
    the configuration file. For more information, refer to [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些云提供商中，支持服务LoadBalancer控制器。这可以通过配置文件中的`status.loadBalancer.ingress`语法与入口集成。有关更多信息，请参阅[https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer)。
- en: 'Since our host is set to `devops.k8s`, it will only return if we access it
    from that hostname. You could either configure the DNS record in the DNS server,
    or modify the host''s file in local. For simplicity, we''ll just add a line with
    the `ip hostname` format in the host file:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的主机设置为 `devops.k8s`，只有从该主机名访问时才能返回。你可以在 DNS 服务器中配置 DNS 记录，或修改本地主机文件。为了简单起见，我们只需在主机文件中添加一行，格式为
    `ip 主机名`：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, we should be able to access our service by the URL directly:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应该能够直接通过 URL 访问我们的服务：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The pod ingress controller dispatches the traffic based on the URL's path. The
    routing path is similar to external-to-service communication. The packet hops
    between nodes and pods. Kubernetes is pluggable; lots of third-party implementation
    is going on. We have only scratched the surface here, even though `iptables` is
    just a default and common implementation. Networking evolves a lot with every
    single release. At the time of writing, Kubernetes had just released version 1.13.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Pod ingress 控制器根据 URL 路径分发流量。路由路径类似于外部到服务的通信。数据包在节点和 pod 之间跳跃。Kubernetes 是可插拔的；有很多第三方实现正在进行。即使
    `iptables` 只是默认的常见实现，我们这里只是触及了表面。每个版本的发布都会使网络发生很大变化。在撰写本文时，Kubernetes 刚刚发布了 1.13
    版本。
- en: Network policy
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络策略
- en: The network policy works as a software firewall to the pods. By default, every
    pod can communicate with each other without any boundaries. The network policy
    is one of the isolations you could apply to these pods. This defines who can access
    which pods in which port by namespace selector and pod selector. The network policy
    in a namespace is additive, and once a pod enables the network policy, it denies
    any other ingress (also known as deny all).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略充当 pod 的软件防火墙。默认情况下，每个 pod 都可以互相通信，没有任何边界。网络策略是你可以对这些 pod 应用的隔离之一。它定义了通过命名空间选择器和
    pod 选择器，谁可以访问哪个 pod 的哪个端口。命名空间中的网络策略是累加的，一旦 pod 启用网络策略，它将拒绝任何其他入站流量（也称为拒绝所有）。
- en: 'Currently, there are multiple network providers that support the network policy,
    such as Calico ([https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/](https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/)),
    Romana ([https://github.com/romana/romana](https://github.com/romana/romana)),
    Weave Net ([https://www.weave.works/docs/net/latest/kube-addon/#npc](https://www.weave.works/docs/net/latest/kube-addon/#npc))),
    Contiv ([http://contiv.github.io/documents/networking/policies.html](http://contiv.github.io/documents/networking/policies.html))),
    and Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)).
    Users are free to choose between any of these. For the purpose of simplicity,
    though, we''re going to use Calico with minikube. To do that, we''ll have to launch
    minikube with the `--network-plugin=cni` option. The network policy is still pretty
    new in Kubernetes at this point. We''re running Kubernetes version v.1.7.0 with
    the v.1.0.7 minikube ISO to deploy Calico by self-hosted solution ([http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/](http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/)).
    Calico can be installed with etcd datastore or the Kubernetes API datastore. For
    convenience, we''ll demonstrate how to install Calico with the Kubernetes API
    datastore here. Since rbac is enabled in minikube, we''ll have to configure the
    roles and bindings for Calico:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有多个网络提供商支持网络策略，如 Calico（[https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/](https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/)）、Romana（[https://github.com/romana/romana](https://github.com/romana/romana)）、Weave
    Net（[https://www.weave.works/docs/net/latest/kube-addon/#npc](https://www.weave.works/docs/net/latest/kube-addon/#npc)）、Contiv（[http://contiv.github.io/documents/networking/policies.html](http://contiv.github.io/documents/networking/policies.html)）和
    Trireme（[https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)）。用户可以自由选择其中任何一种。不过，为了简单起见，我们将使用
    Calico 配合 minikube。为此，我们需要使用 `--network-plugin=cni` 选项启动 minikube。此时，网络策略在 Kubernetes
    中仍然是比较新的功能。我们正在运行 Kubernetes 版本 v.1.7.0，配合 v.1.0.7 minikube ISO 通过自托管解决方案部署 Calico（[http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/](http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/)）。Calico
    可以通过 etcd 数据存储或 Kubernetes API 数据存储进行安装。为了方便起见，我们将在这里演示如何使用 Kubernetes API 数据存储安装
    Calico。由于 minikube 中启用了 rbac，我们需要为 Calico 配置角色和绑定：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s deploy Calico:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们部署 Calico：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After doing this, we can list the Calico pods and see whether it''s launched
    successfully:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们可以列出 Calico pod 并查看它是否成功启动：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s reuse `6-2-1_nginx.yaml` for our example:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在示例中重用 `6-2-1_nginx.yaml`：
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will find that our `nginx` service has an IP address of `10.96.51.143`.
    Let''s launch a simple bash and use `wget` to see whether we can access our `nginx`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将发现我们的`nginx`服务的IP地址是`10.96.51.143`。我们来启动一个简单的bash并使用`wget`查看我们是否能够访问`nginx`：
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `--spider` parameter is used to check whether the URL exists. In this case,
    `busybox` can access `nginx` successfully. Next, let''s apply a `NetworkPolicy`
    to our `nginx` pods:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`--spider`参数用于检查URL是否存在。在这种情况下，`busybox`可以成功访问`nginx`。接下来，我们为`nginx` pods应用一个`NetworkPolicy`：'
- en: '[PRE28]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can see some important syntax here. The `podSelector` is used to select pods
    that should match the labels of the target pod. Another one is `ingress[].from[].podSelector`,
    which is used to define who can access these pods. In this case, all the pods
    with `project=chapter6` labels are eligible to access the pods with `server=nginx`
    labels. If we go back to our busybox pod, we're unable to contact `nginx` any
    more because, right now, the `nginx` pod has `NetworkPolicy` on it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些重要的语法。`podSelector`用于选择应该匹配目标pod标签的pods。另一个是`ingress[].from[].podSelector`，它用于定义谁可以访问这些pods。在这种情况下，所有具有`project=chapter6`标签的pods都有资格访问具有`server=nginx`标签的pods。如果我们回到我们的busybox
    pod，我们将无法再联系`nginx`，因为此时`nginx` pod上已经有了`NetworkPolicy`。
- en: 'By default, it is deny all, so busybox won''t be able to talk to `nginx`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，它会拒绝所有连接，因此busybox将无法与`nginx`进行通信：
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can use `kubectl edit deployment busybox` to add the `project=chapter6` label
    in busybox pods.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`kubectl edit deployment busybox`在busybox pods中添加`project=chapter6`标签。
- en: 'After that, we can contact the `nginx` pod again:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以再次联系`nginx` pod：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'With the help of the preceding example, we now have an idea of how to apply
    a network policy. We could also apply some default polices to deny all, or allow
    all, by tweaking the selector to select nobody or everybody. For example, the
    deny all behavior can be achieved as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 借助前面的示例，我们现在有了如何应用网络策略的思路。我们还可以应用一些默认策略来拒绝所有流量或允许所有流量，通过调整选择器来选择没有人或每个人。例如，拒绝所有流量的行为可以通过以下方式实现：
- en: '[PRE31]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This way, all pods that don''t match labels will deny all other traffic. Alternatively,
    we could create a `NetworkPolicy` whose ingress is listed everywhere. By doing
    this, the pods running in this namespace can be accessed by anyone:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，所有不匹配标签的pods将拒绝所有其他流量。或者，我们可以创建一个`NetworkPolicy`，其入口规则列出所有内容。这样，这个命名空间中的pods可以被任何人访问：
- en: '[PRE32]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Service mesh
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格
- en: 'A service mesh is an infrastructure layer for handling service-to-service communication.
    Especially in the microservice world, the application at hand might contain hundreds
    of thousands of services. The network topology can be very complicated here. A
    service mesh can provide the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是处理服务到服务通信的基础设施层。特别是在微服务世界中，手头的应用可能包含成百上千个服务。这里的网络拓扑可能非常复杂。服务网格可以提供以下功能：
- en: Traffic management (such as A/B testing and canary deployment)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量管理（如A/B测试和金丝雀发布）
- en: Security (such as TLS and key management)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性（如TLS和密钥管理）
- en: Observability (such as providing traffic visibility. This is easy to integrate
    with monitoring systems such as Prometheus ([https://prometheus.io/](https://prometheus.io/)),
    tracing systems such as Jaeger ([https://www.jaegertracing.io](https://www.jaegertracing.io))
    or Zipkin ([https://github.com/openzipkin/zipkin](https://github.com/openzipkin/zipkin)),
    and logging systems)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可观察性（例如提供流量可见性。这很容易与监控系统如Prometheus（[https://prometheus.io/](https://prometheus.io/)）、跟踪系统如Jaeger（[https://www.jaegertracing.io](https://www.jaegertracing.io)）或Zipkin（[https://github.com/openzipkin/zipkin](https://github.com/openzipkin/zipkin)）以及日志系统集成）
- en: 'There are two major service mesh implementations on the market—Istio ([https://istio.io](https://istio.io))
    and Linkerd ([https://linkerd.io](https://linkerd.io)). Both of these deploy network proxy
    containers alongside the application container (the so-called sidecar container)
    and provide Kubernetes support. The following diagram is a simplified common architecture
    of the service mesh:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上有两个主要的服务网格实现——Istio（[https://istio.io](https://istio.io)）和Linkerd（[https://linkerd.io](https://linkerd.io)）。这两者都在应用容器旁边部署网络代理容器（即所谓的sidecar容器），并提供Kubernetes支持。以下图是服务网格的简化通用架构：
- en: '![](img/9227a449-08f6-4594-80bf-ca003f10c738.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9227a449-08f6-4594-80bf-ca003f10c738.png)'
- en: A service mesh normally contains a control plane, which is the brain of the
    mesh. This can manage and enforce the policies for route traffic, as well as collect
    telemetry data that can be integrated with other systems. It also carries out
    identity and credential management for services or end users. The service mesh
    sidecar container, which acts as a network proxy, lives side by side with the
    application container. The communication between services is passed through the
    sidecar container, which means that it can control the traffic by user-defined
    policies, secure the traffic via TLS encryption, do the load balancing and retries,
    control the ingress/egress, collect the metrics, and so on.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格通常包含一个控制平面，这是网格的大脑。它可以管理和执行路由流量的策略，并收集可以与其他系统集成的遥测数据。它还负责服务或最终用户的身份和凭证管理。服务网格的
    sidecar 容器，充当网络代理，与应用容器并排运行。服务之间的通信通过 sidecar 容器传递，这意味着它可以通过用户定义的策略控制流量，通过 TLS
    加密来确保流量的安全，进行负载均衡和重试，控制 ingress/egress，收集指标，等等。
- en: 'In the following section, we''ll use Istio as the example, but you''re free
    to use any implementation in your organization. First, let''s get the latest version
    of Istio. At the time of writing, the latest version is 1.0.5:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将以 Istio 为例，但你可以自由选择组织中任何实现。首先，让我们获取最新版本的 Istio。在撰写本文时，最新版本是 1.0.5：
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, let''s create a **Custom Resource Definition** (**CRD**) for Istio:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为 Istio 创建一个 **自定义资源定义** (**CRD**)：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the following example, we''re installing Istio with default mutual TLS authentication.
    The resource definition is under the `install/kubernetes/istio-demo-auth.yaml` file.
    If you''d like to deploy it without TLS authentication, you can use `install/kubernetes/istio-demo.yaml` instead:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将安装带有默认互信 TLS 身份验证的 Istio。资源定义位于 `install/kubernetes/istio-demo-auth.yaml`
    文件中。如果你希望在没有 TLS 身份验证的情况下部署，可以改用 `install/kubernetes/istio-demo.yaml`：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After deployment, let''s check that the services and pods have all been deployed
    successfully into the `istio-system` namespace:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，让我们检查服务和 pod 是否已经成功部署到 `istio-system` 命名空间：
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After waiting for a few minutes, check that the pods are all in `Running` and
    `Completed` states, as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几分钟后，检查所有的 pod 是否处于 `Running` 和 `Completed` 状态，如下所示：
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Since we have `istio-sidecar-injector` deployed, we can simply use `kubectl
    label namespace default istio-injection=enabled` to enable the sidecar container
    injection for every pod in the `default` namespace. `istio-sidecar-injector` acts
    as a mutating admission controller, which will inject the sidecar container to
    the pod if the namespace is labelled with `istio-injection=enabled`. Next, we
    can launch a sample application from the `samples` folder. Helloworld demonstrates
    the use of canary deployment ([https://en.wikipedia.org/wiki/Deployment_environment](https://en.wikipedia.org/wiki/Deployment_environment)),
    which will distribute the traffic to the helloworld-v1 and helloworld-v2 services:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经部署了 `istio-sidecar-injector`，我们可以简单地使用 `kubectl label namespace default
    istio-injection=enabled` 来为 `default` 命名空间中的每个 pod 启用 sidecar 容器注入。`istio-sidecar-injector`
    作为一个变更型准入控制器，如果命名空间被标记为 `istio-injection=enabled`，它会将 sidecar 容器注入到 pod 中。接下来，我们可以从
    `samples` 文件夹中启动一个示例应用。Helloworld 演示了金丝雀部署的使用 ([https://en.wikipedia.org/wiki/Deployment_environment](https://en.wikipedia.org/wiki/Deployment_environment))，它将流量分发到
    helloworld-v1 和 helloworld-v2 服务：
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If we inspect one of the pods, we''ll find that the `istio-proxy` container
    was injected into it:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查其中一个 pod，会发现 `istio-proxy` 容器已被注入到其中：
- en: '[PRE39]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Taking a closer look, we can see that the `istio-proxy` container was launched
    with the configuration of its control plane address, tracing system address, and
    connection configuration. Istio has now been verified. There are lots of to do
    with Istio traffic management, which is beyond of the scope of this book. Istio
    has a variety of detailed samples for us to try, which can be found in the `istio-1.0.5/samples`
    folder that we just downloaded.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察，我们可以看到 `istio-proxy` 容器已根据其控制平面地址、追踪系统地址和连接配置启动。Istio 已经被验证。对于 Istio 流量管理，还有许多内容超出了本书的范围。Istio
    提供了许多详细的示例供我们尝试，这些示例可以在我们刚下载的 `istio-1.0.5/samples` 文件夹中找到。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how containers communicate with each other. We also
    introduced how pod-to-pod communication works. A service is an abstraction that
    routes traffic to any of the pods underneath it if the label selectors match.
    We also learned how a service works with a pod using `iptables`. We also familiarized
    ourselves with how packet routes from external services to a pod using DNAT and
    un-DAT packets. In addition to this, we looked at new API objects such as ingress,
    which allows us to use the URL path to route to different services in the backend.
    In the end, another `NetworkPolicy` object was introduced. This provides a second
    layer of security, and acts as a software firewall rule. With the network policy,
    we can make certain pods communicate with certain other pods. For example, only
    data retrieval services can talk to the database container. In the last section,
    we got a glimpse at Istio, one of the popular implementations of service mesh. All
    of these things make Kubernetes more flexible, secure, robust, and powerful.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了容器如何相互通信。我们还介绍了 pod 之间如何进行通信。服务是一种抽象，它将流量路由到其下的任何 pod，只要标签选择器匹配。我们还了解了服务如何通过`iptables`与
    pod 协同工作。我们还熟悉了如何使用 DNAT 和 un-DAT 数据包将外部服务的数据包路由到 pod。此外，我们还研究了新的 API 对象，例如 ingress，它允许我们使用
    URL 路径将流量路由到后端的不同服务。最后，我们介绍了另一个 `NetworkPolicy` 对象，它提供了第二层安全性，并充当软件防火墙规则。通过网络策略，我们可以使特定的
    pod 与其他特定 pod 通信。例如，只有数据检索服务才能与数据库容器通信。在最后一节中，我们简要了解了 Istio，这是流行的服务网格实现之一。所有这些功能使
    Kubernetes 更加灵活、安全、可靠和强大。
- en: Before this chapter, we covered the basic concepts of Kubernetes. In [Chapter
    7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml), *Monitoring and Logging*, we'll
    get a clearer understanding of what is happening inside your cluster by monitoring
    cluster metrics and analyzing applications and system logs for Kubernetes. Monitoring
    and logging tools are essential for every DevOps, which also plays an extremely
    important role in dynamic clusters such as Kubernetes. Consequently, we'll get
    an insight into the activities of the cluster, such as scheduling, deployment,
    scaling, and service discovery. [Chapter 7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml),
    *Monitoring and Logging*, will help you to better understand the act of operating
    Kubernetes in the real world.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之前，我们已经介绍了 Kubernetes 的基本概念。在[第 7 章](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml)，*监控与日志记录*中，我们将通过监控集群指标以及分析
    Kubernetes 的应用程序和系统日志，更清晰地了解集群内部发生了什么。监控和日志记录工具对每个 DevOps 来说都是必不可少的，它们在像 Kubernetes
    这样的动态集群中也起着极其重要的作用。因此，我们将深入了解集群的活动，如调度、部署、扩展和服务发现。[第 7 章](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml)，*监控与日志记录*，将帮助你更好地理解在现实世界中操作
    Kubernetes 的过程。
