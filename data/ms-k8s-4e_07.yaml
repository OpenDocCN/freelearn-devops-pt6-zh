- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Running Stateful Applications with Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 运行有状态应用
- en: In this chapter, we will learn how to run stateful applications on Kubernetes.
    Kubernetes takes a lot of work out of our hands by automatically starting and
    restarting pods across the cluster nodes as needed, based on complex requirements
    and configurations such as namespaces, limits, and quotas. But when pods run storage-aware
    software, such as databases and queues, relocating a pod can cause a system to
    break.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在 Kubernetes 上运行有状态应用。Kubernetes 自动处理许多工作，根据复杂的需求和配置（如命名空间、限制和配额），自动启动和重启集群节点上的
    Pod。但是，当 Pods 运行存储感知型软件时，如数据库和队列，移动 Pod 可能会导致系统出现故障。
- en: First, we’ll explore the essence of stateful pods and why they are much more
    complicated to manage in Kubernetes. We will look at a few ways to manage the
    complexity, such as shared environment variables and DNS records. In some situations,
    a redundant in-memory state, a `DaemonSet`, or persistent storage claims can do
    the trick. The main solution that Kubernetes promotes for state-aware pods is
    the `StatefulSet` (previously called `PetSet`) resource, which allows us to manage
    an indexed collection of pods with stable properties. Finally, we will dive deep
    into a full-fledged example of running a Cassandra cluster on top of Kubernetes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨有状态 Pod 的本质，以及为什么它们在 Kubernetes 中的管理更为复杂。我们将看一下几种管理复杂性的方式，比如共享环境变量和
    DNS 记录。在某些情况下，冗余的内存状态、`DaemonSet` 或持久存储声明可以解决问题。Kubernetes 提供的主要有状态 Pod 解决方案是
    `StatefulSet`（之前称为 `PetSet`）资源，它允许我们管理具有稳定属性的按索引排列的 Pod 集合。最后，我们将深入探讨在 Kubernetes
    上运行 Cassandra 集群的完整示例。
- en: 'This chapter will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Stateful versus stateless applications in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中有状态与无状态应用
- en: Running a Cassandra cluster in Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中运行 Cassandra 集群
- en: By the end of this chapter, you will understand the challenges of state management
    in Kubernetes, get a deep look into a specific example of running Cassandra as
    a data store on Kubernetes, and be able to determine the state management strategy
    for your workloads.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将理解 Kubernetes 中状态管理的挑战，深入了解在 Kubernetes 上运行 Cassandra 作为数据存储的具体示例，并能够确定适合你工作负载的状态管理策略。
- en: Stateful versus stateless applications in Kubernetes
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 中有状态与无状态应用
- en: A stateless Kubernetes application is an application that doesn’t manage its
    state in the Kubernetes cluster. All the state is stored in memory or outside
    the cluster, and the cluster containers access it in some manner. A stateful Kubernetes
    application, on the other hand, has a persistent state that is managed in the
    cluster. In this section, we’ll learn why state management is critical to the
    design of a distributed system and the benefits of managing the state within the
    Kubernetes cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态的 Kubernetes 应用是指该应用不在 Kubernetes 集群中管理其状态。所有状态存储在内存中或集群外部，集群中的容器以某种方式访问它。另一方面，有状态的
    Kubernetes 应用有一个持久状态，并在集群中管理。在本节中，我们将学习为什么状态管理对分布式系统设计至关重要，以及在 Kubernetes 集群中管理状态的好处。
- en: Understanding the nature of distributed data-intensive apps
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解分布式数据密集型应用的本质
- en: Let’s start with the basics here. Distributed applications are a collection
    of processes that run on multiple machines, process inputs, manipulate data, expose
    APIs, and possibly have other side effects. Each process is a combination of its
    program, its runtime environment, and its inputs and outputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始。分布式应用是运行在多台机器上的一组进程，这些进程处理输入，操作数据，暴露 API，并可能有其他副作用。每个进程都是其程序、运行时环境以及输入输出的组合。
- en: The programs you write at school get their input as command-line arguments;
    maybe they read a file or access a database, and then write their results to the
    screen, a file, or a database. Some programs keep state in memory and can serve
    requests over a network. Simple programs run on a single machine and can hold
    all their state in memory or read from a file. Their runtime environment is their
    operating system. If they crash, the user has to restart them manually. They are
    tied to their machine.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你在学校写的程序接收命令行参数作为输入；也许它们读取一个文件或访问数据库，然后将结果写入屏幕、文件或数据库。一些程序将状态保存在内存中，并能够通过网络处理请求。简单的程序运行在单台机器上，并且可以将所有状态保存在内存中或从文件中读取。它们的运行时环境是操作系统。如果程序崩溃，用户必须手动重启它们。它们与机器绑定。
- en: A distributed application is a different animal. A single machine is not enough
    to process all the data or serve all the requests quickly enough. A single machine
    can’t hold all the data. The data that needs to be processed is so large that
    it can’t be downloaded cost-effectively into each processing machine. Machines
    can fail and need to be replaced. Upgrades need to be performed over all the processing
    machines. Users may be distributed across the globe.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式应用程序是另一种情况。单台机器不足以快速处理所有数据或响应所有请求。单台机器无法容纳所有数据。需要处理的数据量太大，以至于无法以经济的方式下载到每台处理机器中。机器可能会发生故障，需要更换。所有处理机器都需要进行升级。用户可能分布在全球各地。
- en: Taking all these issues into account, it becomes clear that the traditional
    approach doesn’t work. The limiting factor becomes the data. Users/clients must
    receive only summary or processed data. All massive data processing must be done
    close to the data itself because transferring data is prohibitively slow and expensive.
    Instead, the bulk of processing code must run in the same data center and network
    environment of the data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些问题，很明显传统方法行不通。限制因素变成了数据。用户/客户端只能接收到汇总或处理过的数据。所有大规模的数据处理必须在离数据本身很近的地方完成，因为数据传输过慢且成本过高。相反，大部分处理代码必须在数据所在的数据中心和网络环境中运行。
- en: Why manage the state in Kubernetes?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要在Kubernetes中管理状态？
- en: The main reason to manage the state in Kubernetes itself as opposed to a separate
    cluster is that a lot of the infrastructure needed to monitor, scale, allocate,
    secure, and operate a storage cluster is already provided by Kubernetes. Running
    a parallel storage cluster will lead to a lot of duplicated effort.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes内部管理状态而不是在单独的集群中管理的主要原因是，Kubernetes已经提供了监控、扩展、分配、安全和运营存储集群所需的许多基础设施。运行一个平行的存储集群将导致大量重复的工作。
- en: Why manage the state outside of Kubernetes?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要在Kubernetes外部管理状态？
- en: Let’s not rule out the other option. It may be better in some situations to
    manage the state in a separate non-Kubernetes cluster, as long as it shares the
    same internal network (data proximity trumps everything).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能排除其他选择。在某些情况下，将状态管理在一个独立的非Kubernetes集群中可能更好，只要它共享相同的内部网络（数据的接近性胜过一切）。
- en: 'Some valid reasons are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有效的理由如下：
- en: You already have a separate storage cluster and you don’t want to rock the boat
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经有了一个独立的存储集群，并且不想轻易改变现状
- en: Your storage cluster is used by other non-Kubernetes applications
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的存储集群被其他非Kubernetes应用程序使用
- en: Kubernetes support for your storage cluster is not stable or mature enough
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes对您的存储集群的支持不稳定或不够成熟
- en: You may want to approach stateful applications in Kubernetes incrementally,
    starting with a separate storage cluster and integrating more tightly with Kubernetes
    later
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能希望逐步接近Kubernetes中的有状态应用程序，从独立的存储集群开始，随后与Kubernetes更紧密地集成
- en: Shared environment variables versus DNS records for discovery
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享环境变量与DNS记录进行发现
- en: Kubernetes provides several mechanisms for global discovery across the cluster.
    If your storage cluster is not managed by Kubernetes, you still need to tell Kubernetes
    pods how to find it and access it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为跨集群的全局发现提供了多种机制。如果您的存储集群不是由Kubernetes管理的，您仍然需要告诉Kubernetes的pod如何找到它并访问它。
- en: 'There are two common methods:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见方法：
- en: DNS
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS
- en: Environment variables
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境变量
- en: In some cases, you may want to use both, as environment variables can override
    DNS.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望同时使用这两种方法，因为环境变量可以覆盖DNS。
- en: Accessing external data stores via DNS
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过DNS访问外部数据存储
- en: The DNS approach is simple and straightforward. Assuming your external storage
    cluster is load-balanced and can provide a stable endpoint, then pods can just
    hit that endpoint directly and connect to the external cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: DNS方法简单明了。假设您的外部存储集群是负载均衡的并且可以提供稳定的端点，那么pod只需直接访问该端点并连接到外部集群。
- en: Accessing external data stores via environment variables
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过环境变量访问外部数据存储
- en: Another simple approach is to use environment variables to pass connection information
    to an external storage cluster. Kubernetes offers the `ConfigMap` resource as
    a way to keep configuration separate from the container image. The configuration
    is a set of `key-value` pairs. The configuration information can be exposed in
    two ways. One way is as environment variables. The other way is as a configuration
    file mounted as a volume in the container. You may prefer to use secrets for sensitive
    connection information like passwords.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种简单的方法是使用环境变量将连接信息传递给外部存储集群。Kubernetes 提供了 `ConfigMap` 资源，作为将配置与容器镜像分开的方式。配置是一组
    `key-value` 对。配置可以通过两种方式暴露。一种方式是作为环境变量，另一种是作为挂载到容器中的配置文件。对于像密码这样的敏感连接信息，你可能更倾向于使用秘密管理工具（secrets）。
- en: Creating a ConfigMap
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建 ConfigMap
- en: 'The following file is a `ConfigMap` that keeps a list of addresses:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文件是一个 `ConfigMap`，它保存了地址列表：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Save it as `db-config-map.yaml` and run:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将其保存为 `db-config-map.yaml` 并运行：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `data` section contains all the `key-value` pairs, in this case, just a
    single pair with a key name of `db-ip-addresses`. It will be important later when
    consuming the `ConfigMap` in a pod. You can check out the content to make sure
    it’s OK:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`data` 部分包含所有的 `key-value` 对，在这个例子中，只有一个键值对，键名为 `db-ip-addresses`。当在 Pod 中使用
    `ConfigMap` 时，这一部分将变得很重要。你可以查看内容确认它是否正确：'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are other ways to create a `ConfigMap`. You can directly create one using
    the `--from-value` or `--from-file` command-line arguments.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法可以创建 `ConfigMap`。你可以直接使用 `--from-value` 或 `--from-file` 命令行参数来创建。
- en: Consuming a ConfigMap as an environment variable
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为环境变量使用 ConfigMap
- en: 'When you are creating a pod, you can specify a `ConfigMap` and consume its
    values in several ways. Here is how to consume our configuration map as an environment
    variable:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个 Pod 时，可以指定一个 `ConfigMap` 并以多种方式使用其值。以下是如何将配置映射作为环境变量使用：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This pod runs the `busybox` minimal container and executes an `env bash` command
    and it immediately exists. The `db-ip-addresses` key from the `db-configmap` is
    mapped to the `DB_IP_ADDRESSES` environment variable, and is reflected in the
    logs:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Pod 运行 `busybox` 最小化容器，并执行 `env bash` 命令，随后立即退出。来自 `db-configmap` 的 `db-ip-addresses`
    键映射到 `DB_IP_ADDRESSES` 环境变量，并在日志中反映出来：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using a redundant in-memory state
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用冗余的内存状态
- en: In some cases, you may want to keep a transient state in memory. Distributed
    caching is a common case. Time-sensitive information is another one. For these
    use cases, there is no need for persistent storage, and multiple pods accessed
    through a service may be just the right solution.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你可能希望将临时状态保存在内存中。分布式缓存是一个常见的例子。时间敏感的信息也是如此。对于这些用例，通常不需要持久存储，通过服务访问的多个
    Pods 可能是一个合适的解决方案。
- en: We can use standard Kubernetes techniques, such as labeling, to identify pods
    that belong to the distributed cache, store redundant copies of the same state,
    and expose them through a service. If a pod dies, Kubernetes will create a new
    one and, until it catches up, the other pods will serve the state. We can even
    use the pod’s anti-affinity feature to ensure that pods that maintain redundant
    copies of the same state are not scheduled to the same node.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用标准的 Kubernetes 技术，例如标签，来识别属于分布式缓存的 Pods，存储相同状态的冗余副本，并通过服务暴露它们。如果某个 Pod
    失效，Kubernetes 会创建一个新的 Pod，直到它赶上进度，其他 Pod 将继续提供该状态。我们甚至可以使用 Pod 的反亲和性特性，确保维护相同状态冗余副本的
    Pods 不会被调度到同一节点。
- en: Of course, you could also use something like Memcached or Redis.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你也可以使用像 Memcached 或 Redis 这样的工具。
- en: Using DaemonSet for redundant persistent storage
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 DaemonSet 实现冗余持久存储
- en: Some stateful applications, such as distributed databases or queues, manage
    their state redundantly and sync their nodes automatically (we’ll take a very
    deep look into Cassandra later). In these cases, it is important that pods are
    scheduled to separate nodes. It is also important that pods are scheduled to nodes
    with a particular hardware configuration or are even dedicated to the stateful
    application. The `DaemonSet` feature is perfect for this use case. We can label
    a set of nodes and make sure that the stateful pods are scheduled on a one-by-one
    basis to the selected group of nodes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有状态应用程序，如分布式数据库或队列，管理其状态冗余并自动同步节点（稍后我们会深入了解 Cassandra）。在这些情况下，确保 Pods 被调度到不同的节点上非常重要。同样，确保
    Pods 被调度到具有特定硬件配置的节点上，或者专门分配给有状态应用程序，也是非常重要的。`DaemonSet` 特性非常适合这种用例。我们可以为一组节点打标签，确保有状态的
    Pods 被逐一调度到选定的节点组上。
- en: Applying persistent volume claims
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用持久卷声明
- en: If the stateful application can use effectively shared persistent storage, then
    using a persistent volume claim in each pod is the way to go, as we demonstrated
    in *Chapter 6*, *Managing Storage*. The stateful application will be presented
    with a mounted volume that looks just like a local filesystem.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有状态应用可以有效利用共享的持久存储，那么在每个 pod 中使用持久卷声明是最佳选择，正如我们在*第六章*《存储管理》中展示的那样。有状态应用将看到一个看起来像本地文件系统的挂载卷。
- en: Utilizing StatefulSet
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 StatefulSet
- en: StatefulSets are specially designed to support distributed stateful applications
    where the identities of the members are important, and if a pod is restarted,
    it must retain its identity in the set. It provides ordered deployment and scaling.
    Unlike regular pods, the pods of a `StatefulSet` are associated with persistent
    storage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets 专门设计用于支持分布式有状态应用程序，其中成员的身份非常重要，并且如果 pod 被重启，它必须在集合中保持其身份。它提供了有序的部署和扩展。与常规
    pod 不同，`StatefulSet`的 pod 与持久存储相关联。
- en: When to use StatefulSet
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 何时使用 StatefulSet
- en: '`StatefulSets` are great for applications that necessitate any of the following
    capabilities:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`StatefulSets`非常适合需要以下任何功能的应用程序：'
- en: Consistent and distinct network identifiers
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致且独特的网络标识符
- en: Persistent and enduring storage
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久且持久的存储
- en: Methodical and orderly deployment and scaling
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有条不紊且有组织的部署与扩展
- en: Systematic and organized deletion and termination
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统化和有序的删除与终止
- en: The components of StatefulSet
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: StatefulSet 的组件
- en: 'There are several elements that need to be configured correctly in order to
    have a working `StatefulSet`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个元素需要正确配置才能使`StatefulSet`正常工作：
- en: A headless service responsible for managing the network identity of the `StatefulSet`
    pods
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个负责管理 `StatefulSet` pod 网络身份的无头服务
- en: The `StatefulSet` itself with a number of replicas
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StatefulSet`本身及其副本数'
- en: Local storage on nodes or persistent storage provisioned dynamically or by an
    administrator
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点上的本地存储或由管理员动态或手动配置的持久存储
- en: 'Here is an example of a headless service called `nginx` that will be used for
    a `StatefulSet`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个名为`nginx`的无头服务示例，将用于`StatefulSet`：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, the `StatefulSet` manifest file will reference the service:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`StatefulSet`清单文件将引用该服务：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The next part is the pod template, which includes a mounted volume named `www`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分是 pod 模板，其中包括一个名为`www`的挂载卷：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Last but not least, `volumeClaimTemplates` use a claim named `www` matching
    the mounted volume. The claim requests 1 Gib of storage with `ReadWriteOnce` access:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，`volumeClaimTemplates`使用名为`www`的声明来匹配挂载卷。该声明请求 1 Gib 的存储并具有`ReadWriteOnce`访问权限：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Working with StatefulSets
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 StatefulSets
- en: 'Let’s create the `nginx` headless service and `statefulset`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建`nginx`无头服务和`statefulset`：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can use the `kubectl get all` command to see all the resources that were
    created:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`kubectl get all`命令查看所有已创建的资源：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As expected, we have the `statefulset` with three replicas and the headless
    service. What is not pre-set is a `ReplicaSet`, which you find when you create
    a Deployment. StatefulSets manage their pods directly.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期所示，我们有一个包含三个副本的`statefulset`和无头服务。没有预设的是`ReplicaSet`，你会在创建 Deployment 时看到它。StatefulSets
    直接管理它们的 pods。
- en: 'Note that the `kubectl get all` doesn’t actually show all resources. The StatefulSet
    also creates a persistent volume claim backed by a persistent volume for each
    pod. Here they are:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`kubectl get all`实际上并不显示所有资源。StatefulSet 还会为每个 pod 创建一个由持久卷支持的持久卷声明。它们是：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If we delete a pod, the StatefulSet will create a new pod and bind it to the
    corresponding persistent volume claim. The pod `nginx-1` is bound to the `www-nginx-1`
    pvc:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们删除一个 pod，StatefulSet 会创建一个新 pod 并将其绑定到相应的持久卷声明。pod `nginx-1` 被绑定到`www-nginx-1`
    pvc：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s delete the `nginx-1` pod and check all remaining pods:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除`nginx-1` pod 并检查所有剩余的 pod：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As you can see, the StatefulSet immediately replaced it with a new `nginx-1`
    pod (14 seconds old). The new pod is bound to the same persistent volume claim:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，StatefulSet 立即用一个新的`nginx-1` pod（14秒前创建）替换了它。新 pod 被绑定到相同的持久卷声明：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The persistent volume claim and its backing persistent volume were not deleted
    when the old `nginx-1` pod was deleted, as you can tell by their age:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当旧的`nginx-1` pod 被删除时，持久卷声明及其背后的持久卷并没有被删除，你可以通过它们的创建时间看出这一点：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That means that the state of the StatefulSet is preserved even as pods come
    and go. Each pod identified by its index is always bound to a specific shard of
    the state, backed up by the corresponding persistent volume claim.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着即使 Pods 来来去去，StatefulSet 的状态也会被保持。每个通过其索引标识的 Pod 始终绑定到某个特定的状态分片，并由相应的持久卷声明备份。
- en: At this point, we understand what StatefulSets are all about and how to work
    with them. Let’s dive into the implementation of an industrial-strength data store
    and see how it can be deployed as a StatefulSet in Kubernetes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经理解了 StatefulSets 的概念以及如何使用它们。让我们深入研究如何实现一个工业级数据存储，并看看它如何作为 StatefulSet
    部署在 Kubernetes 中。
- en: Running a Cassandra cluster in Kubernetes
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中运行 Cassandra 集群
- en: 'In this section, we will explore in detail a very large example of configuring
    a Cassandra cluster to run on a Kubernetes cluster. I will dissect and give some
    context for interesting parts. If you wish to explore this even further, the full
    example can be accessed here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细探讨如何配置一个非常大的 Cassandra 集群在 Kubernetes 集群中运行。我将分析并为一些有趣的部分提供背景。如果你想进一步探索完整的示例，可以在这里访问：
- en: '[https://kubernetes.io/docs/tutorials/stateful-application/cassandra](https://kubernetes.io/docs/tutorials/stateful-application/cassandra)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/tutorials/stateful-application/cassandra](https://kubernetes.io/docs/tutorials/stateful-application/cassandra)'
- en: The goal here is to get a sense of what it takes to run a real-world stateful
    workload on Kubernetes and how StatefulSets help. Don’t worry if you don’t understand
    every little detail.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是了解在 Kubernetes 上运行一个真实世界的有状态工作负载需要什么，以及 StatefulSets 如何提供帮助。如果你没有理解每个细节也不用担心。
- en: First, we’ll learn a little bit about Cassandra and its idiosyncrasies, and
    then follow a step-by-step procedure to get it running using several of the techniques
    and strategies we covered in the previous section.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解一些关于 Cassandra 的知识及其独特性，然后按照一步一步的流程，使用我们在前一部分中讨论的几种技术和策略将其运行起来。
- en: A quick introduction to Cassandra
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cassandra 简介
- en: Cassandra is a distributed columnar data store. It was designed from the get-go
    for big data. Cassandra is fast, robust (no single point of failure), highly available,
    and linearly scalable. It also has multi-data center support. It achieves all
    this by having a laser focus and carefully crafting the features it supports and—just
    as importantly—the features it doesn’t support.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra 是一个分布式列式数据存储系统。它从一开始就为大数据而设计。Cassandra 具有快速、强大（没有单点故障）、高可用和线性可扩展的特点。它还支持多数据中心。它通过专注于特定功能并精心设计支持的特性，以及同样重要的是不支持的特性，来实现这一切。
- en: In a previous company, I ran a Kubernetes cluster that used Cassandra as the
    main data store for sensor data (about 100 TB). Cassandra allocates the data to
    a set of nodes (node ring) based on a **distributed hash table** (**DHT**) algorithm.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一家公司，我运行了一个 Kubernetes 集群，使用 Cassandra 作为传感器数据的主要数据存储（大约 100 TB）。Cassandra
    根据 **分布式哈希表**（**DHT**）算法将数据分配到一组节点（节点环）中。
- en: The cluster nodes talk to each other via a gossip protocol and learn quickly
    about the overall state of the cluster (what nodes joined and what nodes left
    or are unavailable). Cassandra constantly compacts the data and balances the cluster.
    The data is typically replicated multiple times for redundancy, robustness, and
    high availability.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 集群节点通过 gossip 协议相互通信，并快速了解集群的整体状态（哪些节点加入、哪些节点离开或不可用）。Cassandra 会不断压缩数据并平衡集群。数据通常会被多次复制以保证冗余、稳健性和高可用性。
- en: From a developer’s point of view, Cassandra is very good for time-series data
    and provides a flexible model where you can specify the consistency level in each
    query. It is also idempotent (a very important feature for a distributed database),
    which means repeated inserts or updates are allowed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从开发者的角度来看，Cassandra 非常适合时间序列数据，并提供一个灵活的模型，允许在每个查询中指定一致性级别。它还是幂等的（这是分布式数据库中非常重要的特性），这意味着允许重复插入或更新操作。
- en: 'Here is a diagram that shows how a Cassandra cluster is organized, how a client
    can access any node, and how a request will be forwarded automatically to the
    nodes that have the requested data:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个图示，展示了 Cassandra 集群的组织方式，客户端如何访问任意节点，以及请求将如何自动转发到具有请求数据的节点：
- en: '![Figure 7.1: Request interacting with a Cassandra cluster](img/B18998_07_01.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1：请求与 Cassandra 集群的交互](img/B18998_07_01.png)'
- en: 'Figure 7.1: Request interacting with a Cassandra cluster'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：请求与 Cassandra 集群的交互
- en: The Cassandra Docker image
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cassandra Docker 镜像
- en: 'Deploying Cassandra on Kubernetes as opposed to a standalone Cassandra cluster
    deployment requires a special Docker image. This is an important step because
    it means we can use Kubernetes to keep track of our Cassandra pods. The Dockerfile
    for an image is available here: [https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile](https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上部署 Cassandra 与在独立 Cassandra 集群中部署不同，需要使用特定的 Docker 镜像。这是一个重要的步骤，因为这意味着我们可以使用
    Kubernetes 来跟踪我们的 Cassandra pods。镜像的 Dockerfile 可以在这里找到：[https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile](https://github.com/kubernetes/examples/blob/master/cassandra/image/Dockerfile)。
- en: See below the Dockerfile that builds the Cassandra image. The base image is
    a flavor of Debian designed for use in containers (see [https://github.com/kubernetes/kubernetes/tree/master/build/debian-base](https://github.com/kubernetes/kubernetes/tree/master/build/debian-base)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是构建 Cassandra 镜像的 Dockerfile。基础镜像是为容器使用设计的 Debian 版本（见 [https://github.com/kubernetes/kubernetes/tree/master/build/debian-base](https://github.com/kubernetes/kubernetes/tree/master/build/debian-base)）。
- en: 'The Cassandra Dockerfile defines some build arguments that must be set when
    the image is built, creates a bunch of labels, defines many environment variables,
    adds all the files to the root directory inside the container, runs the `build.sh`
    script, declares the Cassandra data volume (where the data is stored), exposes
    a bunch of ports, and finally, uses `dumb-init` to execute the `run.sh` scripts:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra Dockerfile 定义了一些构建参数，这些参数在镜像构建时必须设置，创建了一些标签，定义了许多环境变量，将所有文件添加到容器内的根目录，运行
    `build.sh` 脚本，声明 Cassandra 数据卷（数据存储位置），暴露了一些端口，最后使用 `dumb-init` 执行 `run.sh` 脚本：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here are all the files used by the Dockerfile:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Dockerfile 使用的所有文件：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will not cover all of them, but will focus on a couple of interesting scripts:
    the `build.sh` and `run.sh` scripts.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会覆盖所有内容，而是专注于几个有趣的脚本：`build.sh` 和 `run.sh` 脚本。
- en: Exploring the build.sh script
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 `build.sh` 脚本
- en: Cassandra is a Java program. The build script installs the Java runtime environment
    and a few necessary libraries and tools. It then sets a few variables that will
    be used later, such as `CASSANDRA_PATH`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra 是一个 Java 程序。构建脚本安装 Java 运行时环境和一些必要的库与工具。然后设置一些稍后将使用的变量，如 `CASSANDRA_PATH`。
- en: 'It downloads the correct version of Cassandra from the Apache organization
    (Cassandra is an Apache open source project), creates the `/cassandra_data/data`
    directory where Cassandra will store its `SSTables` and the `/etc/cassandra` configuration
    directory, copies files into the configuration directory, adds a Cassandra user,
    sets the readiness probe, installs Python, moves the Cassandra JAR file and the
    seed shared library to their target destination, and then cleans up all the intermediate
    files generated during this process:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 它从 Apache 组织下载正确版本的 Cassandra（Cassandra 是一个 Apache 开源项目），创建了 `/cassandra_data/data`
    目录以存储 Cassandra 的 `SSTables` 和 `/etc/cassandra` 配置目录，复制文件到配置目录，添加 Cassandra 用户，设置就绪探针，安装
    Python，将 Cassandra JAR 文件和种子共享库移动到目标位置，然后清理在此过程中生成的所有中间文件：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Exploring the run.sh script
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 `run.sh` 脚本
- en: 'The `run.sh` script requires some shell skills and knowledge of Cassandra to
    understand, but it’s worth the effort. First, some local variables are set for
    the Cassandra configuration file at `/etc/cassandra/cassandra.yaml`. The `CASSANDRA_CFG`
    variable will be used in the rest of the script:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`run.sh` 脚本需要一些 shell 技能和对 Cassandra 的了解才能理解，但这值得付出努力。首先，设置一些本地变量用于 Cassandra
    配置文件 `/etc/cassandra/cassandra.yaml`。`CASSANDRA_CFG` 变量将在脚本的其余部分使用：'
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If no `CASSANDRA_SEEDS` were specified, then set the `HOSTNAME`, which is used
    by the `StatefulSet` later:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有指定 `CASSANDRA_SEEDS`，则设置 `HOSTNAME`，稍后 `StatefulSet` 将使用该变量：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Then comes a long list of environment variables with defaults. The syntax, `${VAR_NAME:-}`,
    uses the `VAR_NAME` environment variable, if it’s defined, or the default value.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是一个包含默认值的环境变量长列表。语法 `${VAR_NAME:-}` 使用环境变量 `VAR_NAME`，如果已定义，或者使用默认值。
- en: A similar syntax, `${VAR_NAME:=}`, does the same thing but also assigns the
    default value to the environment variable if it’s not defined. This is a subtle
    but important difference.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个类似的语法 `${VAR_NAME:=}` 做了同样的事情，但如果环境变量未定义，还会给环境变量赋予默认值。这是一个微妙但重要的区别。
- en: 'Both variations are used here:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了两种变体：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: By the way, I contributed my part to Kubernetes by opening a PR to fix a minor
    typo here. See [https://github.com/kubernetes/examples/pull/348](https://github.com/kubernetes/examples/pull/348).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，我为 Kubernetes 做出了贡献，通过提交 PR 修复了一个小错误。请参见 [https://github.com/kubernetes/examples/pull/348](https://github.com/kubernetes/examples/pull/348)。
- en: 'The next part configures monitoring JMX and controls garbage collection output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分配置了监控 JMX 并控制垃圾回收输出：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then comes a section where all the variables are printed on the screen. Let’s
    skip most of it:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个部分，所有变量都会在屏幕上打印出来。我们跳过其中的大部分：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The next section is very important. By default, Cassandra uses a simple snitch,
    which is unaware of racks and data centers. This is not optimal when the cluster
    spans multiple data centers and racks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分非常重要。默认情况下，Cassandra 使用一个简单的 Snitch，它不了解机架和数据中心。当集群跨多个数据中心和机架时，这种配置并不理想。
- en: 'Cassandra is rack-aware and data center-aware and can optimize both for redundancy
    and high availability while limiting communication across data centers appropriately:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra 支持机架和数据中心的感知，并可以优化冗余和高可用性，同时合理限制跨数据中心的通信：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Memory management is also important, and you can control the maximum heap size
    to ensure Cassandra doesn’t start thrashing and swapping to disk:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理也很重要，您可以控制最大堆内存大小，确保 Cassandra 不会开始抖动并将数据交换到磁盘：
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The rack and data center information is stored in a simple Java `propertiesfile`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 机架和数据中心信息存储在一个简单的 Java `propertiesfile` 中：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The next section loops over all the variables defined earlier, finds the corresponding
    key in the `Cassandra.yaml` configuration files, and overwrites them. That ensures
    that each configuration file is customized on the fly just before it launches
    Cassandra itself:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分会遍历之前定义的所有变量，找到对应的 `Cassandra.yaml` 配置文件中的键，并覆盖它们。这确保了每个配置文件在启动 Cassandra
    之前会动态定制：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The next section is all about setting the seeds or seed provider depending
    on the deployment solution (`StatefulSet` or not). There is a little trick for
    the first pod to bootstrap as its own seed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分主要是设置种子或种子提供者，这取决于部署解决方案（是否使用 `StatefulSet`）。对于第一个 pod 启动为其自身种子有一个小技巧：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following section sets up various options for remote management and JMX
    monitoring. It’s critical in complicated distributed systems to have proper administration
    tools. Cassandra has deep support for the ubiquitous **JMX** standard:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分设置了远程管理和 JMX 监控的各种选项。在复杂的分布式系统中，拥有合适的管理工具至关重要。Cassandra 对广泛使用的**JMX**标准有深度支持：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, it protects the data directory such that only the `cassandra` user
    can access it, the `CLASSPATH` is set to the Cassandra `JAR` file, and it launches
    Cassandra in the foreground (not daemonized) as the `cassandra` user:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它保护数据目录，只有 `cassandra` 用户可以访问，`CLASSPATH` 设置为 Cassandra 的 `JAR` 文件，并且以 `cassandra`
    用户的身份在前台启动 Cassandra（而非作为守护进程）：
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Hooking up Kubernetes and Cassandra
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接 Kubernetes 和 Cassandra
- en: Connecting Kubernetes and Cassandra takes some work because Cassandra was designed
    to be very self-sufficient, but we want to let it hook into Kubernetes at the
    right time to provide capabilities such as automatically restarting failed nodes,
    monitoring, allocating Cassandra pods, and providing a unified view of the Cassandra
    pods side by side with other pods.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 连接 Kubernetes 和 Cassandra 需要一些工作，因为 Cassandra 设计为非常自给自足，但我们希望它能在适当的时机接入 Kubernetes，提供诸如自动重启失败节点、监控、分配
    Cassandra pod 以及提供 Cassandra pod 与其他 pod 并排显示的统一视图等功能。
- en: Cassandra is a complicated beast and has many knobs to control it. It comes
    with a `Cassandra.yaml` configuration file, and you can override all the options
    with environment variables.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra 是一个复杂的系统，具有许多控制选项。它随附一个 `Cassandra.yaml` 配置文件，您可以通过环境变量覆盖所有选项。
- en: Digging into the Cassandra configuration file
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入了解 Cassandra 配置文件
- en: 'There are two settings that are particularly relevant: the seed provider and
    the snitch. The seed provider is responsible for publishing a list of IP addresses
    (seeds) for nodes in the cluster. Each node that starts running connects to the
    seeds (there are usually at least three) and if it successfully reaches one of
    them, they immediately exchange information about all the nodes in the cluster.
    This information is updated constantly for each node as the nodes gossip with
    each other.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个特别相关的设置：种子提供者和snitch。种子提供者负责发布集群中节点的IP地址列表（种子）。每个启动的节点都会连接到这些种子（通常至少有三个），如果成功连接其中一个，它们会立即交换集群中所有节点的信息。随着节点之间的相互传播，这些信息会不断更新。
- en: 'The default seed provider configured in `Cassandra.yaml` is just a static list
    of IP addresses, in this case, just the loopback interface:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Cassandra.yaml`中配置的默认种子提供者只是一个静态IP地址列表，在此情况下，仅为回环接口：
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The other important setting is the snitch. It has two roles:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的设置是snitch。它有两个作用：
- en: Cassandra utilizes the snitch to gain valuable insights into your network topology,
    enabling it to effectively route requests.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassandra利用snitch来深入了解网络拓扑，从而有效地路由请求。
- en: Cassandra employs this knowledge to strategically distribute replicas across
    your cluster, mitigating the risk of correlated failures. To achieve this, Cassandra
    organizes machines into data centers and racks, ensuring that replicas are not
    concentrated on a single rack, even if it doesn’t necessarily correspond to a
    physical location.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassandra利用这些知识来有策略地在集群中分配副本，降低相关故障的风险。为此，Cassandra将机器组织到数据中心和机架中，确保副本不会集中在同一个机架上，即使这不一定与物理位置相关。
- en: 'Cassandra comes pre-loaded with several snitch classes, but none of them are
    Kubernetes-aware. The default is `SimpleSnitch`, but it can be overridden:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra预装了几种snitch类，但没有一个是Kubernetes感知的。默认是`SimpleSnitch`，但可以被重写：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Other snitches are:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其他的snitch有：
- en: '`GossipingPropertyFileSnitch`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GossipingPropertyFileSnitch`'
- en: '`PropertyFileSnitch`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PropertyFileSnitch`'
- en: '`Ec2Snitch`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ec2Snitch`'
- en: '`Ec2MultiRegionSnitch`'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ec2MultiRegionSnitch`'
- en: '`RackInferringSnitch`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RackInferringSnitch`'
- en: The custom seed provider
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义种子提供者
- en: When running Cassandra nodes as pods in Kubernetes, Kubernetes may move pods
    around, including seeds. To accommodate that, a Cassandra seed provider needs
    to interact with the Kubernetes API server.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Kubernetes中将Cassandra节点作为Pod运行时，Kubernetes可能会移动Pod，包括种子节点。为此，Cassandra种子提供者需要与Kubernetes
    API服务器进行交互。
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Creating a Cassandra headless service
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Cassandra无头服务
- en: The role of the headless service is to allow clients in the Kubernetes cluster
    to connect to the Cassandra cluster through a standard Kubernetes service instead
    of keeping track of the network identities of the nodes or putting a dedicated
    load balancer in front of all the nodes. Kubernetes provides all that out of the
    box through its services.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务的作用是允许Kubernetes集群中的客户端通过标准Kubernetes服务连接到Cassandra集群，而不必跟踪节点的网络身份或在所有节点前面设置专用负载均衡器。Kubernetes通过其服务原生提供了这一切。
- en: 'Here is the `Service` manifest:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`Service`清单：
- en: '[PRE35]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The `app: cassandra` label will group all the pods that participate in the
    service. Kubernetes will create endpoint records and the DNS will return a record
    for discovery. The `clusterIP` is `None`, which means the service is headless
    and Kubernetes will not do any load-balancing or proxying. This is important because
    Cassandra nodes do their own communication directly.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`app: cassandra`标签将所有参与服务的Pod进行分组。Kubernetes会创建端点记录，DNS会返回用于发现的记录。`clusterIP`为`None`，这意味着该服务是无头的，Kubernetes不会进行任何负载均衡或代理。这一点很重要，因为Cassandra节点会直接进行通信。'
- en: The `9042` port is used by Cassandra to serve CQL requests. Those can be queries,
    inserts/updates (it’s always an upsert with Cassandra), or deletes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`9042`端口被Cassandra用于提供CQL请求。这些请求可以是查询、插入/更新（在Cassandra中总是upsert）或删除。'
- en: Using StatefulSet to create the Cassandra cluster
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用StatefulSet创建Cassandra集群
- en: 'Declaring a `StatefulSet` is not trivial. It is arguably the most complex Kubernetes
    resource. It has a lot of moving parts: standard metadata, the StatefulSet spec,
    the pod template (which is often pretty complex itself), and volume claim templates.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 声明一个`StatefulSet`并非易事，它可以说是最复杂的Kubernetes资源之一。它包含许多组成部分：标准元数据、StatefulSet规范、Pod模板（通常本身也相当复杂）和卷声明模板。
- en: Dissecting the StatefulSet YAML file
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解剖StatefulSet YAML文件
- en: Let’s go methodically over this example StatefulSet YAML file that declares
    a three-node Cassandra cluster.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们有条理地回顾一下这个声明了三节点 Cassandra 集群的 StatefulSet YAML 文件示例。
- en: 'Here is the basic metadata. Note the `apiVersion` string is `apps/v1` (`StatefulSet`
    became generally available in Kubernetes 1.9):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基本的元数据。请注意，`apiVersion` 字符串为 `apps/v1`（`StatefulSet` 在 Kubernetes 1.9 中正式发布）：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The StatefulSet spec defines the headless service name, the label selector
    (`app: cassandra`), how many pods there are in the StatefulSet, and the pod template
    (explained later). The `replicas` field specifies how many pods are in the StatefulSet:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'StatefulSet 规格定义了无头服务名称、标签选择器（`app: cassandra`）、StatefulSet 中的 Pod 数量以及 Pod
    模板（稍后解释）。`replicas` 字段指定了 StatefulSet 中的 Pod 数量：'
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The term “replicas” for the pods is an unfortunate choice because the pods are
    not replicas of each other. They share the same pod template, but they have a
    unique identity, and they are responsible for different subsets of the state in
    general. This is even more confusing in the case of Cassandra, which uses the
    same term, “replicas,” to refer to groups of nodes that redundantly duplicate
    some subset of the state (but are not identical, because each can manage an additional
    state too).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的“replicas”这个术语是一个不太恰当的选择，因为这些 Pod 不是彼此的副本。它们共享相同的 Pod 模板，但每个 Pod 都有一个独特的身份，并且通常负责不同的状态子集。对于
    Cassandra 来说，情况更为混淆，因为它使用同样的术语“replicas”来指代那些冗余复制某些状态子集的节点组（但它们并不完全相同，因为每个节点还可以管理额外的状态）。
- en: 'I opened a GitHub issue with the Kubernetes project to change the term from
    replicas to members:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Kubernetes 项目中开了一个 GitHub 问题，建议将“replicas”一词更改为“members”：
- en: '[https://github.com/kubernetes/kubernetes.github.io/issues/2103](https://github.com/kubernetes/kubernetes.github.io/issues/2103)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes/kubernetes.github.io/issues/2103](https://github.com/kubernetes/kubernetes.github.io/issues/2103)'
- en: The pod template contains a single container based on the custom Cassandra image.
    It also sets the termination grace period to 30 minutes. This means that when
    Kubernetes needs to terminate the pod, it will send the containers a `SIGTERM`
    signal notifying them they should exit, giving them a chance to do so gracefully.
    Any container that is still running after the grace period will be killed via
    `SIGKILL`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 模板包含了基于自定义 Cassandra 镜像的单一容器。它还将终止宽限期设置为 30 分钟。这意味着当 Kubernetes 需要终止 Pod
    时，它会向容器发送 `SIGTERM` 信号，通知它们应退出，并给予它们优雅退出的机会。任何在宽限期后仍在运行的容器将通过 `SIGKILL` 被强制终止。
- en: 'Here is the pod template with the `app: cassandra` label:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '这是带有 `app: cassandra` 标签的 Pod 模板：'
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `containers` section has multiple important parts. It starts with a name
    and the image we looked at earlier:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`containers` 部分包含多个重要部分。它以名称和我们之前查看的镜像开始：'
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then, it defines multiple container ports needed for external and internal
    communication by Cassandra nodes:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它定义了 Cassandra 节点所需的多个容器端口，用于外部和内部通信：
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `resources` section specifies the CPU and memory needed by the container.
    This is critical because the storage management layer should never be a performance
    bottleneck due to CPU or memory. Note that it follows the best practice of identical
    requests and limits to ensure the resources are always available once allocated:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`resources` 部分指定了容器所需的 CPU 和内存。这一点至关重要，因为存储管理层绝不应该因为 CPU 或内存的原因成为性能瓶颈。请注意，它遵循了请求和限制一致的最佳实践，以确保资源在分配后始终可用：'
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Cassandra needs access to **inter-process communication** (**IPC**), which
    the container requests through the security context’s capabilities:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra 需要访问**进程间通信**（**IPC**），容器通过安全上下文的能力来请求这一访问权限：
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `lifecycle` section runs the Cassandra `nodetool drain` command to make
    sure data on the node is transferred to other nodes in the Cassandra cluster when
    the container needs to shut down. This is the reason a 30-minute grace period
    is needed. Node draining involves moving a lot of data around:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`lifecycle` 部分运行 Cassandra 的 `nodetool drain` 命令，以确保当容器需要关闭时，节点上的数据会转移到 Cassandra
    集群中的其他节点。这也是为什么需要 30 分钟宽限期的原因。节点排空涉及大量数据转移：'
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `env` section specifies the environment variables that will be available
    inside the container. The following is a partial list of the necessary variables.
    The `CASSANDRA_SEEDS` variable is set to the headless service, so a Cassandra
    node can talk to seed nodes on startup and discover the whole cluster. Note that
    in this configuration we don’t use the special Kubernetes seed provider. `POD_IP`
    is interesting because it utilizes the Downward API to populate its value via
    the field reference to `status.podIP`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`env` 部分指定了容器内可用的环境变量。以下是必要变量的部分列表。`CASSANDRA_SEEDS` 变量设置为无头服务，这样 Cassandra
    节点就可以在启动时与种子节点通信并发现整个集群。请注意，在此配置中，我们没有使用特殊的 Kubernetes 种子提供程序。`POD_IP` 很有趣，因为它利用了向下
    API，通过字段引用 `status.podIP` 来填充其值：'
- en: '[PRE44]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The readiness probe makes sure that requests are not sent to the node until
    it is actually ready to service them. The `ready-probe.sh` script utilizes Cassandra’s
    `nodetool status` command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探针确保在节点实际准备好处理请求之前，不会将请求发送到该节点。`ready-probe.sh` 脚本使用 Cassandra 的 `nodetool
    status` 命令：
- en: '[PRE45]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The last part of the container spec is the volume mount, which must match a
    persistent volume claim:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 容器配置的最后一部分是卷挂载，必须与持久化卷声明匹配：
- en: '[PRE46]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: That’s it for the container spec. The last part is the volume claim templates.
    In this case, dynamic provisioning is used. It’s highly recommended to use SSD
    drives for Cassandra storage, especially its journal. The requested storage in
    this example is 1 GiB. I discovered through experimentation that 1–2 TB is ideal
    for a single Cassandra node. The reason is that Cassandra does a lot of data shuffling
    under the covers, compacting and rebalancing the data. If a node leaves the cluster
    or a new one joins the cluster, you have to wait until the data is properly rebalanced
    before the data from the node that left is properly redistributed or a new node
    is populated.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是容器配置的全部内容。最后一部分是卷声明模板。在这种情况下，使用的是动态供应。强烈建议为 Cassandra 存储使用 SSD 驱动，特别是其日志。此示例中请求的存储为
    1 GiB。我通过实验发现，单个 Cassandra 节点的理想存储大小是 1 到 2 TB。原因是 Cassandra 在后台会做很多数据重排、压缩和重新平衡。如果一个节点离开集群或一个新节点加入集群，你必须等到数据被正确重新平衡后，才能正确地重新分配离开节点的数据，或者新节点会填充数据。
- en: Note that Cassandra needs a lot of disk space to do all this shuffling. It is
    recommended to have 50% free disk space. When you consider that you also need
    replication (typically 3x), then the required storage space can be 6x your data
    size. You can get by with 30% free space if you’re adventurous and maybe use just
    2x replication depending on your use case. But don’t get below 10% free disk space,
    even on a single node. I learned the hard way that Cassandra will simply get stuck
    and will be unable to compact and rebalance such nodes without extreme measures.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Cassandra 需要大量的磁盘空间来进行所有这些数据重排。建议保持 50% 的磁盘空间空闲。当你考虑到还需要复制（通常是 3 倍）时，所需的存储空间可能是数据大小的
    6 倍。如果你敢于冒险，可能只使用 2 倍的复制，并且保持 30% 空闲空间，具体取决于你的使用情况。但即便是在单节点上，也不要让空闲磁盘空间低于 10%。我通过实际经验了解到，Cassandra
    会卡住，无法压缩和重新平衡这种节点，除非采取极端措施。
- en: The storage class `fast` must be defined in this case. Usually, for Cassandra,
    you need a special storage class and can’t use the Kubernetes cluster default
    storage class.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，必须定义存储类 `fast`。通常对于 Cassandra，你需要一个特殊的存储类，而不能使用 Kubernetes 集群默认的存储类。
- en: 'The access mode is, of course, `ReadWriteOnce`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 访问模式当然是 `ReadWriteOnce`：
- en: '[PRE47]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: When deploying a StatefulSet, Kubernetes creates the pods in order per their
    index number. When scaling up or down, it also does so in order. For Cassandra,
    this is not important because it can handle nodes joining or leaving the cluster
    in any order. When a Cassandra pod is destroyed (ungracefully), the persistent
    volume remains. If a pod with the same index is created later, the original persistent
    volume will be mounted into it. This stable connection between a particular pod
    and its storage enables Cassandra to manage the state properly.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 StatefulSet 时，Kubernetes 按照索引号顺序创建 pod。在扩展或缩减时，也会按照顺序进行。对于 Cassandra 来说，这并不重要，因为它可以处理节点按任意顺序加入或离开集群。当一个
    Cassandra pod 被销毁（不优雅地销毁时），持久化卷仍然存在。如果稍后创建具有相同索引的 pod，原始持久化卷将会被挂载到它上面。这种特定 pod
    和其存储之间的稳定连接，使得 Cassandra 能够正确管理状态。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the topic of stateful applications and how to integrate
    them with Kubernetes. We discovered that stateful applications are complicated
    and considered several mechanisms for discovery, such as DNS and environment variables.
    We also discussed several state management solutions, such as in-memory redundant
    storage, local storage, and persistent storage. The bulk of the chapter revolved
    around deploying a Cassandra cluster inside a Kubernetes cluster using a StatefulSet.
    We drilled down into the low-level details in order to appreciate what it really
    takes to integrate a third-party complex distributed system like Cassandra into
    Kubernetes. At this point, you should have a thorough understanding of stateful
    applications and how to apply them within your Kubernetes-based system. You are
    armed with multiple methods for various use cases, and maybe you’ve even learned
    a little bit about Cassandra.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了有状态应用程序以及如何将它们与 Kubernetes 集成。我们发现有状态应用程序非常复杂，并考虑了几种发现机制，如 DNS 和环境变量。我们还讨论了几种状态管理解决方案，如内存冗余存储、本地存储和持久化存储。本章的重点围绕着如何通过
    StatefulSet 在 Kubernetes 集群中部署 Cassandra 集群。我们深入探讨了低级细节，以便更好地理解将像 Cassandra 这样复杂的第三方分布式系统集成到
    Kubernetes 中究竟需要什么。到此为止，您应该对有状态应用程序以及如何在基于 Kubernetes 的系统中应用它们有了透彻的理解。您已经掌握了多种方法，适用于不同的使用场景，也许您还学到了一些关于
    Cassandra 的知识。
- en: In the next chapter, we will continue our journey and explore the important
    topic of scalability, in particular auto-scalability, and how to deploy and do
    live upgrades and updates as the cluster dynamically grows. These issues are very
    intricate, especially when the cluster has stateful apps running on it.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续我们的旅程，探索一个重要话题——可扩展性，特别是自动扩展性，以及如何在集群动态增长时进行部署和实时升级更新。这些问题非常复杂，尤其是当集群中运行着有状态应用程序时。
