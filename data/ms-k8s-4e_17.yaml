- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Running Kubernetes in Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产环境中运行Kubernetes
- en: In the previous chapter, we discussed governance and policy engines. This is
    an important part of managing large-scale Kubernetes-based systems in production.
    However, it is only one part. In this chapter, we will turn our attention to the
    overall management of Kubernetes in production. The focus will be on running multiple
    Managed Kubernetes clusters in the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了治理和策略引擎。这是管理大规模基于Kubernetes的生产系统的一个重要部分。然而，它只是其中的一部分。在本章中，我们将重点关注Kubernetes在生产环境中的整体管理。重点将放在如何在云中运行多个托管Kubernetes集群。
- en: 'The topics we will cover are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖的主题包括：
- en: Understanding Managed Kubernetes in the cloud
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解云中的托管Kubernetes
- en: Managing multiple clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理多个集群
- en: Building effective processes for large-scale Kubernetes deployments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为大规模Kubernetes部署构建有效的流程
- en: Handling infrastructure at scale
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理大规模基础设施
- en: Managing clusters and node pools
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理集群和节点池
- en: Upgrading Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级Kubernetes
- en: Troubleshooting
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除
- en: Cost management
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本管理
- en: Understanding Managed Kubernetes in the cloud
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解云中的托管Kubernetes
- en: '**Managed Kubernetes** is a service provided by cloud providers such as **Amazon
    Web Services** (**AWS**), **Google Cloud Platform** (**GCP**), and **Microsoft
    Azure** that simplifies the deployment, management, and scaling of containerized
    applications in the cloud. With Managed Kubernetes, organizations can focus on
    developing and deploying their applications without worrying too much about the
    underlying infrastructure.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**托管Kubernetes**是由云服务提供商（如**亚马逊Web服务**（**AWS**）、**谷歌云平台**（**GCP**）和**微软Azure**）提供的服务，简化了在云中部署、管理和扩展容器化应用程序的过程。通过托管Kubernetes，组织可以专注于开发和部署应用程序，而无需过多担心底层的基础设施。'
- en: Managed Kubernetes provides a pre-configured and optimized environment for deploying
    containers, eliminating the need for the manual setup and maintenance of a Kubernetes
    cluster. This allows organizations to quickly deploy and scale their applications,
    reducing time to market and freeing up valuable resources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 托管Kubernetes提供了一个预配置且优化的环境，用于部署容器，消除了手动设置和维护Kubernetes集群的需求。这使得组织能够快速部署和扩展应用程序，减少市场响应时间，并释放宝贵的资源。
- en: Additionally, Managed Kubernetes integrates with the cloud providers’ other
    services, such as databases, networking, storage solutions, security, identity,
    and observability features, making it easier to manage and secure the entire application
    stack. This also enables organizations to leverage the providers’ expertise in
    managing large-scale infrastructure, ensuring high availability, and reducing
    downtime.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，托管Kubernetes与云服务提供商的其他服务（如数据库、网络、存储解决方案、安全性、身份验证和可观测性功能）集成，使得管理和保护整个应用堆栈变得更加容易。这也使得组织能够利用服务提供商在管理大规模基础设施、确保高可用性和减少停机时间方面的专业知识。
- en: Overall, Managed Kubernetes provides a simplified and efficient way to deploy
    and manage containerized applications in the cloud, reducing operational overhead
    and improving time to market. This makes it an attractive option for organizations
    of all sizes looking to take advantage of the benefits of containers and the cloud.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，托管Kubernetes提供了一种简化且高效的方式来部署和管理云中的容器化应用程序，减少了运营开销，并提高了市场响应时间。这使得它成为各类组织的一个有吸引力的选择，帮助他们利用容器和云计算的优势。
- en: Deep integration
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度集成
- en: Cloud providers utilize the extensibility of Kubernetes to offer deep integration
    of their Managed Kubernetes solutions with their cloud services via the CNI, the
    CSI, and authentication/authorization plugins. The cloud providers also implement
    the **Cloud Controller Interface** (**CCI**) to allow their compute infrastructure
    to serve Kubernetes nodes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商利用Kubernetes的扩展性，通过CNI、CSI和身份验证/授权插件，提供其托管Kubernetes解决方案与云服务的深度集成。云服务提供商还实现了**云控制器接口**（**CCI**），使其计算基础设施能够为Kubernetes节点提供服务。
- en: However, the integration runs deeper. The cloud providers often configure the
    kubelet, control the container runtime that runs on every node, and deploy various
    DaemonSets on every node.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，集成更深层次。云服务提供商通常会配置kubelet，控制每个节点上运行的容器运行时，并在每个节点上部署各种DaemonSets。
- en: 'For example, AKS leverages many Azure services:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，AKS利用了许多Azure服务：
- en: '**Azure Compute**: AKS leverages Azure Compute resources such as **virtual**
    **machines** (**VMs**), availability sets, and scale sets to provide a managed
    Kubernetes experience.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Compute**：AKS 利用 Azure Compute 资源，如 **虚拟机** (**VMs**)、可用性集和扩展集，提供托管的
    Kubernetes 体验。'
- en: '**Azure Virtual Network**: AKS integrates with Azure Virtual Network, allowing
    users to create and manage their own virtual networks and subnets. This provides
    users with control over their network layout and the ability to tightly control
    network traffic.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Virtual Network**：AKS 与 Azure Virtual Network 集成，使用户能够创建和管理自己的虚拟网络和子网。这为用户提供了对网络布局的控制，并能够紧密控制网络流量。'
- en: '**Azure Blob Storage**: AKS integrates with Azure Blob Storage, allowing users
    to store and manage their application data in the cloud. This provides users with
    scalable, secure, and highly available storage for their applications.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Blob Storage**：AKS 与 Azure Blob Storage 集成，使用户能够在云中存储和管理其应用程序数据。这为用户提供了可扩展、安全且高可用的存储解决方案。'
- en: '**Azure Key Vault**: AKS integrates with Azure Key Vault, allowing users to
    securely manage and store secrets such as passwords, keys, and certificates. This
    provides users with secure storage for their application secrets.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Key Vault**：AKS 与 Azure Key Vault 集成，允许用户安全地管理和存储密码、密钥和证书等机密。这为用户提供了安全存储应用程序机密的解决方案。'
- en: '**Azure Monitor**: AKS integrates with Azure Monitor, allowing users to collect
    and analyze metrics, logs, and traces from their applications. This provides users
    with the ability to monitor and troubleshoot their workloads.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Monitor**：AKS 与 Azure Monitor 集成，使用户能够收集和分析来自其应用程序的度量、日志和跟踪信息。这为用户提供了监控和故障排除其工作负载的能力。'
- en: '**Azure Active Directory** (**AAD**): AKS integrates with AAD to provide a
    secure, reliable, and highly available platform for running Kubernetes clusters.
    AAD provides an efficient and secure way to authenticate and authorize users and
    applications to access the cluster. AAD can also be integrated with Kubernetes
    **RBAC** (**role-based access control**) to provide granular control over access
    to cluster resources.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Active Directory** (**AAD**)：AKS 与 AAD 集成，提供一个安全、可靠且高可用的平台来运行 Kubernetes
    集群。AAD 提供了一种高效且安全的方式来验证和授权用户及应用程序访问集群。AAD 还可以与 Kubernetes **RBAC**（**基于角色的访问控制**）集成，为集群资源访问提供细粒度的控制。'
- en: Let’s move on and discuss one of the key elements of successfully managing a
    production Kubernetes-based system in the cloud.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论成功管理基于 Kubernetes 的生产系统的关键要素之一。
- en: Quotas and limits
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配额和限制
- en: Cloud infrastructure has revolutionized the way organizations store and manage
    their data and run their workloads. However, one major issue that requires consideration
    and attention is the use of quotas and limits by cloud service providers. These
    quotas and limits, while necessary for ensuring the stability and security of
    the cloud infrastructure, can be a major source of frustration and even outages
    for users.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 云基础设施彻底改变了组织存储和管理数据及运行工作负载的方式。然而，云服务提供商使用配额和限制是一个需要考虑和注意的重要问题。这些配额和限制虽然对确保云基础设施的稳定性和安全性至关重要，但也可能是用户面临的主要挫折来源，甚至可能导致宕机。
- en: Quotas and limits are restrictions placed on the number of resources that a
    user can consume. For example, there may be a limit on the number of VMs of a
    particular type that can be created in each region environment, or a quota on
    the amount of storage space that can be used. These quotas and limits are put
    in place to prevent a single user from consuming too many resources and potentially
    disrupting the overall performance of the cloud infrastructure. It also protects
    users from inadvertently provisioning a huge quantity of resources that they don’t
    really need but will have to pay for.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 配额和限制是对用户可消耗资源数量的限制。例如，可能会限制在每个区域环境中可以创建的特定类型的虚拟机数量，或对可用的存储空间数量设定配额。这些配额和限制旨在防止单个用户消耗过多资源，从而可能影响云基础设施的整体性能。它们还保护用户避免无意中配置大量不必要的资源，从而产生不必要的费用。
- en: The cloud is in theory infinitely scalable and elastic. In practice, this is
    true only within the quotas and limits.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，云是无限可扩展和弹性的。但在实际操作中，这只有在配额和限制范围内才成立。
- en: Let’s look at some real-world examples in the next sections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中查看一些实际示例。
- en: Real-world examples of quotas and limits
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配额和限制的实际示例
- en: On GCP, quotas can generally be increased, while limits are fixed. Also, each
    service has its own page of quotas and limits. The **virtual private cloud** (**VPC**)
    page is found at [https://cloud.google.com/vpc/docs/quota](https://cloud.google.com/vpc/docs/quota).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP上，配额通常可以增加，而限制是固定的。此外，每个服务都有自己独立的配额和限制页面。**虚拟专用云**（**VPC**）页面可以在[https://cloud.google.com/vpc/docs/quota](https://cloud.google.com/vpc/docs/quota)找到。
- en: 'You can view the quotas in the GCP console as well as request increases: [https://console.cloud.google.com/iam-admin/quotas](https://console.cloud.google.com/iam-admin/quotas).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GCP控制台中查看配额，并请求增加配额：[https://console.cloud.google.com/iam-admin/quotas](https://console.cloud.google.com/iam-admin/quotas)。
- en: There are currently 9,441 quotas!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当前有9,441个配额！
- en: 'Here is a screenshot that shows some quotas for the GCP Compute Engine service:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是显示GCP计算引擎服务一些配额的截图：
- en: '![](img/B18998_17_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_17_01.png)'
- en: 'Figure 17.1: Screenshot of GCP compute engine quotas'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：GCP计算引擎配额截图
- en: Now that we understand what quotas and limits are, let’s discuss capacity planning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了配额和限制是什么，让我们来讨论容量规划。
- en: Capacity planning
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容量规划
- en: In the olden days, capacity planning meant thinking about how many servers you
    needed in your data centers, how big the disks should be, and the bandwidth of
    your network. This was based on the usage of your workloads as well as keeping
    healthy headroom for redundancy as well as growth. Then, you had to consider upgrades
    and how to phase out obsolete hardware. In the cloud, you don’t need to worry
    about hardware. However, you do need to plan around the quotas and limits. This
    means you need to monitor the quotas and limits and, whenever you get close to
    the current quota, request an increase. For quotas such as the number of VM instances
    of a particular VM family, I recommend staying below 50%-60% if possible. This
    should give you ample room for disaster recovery and growth, as well blue-green
    deployments where you run your new version and old version side by side for a
    while.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，容量规划意味着思考你在数据中心需要多少台服务器、硬盘应该多大，以及网络带宽是多少。这些都基于工作负载的使用情况，并确保有足够的冗余空间和增长空间。然后，你需要考虑硬件升级，以及如何逐步淘汰过时的硬件。在云中，你不需要担心硬件。然而，你需要围绕配额和限制进行规划。这意味着你需要监控配额和限制，并且每当接近当前配额时，提出增加请求。对于像某个VM系列的虚拟机实例数量这样的配额，我建议尽量保持在50%-60%以下。这应该为你提供足够的灾难恢复空间和增长空间，同时也适用于蓝绿部署，即同时运行新版本和旧版本一段时间。
- en: When should you not use Managed Kubernetes?
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候不应该使用托管Kubernetes？
- en: 'Managed Kubernetes is great, but it is not a panacea. There are several situations
    and use cases where you may prefer to manage Kubernetes yourself, such as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 托管Kubernetes很好，但它不是万灵药。有几种情况和使用场景，你可能更愿意自行管理Kubernetes，例如：
- en: The obvious use case is if you run Kubernetes on-prem and a managed solution
    is simply not available. However, you can run a similar stack to cloud-managed
    Kubernetes via platforms like GKE Anthos, AWS Outposts, and Azure Arc.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显而易见的使用场景是，如果你在本地运行Kubernetes，而托管解决方案根本不可用。然而，你可以通过像GKE Anthos、AWS Outposts和Azure
    Arc等平台，运行类似于云托管Kubernetes的技术栈。
- en: You require extreme control over the control plane, the node components, and
    the daemonsets that run on each node.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要对控制平面、节点组件和运行在每个节点上的守护进程进行极端控制。
- en: You already have in-house expertise in running Kubernetes yourself and using
    Managed Kubernetes will require a steep learning curve and might cost more.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经具备了自己运行Kubernetes的内部专业知识，而使用托管Kubernetes将需要陡峭的学习曲线，并且可能成本更高。
- en: You manage highly sensitive information that you must fully control and can’t
    trust the cloud provider with.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你管理着高度敏感的信息，必须完全控制这些信息，且无法将其交给云服务提供商。
- en: You run Kubernetes on multiple cloud providers and/or hybrid environments and
    prefer to have a uniform way to manage Kubernetes in all environments.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在多个云提供商和/或混合环境中运行Kubernetes，并且希望在所有环境中以统一的方式管理Kubernetes。
- en: You want to make sure you are not locked into a particular cloud provider.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望确保不会被锁定到某个特定的云服务提供商。
- en: In short, there are various situations where you may take on managing Kubernetes
    yourself. Let’s look at the various ways you may deploy and manage multiple clusters
    of Kubernetes in different environments.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在各种情况下，你可能会选择自行管理Kubernetes。让我们看看你可能如何在不同环境中部署和管理多个Kubernetes集群。
- en: Managing multiple clusters
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理多个集群
- en: A Kubernetes cluster is powerful and can manage a lot of workloads (thousands
    of nodes, and hundreds of thousands of pods). As a startup, you may get pretty
    far with just one cluster. However, at enterprise scale, you’ll need more than
    one cluster. Let’s consider some use cases.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Kubernetes集群非常强大，可以管理大量的工作负载（成千上万的节点，和数十万个Pod）。作为初创公司，您可能只需一个集群就能走得很远。然而，在企业级规模下，您将需要不止一个集群。让我们来看看一些用例。
- en: Geo-distributed clusters
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地理分布式集群
- en: 'Geo-distributed clusters are clusters that run in different locations. There
    are three main reasons for using geo-distributed clusters:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 地理分布式集群是指在不同位置运行的集群。使用地理分布式集群的主要原因有三：
- en: Keeping your data and workloads close to their consumers.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的数据和工作负载靠近其消费者。
- en: Compliance and data privacy laws where data must remain in its country of origin.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合规性和数据隐私法律要求数据必须保留在其原始国家。
- en: High availability and disaster recovery in case of a regional outage.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在地区性停机的情况下实现高可用性和灾难恢复。
- en: Multi-cloud
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多云
- en: If you run on multiple clouds, then naturally you need at least one cluster
    per cloud provider.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在多个云平台上运行，那么您自然需要每个云提供商至少一个集群。
- en: Running on multiple clouds can be complicated, but at enterprise scale, it may
    be unavoidable and sometimes desirable. For example, your company may run Kubernetes
    on cloud X and acquire a company that runs Kubernetes on cloud Y. Migrating from
    Y to X might be too risky and expensive.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个云平台上运行可能会很复杂，但在企业级规模下，这可能是不可避免的，有时也是必要的。例如，您的公司可能在云X上运行Kubernetes，而收购的公司在云Y上运行Kubernetes。从Y迁移到X可能风险太大且费用过高。
- en: Another valid reason to run on multiple clouds is to have leverage against the
    cloud providers to secure better discounts and ensure you are not fully locked
    in. Finally, it may allow you to tolerate a complete cloud provider outage (this
    is not trivial to pull off).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有效的在多个云平台上运行的理由是，通过与云提供商的谈判，争取更好的折扣，并确保您不会被完全锁定。最后，这也可能让您在完全云服务提供商停机的情况下，仍能保持容错（这可不是简单的事情）。
- en: Hybrid
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合云
- en: Hybrid Kubernetes means running some Kubernetes clusters in the cloud (with
    a single or multiple cloud providers) and some Kubernetes clusters on-prem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 混合Kubernetes意味着在云中（使用一个或多个云提供商）运行一些Kubernetes集群，同时在本地运行一些Kubernetes集群。
- en: This situation may arise as before because of an acquisition or even if you
    are in the process of migrating from on-prem Kubernetes to the cloud. Large systems
    might take years to migrate and during the migration, you’ll have to run a mix
    of Kubernetes clusters running in a hybrid environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况可能会像之前一样发生，因为收购，或者即使您正在将本地Kubernetes迁移到云端。大型系统的迁移可能需要几年时间，而在迁移过程中，您将不得不运行一个混合环境中的Kubernetes集群。
- en: You may also adopt patterns like burst to cloud where most of your Kubernetes
    clusters run on-prem, but you have the flexibility to deploy workloads to Kubernetes
    clusters running in the cloud, which can scale quickly if you are hit with unanticipated
    load or if your on-prem infrastructure is having issues.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以采用像“突发到云”这样的模式，其中大部分Kubernetes集群运行在本地，但您可以灵活地将工作负载部署到云中的Kubernetes集群，这样在遇到无法预料的负载时，或者当您的本地基础设施出现问题时，云端的集群可以迅速扩展。
- en: Kubernetes on the edge
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边缘上的Kubernetes
- en: Most enterprise data (around 90%) is generated in the cloud and private data
    centers; however, that number will drop to just 25% by 2025 according to Gartner.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业数据（约90%）是在云端和私有数据中心生成的；然而，根据Gartner的预测，到2025年，这一比例将降至仅25%。
- en: This is mind-blowing. Edge computing (AKS IoT) will be responsible for this
    massive shift.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这简直让人震撼。边缘计算（AKS IoT）将是这种巨大转变的主要推动力。
- en: A lot of devices spread all over the place will generate constant streams of
    data. Some of that data will be sent back to the backend for processing, aggregation,
    and storage. However, it makes a lot of sense to perform various forms of data
    processing close to the data instead of sending the raw data as it is. In some
    cases, you can even run workloads locally close to the data and serve users completely
    on the edge.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 很多设备分布在各个地方，将会产生不断的数据流。这些数据中的一部分将被送回后端进行处理、聚合和存储。然而，将各种数据处理方式靠近数据本地进行而不是将原始数据直接发送回来，是非常有意义的。在某些情况下，您甚至可以在本地近距离运行工作负载并完全在边缘服务用户。
- en: This is the promise of edge computing. Running Kubernetes at the edge allows
    organizations to bring the processing of data closer to the source of data generation,
    reducing the latency and bandwidth requirements of sending data to a centralized
    data center or cloud. This results in improved response times and real-time processing
    of data, making it an ideal solution for use cases such as industrial IoT, autonomous
    vehicles, and other real-time data processing applications.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是边缘计算的承诺。在边缘运行 Kubernetes 使组织能够将数据处理更接近数据生成源，从而减少将数据发送到集中数据中心或云的延迟和带宽需求。这将提高响应时间并实现数据的实时处理，使其成为工业物联网、自动驾驶车辆以及其他实时数据处理应用场景的理想解决方案。
- en: However, running Kubernetes at the edge comes with its own set of challenges.
    Edge devices are typically resource-constrained, making it necessary to optimize
    the deployment of Kubernetes for the edge environment. Organizations must also
    consider the network connectivity and reliability of edge devices, as well as
    the security and privacy implications of deploying a Kubernetes cluster at the
    edge.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在边缘运行 Kubernetes 会带来一系列挑战。边缘设备通常资源受限，因此必须优化 Kubernetes 的部署以适应边缘环境。组织还必须考虑边缘设备的网络连接性和可靠性，以及在边缘部署
    Kubernetes 集群的安全性和隐私问题。
- en: Projects like CNCF KubeEdge ([https://kubeedge.io](https://kubeedge.io)) can
    get you started.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 像 CNCF KubeEdge ([https://kubeedge.io](https://kubeedge.io)) 这样的项目可以帮助你入门。
- en: However, we will focus for the rest of this chapter on large-scale Kubernetes-based
    systems in the cloud.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本章接下来的部分将专注于云中基于 Kubernetes 的大规模系统。
- en: Building effective processes for large-scale Kubernetes deployments
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为大规模 Kubernetes 部署构建有效的流程
- en: To run multi-cluster Kubernetes systems in production, you must develop a set
    of effective processes and best practices that encompass every aspect of the operation.
    Here are some of the critical areas to address.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要在生产环境中运行多集群 Kubernetes 系统，你必须制定一套有效的流程和最佳实践，涵盖操作的各个方面。以下是一些需要解决的关键领域。
- en: The development lifecycle
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发生命周期
- en: The development lifecycle of a multi-cluster Kubernetes-based system in production
    can be a complex process, but it is possible to streamline it with the right approach.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，多集群 Kubernetes 系统的开发生命周期可能是一个复杂的过程，但通过正确的方法，可以将其简化。
- en: You should absolutely implement a CI/CD pipeline that automatically builds,
    tests, and deploys code changes. This pipeline should be integrated with version
    control systems such as Git, and it should also include automated testing to ensure
    code quality.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该绝对实施一个 CI/CD 流水线，自动构建、测试和部署代码变更。该流水线应与版本控制系统（如 Git）集成，并且还应包含自动化测试以确保代码质量。
- en: It’s important to manage the ownership and approval process for different areas
    of the code base.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 管理不同代码库区域的所有权和审批流程是非常重要的。
- en: Kubernetes namespaces can provide a convenient way to organize workloads and
    corresponding software assets and associate them with teams and stakeholders.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 命名空间可以提供一种方便的方式来组织工作负载及其相应的软件资产，并将它们与团队和相关方关联起来。
- en: You should have a complete track of changes, ongoing builds, and deployments,
    and the ability to freeze activity and roll back changes of each workload.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该对每个工作负载的变更、正在进行的构建和部署有完整的跟踪，并能够冻结活动并回滚变更。
- en: It’s also important to control the gradual deployment to different clusters
    and regions to avoid a situation where a bad change is deployed simultaneously
    across the board and brings the entire system down.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是控制逐步部署到不同集群和区域，以避免在所有系统中同时部署坏的变更，导致整个系统崩溃。
- en: Environments
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境
- en: Code review and careful incremental deployment while monitoring the outcome
    are required, but they are insufficient for large enterprise systems with multiple
    Kubernetes clusters. Some changes might display a negative impact only after running
    for a while or under certain conditions, which will escape the control mechanisms
    we mentioned earlier. The best practice is to have multiple runtime environments
    such as production, staging, and development. The exact division of environments
    can vary, but you typically need at least a production environment, which is the
    actual system that manages all the data and your users interact with, and a staging
    environment, which mimics the production system and where you can test changes
    and new versions without impacting your users and risking bringing the production
    environment down.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 代码审查和小心的增量部署，同时监控结果，是必要的，但对于拥有多个Kubernetes集群的大型企业系统来说，这些措施并不足够。某些变更可能只有在运行一段时间后或在特定条件下才会显示出负面影响，这将逃脱我们前面提到的控制机制。最佳实践是拥有多个运行时环境，例如生产环境、预发布环境和开发环境。环境的具体划分可以有所不同，但通常至少需要一个生产环境，这个环境是实际管理所有数据并且用户进行交互的系统，以及一个预发布环境，它模拟生产系统，你可以在其中测试变更和新版本，而不会影响用户并且避免将生产环境置于风险之中。
- en: Let’s consider some aspects of using multiple environments.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑使用多个环境的一些方面。
- en: Separated environments
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分离的环境
- en: It is critical that the staging environment can’t accidentally contaminate and
    impact the production environment. For example, if you run a stress test in staging
    and some workloads in the staging environment are misconfigured and hit production
    endpoints, you will have a very unpleasant time untangling the mess.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，预发布环境不能意外污染并影响生产环境。例如，如果你在预发布环境中运行压力测试，而某些工作负载在预发布环境中配置错误并访问生产端点，那么你将会经历一段非常不愉快的时间，试图解开这一混乱。
- en: Rigid network segmentation where the staging environment is unable to reach
    the production environment is a good first step. You will still need to be mindful
    of the interaction between staging and production through public endpoints. The
    staging workloads should not have secrets and identities that allow production
    access.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 严格的网络分段使得预发布环境无法访问生产环境，是一个很好的第一步。你仍然需要注意通过公共端点在预发布和生产环境之间的交互。预发布的工作负载不应包含能够访问生产环境的密钥和身份。
- en: Staging environment fidelity
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预发布环境的保真度
- en: The primary motivation for the staging environment is to test changes and interactions
    with other sub-systems before deploying a change to production. This means that
    the staging environment should mimic the production environment as much as possible.
    However, running an exact replica of the production environment is prohibitively
    expensive.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布环境的主要目的是在将变更部署到生产环境之前，测试与其他子系统的交互及变更。这意味着预发布环境应尽可能地模拟生产环境。然而，运行一个完全与生产环境相同的副本成本极高。
- en: The staging environment should be configured and set up using the same automated
    CI/CD pipeline that is able to deploy staging and production.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布环境应使用相同的自动化CI/CD流水线进行配置和设置，该流水线能够部署预发布和生产环境。
- en: Staging infrastructure and resources should also be provisioned using the same
    tools as production although there will typically be fewer resources allocated
    to staging.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布基础设施和资源也应使用与生产环境相同的工具进行配置，尽管通常分配给预发布的资源较少。
- en: You may want to be able to temporarily scale down or even completely shut down
    some parts of the staging environment and be able to bring them back up only when
    necessary for running large-scale tests in staging.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望能够暂时缩减，甚至完全关闭预发布环境中的某些部分，并且仅在需要进行大规模测试时再将其重新启动。
- en: Resource quotas
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源配额
- en: Resource quotas in staging and production can ensure that misconfiguration or
    even an attack doesn’t cause excessive resource usage.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 预发布和生产环境中的资源配额可以确保配置错误或甚至攻击不会导致资源使用过度。
- en: Promotion process
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升过程
- en: Once a change has been thoroughly tested in staging, there should be a clear
    promotion process for deploying it to production. The process may be different
    for different components depending on the scope and impact of the change. The
    promotion may be completely automatic where the CI/CD pipeline detects that staging
    tests are completed successfully and moves ahead with production deployment or
    involve extra steps and possibly another explicit deployment to production.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在暂存环境中彻底测试了更改，就应该有一个明确的推广过程将其部署到生产环境中。这个过程可能因更改的范围和影响而异。推广过程可能完全自动化，CI/CD管道检测到暂存测试成功完成并继续进行生产部署，或者涉及额外的步骤和可能另一个明确的生产部署。
- en: Permissions and access control
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权限和访问控制
- en: When you manage a constellation of Kubernetes clusters running on cloud infrastructure,
    you need to pay a lot of attention to the permission model and your access control.
    This builds on the previous best practices of the development lifecycle and environments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当您管理运行在云基础设施上的Kubernetes集群星座时，需要特别注意权限模型和访问控制。这是在之前开发生命周期和环境的最佳实践基础上构建的。
- en: The principle of least privilege
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小权限原则
- en: The principle of least privilege comes from the security field, but it is useful
    even beyond security for reliability, performance, and cost. Actors – either humans
    or workloads – should not have more permissions than necessary to accomplish their
    tasks. By reducing access to the bare minimum, you ensure that no accidental or
    malicious activity occurs for forbidden resources.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最小权限原则源自安全领域，但即使超越安全性，也对可靠性、性能和成本有用。人员或工作负载不应该比完成任务所需的权限更多。通过将访问权限降至最低限度，确保不会发生禁止资源的意外或恶意活动。
- en: Also, when investigating an incident, it automatically narrows down the possible
    culprits to those who had the permissions to act on the misconfigured resource
    or take the invalid action.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在调查事故时，它会自动将可能的罪魁祸首缩小到那些具有操作不当配置资源或执行无效操作权限的人员。
- en: If you follow the GitOps model, it is possible to create a workflow where every
    change to your clusters and your infrastructure is done by CI/CD and dedicated
    tooling. Human engineers have only read-only access. Some special exceptions can
    be made (see the *Break glass* section in this chapter).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遵循GitOps模型，可以创建一种工作流程，其中对集群和基础设施的每一次更改都由CI/CD和专用工具完成。人类工程师只有只读访问权限。在本章的*打破玻璃*部分可以做一些特殊例外。
- en: Assign permissions to groups
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将权限分配给组
- en: It is highly recommended to assign permissions to groups or teams as opposed
    to individuals. Even if just a single person is currently carrying out a task
    that requires some set of permissions, you should define a group where this person
    is the single member. That will make it easier to add other people later or replace
    the person.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议将权限分配给组或团队，而不是个人。即使只有一个人当前正在执行需要某些权限集的任务，也应定义一个该人员是唯一成员的组。这将使以后添加其他人员或替换该人员更加容易。
- en: Fine-tune your permission model
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化你的权限模型
- en: However, sometimes too strict a permission model can be detrimental. You’ll
    have to maintain a very fine-grained set of permissions to a large set of resources.
    Whenever the slightest change occurs such that another action is needed on some
    resource, you’ll have to modify the permissions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时候过于严格的权限模型可能是有害的。您将不得不对大量资源维护非常细粒度的权限设置。每当发生最轻微的更改以至于需要在某些资源上执行其他操作时，您将不得不修改权限。
- en: Find the golden path between granting super-admin permissions to everyone and
    painstakingly creating hundreds and thousands of roles for each and every resource.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 找到在授予每个人超级管理员权限和为每个资源创建数百甚至数千个角色之间的黄金平衡点。
- en: In particular, consider relaxing the permission model in the development environment
    and potentially in the staging environment too. This is where a lot of experiments
    take place and where you discover what actions you need to perform and what permissions
    are actually necessary and then you can tweak your permissions model before deploying
    to production.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是考虑在开发环境甚至是暂存环境中放宽权限模型。这是许多实验进行的地方，您可以在这里发现需要执行的操作和实际所需的权限，然后在部署到生产环境之前调整您的权限模型。
- en: Break glass
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打破玻璃
- en: Sometimes, your CI/CD pipeline itself will be broken or, due to incomplete coverage,
    some resources may have been provisioned manually. In these cases, human engineers
    must intervene and, so to speak, “*break the glass*” and take direct action against
    Kubernetes or cloud infrastructure.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，CI/CD 流水线本身可能会出现故障，或者由于覆盖不全，一些资源可能是手动配置的。在这些情况下，人工工程师必须介入，可以说是“*打破玻璃*”，直接对
    Kubernetes 或云基础设施进行操作。
- en: It is recommended to have a formal process of acquiring **break glass** access,
    who is allowed to have it, how long it lasts, and record who had it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 建议制定正式的流程来获取**紧急访问**权限，明确谁可以获得该权限、权限持续时间以及记录谁曾经拥有该权限。
- en: This brings us to the next section about observability.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了我们下一部分的内容——可观察性。
- en: Observability
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可观察性
- en: A comprehensive observability stack is an absolute must. Complex systems composed
    of multiple Kubernetes clusters can be reasoned about and understood theoretically.
    You must have a complete record of events from cloud providers, Kubernetes itself,
    and workloads. Your CI/CD pipeline and other tools must also be fully observable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个全面的可观察性栈是绝对必不可少的。由多个 Kubernetes 集群组成的复杂系统可以从理论上推理并理解。你必须拥有来自云服务提供商、Kubernetes
    本身以及工作负载的完整事件记录。你的 CI/CD 流水线及其他工具也必须具备完全的可观察性。
- en: Let’s look at some elements of multi-cluster observability.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看多集群可观察性的一些元素。
- en: One-stop shop observability
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一站式可观察性
- en: Cloud providers and Kubernetes itself provide a lot of observability in the
    form of logs and metrics out of the box. However, those are typically organized
    at the cluster level. If you are dealing with some widespread issue across multiple
    clusters, it is very difficult, and at a certain scale impossible, to go into
    each individual cluster, extract the observability data, and try to make sense
    of it. You must ship all observability data into a single centralized system where
    it can be aggregated, summarized, and be ready for multi-cluster analysis and
    response.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商和 Kubernetes 本身提供了大量的日志和度量指标作为开箱即用的可观察性。然而，这些通常是按集群级别组织的。如果你正在处理跨多个集群的广泛问题，那么进入每个集群、提取可观察性数据并尝试理解它将变得非常困难，甚至在某些规模下是不可能的。你必须将所有可观察性数据传输到一个单一的集中系统中，在那里它可以被汇总、总结，并准备好进行多集群分析和响应。
- en: Troubleshooting your observability stack
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排查你的可观察性栈
- en: Your observability stack is an indispensable component of your system. If it
    is down or degraded, you may be flying blind and unable to effectively respond
    to issues. Moreover, a cross-cluster issue may impact your observability stack
    as it impacts your entire system. Consider this scenario very carefully and make
    sure you have plenty of redundancies and observability alternatives if your primary
    observability stack is not up to the task temporarily. For example, you may rely
    on in-cluster observability solutions if your centralized observability stack
    is compromised. If you want complete redundancy, you may have a parallel observability
    stack on two cloud providers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你的可观察性栈是你系统中不可或缺的组成部分。如果它出现故障或性能下降，你可能会处于“盲飞”状态，无法有效地应对问题。而且，跨集群问题可能会影响你的可观察性栈，进而影响整个系统。非常仔细地考虑这个场景，确保在你的主要可观察性栈暂时无法应对任务时，拥有足够的冗余和可观察性替代方案。例如，如果你的集中式可观察性栈出现问题，你可以依赖集群内部的可观察性解决方案。如果你想要完全冗余，可以在两个云服务提供商上部署并行的可观察性栈。
- en: Consider testing these harsh scenarios of CI/CD pipeline and observability stack
    outages and see how you operate.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑测试这些严苛场景：CI/CD 流水线和可观察性栈出现故障时，看看你的操作流程如何。
- en: Let’s look more specifically into different types of infrastructure and how
    to handle them at scale.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更具体地看看不同类型的基础设施，以及如何在大规模环境中处理它们。
- en: Handling infrastructure at scale
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模处理基础设施
- en: One of the most demanding tasks when running large-scale multi-cluster Kubernetes
    in the cloud is dealing with the cloud infrastructure. In some respects, it is
    much better than being responsible for low-level compute, network, and storage
    infrastructure. However, you lose a lot of control, and troubleshooting issues
    is challenging.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在云端运行大规模多集群 Kubernetes 的任务中，处理云基础设施是最具挑战性的任务之一。从某些方面来说，它比负责底层计算、网络和存储基础设施要好得多。然而，你会失去很多控制权，排查问题变得非常具有挑战性。
- en: Before diving into each category of infrastructure, let’s look at some general
    cloud-level considerations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入每个基础设施类别之前，先来看一些通用的云级别考虑事项。
- en: Cloud-level considerations
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云级别考虑事项
- en: In the cloud, you organize your resources in entities such as AWS accounts,
    GCP projects, and Azure subscriptions. An organization may have multiple such
    groups, and each one has its own limits and quotas. For the sake of brevity, let’s
    call them accounts. Enterprise organizations’ infrastructure requirements will
    exceed the capacity of a single account. It’s critical to decide how to break
    down your infrastructure into different accounts. One good heuristic is to separate
    environments – production, staging, and development – into separate accounts.
    Account-level isolation is beneficial for these environments. However, this may
    not be sufficient and within a single environment, you might need more resources
    than can fit in one account.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中，你会将资源组织为如AWS账户、GCP项目和Azure订阅这样的实体。一个组织可能拥有多个这样的组，每个组都有自己的限制和配额。为了简便起见，我们称之为账户。企业组织的基础设施需求将超过单一账户的容量。决定如何将基础设施拆分为不同账户至关重要。一种好的启发式方法是将不同的环境——生产、预发布和开发——拆分到不同的账户中。账户级别的隔离对于这些环境是有益的。然而，这可能还不够，在单一环境中，你可能需要更多的资源，单一账户无法容纳。
- en: Having a solid account management strategy is key. Accounts can also be a boundary
    of access control as even account administrators can’t access other accounts.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个健全的账户管理策略是关键。账户也可以作为访问控制的边界，因为即使是账户管理员也无法访问其他账户。
- en: Consult with your security team about security-motivated account breakdowns.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请与你的安全团队协商关于基于安全考虑的账户划分。
- en: Another important aspect is the breakdown of regions. If you manage infrastructure
    across multiple regions and workloads in these regions communicate with each other,
    then this has severe latency and cost implications. In particular, cross-region
    egress is typically not free.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要方面是区域划分。如果你在多个区域管理基础设施，并且这些区域中的工作负载需要相互通信，那么这将带来严重的延迟和成本影响。特别是，跨区域出口通常不是免费的。
- en: Let’s look at each category of infrastructure.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下每种基础设施类别。
- en: Compute
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: Compute infrastructure for Managed Kubernetes includes the Kubernetes clusters
    themselves and their worker nodes. The worker nodes are typically grouped into
    node pools, which is not a Kubernetes concept. How you break down your system
    into Kubernetes clusters and what types of node pools exist in each cluster will
    greatly impact your ability to manage the system at scale.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 管理Kubernetes的计算基础设施包括Kubernetes集群本身及其工作节点。工作节点通常被分组到节点池中，这不是Kubernetes的概念。如何将系统拆分为Kubernetes集群以及每个集群中存在的节点池类型，将极大地影响你在规模化管理系统时的能力。
- en: Ideally, you can treat clusters like cattle, provision identical clusters, and
    easily add or remove clusters in different locations. Each cluster will have the
    same node pools.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你可以像管理牲畜一样管理集群，配置相同的集群，并且可以轻松地在不同位置添加或移除集群。每个集群将拥有相同的节点池。
- en: This uniform and consistent organization of clusters is not always possible.
    There are sometimes reasons to have different clusters for particular purposes.
    You should still strive for a small number of cluster types that can be replicated
    easily.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种统一一致的集群组织方式并非总是可能的。有时为了特定目的，可能需要不同的集群。你仍然应该尽量保持少数几种集群类型，以便轻松复制。
- en: Design your cluster breakdown
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计你的集群划分
- en: Clusters in the cloud typically allocate private IP addresses to nodes and pods
    from a virtual network that resides in one region. Yes, it’s possible to have
    wide clusters that cross regions, but this is the exception and not the rule.
    It is highly recommended to manage clusters as cattle if possible and automatically
    provision clusters across all regions of operations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 云中的集群通常从一个区域的虚拟网络中为节点和Pod分配私有IP地址。是的，跨区域的大型集群是可能的，但这是例外而非常规。强烈建议在可能的情况下将集群作为牲畜来管理，并在所有操作区域自动配置集群。
- en: Design your node pool breakdown
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计你的节点池划分
- en: Node pools are groups of nodes that have the same instance type. They can typically
    autoscale to accommodate the needs of the cluster with the help of the cluster
    autoscaler. How you choose what node pools to provision in your clusters is a
    fundamental decision that impacts performance, cost, and operational complexity.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 节点池是具有相同实例类型的节点组。它们通常可以通过集群自动扩展器自动扩展，以满足集群的需求。选择在集群中配置哪些节点池是一个基础性决策，它会影响性能、成本和操作复杂度。
- en: We will dive into a deeper discussion on this later in the chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后深入讨论这一点。
- en: Networking
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络
- en: Networking is a very dynamic area of infrastructure. There are many degrees
    of freedom. The interplay between latency and bandwidth has nuances. Workloads
    can’t request a certain amount of bandwidth or guaranteed latency. In addition,
    there are a lot of external factors that impact the connectivity, reachability,
    and performance of your network. Let’s look at some of the important topics we
    have to consider and plan for.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是基础设施中一个非常动态的领域。它有许多自由度。延迟与带宽之间的相互作用有其细微之处。工作负载不能请求一定的带宽或保证的延迟。此外，还有许多外部因素会影响网络的连接性、可达性和性能。让我们来看看一些我们必须考虑和规划的重要议题。
- en: IP address space management
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IP地址空间管理
- en: 'When you run a multi-cluster Kubernetes-based system, every pod in a cluster
    gets a unique private IP address. However, if you want to connect multiple clusters
    and have workloads in one cluster reach workloads in other clusters via their
    private IP address, then two conditions must exist:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行一个基于多集群的Kubernetes系统时，集群中的每个pod都会获得一个唯一的私有IP地址。然而，如果你想连接多个集群，并且让一个集群中的工作负载通过私有IP地址访问其他集群中的工作负载，那么必须满足以下两个条件：
- en: The networks of the different clusters must be peered together.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同集群的网络必须进行对等连接。
- en: The private IP address of pods must be unique across all clusters.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pods的私有IP地址在所有集群中必须唯一。
- en: This requires centralized management of the private IP address space and carefully
    assigning IP ranges to different clusters.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要集中管理私有IP地址空间，并且要小心地为不同的集群分配IP范围。
- en: Cloud providers differ a little in their approach to assigning IP address ranges
    to clusters. AKS requires that each cluster belongs to a VNet with its own IP
    address range and then subnets are assigned to nodes and pods with sub-ranges
    from the VNet IP address range. GKE comes with a default network that has no IP
    address range of its own. Clusters are provisioned with subnets that have their
    own IP address ranges.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商在为集群分配IP地址范围的方式上有所不同。AKS要求每个集群都属于一个具有自己IP地址范围的VNet，然后子网被分配给节点和pods，子网的IP地址范围来自VNet的IP地址范围。GKE则提供一个没有自己IP地址范围的默认网络。集群是通过具有自己IP地址范围的子网来配置的。
- en: In addition, services require their own IP addresses too, and possibly some
    other components.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，服务也需要自己的IP地址，以及可能的其他组件。
- en: 'The entire private IPv4 address space consists of several blocks:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 整个私有IPv4地址空间由多个块组成：
- en: '`10.0.0.0/8` (class A)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`10.0.0.0/8`（A类）'
- en: '`172.16.0.0/12` (class B)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`172.16.0.0/12`（B类）'
- en: '`192.168.0.0/8` (class C)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`192.168.0.0/8`（C类）'
- en: At scale, the most important one is `10.0.0.0/8`, which consists of 2^24 IP
    addresses, which is more than 16 million addresses. That is a lot of IP addresses,
    but if you don’t plan carefully, you may cause fragmentation and run out of large
    blocks even if you have plenty of unused IP addresses.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模部署中，最重要的是`10.0.0.0/8`，它包含了2^24个IP地址，总数超过1600万个地址。这是一个非常庞大的IP地址范围，但如果不仔细规划，可能会导致碎片化，即使有很多未使用的IP地址，也可能会耗尽大块地址空间。
- en: 'Here are some best practices for managing the IP address space:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是管理IP地址空间的一些最佳实践：
- en: Allocate CIDR blocks to pods, nodes, and services that will be sufficient and
    utilized without too much space.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为pods、nodes和services分配足够并且不会浪费过多空间的CIDR块。
- en: Be aware of Kubernetes and cloud provider limits in terms of the number of supported
    nodes and pods.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Kubernetes和云服务提供商在支持的节点和pods数量方面的限制。
- en: Consider network peering and service meshes spanning clusters.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑跨集群的网络对等连接和服务网格。
- en: Make sure you use non-overlapping CIDR blocks for connected clusters.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保连接的集群使用不重叠的CIDR块。
- en: Use proper tools to manage the address space.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适当的工具来管理地址空间。
- en: Consider the impact of pod density on IP address space (e.g., on AKS, IP addresses
    are pre-allocated to the max number of pods on a node even if not utilized).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑pod密度对IP地址空间的影响（例如，在AKS上，即使没有使用，IP地址也会预先分配给节点上pod的最大数量）。
- en: Be aware of limits such as the maximum number of IP addresses available in a
    region.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意一些限制，例如一个区域中可用的最大IP地址数量。
- en: Network topology
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络拓扑
- en: All cloud providers offer a virtual network or VPC concept. All cloud providers
    also have the concept of a region, which is a geographical area where cloud providers
    host resources. Specifically, virtual networks are always confined to a single
    region. Since a Kubernetes cluster in the cloud is associated with a virtual network,
    it follows that a single Kubernetes cluster can’t span more than one region. This
    has implications for availability and reliability. If you want to survive a regional
    outage, you need to run each critical workload across multiple clusters in different
    regions. Moreover, all these clusters typically need to be connected to each other.
    We will discuss this more in the *Cross-cluster communication* section that follows.
    However, as far as network topology goes, it may be better to have multiple clusters
    in the same region share the same virtual network.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所有云提供商都提供虚拟网络或 VPC 概念。所有云提供商也都有地区的概念，地区是云提供商托管资源的地理区域。具体来说，虚拟网络总是局限于一个单一的地区。由于云中的
    Kubernetes 集群与虚拟网络相关联，因此一个单一的 Kubernetes 集群不能跨越多个地区。这对可用性和可靠性有影响。如果你想在地区性故障中存活下来，你需要在不同地区的多个集群中运行每个关键工作负载。此外，所有这些集群通常需要相互连接。我们将在接下来的
    *跨集群通信* 部分中进一步讨论这个问题。然而，就网络拓扑而言，将同一区域内的多个集群共享同一虚拟网络可能会更好。
- en: Network segmentation
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络分段
- en: Network segmentation is about dividing a network into smaller subnets. In the
    context of Kubernetes, the most important subnets are the subnets for nodes, pods,
    and services. In some cases, the nodes and pods share the same subnet and in other
    cases, there are separate subnets for nodes and pods. Regardless, you need to
    plan and understand how many nodes and pods your cluster can accommodate and size
    your cluster subnets accordingly.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 网络分段是将一个网络划分为更小的子网。在 Kubernetes 的上下文中，最重要的子网是节点、Pod 和服务的子网。在某些情况下，节点和 Pod 共享同一子网，而在其他情况下，节点和
    Pod 会有单独的子网。无论如何，你需要规划并理解你的集群能够容纳多少个节点和 Pod，并据此调整集群子网的规模。
- en: Cross-cluster communication
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨集群通信
- en: 'When running multiple Kubernetes clusters, it is often beneficial to consider
    groups of clusters as a single conceptual cluster. This means that pods in any
    cluster can directly reach pods in other clusters via their private IP address.
    This flat IP address model is an extension of the standard Kubernetes networking
    model within a single cluster to multiple clusters. This requires two elements
    we discussed earlier:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行多个 Kubernetes 集群时，考虑将集群组视为一个单一的概念集群通常是有益的。这意味着，任何集群中的 Pod 都可以通过它们的私有 IP 地址直接访问其他集群中的
    Pod。这种扁平化的 IP 地址模型是将标准 Kubernetes 网络模型从单一集群扩展到多个集群。这需要我们之前讨论过的两个要素：
- en: Non-conflicting IP address ranges for pods across all connected clusters.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有连接的集群之间的 Pod 应该使用不冲突的 IP 地址范围。
- en: Network peering between clusters.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群之间的网络对等连接。
- en: The network peering requirement might be tedious as clusters come and go. Having
    a single regional virtual network reduces the overhead and allows peering just
    the regional virtual networks. However, if you run a lot of clusters in the same
    region sharing the same virtual network, you might run into cloud provider limits
    that will stunt your growth. For example, on Azure, a VNet can have at most 64K
    unique IP addresses.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 网络对等连接要求可能会很繁琐，因为集群会不断地变动。拥有一个单一的区域虚拟网络可以减少开销，并且只需要对区域虚拟网络进行对等连接。然而，如果你在同一地区运行大量共享同一虚拟网络的集群，你可能会遇到云服务商的限制，这会限制你的发展。例如，在
    Azure 上，一个虚拟网络最多可以有 64K 个唯一的 IP 地址。
- en: Cross-cloud communication
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨云通信
- en: If your system spans multiple cloud providers, you need to consider how to connect
    your Kubernetes clusters across cloud providers. There are several ways, with
    different pros and cons.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的系统跨多个云提供商，你需要考虑如何在不同云提供商之间连接你的 Kubernetes 集群。有几种方法，各有优缺点。
- en: First, you may decide to avoid direct communication between clusters on different
    cloud providers. If Kubernetes clusters deployed on different clouds need to communicate
    with each other, they can do so through public APIs. This approach is simple but
    eliminates the idea of a unified conceptual cluster where pods can communicate
    directly with each other regardless of where they are.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可能决定避免在不同云提供商上的集群之间进行直接通信。如果部署在不同云上的 Kubernetes 集群需要相互通信，它们可以通过公共 API 实现。这种方法简单，但会排除统一概念集群的想法，即
    Pod 可以直接相互通信，无论它们位于何处。
- en: A site-to-site VPN is a communication method where different cloud providers
    can connect systems via BGP and establish a VPN connection to networks managed
    by another cloud provider via a VPN gateway that sits in front of the virtual
    networks. This establishes a secure channel; however, a VPN gateway is not trivial
    to set up and incurs a significant overhead.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点VPN是一种通信方法，通过BGP连接不同的云提供商系统，并通过位于虚拟网络前面的VPN网关与另一云提供商管理的网络建立VPN连接。这建立了一个安全通道；然而，设置VPN网关并不是件小事，并会产生显著的开销。
- en: Direct connect (AKA direct peering) is another option that requires installing
    a router in a cloud provider point of presence. This method allows connecting
    Kubernetes clusters running in private data centers to clusters in the cloud.
    In addition, the performance is much better because there is no VPN gateway in
    the middle. The downside is that it is quite complicated to set up and you might
    have to comply with various requirements. It’s a good option for organizations
    with deep low-level networking expertise.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 直接连接（也称为直接对等连接）是另一种选项，需要在云提供商的点对点位置安装路由器。该方法允许将运行在私有数据中心中的Kubernetes集群连接到云中的集群。此外，性能要好得多，因为中间没有VPN网关。缺点是设置相当复杂，您可能需要遵守各种要求。对于具有深入低级网络专业知识的组织来说，这是一个不错的选择。
- en: Carrier or partner peering is similar to direct connect; however, you take advantage
    of the expertise of an established third party that specializes in providing this
    service and already has an established relationship and is certified with the
    cloud provider. You will have to pay for the service, of course.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 运营商或合作伙伴对等连接类似于直连；但是，您可以利用专业提供此服务的第三方的专业知识，该第三方已经与云提供商建立了关系并获得了认证。当然，您需要为此服务付费。
- en: Cross-cluster service meshes
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨集群服务网格
- en: Service meshes bring tremendous value to Kubernetes, as we discussed in *Chapter
    14*, *Utilizing Service Meshes*. When running multiple Kubernetes clusters in
    production, it is arguably even more important to connect all the clusters via
    a service mesh. The advanced capabilities and policies of a service mesh can be
    applied and manage connectivity and routing across all clusters.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第14章* *利用服务网格*中所讨论的那样，服务网格为Kubernetes带来了巨大的价值。在生产环境中运行多个Kubernetes集群时，通过服务网格连接所有集群可能更加重要。服务网格的高级功能和策略可以应用于管理所有集群之间的连接和路由。
- en: 'There are two approaches to consider here:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在此需要考虑两种方法：
- en: A single fully connected service mesh.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个完全连接的服务网格。
- en: Divide your clusters into multiple meshes.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的集群划分为多个网格。
- en: The single fully connected single mesh aligns conceptually with the single conceptual
    Kubernetes cluster approach. Everything is straightforward. New clusters just
    join the mesh, and the mesh is configured to allow every pod to talk to every
    other pod (as long as the routing policies allow it).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 单个完全连接的单一网格在概念上与单一概念的Kubernetes集群方法一致。一切都很简单。新集群只需加入网格，并配置网格以允许每个Pod与其他所有Pod通信（只要路由策略允许）。
- en: However, you might eventually run into scalability barriers as a single mesh
    means that the mesh control plane needs to handle policies for all workloads in
    all the clusters, and updating sidecars for all pods can put a lot of burden on
    your clusters.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单个网格可能会遇到可扩展性障碍，因为网格控制平面需要处理所有集群中所有工作负载的策略，并且为所有Pod更新Sidecar可能会给您的集群带来很大负担。
- en: An alternative approach is to have multiple independent meshes. Pods in clusters
    that belong to a particular mesh can directly talk to pods in all other clusters
    in the same service mesh but must go through public endpoints to access clusters
    in other service meshes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是拥有多个独立的网格。属于特定网格的集群中的Pod可以直接与同一服务网格中所有其他集群中的Pod通信，但必须通过公共端点访问其他服务网格中的集群。
- en: The multi-mesh approach is more scalable but much more complicated. You need
    to consider how to divide your system into different service meshes and when new
    clusters join or leave the system, and how it impacts your overall architecture.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 多网格方法更具可扩展性，但更为复杂。您需要考虑如何将系统划分为不同的服务网格，以及新集群加入或离开系统时对整体架构的影响。
- en: The private IP address space management in the multi-service mesh case can be
    more nuanced too where different service meshes can have conflicting IP addresses.
    This means that you can manage the IP address space for each mesh separately.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在多服务网格的情况下，私有IP地址空间管理可能会更加复杂，因为不同的服务网格可能会有冲突的IP地址。这意味着你可以为每个网格单独管理IP地址空间。
- en: Service meshes offer another interesting solution to the cross-cluster connectivity
    story, which is the east-west gateway. With the east-west gateway approach, workloads
    in different clusters communicate indirectly through dedicated gateways in each
    cluster. This means that the private IP addresses of each cluster are unknown
    and there is an extra hop for each cross-cluster communication.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格提供了另一种有趣的解决方案来处理跨集群的连接问题，那就是东西向网关。通过东西向网关方法，不同集群中的工作负载通过每个集群中的专用网关间接通信。这意味着每个集群的私有IP地址是未知的，并且每次跨集群通信都会多出一个跳跃。
- en: Managing egress at scale
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大规模管理出口流量
- en: Some systems need to access external systems aggressively. Maybe you frequently
    fetch data from external systems or maybe the purpose of your system is to manage
    some external systems via APIs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统需要积极地访问外部系统。也许你经常从外部系统获取数据，或者你的系统的目的是通过API管理一些外部系统。
- en: There may be unique issues for egress traffic that require special attention.
    Some third-party organizations or even countries may have policies that block
    or throttle traffic coming from certain geographical areas or specific IP CIDR
    blocks. For example, China and its great firewall are famous for blocking and
    censoring a vast number of companies, such as Google and Facebook. If you run
    on GCP and need to access China, it might be a serious issue.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于出口流量，可能会有一些特殊问题需要关注。一些第三方组织或甚至某些国家可能会有政策，阻止或限制来自特定地理区域或特定IP地址范围（CIDR块）的流量。例如，中国及其“长城防火墙”因屏蔽和审查大量公司（如谷歌和Facebook）而闻名。如果你在GCP上运行并且需要访问中国，这可能会是一个严重的问题。
- en: Beyond total blocking, there may be limits and throttling in place if you try
    to access some third-party APIs at scale.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了完全阻止之外，如果你试图大规模访问某些第三方API，可能还会有流量限制和限速措施。
- en: If you persist in accessing those third-party APIs, you could even be reported,
    and your cloud provider could potentially impose various sanctions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你持续访问这些第三方API，你甚至可能会被举报，云服务提供商也有可能对你采取各种制裁措施。
- en: Let’s consider some solutions to deal with these real-world problems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些解决方案来应对这些现实世界中的问题。
- en: If your current cloud provider is not allowed to access your target destination,
    then you must establish an egress presence outside your cloud provider. This can
    be on another cloud provider or via an intermediate organization in good standing.
    This proxy approach can take many shapes, which is beyond the scope of this section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你当前的云服务提供商无法访问你的目标地址，那么你必须在云服务提供商之外建立一个出口连接。这可以是在另一个云提供商上，或通过一个信誉良好的中介组织。这种代理方法有许多形式，超出了本节的讨论范围。
- en: If you are getting throttled, then the issue may be that you send too many requests
    from the same source IP address. A good solution here is to create a pool of egress
    nodes with different public IP addresses and distribute your requests through
    multiple different IP addresses. It can also help if you rotate your public IP
    addresses periodically, which is pretty easy in the cloud by just re-creating
    instances, which receive new public IP addresses.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到限流问题，那么可能是因为你从同一个源IP地址发送了过多请求。一个有效的解决方案是创建一个具有不同公网IP地址的出口节点池，并通过多个不同的IP地址分发请求。如果定期轮换公网IP地址也会有所帮助，在云平台中通过重新创建实例就可以很容易地获得新的公网IP地址。
- en: The opposite issue is if you have an agreement with some third-party company
    and they specifically allow traffic from your organization by whitelisting some
    IP addresses you provide. In this case, you need to manage static public IP addresses
    that don’t change and ensure that all requests to that third-party organization
    go out through the whitelisted IP addresses.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种问题是，如果你与某个第三方公司有协议，并且他们通过白名单允许你提供的某些IP地址访问流量，那么你需要管理那些不会变化的静态公网IP地址，并确保所有发往该第三方组织的请求都通过这些白名单IP地址发送。
- en: Finally, to address the risk of being reported and flagged by your cloud provider,
    you may need to isolate your egress access to a separate account. Most cloud provider
    sanctions are at the account level. If your egress account is disabled by your
    cloud provider, at least it will not bring down the entire enterprise.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了应对被云服务提供商报告并标记的风险，你可能需要将出口访问隔离到一个单独的账户。大多数云服务提供商的制裁是基于账户级别的。如果你的出口账户被云服务提供商禁用，至少不会影响整个企业。
- en: Managing the DNS at the cluster level
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群级别的 DNS 管理
- en: Large-scale clusters with lots of pods and services may put a high load on CoreDNS,
    which is the internal DNS server of Kubernetes. It’s important to ensure sufficient
    DNS capacity since most internal communication between workloads in the cluster
    uses DNS names for addressing and not direct IP addresses.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模的集群，包含大量的 pod 和服务，可能会给 CoreDNS（Kubernetes 的内部 DNS 服务器）带来较高的负载。确保足够的 DNS 容量非常重要，因为集群内大多数工作负载之间的内部通信是通过
    DNS 名称进行寻址，而不是直接使用 IP 地址。
- en: It is recommended to use DNS autoscaling, which is often not enabled by default.
    See [https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)
    for more details.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用 DNS 自动扩展，默认情况下通常不会启用。详情请参见 [https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)。
- en: Storage
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: Storage is arguably the most critical element of your infrastructure. This is
    where your persistent data lives, which is the long-term memory of organizations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 存储可以说是你基础设施中最为关键的元素。这里存储着你的持久化数据，它是组织的长期记忆。
- en: Choose the right storage solutions
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的存储解决方案
- en: There are many storage solutions available for Kubernetes clusters in the cloud,
    such as cloud-native blob storage, managed storage services, managed databases,
    and managed file systems. You should develop a deep understanding of the performance,
    durability, and cost of each storage solution and match them against your storage
    use cases.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 云中有许多适用于 Kubernetes 集群的存储解决方案，如云原生对象存储、托管存储服务、托管数据库和托管文件系统。你应该深入了解每种存储解决方案的性能、耐用性和成本，并根据存储使用场景进行匹配。
- en: Your baseline should always be cloud-native blob storage (AKA buckets) like
    AWS S3, GCP Google Cloud Storage, or Azure Blob Storage. It’s hard to imagine
    a large-scale Managed Kubernetes enterprise that doesn’t use buckets.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你的基础设施应该始终以云原生的对象存储（即桶存储），如 AWS S3、GCP Google Cloud Storage 或 Azure Blob Storage
    为基础。很难想象一个大型的托管 Kubernetes 企业不使用桶存储。
- en: Then, consider more structured or high-level storage solutions. If you aim to
    stay cloud-agnostic, you may ignore cloud-based managed storage solutions and
    deploy your own solutions, as we saw in *Chapter 6*, *Managing Storage*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，考虑更为结构化或高层次的存储解决方案。如果你希望保持云中立，你可以忽略基于云的托管存储解决方案，部署你自己的解决方案，就像我们在*第 6 章*《管理存储》中所看到的那样。
- en: At an enterprise scale, it may be worthwhile considering different levels of
    access speed and cost for data at different levels of importance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业规模下，考虑不同重要性层级的数据存取速度和成本可能是值得的。
- en: Data backup and recovery
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据备份与恢复
- en: Plan for data backup and recovery. Your data is valuable. Data backup and recovery
    are crucial for production environments. Consider implementing data backup and
    recovery processes that are reliable and scalable, and make sure they are regularly
    tested and updated.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 规划数据备份与恢复。你的数据是非常宝贵的，数据备份与恢复对于生产环境至关重要。考虑实施可靠且可扩展的数据备份与恢复流程，并确保它们定期经过测试与更新。
- en: You should also consider data retention policies and not automatically assume
    that all data must be kept forever.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该考虑数据保留策略，并且不要自动假设所有数据都必须永久保存。
- en: Of course, to comply with data privacy laws and regulations like the GDPR, you
    will need to build the ability to selectively delete data too.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了遵守数据隐私法和类似 GDPR 的法规，你还需要具备选择性删除数据的能力。
- en: Storage monitoring
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储监控
- en: Set up storage monitoring. Period. Monitoring storage performance, usage, and
    capacity is essential for identifying and resolving issues before they impact
    the availability or performance of your applications. Set up monitoring and alerting
    for storage utilization, latency, and throughput. This is important for managed
    storage, but also for node storage where logs can easily accumulate and render
    a node un-operational.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 设置存储监控。仅此而已。监控存储性能、使用情况和容量对于在问题影响应用程序的可用性或性能之前识别并解决问题至关重要。设置监控和警报，以便及时了解存储使用、延迟和吞吐量。这对托管存储非常重要，也适用于节点存储，因为日志容易积累并导致节点无法正常工作。
- en: Data security
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据安全
- en: Implement data security measures. Protecting sensitive data and ensuring compliance
    with data protection regulations is critical for production environments. Implement
    access controls, encryption, and data security policies to safeguard your data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 实施数据安全措施。保护敏感数据并确保符合数据保护法规对于生产环境至关重要。实施访问控制、加密和数据安全策略以保护数据。
- en: Optimize storage usage
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化存储使用
- en: Kubernetes clusters in the cloud can be expensive, and storage costs can add
    up quickly. Optimize your storage usage by deleting unused data, using data compression
    or deduplication, and setting up storage tiering.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 云中的Kubernetes集群可能会很昂贵，存储成本也可能迅速累积。通过删除未使用的数据、使用数据压缩或去重，并设置存储分层，优化存储使用。
- en: Test and validate storage performance
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试并验证存储性能
- en: Before deploying applications in a production environment, test and validate
    the performance of your storage solution to ensure it meets the performance requirements
    of your workloads.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中部署应用程序之前，请测试并验证您的存储解决方案的性能，以确保它满足工作负载的性能要求。
- en: By considering these factors and implementing best practices for managing storage
    in production for Kubernetes clusters in the cloud, you can ensure reliable and
    scalable storage performance for your applications.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑这些因素并实施最佳实践来管理云中Kubernetes集群的生产环境存储，您可以确保应用程序的存储性能既可靠又可扩展。
- en: Now that we have covered a lot of guidelines and best practices for managing
    cloud infrastructure at scale for Kubernetes, let’s shine a spotlight on the management
    of clusters and node pools, which is at the heart of managing multi-cluster Kubernetes
    in production.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了大规模管理Kubernetes云基础设施的许多指南和最佳实践，让我们把重点放在集群和节点池的管理上，这是在生产环境中管理多集群Kubernetes的核心。
- en: Managing clusters and node pools
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理集群和节点池
- en: Managing your clusters and node pools is the top infrastructure administration
    activity for a large-scale Kubernetes-based enterprise. In this section, we will
    look at several crucial aspects, including provisioning, bin packing and utilization,
    upgrades, troubleshooting, and cost management.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 管理您的集群和节点池是大规模Kubernetes企业基础设施管理的核心活动。在本节中，我们将讨论几个重要方面，包括配置、资源利用率、升级、故障排除和成本管理。
- en: Provisioning managed clusters and node pools
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置托管集群和节点池
- en: There are different methods for provisioning clusters and node pools. You should
    choose the method that is best for your use case wisely because failure here can
    result in devastating outages. Let’s review some options. All cloud providers
    offer cluster and node pool provisioning via APIs, CLIs, and UIs. I highly recommend
    avoiding directly using any of these methods and instead using GitOps-based declarative
    approaches. Here are some solid options to consider.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 配置集群和节点池有不同的方法。您应根据实际情况明智地选择最适合的方法，因为在这一环节的失败可能导致灾难性的停机。让我们来回顾一下几种选项。所有云服务提供商都通过API、CLI和UI提供集群和节点池的配置。我强烈建议避免直接使用这些方法，而应使用基于GitOps的声明式方法。这里有一些值得考虑的可靠选项。
- en: The Cluster API
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群API
- en: The Cluster API is an open-source project from the Cluster Lifecycle SIG. Its
    goal is to make provisioning, upgrading, and operating multiple Kubernetes clusters
    easier. It is focused on clusters’ and node pools’ lifecycles. However, it started
    mostly as a way to provision clusters using kubeadm. Managed cluster support on
    different cloud providers was added later, and it is still young. In particular,
    GKE is not supported (although you can provision Kubernetes clusters on GCP as
    an infrastructure layer). AKS and EKS are supported.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster API是来自Cluster Lifecycle SIG的开源项目。它的目标是使多个Kubernetes集群的配置、升级和操作更加简单。它专注于集群和节点池的生命周期。然而，它最初主要是使用kubeadm来配置集群的一种方法。稍后添加了对不同云提供商的托管集群支持，但它仍然很年轻。特别是，GKE不受支持（尽管您可以作为基础设施层在GCP上配置Kubernetes集群）。AKS和EKS是支持的。
- en: The Cluster API has a lot of momentum, and if you don’t operate GKE, you should
    definitely look into it.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster API有很大的动力，如果您不操作GKE，您应该确实研究一下它。
- en: Terraform/Pulumi
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Terraform/Pulumi
- en: Terraform and Pulumi are similar in their approach. They can provision clusters
    and node pools on all cloud providers. However, these tools on their own can’t
    respond to out-of-band changes and don’t monitor the state of the infrastructure
    after provisioning. Their internal state can deviate from the real world and that
    can cause difficult to recover from situations that require careful “surgery.”
    In particular, node pools often need to be provisioned or updated, and Terraform
    or Pulumi might not be up to the task. If you have a lot of experience with these
    tools and are aware of their quirks and special requirements, they may still be
    a good option for you.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform和Pulumi在方法上类似。它们可以在所有云提供商上为您配置集群和节点池。然而，这些工具本身无法响应带外变更，并且在配置后不监控基础设施的状态。它们的内部状态可能会偏离真实世界，这可能导致需要小心“手术”的难以恢复的情况。特别是，节点池经常需要配置或更新，而Terraform或Pulumi可能无法胜任。如果您对这些工具有很多经验，并且了解它们的怪癖和特殊要求，它们仍然可能是一个不错的选择。
- en: Kubernetes operators
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes运算符
- en: Another alternative is to use Kubernetes operators that reconcile CRDs with
    cluster and node pool specs with the cloud provider. Under the covers, the operator
    will invoke Managed Kubernetes APIs from the cloud provider. This requires non-trivial
    work and expertise in writing Kubernetes operators but gives you the ultimate
    control.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用Kubernetes运算符，它们可以将CRD与集群和节点池规范与云提供商进行协调。在幕后，运算符将调用云提供商的托管Kubernetes
    API。这需要非平凡的工作和编写Kubernetes运算符的专业知识，但可以给您带来最终的控制。
- en: You may try to use Crossplane instead of writing your own operator; however,
    Crossplane support seems pretty basic and incomplete at the moment. One option
    to expand the scope is to use the Upjet project ([https://github.com/upbound/upjet](https://github.com/upbound/upjet))
    to generate Crossplane controllers from Terraform providers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试使用Crossplane而不是编写自己的运算符；然而，目前Crossplane的支持似乎相当基础和不完整。扩展范围的一个选择是使用Upjet项目（[https://github.com/upbound/upjet](https://github.com/upbound/upjet)）从Terraform提供程序生成Crossplane控制器。
- en: Utilizing managed nodes
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用托管节点
- en: You can also try to use managed nodes, so you never need to deal with provisioning
    node pools and nodes directly. All cloud providers offer some form of managed
    nodes such as GKE AutoPilot, EKS + Fargate, and AKS + ACI. For enterprise use
    cases, I believe you will need more control than fully managed node pools provide.
    It may be a good option for a subset of your workloads. However, at scale, you
    will want to optimize your resource usage and performance and the limitations
    of managed node pools might be too severe.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以尝试使用托管节点，这样您就永远不需要直接处理节点池和节点的配置。所有云提供商都提供某种形式的托管节点，如GKE AutoPilot、EKS +
    Fargate和AKS + ACI。对于企业使用情况，我认为您可能需要比完全托管的节点池提供更多的控制。这可能是您工作负载的一部分的一个不错选择。然而，在规模化之后，您将希望优化资源使用和性能，而托管节点池的限制可能会过于严格。
- en: Once you have figured out how to provision and manage your clusters and node
    pools, you should turn your attention to effectively using the resources you provisioned.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您找出如何配置和管理您的集群和节点池，您应该将注意力转向有效地使用您配置的资源。
- en: Bin packing and utilization
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 装箱和利用率
- en: 'Cloud resources are expensive. Efficient usage of resources on Kubernetes has
    two parts: efficiently scheduling pods to nodes based on their resource requests,
    and pods actually using the resources they requested.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 云资源是昂贵的。在Kubernetes上有效使用资源有两个部分：根据其资源请求有效地调度pod到节点，以及实际使用它们请求的资源的pod。
- en: Bin packing means ensuring that the total sum of resource requests is as close
    as possible to the allocatable resources on the target node. Once a workload is
    scheduled to a node, it will not be evicted under normal conditions even if the
    node is highly underutilized, but components like the cluster autoscaler can help
    here.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 容器打包（Bin packing）是指确保资源请求的总和尽可能接近目标节点上可分配的资源。一旦工作负载被调度到某个节点，通常情况下，即使该节点的资源使用率很低，工作负载也不会被驱逐，但像集群自动扩展器这样的组件可以在这里提供帮助。
- en: Resource utilization measures what percentage of the requested resource is actually
    used. Resource utilization is in general not fixed as the resource usage of workloads
    may vary widely throughout their lifetimes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 资源利用率衡量的是实际使用的资源占请求资源的百分比。一般来说，资源利用率并不是固定的，因为工作负载的资源使用情况可能会在其生命周期中发生显著变化。
- en: There are a lot of nuances to bin packing, resource utilization, and the interplay
    between them. For example, there are different resources such as CPU, memory,
    disk, and network. A node may have 100% bin packing for CPU, but only 20% bin
    packing memory. Network and non-ephemeral disks on the node are shared resources
    that pods can request to ensure they will always have a certain amount. This complicates
    operation and reliability. Let’s discuss some principles and concepts that can
    assist in navigating this complicated topic.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 容器打包、资源利用率及它们之间的相互作用有许多细微之处。例如，存在不同类型的资源，如 CPU、内存、磁盘和网络。一个节点可能对 CPU 实现了 100%
    的容器打包，但内存的容器打包率可能只有 20%。该节点上的网络和非临时磁盘是共享资源，Pod 可以请求它们以确保始终获得一定的资源量。这增加了操作的复杂性和可靠性问题。我们将讨论一些原则和概念，帮助理解这一复杂话题。
- en: Understanding workload shape
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解工作负载形态
- en: Workload shape is the ratio between the workload CPU requests and its memory
    requests. In the cloud, there is a standard ratio of 1 CPU to 4 GiB of memory.
    As a result, most VM types that you can choose offer resource capacities with
    this ratio. Some workloads need more memory or more CPU than this ratio. All cloud
    providers also offer high-memory VM types with a ratio of 1 CPU to 8 GiB of memory
    as well as high-CPU VM types with 1 CPU to 2 GiB of memory.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载形态是工作负载 CPU 请求与内存请求的比率。在云中，标准的比例是 1 个 CPU 对应 4 GiB 的内存。因此，您可以选择的大多数虚拟机类型都提供此比例的资源容量。有些工作负载需要比这个比例更多的内存或
    CPU。所有云服务提供商也提供高内存虚拟机类型，比例为 1 个 CPU 对 8 GiB 内存，以及高 CPU 虚拟机类型，比例为 1 个 CPU 对 2 GiB
    内存。
- en: Understanding the resource shape of your workloads is necessary to inform the
    VM types you choose to optimize your resource usage.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 理解工作负载的资源形态对于选择合适的虚拟机类型以优化资源使用至关重要。
- en: For example, if a workload requires 1 CPU and 8 GiB of memory and you schedule
    it on a VM type with a ratio of 1:4, you will need to run it on a node that has
    2 CPUs and 8 GiB of memory. No other pod can run on this node since the original
    workload uses all the 8 GiB of memory. However, 1 CPU out of 2 is not used at
    all. It would have been much better to schedule the workload on a node with a
    VM type of 1:8 ratio, which ensures optimal bin packing.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果某个工作负载需要 1 个 CPU 和 8 GiB 的内存，并且你将其调度到一个 CPU 与内存比例为 1:4 的虚拟机类型上，那么你需要将其运行在一个有
    2 个 CPU 和 8 GiB 内存的节点上。由于原始工作负载使用了全部的 8 GiB 内存，因此该节点不能运行其他任何 Pod。然而，2 个 CPU 中有
    1 个完全没有使用。将该工作负载调度到一个 CPU 与内存比例为 1:8 的节点上会更好，这样可以确保最优的容器打包。
- en: Setting requests and limits
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置请求和限制
- en: Setting requests and limits for your workloads is a key for proper resource
    utilization. As you recall when you set requests for your pod’s containers, it
    will be scheduled to a node that has at least the requested number of resources
    available for the total sum of the requests of all containers. The requested resources
    are allocated for the exclusive use of each container for as long as the pod running
    on the node. The containers may use more resources than the requests if available.
    If you specify CPU limits and the container tries to use more CPU than the limit,
    then the pod may get throttled. If you specify a memory limit and the container
    tries to use more memory than the limit, then the container will be OOMKilled
    and restarted.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为工作负载设置请求和限制是实现适当资源利用的关键。正如你在为 Pod 的容器设置请求时会注意到的那样，Pod 会被调度到一个节点，该节点上至少有足够的资源来满足所有容器请求资源的总和。这些请求的资源将被专门分配给每个容器，直到
    Pod 在该节点上运行时。容器如果有空闲资源，可能会使用超出请求的资源。如果你为 CPU 设置了限制，并且容器尝试使用超出限制的 CPU，那么该 Pod 可能会被限制使用。如果你为内存设置了限制，并且容器尝试使用超出限制的内存，那么容器将会因为
    OOM（内存溢出）被杀死并重启。
- en: It is best practice to set resource requests for CPU, memory, and even ephemeral
    storage if the container uses any. How do you know how much to request? You can
    start with a rough estimate and monitor the actual resource usage over time and
    fine-tune it later.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是为CPU、内存，甚至是短暂存储（如果容器使用的话）设置资源请求。你如何知道应该请求多少资源？你可以从一个粗略的估算开始，随着时间的推移监控实际的资源使用情况，并在后续进行微调。
- en: But even this straightforward method has some subtleties. Suppose a workload
    uses between 2 CPUs and 4 CPUs with an average of 3 CPUs. Should you request 4
    CPUs and know for sure that the workload will never get throttled? But then, you
    waste a whole CPU because the average usage is just 3 CPUs. If you request 3 CPUs,
    are you going to get throttled every time the workload needs more than 3 CPUs?
    That depends on the available CPU on the node the pod is scheduled to. If the
    overall CPU on the node is saturated because all the pods need a lot of CPU, then
    it is possible.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 但即便是这种直接的方法也有一些微妙之处。假设一个工作负载使用2个至4个CPU，平均为3个CPU。你应该请求4个CPU并确保工作负载永远不会被限速吗？但这样一来，你就浪费了一个CPU，因为平均使用量只有3个CPU。如果你请求3个CPU，每当工作负载需要超过3个CPU时，是否会被限速？这取决于Pod调度到的节点上的可用CPU。如果节点上的总体CPU已经饱和，因为所有Pod都需要大量的CPU，那么这种情况是可能发生的。
- en: On top of plain requests, you can also assign priorities to workloads, which
    allow you to control the destiny of high-priority workloads and ensure they take
    precedence over non-prioritized or low-priority workloads.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 除了普通的资源请求，你还可以为工作负载分配优先级，这样可以控制高优先级工作负载的调度顺序，并确保它们优先于那些没有优先级或低优先级的工作负载。
- en: Yes, scheduling is far from trivial. If you need a refresher, check out the
    *Understanding the design of the Kubernetes scheduler* section in *Chapter 15*,
    *Extending Kubernetes*.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，调度远非简单。如果你需要复习，可以查看*第15章*中*理解Kubernetes调度器的设计*部分，章节来自于*扩展Kubernetes*。
- en: Let’s turn our attention to limits. A simple approach is to set limits equal
    to the requests. This ensures in general that containers will not use more resources
    than they requested, which makes bin packing easy. However, in real-world situations,
    the resource usage of workloads varies. It is often more economical to request
    less than the maximal usage or sometimes even less than the average usage. In
    this case, you may opt not to set limits at all or set the limits higher than
    the requests. For example, if a workload uses 1 to 4 CPUs, then you may decide
    to request 2 CPUs and set the limits to 4 CPUs. Requesting just 2 CPUs will allow
    packing more pods into the same node or schedule the pod into a smaller node.
    So, why set limits at all? Well, setting some limits ensures the pods don’t get
    out of control, hog all the CPU, and starve all other pods that may also set lower
    requests but actually need additional CPU.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将注意力转向限制。一个简单的方法是将限制设置为与请求相等。这通常可以确保容器不会使用超过请求的资源，这使得容器打包变得容易。然而，在实际情况中，工作负载的资源使用量是变化的。通常，申请少于最大使用量，或者有时甚至少于平均使用量，是更经济的。在这种情况下，你可以选择不设置限制，或者将限制设置得比请求值更高。例如，如果一个工作负载使用1到4个CPU，你可以决定请求2个CPU，并将限制设置为4个CPU。只请求2个CPU将允许在同一个节点上容纳更多的Pod，或者将Pod调度到较小的节点上。那么，为什么还要设置限制呢？嗯，设置一些限制可以确保Pods不会失控，霸占所有CPU，导致其他Pods的请求资源较低但实际上需要更多CPU的Pods受到资源饥饿。
- en: Setting memory high limits is even more important, especially for workloads
    that are more sensitive and that shouldn’t be restarted often since any attempt
    to use more allocated memory than the limit will result in a container restart.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 设置内存高限制更加重要，特别是对于那些更敏感且不应频繁重启的工作负载，因为任何超出分配内存限制的尝试都会导致容器重启。
- en: Utilizing init containers
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用初始化容器
- en: Some workloads need to do a lot of work when they just start and then their
    resource requirements are lower. For example, a workload needs 10 GiB of memory
    and 4 CPUs to fetch some data and process it in memory before it is ready to handle
    requests. However, once it’s running, it doesn’t need more than 1 CPU and 4 GiB.
    It would be pretty wasteful to request 4 CPUs and 10 GiB if the pod is a long-running
    one. This is where init containers are very useful. You can split your workload
    into two containers. All the initialization work that requires 4 CPUs and 10 GiB
    can be done by an init container and then the main container can request just
    1 CPU and 4 GiB.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作负载在启动时需要做大量工作，然后它们的资源需求较低。例如，一个工作负载需要 10 GiB 的内存和 4 个 CPU 来获取一些数据并在内存中处理它，然后才准备好处理请求。然而，一旦运行，它不需要超过
    1 个 CPU 和 4 GiB 内存。如果 Pod 是一个长期运行的工作负载，申请 4 个 CPU 和 10 GiB 会非常浪费。在这种情况下，init 容器非常有用。你可以将工作负载拆分为两个容器。所有需要
    4 个 CPU 和 10 GiB 内存的初始化工作可以由 init 容器完成，然后主容器只需要请求 1 个 CPU 和 4 GiB 内存。
- en: Shared nodes vs. dedicated nodes
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享节点与专用节点
- en: When designing your node pools, you have two fundamental choices to make. Shared
    node pools have multiple different workloads scheduled and run side by side on
    the same node. Dedicated node pools have a single workload taking over a single
    node (possibly multiple instances of the same workload).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计节点池时，你需要做出两个基本的选择。共享节点池会调度多个不同的工作负载，并将它们并排运行在同一节点上。专用节点池则会将一个工作负载分配到单一节点（可能是同一工作负载的多个实例）。
- en: Shared node pools are simple. The extreme case is that you have just a single
    node pool and all pods are scheduled to nodes from this node pool. If you have
    multiple shared node pools (e.g., one with regular nodes and one with spot instances),
    then you need to assign taints to node pools and tolerations to workloads as well
    as dealing with node and pod affinity.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 共享节点池比较简单。极端情况下，你只有一个节点池，所有 Pod 都调度到该节点池的节点上。如果你有多个共享节点池（例如，一个包含常规节点，另一个包含抢占实例），那么你需要为节点池分配污点，并为工作负载分配容忍度，还需要处理节点和
    Pod 的亲和性。
- en: Since you don’t know exactly which combination of pods will end up on which
    node, there might be inefficiencies with bin packing. However, as long as the
    overall average workload shape matches the resource ratio of your nodes, bin packing
    at a large scale should be close to optimal.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你无法确切知道哪一组 Pod 最终会被调度到哪个节点，因此可能会出现装箱效率低下的情况。然而，只要整体的平均工作负载形态与节点的资源比例匹配，大规模的装箱应该接近最优。
- en: Workloads can request the CPU, memory, and ephemeral storage they need. However,
    there are some shared resources on the node, like network and disk I/O, that you
    can’t easily carve out for your workload when other workloads on the same node
    might try to use the same resource.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载可以请求所需的 CPU、内存和临时存储。然而，节点上存在一些共享资源，如网络和磁盘 I/O，当同一节点上的其他工作负载也可能尝试使用这些资源时，你无法轻松为你的工作负载划分出这些资源。
- en: This is where dedicated node pools come in. Critical workloads like databases
    or event queues require predictable network and disk I/O. Scheduling such workloads
    on a dedicated node ensures the workload doesn’t have to worry about other workloads
    cannibalizing the shared resources.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是专用节点池派上用场的地方。像数据库或事件队列这样的关键工作负载需要可预测的网络和磁盘 I/O。将这些工作负载调度到专用节点上，确保它们不必担心其他工作负载争夺共享资源。
- en: It makes sense in this case for the workload to request more than 50% of the
    standard resources like CPU or memory to ensure exactly one pod of the critical
    workload is scheduled on the node.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，工作负载请求超过 50% 的标准资源（如 CPU 或内存）是合理的，以确保关键工作负载的确切一个 Pod 被调度到节点上。
- en: Remember that system daemons will also run on the node and have higher priority.
    If your dedicated workload requests too many resources, it might become unschedulable.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，系统守护进程也会在节点上运行并具有更高的优先级。如果你的专用工作负载请求了过多的资源，它可能变得无法调度。
- en: I have run into this issue after an upgrade where the daemonsets on the node
    required more resources and caused the dedicated workload to be unschedulable
    until it reduced its resource requests.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在升级后，我遇到过这种问题：节点上的守护进程需要更多资源，导致专用工作负载无法调度，直到它减少了资源请求。
- en: Large nodes vs, small nodes
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大节点与小节点
- en: In the cloud, nodes come in a variety of sizes, from 1 core to tens or even
    hundreds of cores. Should you have lots of small nodes or fewer large nodes?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在云环境中，节点有多种尺寸，从 1 个核心到数十个甚至数百个核心。你是应该使用大量小节点还是使用较少的大节点？
- en: First and foremost, you must have nodes that your largest workloads fit into.
    For example, if a workload requests 8 CPUs, then you must have a node with at
    least 8 CPUs available.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您必须确保有足够的节点可以容纳您最大的工作负载。例如，如果一个工作负载请求 8 个 CPU，那么您必须有一个至少可用 8 个 CPU 的节点。
- en: But what about much bigger nodes? There are advantages in terms of efficiency
    for larger nodes. In the cloud, when you provision (for example) a node with 1
    CPU core and 4 GIB of memory, you don’t really get all these resources. First,
    the OS, the container runtime, and kube-proxy take their resources, then the additional
    processes the cloud provider decides to run on each node, then various sys daemonsets
    and your own daemonsets. Finally, what’s left is available for your workloads.
    All these processes and workloads that always run on every node need a lot of
    resources. However, the resources they require are not proportional to the size
    of the node. This means that on small nodes, a much smaller percentage of the
    resources you pay for will be available for your pods. Let’s look at an example.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 那么更大的节点呢？对于更大的节点，效率上有优势。在云中，当您配置（例如）一个具有 1 个 CPU 核心和 4 GiB 内存的节点时，您实际上并没有真正获得这些资源。首先，操作系统、容器运行时和
    kube-proxy 会占用一些资源，然后是云提供商决定在每个节点上运行的额外进程，再然后是各种系统守护进程和您自己的守护进程。最终，剩下的资源才可供您的工作负载使用。所有这些在每个节点上始终运行的进程和工作负载需要大量资源。然而，它们所需的资源并不与节点的大小成正比。这意味着在小型节点上，您支付的资源中，只有较小一部分会真正分配给您的
    pod。我们来看看一个例子。
- en: 'Here is the resource breakdown for a real node running on an AKS production
    cluster. It has a VM type of **Standard_F2s_v2** ([https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series](https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series)).
    It has 2 CPUs and 4 GIB of memory. However, the allocatable CPU and memory is
    1.9 CPU and 2.1 GiB. Yes, this is correct. You barely get a little more than 50%
    of the memory available on the node:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在 AKS 生产集群上运行的真实节点的资源明细。它的虚拟机类型为 **Standard_F2s_v2** ([https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series](https://learn.microsoft.com/en-us/azure/virtual-machines/fsv2-series))。它有
    2 个 CPU 和 4 GiB 内存。然而，可分配的 CPU 和内存为 1.9 个 CPU 和 2.1 GiB。是的，这是正确的。您几乎只能使用节点上 50%
    以上的内存：
- en: '[PRE0]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'But the story doesn’t end here. There are system daemonsets running in kube-system.
    You can find them with the following command:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 但故事并未结束。kube-system 中运行着系统守护进程。您可以使用以下命令找到它们：
- en: '[PRE1]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s look at the resources requested by these workloads on our node:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些工作负载在我们节点上请求的资源：
- en: '[PRE2]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s a total of 0.56 CPU and 520Mi of memory. If we subtract it from the allocatable
    CPU and memory, we end up with 1.4 CPU and 1.58 GiB of memory.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这总共需要 0.56 个 CPU 和 520Mi 内存。如果从可分配的 CPU 和内存中减去这些，我们将剩下 1.4 个 CPU 和 1.58 GiB
    的内存。
- en: This is quite eye-opening. On a small node with 2 CPUs and 4 GIB of memory,
    we end up with 70% of the CPU and less than 40% of the memory. Beyond the cost
    implications, if you miscalculate and assume you can schedule, for example, a
    pod that requests 2 GIB of memory on a 4 GiB node, you’ll have an unpleasant surprise
    when your pod remains pending because it doesn’t fit on this node.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是让人眼前一亮。在一个 2 个 CPU 和 4 GiB 内存的小节点上，我们只能使用 70% 的 CPU 和不到 40% 的内存。除了成本影响之外，如果您计算错误并假设您可以在一个
    4 GiB 的节点上调度一个请求 2 GiB 内存的 pod，当您的 pod 因为无法在此节点上运行而处于待定状态时，您将会有一个不愉快的惊讶。
- en: 'Let’s look at large nodes. A Standard_D64ads_v5 Azure VM has a whopping 64
    cores and 256 GiB of memory. It is undoubtedly a beast. Let’s look at its capacity
    and allocatable resources:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看大节点。一个 **Standard_D64ads_v5** 的 Azure 虚拟机有 64 个核心和 256 GiB 的内存。它无疑是一个庞然大物。让我们看看它的容量和可分配资源：
- en: '[PRE3]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we lost 740 mcpu (as opposed to 100 mcpu on the small node) and 17 GiB
    of memory. This sounds like a lot, but proportionally, it is much better. Let’s
    look at system workloads to get the full picture:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们失去了 740 mcpu（相比之下，小节点上是 100 mcpu）和 17 GiB 的内存。听起来很多，但按比例计算，这要好得多。让我们看看系统工作负载，以便了解整个情况：
- en: '[PRE4]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s a total of 0.365 CPU and 520Mi of memory. Surprisingly, less CPU is requested
    than the small node and the memory requests are the same. If we subtract it from
    the allocatable CPU and memory, we end up with 62.9 CPU and 238.48 GiB of memory.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这总共需要 0.365 个 CPU 和 520Mi 内存。令人惊讶的是，所请求的 CPU 比小型节点还少，内存请求相同。如果从可分配的 CPU 和内存中减去这些，我们将剩下
    62.9 个 CPU 和 238.48 GiB 的内存。
- en: On a large node with 64 cores and 256 GiB of memory, we end up with more than
    98% of the CPU and more than 93% of the memory.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个拥有 64 个核心和 256 GiB 内存的大节点上，我们可以使用超过 98% 的 CPU 和超过 93% 的内存。
- en: This is a pretty clear victory for large nodes in terms of resource provisioning
    efficiency and getting more resources available for your workloads.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源配置效率方面，大型节点显然在为工作负载提供更多资源方面占据优势。
- en: However, there are additional nuances and considerations to consider. Let’s
    consider small and short-lived workloads.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有更多的细节和考虑事项需要关注。让我们来看看小型且短暂的工作负载。
- en: Small and short-lived workloads
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小型且短暂的工作负载
- en: Suppose we use large nodes, and our cluster is bin-packed very efficiently.
    Some deployment needs to scale up and create a new pod. If there is no room for
    the new pod in any of the existing nodes, then a new node must be provisioned.
    However, if the new pod is small, then we actually waste a lot of resources by
    running just one small pod on a large node. At scale, and when a lot of pods come
    and go pretty quickly, this may not be a problem. However, consider the following
    scenario – our cluster is normally running on 100 large nodes. During a temporary
    spike of activity, our clusters scaled up to 200 large nodes and then the activity
    went back to normal. Our resource utilization is now 50% (the cluster needs 100
    nodes out of 200). In an ideal world, the cluster autoscaler will eventually scale
    down empty nodes until we have 100 properly bin-packed nodes. But, in the real
    world, especially in the presence of small short-lived pods, new pods may get
    scheduled arbitrarily to all 200 nodes and the autoscaler might have a difficult
    time scaling down. We will see later, in the custom scheduling section, some options.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用大型节点，并且我们的集群已经进行了非常高效的二进制打包。一些部署需要扩展并创建一个新的 pod。如果现有的任何节点都没有足够的空间容纳新 pod，那么就必须为其配置一个新节点。然而，如果新
    pod 很小，那么我们实际上会浪费大量资源，因为只在一个大节点上运行一个小 pod。在大规模应用中，尤其是当大量 pod 快速进出时，这可能不是一个问题。然而，考虑以下场景——我们的集群通常运行在
    100 个大型节点上。在一次短暂的活动峰值期间，我们的集群扩展到 200 个大型节点，然后活动恢复正常。此时我们的资源利用率为 50%（集群需要 100 个节点中的
    200 个）。在理想的情况下，集群自动缩放器最终会缩减空闲节点，直到我们有 100 个合适的二进制打包节点。但在现实世界中，尤其是在存在小型短暂生命周期 pod
    的情况下，新的 pod 可能会被任意调度到所有 200 个节点，自动缩放器可能很难进行缩减。稍后我们将在自定义调度部分看到一些选项。
- en: Another issue with short-lived workloads is that even if they have room on an
    existing node, they can still waste resources if they take a while to get ready.
    Consider a pod that takes 1 minute to get ready and runs, on average, for 1 minute.
    This pod with optimal utilization of its resources still can do better than 50%
    because after it gets scheduled, it reserves its resources on the node for 2 minutes,
    but actually does work for only 1 minute. If the pod needs to pull its image,
    then it can easily take several minutes to get ready.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 短暂工作负载的另一个问题是，即使它们在现有节点上有空间，如果它们准备时间较长，也可能会浪费资源。考虑一个需要 1 分钟才能准备好，并且平均运行 1 分钟的
    pod。这个 pod 即便资源得到了最佳利用，仍然无法做到 50% 以上的效率，因为在调度后，它会在节点上保留 2 分钟的资源，但实际上只工作 1 分钟。如果
    pod 需要拉取镜像，那么准备过程可能需要几分钟。
- en: The Kubernetes scheduler is very sophisticated and can be extended too, as we
    covered in *Chapter 15*, *Extending Kubernetes*. The issues with the inefficient
    scheduling of pods in the different use cases we mentioned could potentially be
    addressed by choosing a different scoring strategy. The default scoring strategy
    of **RequestedToCapacityRatio** is intended to evenly distribute workloads across
    all nodes. This is not ideal for tight bin packing. The **MostAllocated** scoring
    strategy may be preferable here.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 调度器非常复杂，并且也可以扩展，正如我们在 *第 15 章*、*扩展 Kubernetes* 中所讨论的那样。我们提到的在不同使用案例中出现的无效
    pod 调度问题，可能通过选择不同的评分策略来解决。默认的评分策略 **RequestedToCapacityRatio** 旨在将工作负载均匀分配到所有节点。这对于紧密的二进制打包并不理想。此时，**MostAllocated**
    评分策略可能更为适合。
- en: Check out [https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing](https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing)
    for more details.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing](https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing)
    获取更多详细信息。
- en: Pod density
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod 密度
- en: Pod density is the maximum number of pods per node (the Kubernetes default is
    110). As mentioned earlier, some resources like private IP addresses or system
    daemon CPU and memory may be correlated with the pod density. If your pod density
    is too high, then you may waste the resources that were pre-allocated to support
    many pods on each node. However, if you set the pod density too low, then you
    may not be able to schedule enough pods to run on the node. Let’s consider a large
    node with 64 CPU cores and 256 GiB of memory. If the pod density is 100, then
    at most 100 pods can run on this node. Suppose we have a lot of small pods that
    use only 10 mcpu and 100 MiB of memory. 100 pods need only 10 CPU cores and 10
    GiB of memory combined. If 100 such pods get scheduled to one large node, the
    node will be highly underutilized. 54 CPU cores and 246 GIB of memory will be
    wasted.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 密度是每个节点的最大 Pod 数量（Kubernetes 默认为 110）。正如前面提到的，某些资源如私有 IP 地址或系统守护程序的 CPU
    和内存可能与 Pod 密度相关。如果 Pod 密度过高，则可能浪费预分配给每个节点的资源以支持多个 Pod。但如果将 Pod 密度设置过低，则可能无法在节点上调度足够多的
    Pod。让我们考虑一个具有 64 个 CPU 核心和 256 GiB 内存的大节点。如果 Pod 密度为 100，则最多可以在此节点上运行 100 个 Pod。假设我们有很多只使用
    10 mcpu 和 100 MiB 内存的小 Pod。100 个 Pod 只需合计 10 CPU 核心和 10 GiB 内存。如果将 100 个这样的 Pod
    安排到一个大节点上，该节点将被严重地低效利用。将浪费 54 个 CPU 核心和 246 GiB 内存。
- en: If you go with the shared node pool model, then it’s an arbitrary mix of pods
    with different workload shapes, and resource requirements can get scheduled to
    nodes.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择共享节点池模型，则是各种工作负载形状的任意混合，资源需求可以安排到节点上。
- en: Fallback node pools
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 备用节点池
- en: Cloud providers suffer from temporary capacity issues from time to time, and
    as a result, are unable to provision new nodes. In addition, spot instances may
    disappear at any time if there is a lot of demand for regular nodes. The good
    news is that these outages are a zonal affair and also are typically limited to
    a specific instance type or VM family.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商偶尔会遇到临时容量问题，因此无法提供新节点。此外，如果普通节点需求量很大，抢占式实例随时可能消失。好消息是，这些故障是区域性的，通常仅限于特定的实例类型或虚拟机家族。
- en: A good strategy to address this issue is to use fallback node pools.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个好策略是使用备用节点池。
- en: A fallback node pool is an empty node pool with autoscaling disabled that has
    the same labels and taints as another active node pool but with a different VM
    family or a different node type (e.g., regular vs. spot). If the active node pool
    is unable to provision more nodes and there are pending pods, then the back node
    pool can be resized and/or become auto-scaling. This will allow the pending pods
    to be scheduled to the backup node pool until the situation with the native node
    pool is resolved.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 备用节点池是一个空的节点池，禁用自动扩展，具有与另一个活动节点池相同的标签和污点，但使用不同的 VM 家族或不同的节点类型（例如普通与抢占式）。如果活动节点池无法提供更多节点且有未调度的
    Pod，则备用节点池可以调整大小并/或变为自动扩展。这将允许未调度的 Pod 被安排到备用节点池，直到与本地节点池的情况得到解决。
- en: If you choose this path, you need to come up with a proper procedure to activate
    the backup node pool, which includes detection of issues in the active node pool,
    a manual or automated process for backup node pool activation, and a scale-back
    process when the active node pool is back to normal.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择这条路线，则需要制定适当的流程来激活备用节点池，其中包括检测活动节点池中的问题、备用节点池激活的手动或自动化过程，以及在活动节点池恢复正常时的缩减过程。
- en: It is very important to ensure the backup node pool has enough quota to replace
    its active node pool when needed.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 确保备用节点池在需要时有足够的配额来替换其活动节点池非常重要。
- en: This was a very thorough treatment of bin packing and resource utilization.
    Let’s turn our attention to upgrades.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对装箱和资源利用的非常彻底的处理。现在让我们把注意力转向升级。
- en: Upgrading Kubernetes
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 升级 Kubernetes
- en: Upgrading Kubernetes can be a very stressful operation. A hasty upgrade might
    remove support for resource versions, and if you have unsupported resources deployed,
    you will encounter catastrophic failures. Using Managed Kubernetes has its pros
    and cons. When it comes to upgrades, there is, at any point in time, a range of
    supported versions.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 升级 Kubernetes 可能是一个非常紧张的操作。仓促的升级可能会删除对资源版本的支持，如果部署了不受支持的资源，将会遇到灾难性的故障。使用托管 Kubernetes
    有其利弊。在升级方面，任何时候都有一系列支持的版本。
- en: You may upgrade to more recent supported versions. However, if you delay and
    neglect to upgrade, then the cloud provider will upgrade your clusters and node
    pools automatically once you fall behind the cutting edge of supported versions.
    Let’s look at the various elements of upgrading Kubernetes you must be on top
    of.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以升级到更近期的受支持版本。然而，如果你拖延并忽视升级，云服务提供商将在你落后于支持版本的前沿时自动升级你的集群和节点池。让我们来看看升级Kubernetes时你必须掌握的各个要素。
- en: Know the lifecycle of your cloud provider
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解你的云服务提供商的生命周期
- en: 'Cloud providers can’t support just any Kubernetes version in existence. It
    is critical to know when the current version of your clusters and node pools is
    going to be defunct. All cloud providers have a methodical process and share the
    information broadly. Here are the locations for each of the three major cloud
    providers:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商不能支持所有存在的Kubernetes版本。了解你的集群和节点池当前版本的淘汰时间是至关重要的。所有云服务提供商都有一套系统化的流程，并广泛分享相关信息。以下是三个主要云服务提供商的相关链接：
- en: 'AWS EKS: [https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AWS EKS: [https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html)'
- en: 'Azure AKS: [https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions](https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azure AKS: [https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions](https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions)'
- en: 'Google GKE: [https://cloud.google.com/kubernetes-engine/docs/release-schedule](https://cloud.google.com/kubernetes-engine/docs/release-schedule)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Google GKE: [https://cloud.google.com/kubernetes-engine/docs/release-schedule](https://cloud.google.com/kubernetes-engine/docs/release-schedule)'
- en: For example, AKS, at the time of writing, supports versions 1.23 through 1.26\.
    In addition, each version has an official end-of-life date. For example, the end-of-life
    date for 1.23 is April 2023\. If your cluster is still on 1.23, then AKS may upgrade
    your cluster automatically to version 1.24\. The process of cloud provider upgrade
    is gradual, done region by region, and might take several weeks.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当前在写作时，AKS支持版本1.23至1.26。此外，每个版本都有一个官方的生命周期结束日期。例如，1.23的生命周期结束日期是2023年4月。如果你的集群仍在1.23版本上，那么AKS可能会自动将你的集群升级到1.24版本。云服务提供商的升级过程是渐进的，按区域进行，可能需要几周时间。
- en: All cloud providers offer an API and CLI to check the exact list of versions
    (including patch versions) in every region.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 所有云服务提供商都提供API和CLI，用于检查每个区域的版本（包括补丁版本）的准确列表。
- en: 'For example, at the moment, these are versions supported on AKS for the Central
    US region:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，目前这是支持的在美国中部地区的AKS版本：
- en: '[PRE5]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, for each minor version, there are several patch versions. It
    is even nice enough to mention which versions you may upgrade to yourself. Due
    to security concerns, the cloud provider may drop support for patch versions at
    any time.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，对于每个次要版本，都有多个补丁版本。甚至会明确指出你可以自行升级到哪些版本。由于安全原因，云服务提供商可能随时停止对某些补丁版本的支持。
- en: Let’s talk about the upgrade process of the control plane.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来讨论控制平面升级的过程。
- en: Upgrading clusters
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级集群
- en: 'When using Managed Kubernetes in the cloud, you are not responsible for the
    operation of the control plane, but you still need to manage the upgrade process.
    You have two options:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云中的托管Kubernetes时，你不需要负责控制平面的操作，但你仍然需要管理升级过程。你有两个选择：
- en: Auto upgrade
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动升级
- en: Manual upgrade
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动升级
- en: In an auto upgrade, the cloud provider will update your cluster according to
    their schedule, but you still must ensure that the versions of resources in your
    cluster are compatible with the new version. A manual upgrade requires you to
    upgrade yourself but gives you more control over timing. For example, you may
    choose to update earlier to benefit from some new features.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动升级中，云服务提供商会根据他们的计划更新你的集群，但你仍然需要确保集群中的资源版本与你的新版兼容。手动升级则要求你自己进行升级，但可以让你对升级时机有更多的控制。例如，你可以选择提前更新，以便享受一些新功能的好处。
- en: Remember that a manual upgrade doesn’t mean you can stay on the same Kubernetes
    version forever. The cloud provider will forcefully upgrade you if you fall behind
    the minimal supported version.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，手动升级并不意味着你可以永远停留在同一个Kubernetes版本上。如果你落后于最小支持版本，云服务提供商将强制进行升级。
- en: Kubernetes releases a new version roughly every 3 months. Cloud providers support
    roughly 4 versions. This means that if you just upgraded to the latest supported
    version, you may hold off for about a year on upgrades, but then you will be on
    the minimal supported version, which means you will now have to upgrade every
    3 months.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 每大约三个月发布一个新版本。云服务提供商通常支持大约 4 个版本。这意味着，如果你刚刚升级到最新的支持版本，你可以大约等待一年再进行升级，但那时你将处于最低支持版本，这意味着你将需要每三个月进行一次升级。
- en: Note that you should upgrade the control plane one minor version at a time.
    If you are on version 1.24, and you want to upgrade to 1.26, you have to upgrade
    to 1.25 first and then from 1.25 to 1.26.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，应该一次只升级一个小版本的控制平面。如果你当前使用的是 1.24 版本，并且想要升级到 1.26，你必须先从 1.24 升级到 1.25，然后再从
    1.25 升级到 1.26。
- en: The bottom line is that upgrading the Kubernetes control plane is a standard
    operation that takes place multiple times per year. You should have a streamlined
    process for it.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结论是，升级 Kubernetes 控制平面是一个标准操作，每年会进行多次。你应该为此制定一个简化的流程。
- en: Let’s look at what is involved.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看涉及的内容。
- en: Planning an upgrade
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 升级计划
- en: You should plan your upgrades and coordinate them with cluster users. Control
    plane upgrades typically take 20-45 minutes. This is a non-disruptive operation
    for your workloads. Your workloads will keep running, and new pods will be scheduled
    to existing nodes. However, node pool operations might be blocked during the control
    plane upgrade.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该规划你的升级，并与集群用户协调。控制平面升级通常需要 20-45 分钟。这是一个不会干扰工作负载的操作。你的工作负载会继续运行，并且新 pod 会被调度到现有节点。然而，节点池操作在控制平面升级期间可能会被阻塞。
- en: If you’re running multiple clusters with a redundancy scheme, it is best to
    perform the upgrades gradually and start with non-critical clusters (e.g., a development
    or staging environment).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行多个集群，并采用冗余方案，最好逐步进行升级，并从非关键集群（例如开发或预生产环境）开始。
- en: I recommend having owners (engineers or teams) for every namespace. Notify all
    owners about upcoming upgrades so they can reserve time for converting incompatible
    resources.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议为每个命名空间指定负责人（工程师或团队）。通知所有负责人有关即将进行的升级，以便他们为转换不兼容资源预留时间。
- en: Detecting incompatible resources
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测不兼容的资源
- en: The main concern with an upgrade is that the functionality of your system will
    be compromised or completely broken because it uses resources that are not supported
    anymore. In most cases, a specific version of a resource will be removed, and
    a newer version will be available.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 升级的主要问题是，系统的功能可能会受到影响或完全损坏，因为它使用了不再支持的资源。在大多数情况下，一个特定版本的资源将被移除，并且会提供更新的版本。
- en: But you don’t have to wait until the last minute to scramble and replace removed
    resources or versions. Kubernetes has a deprecation policy and resources will
    be deprecated for several versions before they are fully removed. I suggest making
    sure before each upgrade that all deprecated resources are updated or replaced.
    This will ensure that the upgrade process is not stressful because, even if you
    didn’t manage to update all resources, the deprecated resources are still going
    to be supported by the new version and you will have some extra time to update
    them before they are fully removed.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 但你不必等到最后一刻才去忙着替换已移除的资源或版本。Kubernetes 有一个弃用政策，资源会在完全移除之前被弃用几个版本。我建议在每次升级之前确保所有弃用的资源都已更新或替换。这将确保升级过程不那么紧张，因为即使你没有及时更新所有资源，弃用的资源仍然会被新版本支持，你将有更多时间在它们被完全移除之前更新它们。
- en: Kubernetes publishes a migration guide with details about deprecated and removed
    APIs in each version. See [https://kubernetes.io/docs/reference/using-api/deprecation-guide](https://kubernetes.io/docs/reference/using-api/deprecation-guide).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 会发布迁移指南，详细说明每个版本中已弃用和移除的 API。请参阅 [https://kubernetes.io/docs/reference/using-api/deprecation-guide](https://kubernetes.io/docs/reference/using-api/deprecation-guide)。
- en: For example, Kubernetes 1.25 stopped serving the `CronJob` resource with the
    API version of `batch/v1beta1`. Instead, the `batch/v1` `CronJob` resource has
    been available since Kubernetes 1.21\. Ideally, after you upgrade to Kubernetes
    1.21, you have updated all your `CronJob` resources to use `batch/v1`, and by
    the time you upgrade to Kubernetes 1.25, the fact that `batch/v1beta1` is removed
    is not an issue because you are already on the supported version.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Kubernetes 1.25停止提供API版本为`batch/v1beta1`的`CronJob`资源。从Kubernetes 1.21起，`batch/v1`版本的`CronJob`资源已经可用。理想情况下，在你升级到Kubernetes
    1.21后，你应该已将所有的`CronJob`资源更新为使用`batch/v1`，并且当你升级到Kubernetes 1.25时，`batch/v1beta1`被移除就不会成为问题，因为你已经使用了受支持的版本。
- en: There are several ways to make sure you detect all deprecated and/or removed
    resources that you currently use. You can use the manual method of reading the
    deprecation guide and just scanning your code and detecting incompatible resources.
    Most releases don’t have a lot of deprecations or removals. However, some releases
    may have up to ten different resources that are being deprecated or removed. For
    example, Kubernetes 1.25 stopped serving seven different resource versions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以确保你检测到当前使用的所有已弃用和/或已移除的资源。你可以通过手动阅读弃用指南，并扫描你的代码来检测不兼容的资源。大多数发布版本并没有很多弃用或移除的资源。然而，某些版本可能有多达十个不同的资源被弃用或移除。例如，Kubernetes
    1.25停止提供七个不同版本的资源。
- en: A more systematic way is to use a tool like `kube-no-trouble` ([https://github.com/doitintl/kube-no-trouble](https://github.com/doitintl/kube-no-trouble)),
    which scans your clusters and can output a list of deprecated resources.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 更系统的方法是使用像`kube-no-trouble`这样的工具（[https://github.com/doitintl/kube-no-trouble](https://github.com/doitintl/kube-no-trouble)），它扫描你的集群，并能输出一个弃用资源的列表。
- en: 'Here is how to install it:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何安装它的方法：
- en: '[PRE6]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I have a 1.25 cluster that doesn’t contain any deprecated resources at the
    moment. However, in Kubernetes 1.26, the **HorizontalPodAutoscaler** of version
    autoscaling/v2beta2 will be removed as it has been deprecated since Kubernetes
    1.23\. Let’s create such a resource. There is a `kyverno` deployment in the cluster:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个1.25集群，目前没有包含任何已弃用的资源。然而，在Kubernetes 1.26中，版本autoscaling/v2beta2的**HorizontalPodAutoscaler**将被移除，因为它自Kubernetes
    1.23起已被弃用。让我们创建这样的资源。集群中有一个`kyverno`部署：
- en: '[PRE7]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is an HPA that sets the min replicas to 1 and the max replicas to 3:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将最小副本数设置为1，最大副本数设置为3的HPA：
- en: '[PRE8]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, kubectl gives a very nice warning when you create a deprecated
    resource version that tells when the resource was deprecated (1.23), when it will
    be removed (1.26), and which version to replace it with (autoscaling/v2).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，当你创建一个已弃用的资源版本时，kubectl会给出一个非常好的警告，告诉你该资源何时被弃用（1.23），何时会被移除（1.26），以及用哪个版本来替代它（autoscaling/v2）。
- en: This is nice, but it is not sufficient. You probably create your resources through
    CI/CD, which might not receive the same warning, and even if it does, might not
    surface it, because it is not an error. However, if you created the HPA when your
    cluster was on an earlier version of Kubernetes than 1.23, then you wouldn’t get
    any warning because at the time it wasn’t deprecated.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，但还不够。你可能通过CI/CD创建资源，这可能不会收到相同的警告，甚至即使收到，可能也不会显示出来，因为这不是一个错误。然而，如果你在集群版本低于1.23时创建了HPA，那么你就不会收到任何警告，因为当时它并未被弃用。
- en: 'Let’s see if kubent can detect the deprecated HPA:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看kubent是否能检测到已弃用的HPA：
- en: '[PRE9]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Yes, it does. You get the same information: when it will be removed, when it
    was deprecated, and what to replace it with.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，它能。你会得到相同的信息：什么时候会被移除，什么时候被弃用，以及该用什么替代它。
- en: Updating incompatible resources
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新不兼容的资源
- en: Updating an incompatible resource may require some changes to your manifests.
    If the API change just adds new fields, then you may just change the API version
    and be done with it. However, sometimes it may require additional changes.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 更新不兼容的资源可能需要对你的清单文件进行一些更改。如果API的更改只是增加了新的字段，那么你只需要更改API版本就可以了。然而，有时可能需要额外的更改。
- en: 'OK. We’re about to upgrade our cluster, and we detected some incompatible resources.
    Kubectl and the kubectl-convert plugin can help here. Follow the instructions
    here to install the plugin: [https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).
    Let’s convert our HPA manifest and see what it looks like:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们即将升级我们的集群，并且检测到了一些不兼容的资源。Kubectl和kubectl-convert插件可以帮助我们。按照此处的说明安装插件：[https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)。让我们转换我们的HPA清单并看看它的样子：
- en: '[PRE10]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The conversion succeeded but created a few unnecessary fields. The `creationTimestamp:
    null` is useless as it will be updated on a live resource. Also, the status is
    useless as this is just a manifest file, and the status will be updated at runtime.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '转换成功，但创建了一些不必要的字段。`creationTimestamp: null` 是无用的，因为它会在资源实时更新时被更新。另外，状态也是无用的，因为这只是一个清单文件，状态会在运行时更新。'
- en: 'However, the main differences are that `apiVersion` was changed to `apiVersion:
    autoscaling/v1` and that the target CPU percentage is now specified as a single
    field:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，主要的区别在于 `apiVersion` 已更改为 `apiVersion: autoscaling/v1`，并且目标 CPU 百分比现在作为单一字段指定：'
- en: '[PRE11]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using kubectl-convert saves time, and it is a well-tested tool.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl-convert` 可以节省时间，并且它是一个经过充分测试的工具。
- en: Dealing with removed features
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理移除的功能
- en: 'There is one other situation we need to address, which is the complete removal
    of a feature without an upgrade path. Kubernetes 1.25 completely removed support
    for **Pod Security Policies** (**PSPS**). The application of PSPs to pods has
    caused confusion for many users who have attempted to utilize them. Check this
    link for more details: [https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种情况需要处理，那就是完全移除一个没有升级路径的功能。Kubernetes 1.25 完全移除了对 **Pod 安全策略**（**PSP**）的支持。PSP
    应用于 Pod 的做法曾让许多用户感到困惑。请查看此链接了解更多详情：[https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/)。
- en: 'If you used PSPs, then when the time comes to upgrade to Kubernetes 1.25, your
    PSPs will no longer work. The Kubernetes developers didn’t just remove the feature
    with no alternative. There are two alternatives to PSPs:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用了 PSP，那么当升级到 Kubernetes 1.25 时，你的 PSP 将不再有效。Kubernetes 开发者并非直接移除该功能而没有替代方案。实际上，PSP
    有两个替代方案：
- en: Pod security admission
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 安全准入
- en: A third-party admission controller
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方准入控制器
- en: 'The pod security admin is a simplified solution that may or may not be a complete
    replacement for PSPs. The Kubernetes developers published a detailed guide for
    migration. Check it out: [https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/](https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 安全管理员是一个简化的解决方案，可能是 PSP（Pod 安全策略）的完整替代品，也可能不是。Kubernetes 开发者发布了一个详细的迁移指南。请查看：[https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/](https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/)。
- en: If you choose a third party (e.g., Kyverno), then you should check its documentation.
    Kyverno comes with a lot of sample policies for pod security and the transition
    is pretty straightforward.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择第三方（例如 Kyverno），则应查看其文档。Kyverno 提供了许多用于 Pod 安全的示例策略，迁移过程相当简单直接。
- en: Upgrading node pools
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级节点池
- en: Upgrading the node pools of multiple clusters can be a major undertaking. If
    you have tens to hundreds of clusters and in each cluster, you have multiple node
    pools (5-20 is not uncommon), then be ready for a serious adventure. Control plane
    upgrades (once you have ensured your workloads are compatible with the new version)
    are pretty quick and painless. Node pool upgrades are very difficult. Realistically,
    it can take several weeks to upgrade all the node pools in a large Kubernetes-based
    system with tens to hundreds of node pools with thousands of nodes.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 升级多个集群的节点池可能是一项重大任务。如果你有几十到几百个集群，并且每个集群都有多个节点池（5-20个节点池并不少见），那么你得准备好进行一场大冒险。控制平面升级（在确保工作负载与新版本兼容后）通常是快速且无痛的。节点池升级则非常困难。实际上，升级一个大型基于
    Kubernetes 的系统，涉及数十到数百个节点池、成千上万个节点，可能需要几周的时间才能完成。
- en: Syncing with control plane upgrades
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与控制平面升级同步
- en: Kubernetes imposes constraints on the versions of the control plane and the
    worker nodes. The Kubernetes node components may be two minor versions behind
    the control plane. If the control plane is on version N, then the node pools may
    be on version N-2\. Since node pool upgrades are much more disruptive and labor-intensive
    than control plane upgrades, I recommend upgrading node pools only to every other
    version of Kubernetes. For example, suppose we start with a Kubernetes cluster
    where both the control plane and the nodes are on version 1.24\. When we upgrade
    to version 1.25, we upgrade only the control plane to 1.25 and keep the node pools
    on 1.24, which is compatible. Then, when it’s time to upgrade to 1.26, we upgrade
    the control plane first from 1.25 to 1.26, and then we start upgrading all the
    node pools from 1.24 directly to 1.26\. Let’s see how to go about upgrading node
    pools.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 对控制平面和工作节点的版本施加了限制。Kubernetes 节点组件的版本可能比控制平面落后两个小版本。如果控制平面处于版本 N，那么节点池可能处于版本
    N-2。由于节点池的升级比控制平面的升级更具破坏性且劳动强度更大，我建议只将节点池升级到每个 Kubernetes 的偶数版本。例如，假设我们从一个 Kubernetes
    集群开始，控制平面和节点的版本都是 1.24。当我们升级到 1.25 时，我们只升级控制平面到 1.25，并将节点池保持在兼容的 1.24 版本。然后，当需要升级到
    1.26 时，我们首先将控制平面从 1.25 升级到 1.26，接着开始将所有节点池从 1.24 直接升级到 1.26。让我们看看如何进行节点池的升级。
- en: How to perform node pool upgrades
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何执行节点池升级
- en: Node pool upgrades require a new node pool. It is not possible to upgrade nodes
    within the node pool. It is not even possible to add new nodes with a new version
    to an existing node pool. The node version is one of the essential properties
    of a node pool. What it means is that you actually don’t upgrade an existing node
    pool. You replace your node pool. First, you create a new node pool, scale it
    up, and start draining nodes from the old node pool until all the pods run on
    the new node pool and then you can delete the old (now empty) node pool.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 节点池的升级需要创建一个新的节点池。无法在现有节点池中升级节点，也不能将新版本的节点添加到现有节点池中。节点版本是节点池的基本属性之一。这意味着你实际上并不是在升级现有的节点池，而是在替换你的节点池。首先，你创建一个新的节点池，扩展它，并开始从旧节点池排空节点，直到所有的
    pods 都运行在新的节点池中，然后你就可以删除旧的（现在空的）节点池。
- en: 'If your original node pool was an autoscaling node pool, then before starting
    the upgrade process, you must turn off autoscaling; otherwise, the pods you evicted
    from a node in the old node pool might get scheduled right back to the old node
    pool. Let’s list the exact steps you need to take to upgrade a node pool:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你原先的节点池是一个自动扩展节点池，那么在开始升级过程之前，必须关闭自动扩展；否则，你从旧节点池中驱逐的 pods 可能会重新调度到旧节点池。以下是升级节点池时你需要采取的确切步骤：
- en: Create a new node pool with the exact same specifications (instance type, labels,
    tolerations) as the existing node with the new version.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的节点池，确保其规格（实例类型、标签、容忍度）与现有节点池完全相同，并使用新的版本。
- en: You may pre-allocate some instances to the new pool, so they are ready to schedule
    pods from the old node pool.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以提前为新节点池分配一些实例，以便它们准备好调度来自旧节点池的 pods。
- en: Turn autoscaling off in the old node pool.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在旧节点池中关闭自动扩展。
- en: Cordon all the nodes in the old node pool to prevent the scheduling of new pods
    to the old pool.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将旧节点池中的所有节点设置为隔离状态，以防新 pods 被调度到旧池。
- en: Drain the nodes of the old node pool.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排空旧节点池的节点。
- en: Observe and deal with problems.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察并处理问题。
- en: Wait for the cluster autoscaler to delete empty old pool nodes or delete them
    yourself to expedite the process.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待集群自动扩展器删除空的旧池节点，或者你自己删除它们以加速过程。
- en: Let’s look at some problems that can delay or even hold up the upgrade process.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些可能会延迟甚至阻碍升级过程的问题。
- en: Concurrent draining
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行排空
- en: If you need to upgrade many node pools with lots of nodes in each, you may decide
    to provision new node pools and start draining all your node pools at once or
    in large batches. This can cause you to exceed your quota or hit cloud provider
    capacity issues.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要升级多个节点池，每个节点池中有大量节点，你可能会决定配置新的节点池，并开始同时或大批量地排空所有的节点池。这可能会导致你超出配额或遇到云提供商的容量问题。
- en: You should pay attention to your quota and ensure you have a sufficient quota
    regardless of upgrades. If you’re getting close to your quota ceiling, I suggest
    bumping it before engaging in a complex operation like a node pool upgrade. The
    last thing you want is to be in the middle of an upgrade when you need to scale
    your capacity due to business needs and realize you maxed out your quota.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该关注配额，并确保在升级时你的配额充足。如果你接近配额上限，建议在进行复杂操作（如节点池升级）之前先提高配额。你最不希望发生的情况是，在进行升级过程中，由于业务需求需要扩展容量时，才发现已经达到了配额上限。
- en: A good strategy for handling capacity issues and ramping up speed (how fast
    the cloud provider provision can instances for your new nodes) is to pre-allocate
    those instances. Again, this requires that you have a sufficient quota for the
    old node pool and the new node pool at the same time.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 处理容量问题并加速云服务提供商为新节点分配实例的速度的一个好策略是预先分配这些实例。同样，这要求你同时为旧节点池和新节点池拥有足够的配额。
- en: Let’s understand what happens if you don’t pre-allocate nodes in the new node
    pool. When you drain a node from the old node pool, all the pods are evicted from
    the node and become pending pods. Kubernetes will try to schedule these pods to
    existing nodes if any are available. The old node pool is cordoned, so either
    Kubernetes can find suitable nodes on other existing node pools (it’s a good thing
    that improves bin packing) or the cluster autoscaler will need to provision a
    new node. That takes several minutes. If you drain multiple nodes at the same
    time, then all the pods from these nodes will be pending for a few minutes until
    new nodes can be provisioned and your system’s capacity is degraded. In addition,
    if the cloud provider has capacity issues, maybe it can’t provision new nodes
    and your pods will remain pending until then.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来了解一下如果你没有预先分配新节点池中的节点会发生什么。当你从旧节点池中清理一个节点时，所有的 Pod 会被驱逐出该节点并变为待处理 Pod。Kubernetes
    会尝试将这些 Pod 调度到现有的节点上，如果有可用的节点。旧节点池会被标记为不可用，因此 Kubernetes 要么可以在其他现有节点池中找到合适的节点（这是一件提高箱子装载效率的好事），要么集群自动扩展器需要为新节点池配置一个新的节点。这需要几分钟的时间。如果你同时清理多个节点，那么这些节点上的所有
    Pod 都会在几分钟内处于待处理状态，直到可以为其配置新的节点，这时你的系统容量会下降。另外，如果云服务提供商的容量出现问题，可能它无法为新节点分配资源，你的
    Pod 会一直处于待处理状态，直到资源能够提供。
- en: Pre-allocating nodes means that the new node pool will have nodes ready to go.
    The moment a pod is evicted from the old node pool, it will immediately be scheduled
    to an available node in the new node pool.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 预先分配节点意味着新节点池中的节点已经准备就绪。只要一个 Pod 从旧节点池中被驱逐，它就会立即被调度到新节点池中的可用节点上。
- en: Dealing with workloads with no PDB
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理没有 PDB 的工作负载
- en: When draining a node, Kubernetes is mindful of **pod disruption budgets** (**PDBs**).
    If a deployment has a PDB that says only one pod can be unavailable and there
    are two pods of this deployment on the drained node, then Kubernetes will evict
    just one pod and wait until it is eventually scheduled before evicting the other
    pod. However, if you have workloads without PDBs, then that means Kubernetes is
    allowed to evict all the pods of those workloads at the same time. For most workloads,
    this is unacceptable. You should identify these workloads and work with their
    owner to add a PDB. Note that in the scenario of draining all nodes at once, workloads
    with no PDB are vulnerable even if they have many pods running on different nodes.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 当清理节点时，Kubernetes 会考虑 **Pod 中断预算** (**PDBs**) 的情况。如果某个部署的 PDB 设置为只能有一个 Pod 不可用，而该节点上有两个此部署的
    Pod，那么 Kubernetes 会只驱逐一个 Pod，并等待它被调度完成后再驱逐另一个 Pod。然而，如果你的工作负载没有 PDB，这意味着 Kubernetes
    可以同时驱逐这些工作负载的所有 Pod。对于大多数工作负载来说，这是不可接受的。你应该识别出这些工作负载，并与其所有者协作为其添加 PDB。需要注意的是，在一次清理所有节点的场景下，即使工作负载在不同节点上运行了多个
    Pod，没有 PDB 的工作负载仍然非常脆弱。
- en: Dealing with workloads with PDB zero
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理 PDB 为零的工作负载
- en: However, the opposite problem of unevictable pods is the bane of node pool upgrades.
    If a node contains an unevictable pod, then it can’t be drained, and Kubernetes
    will wait forever (or until the pod is manually deleted) before it fully drains
    the node. This can halt the upgrade process indefinitely and typically requires
    coordinating with the workload owner to resolve it.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无法驱逐的 Pod 是节点池升级中的另一大问题。如果一个节点上有无法驱逐的 Pod，那么该节点就无法被清理，Kubernetes 会无限期地等待（或者直到该
    Pod 被手动删除）才能完成节点的清理过程。这可能会无限期地阻碍升级进程，通常需要与工作负载所有者协作来解决这一问题。
- en: 'If a workload has a PDB with `minUnavailable: 0`, it means that Kubernetes
    is not allowed to evict even a single pod from the workload regardless of how
    many replicas the workload has.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '如果某个工作负载有一个 PDB，且其 `minUnavailable: 0`，则意味着 Kubernetes 不允许驱逐任何一个 Pod，无论该工作负载有多少副本。'
- en: Some workloads (usually stateful) are more sensitive than others and prefer
    not to be disrupted at all. This is, of course, an unrealistic expectation because
    the node itself might go bad or the underlying VM might disappear due to cloud
    provider issues, and then the pods scheduled on it will have to be evicted. It’s
    best to work with workload owners and come up with a solution that minimizes disruption,
    but still allows upgrades to progress. This has to be worked out before the upgrade
    process starts. You don’t want to be in a situation where a single workload holds
    a node pool upgrade process hostage, and you have to beg the workload owner to
    allow you to evict a pod.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作负载（通常是有状态的）比其他工作负载更为敏感，更不希望被打扰。当然，这是不现实的期望，因为节点本身可能出现故障，或者底层虚拟机由于云服务提供商问题可能消失，导致调度到该节点上的
    Pod 必须被驱逐。最好与工作负载的拥有者合作，提出一个最小化干扰的解决方案，同时仍能让升级过程继续进行。这些工作需要在升级过程开始之前就做好准备。你不希望遇到某个工作负载让整个节点池升级过程停滞不前的情况，并且还不得不请求工作负载的拥有者允许你驱逐一个
    Pod。
- en: 'But, in addition to strict PDB-zero workloads, you might run into effective
    PDB-zero situations. Consider a workload with a PDB of `minUnavailable: 1`. This
    is pretty common and means the workload allows one pod at a time to be unavailable.
    When draining the pods of this workload, Kubernetes is allowed to evict one pod
    as long as all the other pods are running. However, if even one of the pods of
    this workload is pending or unable to be ready due to any reason, then effectively
    the workload already has one pod unavailable, and the upgrade process will be
    halted again.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '但是，除了严格的 PDB-zero 工作负载之外，你还可能遇到有效的 PDB-zero 情况。考虑一个 PDB 为 `minUnavailable:
    1` 的工作负载。这种情况比较常见，意味着工作负载允许一次一个 Pod 不可用。在对该工作负载的 Pod 进行驱逐时，Kubernetes 允许驱逐一个 Pod，只要所有其他
    Pod 都在正常运行。然而，如果该工作负载的任何一个 Pod 因为某种原因处于待处理状态或无法准备就绪，那么实际上该工作负载已经有一个 Pod 不可用了，从而导致升级过程再次被暂停。'
- en: The best practice here is to identify these workloads before the upgrade process
    starts and ensure that all workloads are healthy and can participate in the node
    pool upgrade process.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最佳实践是，在升级过程开始之前识别这些工作负载，并确保所有工作负载都处于健康状态，并能够参与节点池的升级过程。
- en: However, even if you did all the preparation work ahead of time, some workloads
    might get into an unhealthy state during the upgrade process (remember we’re talking
    about a process that can take weeks).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使你提前做好了所有准备工作，在升级过程中一些工作负载可能会进入不健康状态（记住，我们讨论的是一个可能持续数周的过程）。
- en: I recommend having strong monitoring of the progress of the upgrade process,
    detecting stuck pods, and working with owners to resolve issues. In the case of
    pods that are scheduled on the old node pool and can’t be ready, it is a simple
    solution to just delete the pod, see it is scheduled to the new node pool, and
    let the workload owner resolve the problem on the new node pool.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议对升级过程的进度进行强有力的监控，及时发现卡住的 Pod，并与相关人员协作解决问题。如果某些 Pod 被调度到旧的节点池且无法准备就绪，那么简单的解决方案是删除这些
    Pod，确保它们被调度到新的节点池，然后让工作负载的拥有者在新节点池上解决问题。
- en: Other cases might require more creative solutions.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 其他情况可能需要更具创造性的解决方案。
- en: Let’s turn our attention to various problems that can occur in a cluster and
    how to handle them, especially at scale.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将注意力转向集群中可能出现的各种问题，以及如何处理它们，特别是在大规模情况下。
- en: Troubleshooting
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除
- en: In this section, we will cover the troubleshooting process in a production cluster
    and the logical procession of actions to take. The pod lifecycle involves multiple
    phases and failures can occur at each phase. In addition, pod containers go through
    their own mini lifecycle where init containers are running to completion and then
    the main containers start running. Let’s see what can go wrong along the way and
    how to handle it.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍生产集群中的故障排除过程以及采取的逻辑步骤。Pod 生命周期涉及多个阶段，且每个阶段都有可能发生故障。此外，Pod 容器经历自己的小生命周期，其中初始化容器运行完成后，主容器才会开始运行。让我们看看在这一过程中可能出现的问题以及如何处理它们。
- en: First, let’s look at pending pods.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看待处理待处理的 Pod。
- en: Handling pending pods
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理待处理的 Pod
- en: When a new pod was created, Kubernetes used to place it in the Pending state
    and try to find a node to schedule it on. However, since Kubernetes 1.26, there
    is an even earlier state where a pod can’t be scheduled.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个新 Pod 被创建时，Kubernetes 过去会将其置于待处理状态，并尝试找到一个节点来调度它。然而，从 Kubernetes 1.26 开始，Pod
    可以进入一个更早的状态，此时 Pod 无法被调度。
- en: 'Let’s create a new 1.26 kind cluster called “`trouble`" and enable the pod
    scheduling readiness feature. Here is the configuration file (`cluster-config.yaml`):'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的 1.26 版本的 kind 集群，命名为“`trouble`”，并启用 Pod 调度就绪功能。这里是配置文件（`cluster-config.yaml`）：
- en: '[PRE12]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And here is how to create the kind cluster:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建 kind 集群的方法：
- en: '[PRE13]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we’ll create a new namespace called `trouble` and take it from there.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个新的命名空间，名为 `trouble`，然后从那里开始。
- en: '[PRE14]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s create a pod with a scheduling gate called `no-scheduling-yet`:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个带有调度门控的 Pod，名为 `no-scheduling-yet`：
- en: '[PRE15]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, the pod has a status of `SchedulingGated`.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该 Pod 的状态为 `SchedulingGated`。
- en: The benefit of the scheduling gate is that if the pod can’t be scheduled yet
    due to issues like the quota, which need to be resolved externally, then the pod
    in this state will not cause a lot of churns to the Kubernetes scheduler, which
    will ignore it. After the external issue is resolved, you (or more likely an operator)
    can remove the feature gate and the pod will become a pending pod ready to be
    scheduled.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 调度门控的好处在于，如果 Pod 因为配额等问题无法调度，而这些问题需要外部解决，那么处于这种状态的 Pod 不会对 Kubernetes 调度器产生太多的干扰，调度器会忽略它。外部问题解决后，你（或更可能是运维人员）可以移除该功能门控，Pod
    将变为待处理状态，准备被调度。
- en: Now, let’s turn our attention to pending pods. It’s okay for a pod to be pending;
    however, if a pod is pending for more than a few minutes, something is wrong and
    we need to investigate it. I suggest having an alert set up for pods pending for
    more than X minutes (reasonable values for X can be between 10 and 60 minutes).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把注意力转向待处理的 Pods。Pod 处于待处理状态是正常的；然而，如果一个 Pod 已经待处理超过几分钟，说明有问题，我们需要进行调查。我建议为待处理超过
    X 分钟的 Pods 设置一个警报（X 的合理范围通常是 10 到 60 分钟）。
- en: 'There are two types of pending pods: temporarily pending pods and permanent
    pending pods. Temporarily pending pods may be scheduled to one of the existing
    node pools; however, there is currently no room on any of the nodes. If the node
    pool has autoscaling enabled, the cluster autoscaler will try to provision a new
    node. If the node pool has autoscaling disabled, then the pod will remain pending
    until some other pods complete or are evicted from a node to make room. Another
    category of temporarily unschedulable pods is if the target namespace has a resource
    quota that is maxed out at the moment. Here is an example, where the namespace
    has a resource quota of 1 CPU and a deployment with 3 replicas is created where
    each pod requests 0.5 CPU. Only 2 pods can fit with the namespace quota. The third
    pod will be pending:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 待处理的 Pods 有两种类型：暂时待处理的 Pods 和永久待处理的 Pods。暂时待处理的 Pods 可能会被调度到现有的某个节点池；然而，当前没有任何节点有足够的空间。如果节点池启用了自动扩容，集群自动扩容器将尝试创建新节点。如果节点池没有启用自动扩容，那么该
    Pod 将保持待处理状态，直到其他 Pods 完成或被驱逐以腾出空间。另一类暂时不可调度的 Pods 是当目标命名空间的资源配额已达到上限时。例如，命名空间的资源配额为
    1 CPU，并创建了一个包含 3 个副本的部署，其中每个 Pod 请求 0.5 CPU。此时，只有 2 个 Pods 能符合命名空间配额，第三个 Pod 会处于待处理状态：
- en: '[PRE16]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let’s create the deployment and see what happens:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建部署并看看会发生什么：
- en: '[PRE17]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We end up with just two running pods as expected:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们只会得到两个正在运行的 Pods，正如预期的那样：
- en: '[PRE18]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note, that there is no third pending pod in this case. Kubernetes is smart enough
    to create only two pods in this case.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下并没有第三个待处理的 Pod。Kubernetes 足够智能，能够在这种情况下只创建两个 Pods。
- en: 'The reason is the namespace quota:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是命名空间配额：
- en: '[PRE19]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Permanent pending pods are pods that can’t be scheduled on any of the available
    node pools, so provisioning a new node will not help. There are several categories
    of such permanently unschedulable pods:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 永久待处理的 Pods 是指无法在任何可用的节点池上调度的 Pods，因此，创建一个新节点也无法解决问题。这类永久不可调度的 Pods 分为几类：
- en: All the node pools have taints, and the pod doesn’t have the proper tolerations.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有节点池都有污点，而该 Pod 没有适当的容忍度。
- en: The pod requests more resources than are available on any of the node pools.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 请求的资源超过了任何节点池上的可用资源。
- en: The pod is waiting for a persistent volume.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该 Pod 正在等待持久卷。
- en: The pod has incorrect `nodeSelector` or `nodeAffinity`.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 的 `nodeSelector` 或 `nodeAffinity` 设置不正确。
- en: 'Let’s delete the previous deployment and resource quota and look at an example
    of a pod that just requests way too many resources (666 CPU cores):'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除之前的部署和资源配额，并查看一个请求了过多资源（666 个 CPU 核心）的 Pod 示例：
- en: '[PRE20]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we look at the pod that was created now, we can see it is indeed pending:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看当前创建的 Pod，能够看到它确实处于等待状态：
- en: '[PRE21]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To understand why it is pending, we can look at the status of the pod:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么它处于待处理状态，我们可以查看 Pod 的状态：
- en: '[PRE22]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The message is pretty clear and explains that 0 out of 1 node is available to
    schedule. It even says that 1 node has insufficient CPU. If there were other nodes
    in the cluster with other reasons, it would list them too.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 信息非常明确，说明 1 个节点中有 0 个可用于调度。它甚至指出 1 个节点的 CPU 不足。如果集群中有其他节点存在其他原因，信息中也会列出。
- en: Pending pods don’t use resources and don’t take up space in nodes; however,
    they put some pressure on the API server and also it means that some workloads
    don’t get to do their work and they are waiting for a node to be scheduled on,
    which might be very serious in production. Mind your pending pods and make sure
    to resolve any issues quickly.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 待处理的 Pod 不占用资源，也不占用节点空间；然而，它们会给 API 服务器带来一定压力，同时这意味着一些工作负载无法完成任务，它们在等待节点的调度，这在生产环境中可能非常严重。请留意你的待处理
    Pod，确保快速解决任何问题。
- en: The next category of problems is about pods that are scheduled to a node but
    are unable to run.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 下一类问题是关于已调度到节点但无法运行的 Pod。
- en: Handling scheduled pods that are not running
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理调度到节点但未运行的 Pod
- en: 'There may be several reasons why a scheduled pod is not running. One of the
    most common ones is failure to pull an image required by one of the pod’s containers.
    The kubelet will just keep trying and the pod’s status will show as `ErrImagePull`:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种原因可能导致调度的 Pod 无法运行。其中一个最常见的原因是无法拉取 Pod 容器所需的镜像。kubelet 会不断尝试，而 Pod 的状态将显示为
    `ErrImagePull`：
- en: '[PRE23]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s check the pods:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下 Pod：
- en: '[PRE24]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To see a more elaborate message, we can check the `containerStatuses` field
    of the status:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看更详细的消息，我们可以检查状态中的 `containerStatuses` 字段：
- en: '[PRE25]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Image pull errors could relate to a misconfigured image. The image name or the
    image tag might be wrong. However, the image may be correct but might have been
    deleted accidentally from the registry. If you try to pull from a private registry,
    then possibly you don’t have the correct permissions. Finally, the image registry
    may be unavailable. For example, Docker Hub often has rate limits.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像拉取错误可能与镜像配置错误有关。镜像名称或标签可能错误。然而，镜像本身可能正确，但可能已被意外删除。如果尝试从私有注册表拉取镜像，可能是因为没有正确的权限。最后，镜像注册表可能不可用。例如，Docker
    Hub 经常会有速率限制。
- en: You may prefer to pull all your images from a single source you control, where
    you can scan and curate the images and ensure that images don’t disappear from
    you. If you’re on the cloud, then every cloud provider offers their image registry.
    This should be the preferred option in most cases. You may save some money by
    using a private registry, and you may prefer a different solution in hybrid cloud
    scenarios.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能更倾向于从一个你控制的单一来源拉取所有镜像，在那里你可以扫描和管理镜像，并确保镜像不会从你这里消失。如果你使用的是云服务，每个云提供商都会提供自己的镜像注册表。在大多数情况下，这应该是首选选项。使用私有注册表可能能节省一些费用，且在混合云场景下，你可能更倾向于使用不同的解决方案。
- en: 'Another reason that a pod doesn’t start running is an init container that doesn’t
    complete:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个导致 Pod 无法启动的原因是 init 容器没有完成：
- en: '[PRE26]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Checking the pod status, we can see that it is stuck in the init phase because
    our init container is in the pause container, which never completes:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Pod 状态时，我们可以看到它卡在 init 阶段，因为我们的 init 容器处于暂停状态，这个状态永远不会完成：
- en: '[PRE27]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Sometimes the pod starts running, but the container keeps failing. In this
    case, you need to check your pod’s logs or your Dockerfile for the reason. Here
    is a pod that keeps crashing because its Dockerfile command just exists with exit
    code 1:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 有时 Pod 启动了，但容器不断失败。在这种情况下，你需要检查 Pod 的日志或 Dockerfile 以查明原因。下面是一个因为 Dockerfile
    命令以退出代码 1 退出而不断崩溃的 Pod：
- en: '[PRE28]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The result is that the pod will have a status of `RunContainerError`. Kubernetes
    will keep restarting the pod (assuming the default restart policy of always):'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 Pod 会有 `RunContainerError` 状态。Kubernetes 会继续重启 Pod（假设默认的重启策略是一直重启）：
- en: '[PRE29]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Getting your pods and containers to a running state is a good start, but it
    is not enough. In order for Kubernetes to send requests to your pods, all the
    containers must be ready. Let’s see what problems you might run into.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 让你的 Pods 和容器处于运行状态是一个好的开始，但这还不够。为了让 Kubernetes 向你的 Pods 发送请求，所有容器必须处于就绪状态。让我们看看你可能会遇到哪些问题。
- en: Handling running pods that are not ready
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理未就绪的运行中 Pods
- en: If all your init containers are completed and all your main containers are running
    with no errors, then probes come into play. Kubernetes considers a pod with running
    containers ready to receive requests if no probes are defined or if all probes
    for all containers succeed. The startup probe is checked initially until it succeeds.
    If it fails, the pod is not considered ready. If your container has a hung startup
    pod, it will never be ready.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有初始化容器都已完成，且所有主容器都在没有错误的情况下运行，那么探针将发挥作用。如果没有定义任何探针，或者所有容器的所有探针都成功，Kubernetes
    会认为具有运行容器的 Pod 准备好接收请求。启动探针最初会检查，直到它成功。如果失败，Pod 将不被认为是就绪的。如果容器的启动挂起，Pod 将永远不会就绪。
- en: Kubernetes will eventually kill and restart your container and the startup probe
    will have another chance.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 最终会杀死并重启你的容器，启动探针将有另一次机会。
- en: 'Here is a deployment where the main container has a startup probe that will
    always fail (the pause container doesn’t even listen on port 80). The pod will
    never get to the ready state. After some retries and delays defined by the startup
    probe, Kubernetes will restart the container and the cycle will repeat:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个部署示例，其中主容器有一个总是失败的启动探针（暂停容器甚至不在端口 80 上监听）。Pod 永远不会进入就绪状态。经过启动探针定义的几次重试和延迟后，Kubernetes
    会重启容器，循环将继续：
- en: '[PRE30]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Checking the pod shows that the pod is in `CrashloopBackoff` and Kubernetes
    keeps restarting the container:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Pod 显示 Pod 处于 `CrashloopBackoff` 状态，Kubernetes 一直在重启容器：
- en: '[PRE31]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that the delay between restarts grows exponentially to avoid a bad container
    putting a lot of stress on the API server and the kubelet having to keep restarting
    often.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，重启之间的延迟会呈指数增长，以避免一个坏容器给 API 服务器带来很大的压力，并且 kubelet 需要频繁重启容器。
- en: If there is no startup probe or it succeeds, the hurdle is the readiness probe.
    It works very similar to a startup probe, except Kubernetes will not restart the
    container. It will just keep checking the readiness probe. When a readiness probe
    fails, the pod will be removed from the endpoints list of any service that matches
    its labels, so it doesn’t get to handle any requests. However, the pod remains
    alive and consumes resources on the node.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有启动探针或启动探针成功，接下来的障碍就是就绪探针。它的工作方式与启动探针非常相似，但 Kubernetes 不会重启容器。它只会继续检查就绪探针。当就绪探针失败时，Pod
    将从任何匹配其标签的服务的端点列表中移除，因此它不会处理任何请求。然而，Pod 会保持存活并消耗节点上的资源。
- en: 'Let’s see it in action:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看实际操作：
- en: '[PRE32]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you can see, the pod is running for an hour, it never gets ready, and it
    is not restarted:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Pod 已运行了一小时，但从未处于就绪状态，并且没有被重启：
- en: '[PRE33]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The last type of probe is the liveness probe. It works just like the startup
    probe (the container will get restarted and the pod will be in `CrashloopBackoff`),
    except it is checked periodically, even if it succeeds while the startup probe
    is a one-time deal. Once it succeeds, it is never checked again.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种探针是存活探针。它的工作方式与启动探针类似（容器将被重启，Pod 将进入 `CrashloopBackoff`），不同之处在于它会定期检查，即使启动探针只会检查一次，一旦成功，就不会再进行检查。
- en: The reason both startup and liveness probes are needed is that some containers
    need a longer startup period, but once they are initialized, then periodic liveness
    checks should be shorter.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 启动探针和存活探针都需要的原因是，有些容器需要更长的启动时间，但一旦初始化完成，存活检查的周期应该较短。
- en: That covers troubleshooting the pod lifecycle. When pods are scheduled but are
    not running or are not ready, it has cost implications. Let’s move on and consider
    cost management.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了故障排除 Pod 生命周期的问题。当 Pods 被调度但没有运行或未就绪时，会带来成本影响。让我们继续讨论成本管理。
- en: Cost management
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本管理
- en: 'When running Kubernetes at a large scale in the cloud, one of the major concerns
    is the cost of the infrastructure. Cloud providers offer a variety of infrastructure
    options and services for your Kubernetes clusters. These are expensive. To harness
    your costs, make sure you follow best practices such as:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中大规模运行 Kubernetes 时，一个主要的关注点是基础设施的成本。云提供商为你的 Kubernetes 集群提供了各种基础设施选项和服务，这些服务非常昂贵。为了控制成本，确保你遵循最佳实践，如：
- en: Having a cost mindset
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有成本意识
- en: Cost observability
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本可观察性
- en: The smart selection of resources
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能资源选择
- en: Efficient usage of resources
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效使用资源
- en: Discounts, reserved instances, and spot instances
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣、预留实例和抢占实例
- en: Invest in local environments
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投资本地环境
- en: Let’s review them one by one.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一回顾它们。
- en: Cost mindset
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本思维
- en: 'Engineers often neglect cost or put it way down the priority list. I often
    think in this order: make it work, make it fast, make it last, make it secure,
    and only then make it cheap. This is not necessarily a bad thing, especially for
    startup companies or new projects. Growth and velocity are often the top priorities.
    After all, if you don’t have a good product, and you don’t have customers, then
    the business will fail even if your costs are zero.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师通常忽视成本或将其排在优先级列表的后面。我通常按以下顺序思考：让它工作，确保它快速，让它持久，确保它安全，最后才考虑让它便宜。这不一定是坏事，特别是对于初创公司或新项目来说。增长和速度通常是最优先的。毕竟，如果你没有一个好的产品，也没有客户，那么即使成本为零，业务也会失败。
- en: In addition, when the system is small, the absolute cost might be relatively
    small even if there is a lot of waste. Add to that the fact that cloud providers
    lure companies in with generous credits.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当系统规模较小时，即使存在大量浪费，绝对成本可能相对较小。再加上云提供商通过慷慨的积分吸引公司。
- en: However, if you are part of a large enterprise or your startup succeeds and
    grows, at some point, cost will become a significant concern. At this point, you
    need to shift your thinking and have cost as the primary concern for everything
    you do. Cost may or may not be aligned with other initiatives. For example, everybody
    loves Pareto improvements. If you just manage to use 20% fewer VMs to accomplish
    the same task, then you will automatically save a lot of money without negatively
    impacting any other aspect.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你是一个大型企业的一部分，或者你的创业公司成功并发展壮大，那么在某个时刻，成本将成为一个重要的关注点。到那时，你需要转变思维，把成本作为你所做一切事情的首要考虑。成本可能与其他计划不一致。例如，每个人都喜欢帕累托改善。如果你通过使用少20%的虚拟机来完成同样的任务，那么你将自动节省大量资金，而不会对其他方面产生负面影响。
- en: But those easy wins and low-hanging fruit will eventually dry up. Then, you
    get to harder decisions. For example, caching the last week of data in memory
    will give you excellent response times, but at a large cost. What if you just
    cache one day?
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 但那些轻松的胜利和低挂的果实最终会枯竭。然后，你将面临更困难的决策。例如，将过去一周的数据缓存到内存中将提供非常好的响应时间，但代价很大。如果你只缓存一天呢？
- en: Availability and redundancy are often at odds with cost as well. Do you really
    need a full-fledged zero-downtime active-active setup across multiple availability
    zones, regions, and cloud providers or can you get by with some downtime and recovery
    from backups in the event of catastrophic failure?
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性和冗余通常与成本相冲突。你是否真的需要在多个可用区、区域和云提供商之间设置一个完整的零停机时间的主动-主动架构，还是在发生灾难性故障时，可以接受一些停机并从备份中恢复？
- en: You may end up choosing the more expensive option, but you should do it with
    an explicit understanding of how much you pay for it and ensure the value you
    receive is greater.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能最终会选择更昂贵的选项，但你应该明确了解你为此支付了多少，并确保你获得的价值大于成本。
- en: That takes us to the next topic of cost observability.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了下一个话题：成本可观察性。
- en: Cost observability
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本可观察性
- en: To manage your infrastructure cost on Kubernetes and in the cloud in general,
    you must have strong cost-oriented observability. Let’s look at some of the ways
    to accomplish it.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 和云环境中管理你的基础设施成本，你必须拥有强大的成本导向可观察性。让我们来看看一些实现这一目标的方法。
- en: Tagging
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签
- en: Tagging is associating every resource with a set of tags or labels. From a cost
    perspective, tags should enable you to attribute the cost of any infrastructure
    resource to the relevant stakeholders. For example, you may have a team tag or
    an owner tag. If the resources provisioned by a specific team suddenly grow rapidly,
    you can narrow down the issue more quickly. The specific tags are up to you. Common
    tags may include environment (production, staging, and development), release,
    and git sha. Many resources in the cloud come with cloud-provider tags that you
    can take advantage of.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是将每个资源与一组标签或标识相关联。从成本的角度来看，标签应该使你能够将任何基础设施资源的成本归因于相关的利益相关者。例如，你可能有团队标签或所有者标签。如果某个特定团队所配置的资源突然快速增长，你就可以更快地缩小问题范围。具体标签由你决定。常见的标签可能包括环境（生产、预发布和开发）、发布版本和
    git sha。许多云资源附带云提供商标签，你可以利用这些标签。
- en: Policies and budgets
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 政策和预算
- en: 'Policies and budgets let you rein in wild spending on infrastructure. Some
    policies are implicit, such as a namespace resource quota that will block the
    provisioning of excess resources. However, other policies may be more cost-specific
    and be informed by cost tracking. Budgets let you set a hard limit on spending
    at different scopes. All cloud providers offer budgets as part of their cost management
    solutions:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 策略和预算让你能够控制基础设施上的过度支出。一些策略是隐性的，例如命名空间资源配额，它会阻止超额资源的配置。然而，其他策略可能更具成本针对性，并通过成本追踪来提供信息。预算让你在不同的范围内设置硬性支出上限。所有云服务提供商都将预算作为其成本管理解决方案的一部分：
- en: 'Tutorial: Create and manage Azure budgets: [https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets)'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 教程：创建和管理 Azure 预算：[https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets)
- en: 'Managing your costs with AWS Budgets: [https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html](https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html)'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 AWS Budgets 管理成本：[https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html](https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html)
- en: 'Create, edit, or delete budgets and budget alerts in GCP: [https://cloud.google.com/billing/docs/how-to/budgets](https://cloud.google.com/billing/docs/how-to/budgets)'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 GCP 中创建、编辑或删除预算和预算警报：[https://cloud.google.com/billing/docs/how-to/budgets](https://cloud.google.com/billing/docs/how-to/budgets)
- en: Policies and budgets are great, but sometimes they are not sufficient, or you
    don’t have the cycles to specify and update them. This is where alerting comes
    into play.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 策略和预算很重要，但有时它们不足以应对所有情况，或者你没有时间去指定和更新它们。这时，警报功能就发挥作用了。
- en: Alerting
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 警报
- en: Budgets are often the last resort to mitigate against rogue infrastructure provisioning
    or accidental runaway provisioning. For example, you may set broad budgets for
    several overall categories, like no more than $500,000 of compute spending. These
    budgets of course need to align with business growth and make sure that they don’t
    cause an incident if you temporarily need to provision more infrastructure to
    handle a temporary spike in demand. Budgets are often set or modified only with
    top management approval.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 预算通常是应对不当基础设施配置或意外超支配置的最后一道防线。例如，你可能会为几个总体类别设置宽泛的预算，如计算支出不超过 500,000 美元。这些预算当然需要与业务增长保持一致，并确保在你临时需要配置更多基础设施以应对需求短期激增时，不会引发任何事故。预算通常只在高层管理审批后才会设定或修改。
- en: Fine-grained and day-to-day cost management alerts are much more nimble and
    practical. If you cross or come close to some cost limit, you can set alerts that
    will let you know and escalate as necessary. The alerts rely on proper tagging,
    so you can have meaningful information to evaluate the cause of the cost increase
    and the responsible party.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化的、日常的成本管理警报更加灵活和实用。如果你超过或接近某个成本限制，可以设置警报来提醒你，并在必要时进行升级。警报依赖于适当的标签，这样你就可以获得有意义的信息来评估成本增加的原因和负责方。
- en: As you can see, managing costs is a dynamic and complex activity. You need good
    tools to help you.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，成本管理是一个动态且复杂的活动。你需要好的工具来帮助你。
- en: Tools
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工具
- en: 'All the cloud providers have strong cost management tools. Check out:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 所有云服务提供商都拥有强大的成本管理工具。了解更多：
- en: 'AWS Cost Explorer: [https://aws.amazon.com/aws-cost-management/aws-cost-explorer/](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/)'
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS 成本探查器：[https://aws.amazon.com/aws-cost-management/aws-cost-explorer/](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/)
- en: 'Azure Cost Analyzer: [https://www.serverless360.com/azure-cost-analysis](https://www.serverless360.com/azure-cost-analysis)'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Azure 成本分析器：[https://www.serverless360.com/azure-cost-analysis](https://www.serverless360.com/azure-cost-analysis)
- en: 'GCP Cost Management: [https://cloud.google.com/cost-management/](https://cloud.google.com/cost-management/)'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GCP 成本管理：[https://cloud.google.com/cost-management/](https://cloud.google.com/cost-management/)
- en: In addition, you may opt to use a multi-cloud open-source tool like kubecost
    ([https://www.kubecost.com](https://www.kubecost.com)) or a paid product like
    cast.ai ([https://cast.ai](https://cast.ai)). Of course, you can do everything
    yourself, ingest cost metrics from the cloud provider into Prometheus, and build
    your Grafana dashboards and alerts on top of them.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以选择使用像 kubecost ([https://www.kubecost.com](https://www.kubecost.com))
    这样的多云开源工具，或者像 cast.ai ([https://cast.ai](https://cast.ai)) 这样的付费产品。当然，你也可以自己做这一切，从云服务提供商获取成本指标到
    Prometheus，然后在其上构建 Grafana 仪表盘和警报。
- en: Remember that picking the right set of tools can literally pay for itself very
    quickly.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，选择合适的工具组合可以迅速为你带来收益。
- en: The smart selection of resources
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智能选择资源
- en: The cloud offers a plethora of choices and combinations for resources like VMs,
    networking, and disks. We covered all the considerations in the *Bin packing and
    utilization* section earlier in this chapter. However, with a focus on cost, you
    should ensure that you understand that you may be able to get the job done with
    a cheaper alternative and reduce your costs significantly. Familiarize yourself
    with the inventory of resources and stay up to date as cloud providers update
    their offerings and may change prices too.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供了丰富的资源选择和组合，如虚拟机、网络和磁盘等。我们在本章前面的 *Bin packing 和利用率* 部分已经覆盖了所有相关的考虑因素。然而，在关注成本的同时，你应该确保自己理解，可能会有更便宜的替代方案能够完成工作，并大幅降低成本。熟悉资源清单，并保持更新，因为云服务商会不断更新其产品并可能调整价格。
- en: Efficient usage of resources
  id: totrans-509
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源的高效使用
- en: When you’re cutting costs, any unused resources are a red flag. You leave money
    on the table. It is often necessary to build in flexibility. For example, the
    industry average CPU utilization is in the range of 40%-60%. This might seem low,
    but it is not easy to improve on that in a very dynamic environment with constraints
    like high availability, quick recovery, and the ability to scale up quickly.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在削减成本时，任何未使用的资源都应该引起警惕。你正在浪费金钱。通常有必要增加灵活性。例如，行业平均的CPU利用率在40%-60%之间。这看起来可能比较低，但在一个非常动态的环境中，要在高可用性、快速恢复和快速扩展等限制下提升这一点并不容易。
- en: Discounts, reserved instances, and spot instances
  id: totrans-511
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折扣、预留实例和竞价实例
- en: One of the best ways to reduce costs is to just pay less money to the cloud
    providers for the same resources. The common ways to accomplish this are discounts,
    reserved instances, and spot instances.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 降低成本的最佳方法之一就是为相同的资源支付更少的钱。常见的实现方式包括折扣、预留实例和竞价实例。
- en: Discounts and credits
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 折扣和积分
- en: Discounts are the best. They only have an upside. You simply pay less than the
    sticker price. That’s it. Well, it’s not that easy. You will need to negotiate
    to get the best prices and often show promise for growth and commitment to stay
    longer on the platform.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣是最好的选择。它们只有好处。你只需支付低于标价的费用。就是这么简单。嗯，事情并非那么简单。你需要通过谈判获得最优惠的价格，并且通常需要展示出增长潜力和承诺，表示你会长期使用该平台。
- en: Credits are another great way to offset initial cloud spending. All the major
    cloud providers offer various credit programs, and you may be able to negotiate
    more credits too.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 积分是另一种抵消初期云开支的好方法。所有主要的云服务商都提供各种积分计划，你也许还可以协商获得更多的积分。
- en: AWS has the Activate program, targeted mostly at startups, where you can get
    up to $100,000 of AWS credit. See [https://aws.amazon.com/activate/](https://aws.amazon.com/activate/).
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 有 Activate 计划，主要面向初创公司，在该计划下，你可以获得最多 $100,000 的 AWS 积分。详情请见 [https://aws.amazon.com/activate/](https://aws.amazon.com/activate/)。
- en: Azure has the Microsoft for Startups program, which offers up to $150,000 of
    Azure credit. See [https://www.microsoft.com/en-us/startups](https://www.microsoft.com/en-us/startups).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 有 Microsoft for Startups 计划，提供最多$150,000的 Azure 积分。详情请见 [https://www.microsoft.com/en-us/startups](https://www.microsoft.com/en-us/startups)。
- en: GCP has the Google for Startups cloud program, which offers (like AWS) up to
    $100,000 of GCP credit. See [https://cloud.google.com/startup](https://cloud.google.com/startup).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 有 Google for Startups 云计划，提供（与 AWS 类似）最多 $100,000 的 GCP 积分。详情请见 [https://cloud.google.com/startup](https://cloud.google.com/startup)。
- en: These programs are designed to boost young startups without a lot of revenue.
    Let’s move on to options for established enterprise organizations that still want
    to reduce their cloud spending.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 这些程序旨在帮助年轻的初创公司，在收入不多的情况下推动其发展。接下来我们来看看一些适用于已经成立的企业组织的选项，这些组织仍然希望减少云开支。
- en: Reserved instances
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预留实例
- en: Reserved instances are a very good way to reduce your costs. They require that
    you purchase capacity in bulk and commit for a long period (one year or three
    years). The longer period carries better discounts. Overall, the discounts are
    significant and can vary from 30% to 75% compared to on-demand prices.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 预留实例是降低成本的一个非常好的方法。它们要求你批量购买容量，并承诺长期使用（一年或三年）。承诺更长时间将带来更好的折扣。总体而言，折扣非常可观，与按需价格相比，折扣可以从30%到75%不等。
- en: Beyond the price, reserved instances also ensure capacity compared to on-demand
    instances, which may temporarily be unavailable for particular instance types
    in a particular availability zone.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 除了价格外，预留实例相比按需实例还能确保容量，后者在特定的可用区内可能会暂时无法提供某些实例类型。
- en: The downsides of reserved instances are that you typically have to prepay, and
    the commitment is often tied to specific instance types and regions. You may be
    able to exchange equivalent reservations, but you’ll have to check the terms and
    conditions of the cloud provider. Also, if you are unable to use all your reserved
    capacity, you still pay for it (although at a very discounted price).
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 预留实例的缺点是你通常需要预付费，而且承诺通常与特定的实例类型和区域挂钩。你可能可以交换等效的预留实例，但你需要查看云提供商的条款和条件。此外，如果你未能使用完所有的预留容量，你仍然需要为其支付费用（尽管价格大幅折扣）。
- en: If you consider reserved instances (RIs), you may opt for a limited capacity
    of reserved instances, which you know you can always utilize, and then use on-demand
    and spot instances to handle spikes and take advantage of the elasticity of the
    cloud. If you discover later that you spend too much on on-demand, you can always
    reserve more instances and come up with a mix of three-year reserved instances,
    one-year reserved instances, on-demand, and spot instances. This is a great segue
    to the next item on the list, which is spot instances.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑使用预留实例（RIs），你可以选择一个有限容量的预留实例，这样你就可以保证始终能够使用它们，然后使用按需实例和抢占式实例来处理流量高峰并利用云的弹性。如果后来发现你在按需实例上的支出过多，你总是可以预留更多实例，并制定一个三年期、1年期预留实例、按需实例和抢占式实例的组合。这也是引入下一个话题的好时机——抢占式实例。
- en: Spot instances
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抢占式实例
- en: Cloud providers love reserved instances. They sell them, provision them, make
    their profit, and can forget about them (except for making sure they’re up and
    running). The on-demand side is very different. Cloud providers have to make sure
    they can reasonably provision more capacity when their customers demand it. In
    particular, cloud providers should roughly have enough capacity to handle the
    quota of each customer even if the customer significantly underutilizes their
    quota. That means that, in practice, under normal conditions, cloud providers
    have a lot of idle or underutilized capacity. This is where spot instances come
    in. Cloud providers can sell this excess capacity, which is in theory allocated
    for on-demand use. If it is needed, then the cloud provider just takes away the
    spot instances and provides them to the on-demand customers.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商喜欢预留实例。他们销售这些实例、提供这些实例、从中获利，并可以忘记它们（除了确保它们正常运行）。按需服务则完全不同。云服务提供商必须确保，当客户需要时，他们能够合理地提供更多的容量。特别是，云服务提供商应该大致有足够的容量来处理每个客户的配额，即使客户的配额使用量非常低。这意味着在实际操作中，通常情况下，云服务提供商有大量的闲置或未充分利用的容量。这就是抢占式实例的作用。云服务提供商可以销售这些多余的容量，这些容量理论上是为按需使用分配的。如果需要，云服务提供商就可以收回抢占式实例，并将其提供给按需客户。
- en: Why should you use spot instances? Well, because they are much cheaper. Due
    to their potentially ephemeral nature, they carry significant discounts of up
    to 90%. Remember, from the cloud provider’s point of view, this is free money.
    Those instances are already accounted for and paid for by the markup of on-demand
    instances in use and the quota ratio of each customer.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要使用抢占式实例？因为它们便宜得多。由于它们可能是临时性的，因此提供了高达 90% 的显著折扣。记住，从云服务提供商的角度来看，这其实是免费的收入。这些实例已经通过按需实例的加价和每个客户的配额比例计算和支付过了。
- en: In practice, spot instances don’t disappear from under you very quickly. The
    Kubernetes way advocates that workloads shouldn’t care too much about specific
    nodes. If you run on a spot instance and it goes away, all your pods will be evicted
    and scheduled to other nodes. If you have sensitive workloads that don’t handle
    eviction from their node well, then these workloads are not good Kubernetes citizens
    in the first place. Nodes become unhealthy all the time, regardless, and your
    workloads should be able to handle eviction.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，抢占式实例不会很快从你下面消失。Kubernetes 的方式提倡工作负载不应过于关心特定节点。如果你在抢占式实例上运行并且它消失了，所有你的
    Pod 都会被驱逐并调度到其他节点。如果你有敏感的工作负载，无法很好地处理从节点上的驱逐，那么这些工作负载本身就不是好的 Kubernetes 公民。节点时常变得不健康，无论如何，你的工作负载应该能够处理驱逐。
- en: However, there is one situation that needs to be addressed, specifically if
    you run critical workloads on spot instances. In case of a zonal outage of a specific
    instance type, it is possible that many spot instances will be taken by the cloud
    provider. I suggest having empty fallback on-demand node pools with the same or
    similar instance type and the same labels and taints. If a node pool using spot
    instances suddenly loses a lot of nodes and is unable to scale up (because the
    spot instances are unavailable), then you can scale up your empty on-demand node
    pool and your pods will be scheduled there until spot instances become available
    again.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一种情况需要特别关注，尤其是在你使用抢占实例运行关键工作负载时。如果某个实例类型所在的可用区发生故障，可能会有许多抢占实例被云服务提供商回收。我建议你配置空的后备按需节点池，使用相同或类似的实例类型，并且设置相同的标签和污点。如果使用抢占实例的节点池突然失去大量节点，且无法扩展（因为抢占实例不可用），你就可以扩展空的按需节点池，直到抢占实例重新可用，这时你的Pod会被调度到空的节点池中。
- en: Make sure you have enough quota for the fallback node pools to pick up the slack
    if the equivalent spot instances are unavailable.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你为后备节点池预留足够的配额，以便在相应的抢占实例不可用时能够顶上。
- en: Next, let’s talk about local environments and how they can help us save money.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论本地环境以及它们如何帮助我们节省成本。
- en: Invest in local environments
  id: totrans-532
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投资本地环境
- en: Organizations practice different protocols of development and testing. Some
    organizations do a lot of testing in the cloud in staging and development environments.
    Sometimes, engineers provision infrastructure for various experiments and tests.
    Such development and test environments can be difficult to manage effectively
    as infrastructure is often provisioned in an ad hoc manner. There are solutions
    like disposable Kubernetes clusters and virtual clusters. Another direction is
    to invest in local development environments that engineers can run on their local
    machines. The advantages are that these environments are often very quick to set
    up and discard, and they don’t incur any expensive cloud costs. The downside is
    that such environments might not be fully representative of the staging or production
    environments. I suggest looking into local environments and finding use cases
    that will save cloud costs without compromising other critical aspects of the
    system.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 各个组织采用不同的开发和测试协议。有些组织在云环境中进行大量的测试，尤其是在预发布和开发环境中。有时，工程师会为各种实验和测试配置基础设施。这类开发和测试环境通常很难有效管理，因为基础设施往往是临时配置的。像一次性Kubernetes集群和虚拟集群这样的解决方案已经出现。另一种方向是投资本地开发环境，工程师可以在本地机器上运行这些环境。它们的优点是，这些环境通常非常快速地搭建和销毁，而且不会产生高昂的云费用。缺点是，这些环境可能无法完全代表预发布或生产环境。我建议考虑本地环境，并寻找可以节省云费用的使用场景，同时不影响系统的其他关键方面。
- en: Summary
  id: totrans-534
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered in depth what it takes to run large-scale Managed
    Kubernetes systems in production. We looked at managing multiple clusters, building
    effective processes, handling infrastructure at scale, managing clusters and node
    pools, bin packing and utilization, upgrading Kubernetes, and troubleshooting
    and cost management. That’s a lot, but even that is just the tip of the iceberg.
    There is no substitute for in-depth familiarity with your use cases and special
    concerns. The bottom line is that large-scale enterprise systems are complex to
    manage, but Kubernetes gives you a lot of industrial-strength tools to accomplish
    it.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们深入探讨了运行大规模托管Kubernetes系统所需的各项要求。我们讨论了如何管理多个集群、建立有效的流程、处理大规模的基础设施、管理集群和节点池、二进制打包和资源利用、升级Kubernetes、故障排除以及成本管理。这些内容涵盖了很多，但即便如此，也只是冰山一角。对于你的用例和特殊问题，深入的了解是不可替代的。最终结论是，大规模的企业系统管理起来非常复杂，但Kubernetes为你提供了大量工业级的工具来完成这项工作。
- en: 'The next chapter will conclude the book. We will look at the future of Kubernetes
    and the road ahead. Spoiler alert: the future is very bright. Kubernetes has established
    itself as the gold standard for cloud-native computing. It is being used across
    the board, and it keeps evolving responsibly. An entire support system has developed
    around Kubernetes, including training, open-source projects, tools, and products.
    The community is amazing, and the momentum is very strong.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将总结本书的内容。我们将展望Kubernetes的未来及其前景。剧透警告：未来非常光明。Kubernetes已经确立了自己在云原生计算领域的黄金标准地位。它正在各个领域广泛使用，并且不断负责任地发展。围绕Kubernetes，已经建立了完整的支持体系，包括培训、开源项目、工具和产品。社区非常强大，发展势头十足。
- en: Join us on Discord!
  id: totrans-537
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord！
- en: Read this book alongside other users, cloud experts, authors, and like-minded
    professionals.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他用户、云计算专家、作者及志同道合的专业人士一起阅读这本书。
- en: Ask questions, provide solutions to other readers, chat with the authors via.
    Ask Me Anything sessions and much more.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 提问、为其他读者提供解决方案、通过问我任何问题的环节与作者互动，还有更多。
- en: Scan the QR code or visit the link to join the community now.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描二维码或访问链接立即加入社区。
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
- en: '![](img/QR_Code844810820358034203.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code844810820358034203.png)'
