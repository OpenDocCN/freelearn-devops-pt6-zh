- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Implementing Service Mesh for Cross-Cutting Concerns
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现跨切关注点的服务网格
- en: In the previous chapter, we looked at OpenEBS cloud-native storage solutions
    so that we can provide persistent storage for our container applications. We also
    looked at how **Container Attached Storage** (**CAS**) is swiftly gaining acceptance
    as a viable solution for managing stateful workloads and utilizing persistent,
    fault-tolerant stateful applications. MicroK8s comes with built-in support for
    OpenEBS, making it the ideal option for running Kubernetes clusters in air-gapped
    Edge/IoT environments. Using the OpenEBS storage engine, we configured and implemented
    a PostgreSQL stateful workload. We also went over some best practices to keep
    in mind when creating a persistent volume and while selecting OpenEBS data engines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了 OpenEBS 云原生存储解决方案，以便为我们的容器应用程序提供持久存储。我们还讨论了 **容器附加存储**（**CAS**）如何迅速获得作为管理有状态工作负载并利用持久、容错的有状态应用程序的可行解决方案的接受。MicroK8s
    内置支持 OpenEBS，使其成为在隔离的 Edge/IoT 环境中运行 Kubernetes 集群的理想选择。通过 OpenEBS 存储引擎，我们配置并实现了
    PostgreSQL 有状态工作负载。我们还回顾了一些创建持久卷和选择 OpenEBS 数据引擎时需要牢记的最佳实践。
- en: The emergence of cloud-native applications is linked to the rise of the service
    mesh. In the cloud-native world, an application could be made up of hundreds of
    services, each of which could have thousands of instances, each of which could
    be constantly changing due to an orchestrator such as Kubernetes dynamically scheduling
    them. Not only is service-to-service communication tremendously complex, but it’s
    also a critical component of an application’s runtime behavior. It is critical
    to manage it to ensure end-to-end performance, dependability, and security.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生应用程序的兴起与服务网格的出现密切相关。在云原生世界中，一个应用程序可能由数百个服务组成，每个服务可能有成千上万个实例，而这些实例由于 Kubernetes
    等调度器的动态调度而可能不断变化。服务之间的通信不仅极为复杂，而且是应用程序运行时行为的一个关键组成部分。管理它对于确保端到端的性能、可靠性和安全性至关重要。
- en: A service mesh, such as Linkerd or Istio, is a tool for transparently embedding
    observability, security, and reliability features into cloud-native applications
    at the infrastructure layer rather than the application layer. The service mesh
    is quickly becoming an essential component of the cloud-native stack, particularly
    among Kubernetes users. Typically, the service mesh is built as a scalable set
    of network proxies that run alongside application code (the sidecar pattern).
    These proxies mediate communication between microservices and serve as a point
    where service mesh functionalities can be implemented.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格，如 Linkerd 或 Istio，是一种工具，用于将可观察性、安全性和可靠性功能透明地嵌入到云原生应用程序的基础设施层，而不是应用程序层。服务网格正在迅速成为云原生技术栈的一个关键组成部分，尤其是在
    Kubernetes 用户中。通常，服务网格构建为一组可扩展的网络代理，这些代理与应用程序代码（边车模式）一起运行。这些代理调解微服务之间的通信，并作为实现服务网格功能的一个切入点。
- en: 'The service mesh layer can run in a container alongside the application as
    a sidecar. Each of the applications has many copies of the same sidecar attached
    to it, as shown in the following diagram:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格层可以作为边车与应用程序一起在容器中运行。每个应用程序都有多个相同的边车副本附加到其上，如下图所示：
- en: '![Figure 12.1 – The service mesh sidecar pattern ](img/Figure_12.01_B18115.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – 服务网格边车模式](img/Figure_12.01_B18115.jpg)'
- en: Figure 12.1 – The service mesh sidecar pattern
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 服务网格边车模式
- en: The sidecar proxy handles all incoming and outgoing network traffic from a single
    service. As a result, the sidecar controls traffic between microservices, collects
    telemetry data, and applies policies. In certain ways, the application service
    is unaware of the network and is just aware of the sidecar proxy connecting to
    it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 边车代理处理来自单一服务的所有进出网络流量。因此，边车控制微服务之间的流量，收集遥测数据并应用策略。在某些方面，应用服务对网络并不知情，只知道连接到它的边车代理。
- en: 'Within a service mesh, there’s a data plane and a control plane:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务网格中，存在数据平面和控制平面：
- en: The **data plane** coordinates communication between the mesh’s services and
    performs functions such as service discovery, load balancing, traffic management,
    health checks, and so on.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据平面**协调网格中各服务之间的通信，并执行诸如服务发现、负载均衡、流量管理、健康检查等功能。'
- en: The **control plane** manages and configures sidecar proxies so that policies
    can be enforced and telemetry can be collected.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面**管理和配置边车代理，以便执行策略并收集遥测数据。'
- en: The service mesh provides features for service discovery, automatic load balancing,
    fine-grained control of traffic behavior with routing rules, retries, failovers,
    and more. It also has a pluggable policy layer and API configuration that supports
    access controls, rate limits, and quotas. Finally, it provides service monitoring
    with automatic metrics, logs, and traces for all traffic, as well as secure service-to-service
    communication in the mesh.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格提供服务发现、自动负载均衡、通过路由规则、重试、故障转移等对流量行为的精细控制等功能。它还具有可插拔的策略层和API配置，支持访问控制、速率限制和配额。最后，它还提供服务监控，自动收集所有流量的度量、日志和跟踪数据，并在网格中实现安全的服务间通信。
- en: 'In this chapter, we will look at two popular service mesh providers to implement
    this pattern: Linkerd and Istio. We won’t be looking at all the capabilities of
    a service mesh; instead, we will touch upon the monitoring aspect using a sample
    application.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看两个流行的服务网格提供商来实现此模式：Linkerd和Istio。我们不会深入探讨服务网格的所有功能，而是将重点介绍使用示例应用程序进行的监控方面。
- en: 'In this chapter, we’re going to cover the following topics:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Overview of the Linkerd service mesh
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd服务网格概述
- en: Enabling the Linkerd add-on and running a sample application
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用Linkerd附加组件并运行示例应用程序
- en: Overview of the Istio service mesh
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio服务网格概述
- en: Enabling the Istio add-on and running a sample application
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用Istio附加组件并运行示例应用程序
- en: Common use cases for a service mesh
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务网格的常见用例
- en: Guidelines on choosing a service mesh
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择服务网格的指南
- en: Best practices for configuring a service mesh
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置服务网格的最佳实践
- en: Overview of the Linkerd service mesh
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linkerd服务网格概述
- en: Linkerd is a Kubernetes-based service mesh. It simplifies and secures how services
    operate by providing runtime debugging, observability, dependability, and security
    all without requiring any code changes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd是一个基于Kubernetes的服务网格。它通过提供运行时调试、可观察性、可靠性和安全性，简化并保障了服务的运行，而无需任何代码更改。
- en: Each service instance is connected to Linkerd by a system of ultralight, transparent
    proxies. These proxies handle all traffic to and from the service automatically.
    These proxies function as highly instrumented out-of-process network stacks, sending
    telemetry to and receiving control signals from the control plane. Linkerd can
    also measure and manage traffic to and from the service without introducing unnecessary
    latency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务实例通过一套超轻量、透明的代理系统与Linkerd连接。这些代理自动处理进出服务的所有流量。它们作为高度仪表化的外部进程网络堆栈运行，向控制平面发送遥测数据并接收控制信号。Linkerd还可以在不引入不必要延迟的情况下，衡量和管理进出服务的流量。
- en: As discussed in the previous chapter, Linkerd is made up of a control plane,
    which is a collection of services that control Linkerd as a whole, and a data
    plane, which is made up of transparent micro-proxies that run closer to each service
    instance in the pods as sidecar containers. These proxies handle all TCP traffic
    to and from the service automatically and communicate with the control plane for
    configuration.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所讨论的，Linkerd由控制平面组成，控制平面是一组服务，用于控制整个Linkerd，并由数据平面组成，数据平面由透明的微代理构成，这些代理更靠近每个服务实例，并作为Sidecar容器运行在Pod中。这些代理自动处理所有进出服务的TCP流量，并与控制平面进行配置通信。
- en: 'The following diagram shows the architecture of Linkerd:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了Linkerd的架构：
- en: '![Figure 12.2 – Linkerd service mesh components ](img/Figure_12.02_B18115.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – Linkerd服务网格组件](img/Figure_12.02_B18115.jpg)'
- en: Figure 12.2 – Linkerd service mesh components
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – Linkerd服务网格组件
- en: 'Now that we’ve provided a high-level overview and looked at the architecture,
    let’s look at each component in more detail:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提供了一个高层次的概述并查看了架构，接下来让我们更详细地了解每个组件：
- en: '**Destination service**: The data plane proxies use the destination service
    to determine various aspects of their behavior. It is used to retrieve service
    discovery information, retrieve policy information about which types of requests
    are permitted, and retrieve service profile information that’s used to inform
    per-route metrics, retries, timeouts, and more.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标服务**：数据平面代理使用目标服务来确定它们行为的各个方面。它用于检索服务发现信息，获取关于哪些类型的请求被允许的策略信息，以及检索服务配置文件信息，这些信息用于指导每条路由的度量、重试、超时等。'
- en: '**Identity service**: The identity service functions as a TLS Certificate Authority,
    accepting CSRs from proxies and issuing signed certificates. These certificates
    are issued at proxy initialization time and are used to implement mTLS on proxy-to-proxy
    connections.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份服务**：身份服务充当 TLS 证书颁发机构，接受来自代理的 CSR 并签发证书。这些证书在代理初始化时颁发，并用于在代理间连接上实现 mTLS。'
- en: '`linkerd.io/inject: enabled`) in resources. When that annotation is present,
    the injector modifies the pod’s specification and adds the `proxy-init` and `linkerd-proxy`
    containers, as well as the relevant start-time configuration, to the pod.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linkerd.io/inject: enabled`) 在资源中。当该注解存在时，注入器会修改 pod 的规格，添加 `proxy-init` 和
    `linkerd-proxy` 容器，并将相关的启动时配置添加到 pod 中。'
- en: '**Linkerd2-proxy**: Linkerd2-proxy is an ultralight, transparent micro-proxy
    that was created specifically for the service mesh use case and is not intended
    to be a general-purpose proxy.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linkerd2-proxy**：Linkerd2-proxy 是一个超轻量级、透明的微代理，专为服务网格用例设计，并非通用代理。'
- en: '`linkerd-init` container added as a Kubernetes `init` container that runs before
    any other containers are started. All TCP traffic to and from the pod is routed
    through the proxy using iptables.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linkerd-init` 容器作为 Kubernetes 的 `init` 容器，在其他容器启动之前运行。所有进出 pod 的 TCP 流量都通过
    iptables 路由到代理。'
- en: Now that we’ve grasped the fundamentals, let’s enable the Linkerd add-on and
    run a sample application.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了基础知识，接下来启用 Linkerd 插件并运行一个示例应用程序。
- en: Enabling the Linkerd add-on and running a sample application
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用 Linkerd 插件并运行示例应用程序
- en: In this section, you will enable the Linkerd add-on in your MicroK8s Kubernetes
    cluster. Then, to demonstrate Linkerd’s capabilities, you will deploy a sample
    application.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将启用 MicroK8s Kubernetes 集群中的 Linkerd 插件。然后，为了展示 Linkerd 的功能，您将部署一个示例应用程序。
- en: Note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I’ll be using an Ubuntu virtual machine for this section. The instructions for
    setting up a MicroK8s cluster are the same as those in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本节中使用 Ubuntu 虚拟机。设置 MicroK8s 集群的说明与[*第 5 章*](B18115_05.xhtml#_idTextAnchor070)中的内容相同，*创建和实施多节点
    Raspberry Pi Kubernetes 集群的更新*。
- en: Step 1 – Enabling the Linkerd add-on
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 – 启用 Linkerd 插件
- en: 'Use the following command to enable the Cilium add-on:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启用 Cilium 插件：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following output indicates that the Linkerd add-on has been enabled:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表示 Linkerd 插件已经启用：
- en: '![Figure 12.3 – Enabling the Linkerd add-on ](img/Figure_12.03_B18115.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3 – 启用 Linkerd 插件 ](img/Figure_12.03_B18115.jpg)'
- en: Figure 12.3 – Enabling the Linkerd add-on
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – 启用 Linkerd 插件
- en: 'It will take some time to finish activating the add-on. The following output
    shows that Linkerd has been successfully enabled:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 启用插件的过程需要一些时间。以下输出表明 Linkerd 已成功启用：
- en: '![Figure 12.4 – Linkerd enabled successfully ](img/Figure_12.04_B18115.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – 成功启用 Linkerd ](img/Figure_12.04_B18115.jpg)'
- en: Figure 12.4 – Linkerd enabled successfully
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 成功启用 Linkerd
- en: 'Before we move on to the next step, let’s make sure that all of the Linkerd
    components are up and running by using the following command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一步之前，让我们使用以下命令确保所有 Linkerd 组件都已启动并运行：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following output indicates that all the components are `Running`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表示所有组件均处于 `Running` 状态：
- en: '![Figure 12.5 – The Linkerd pods are running ](img/Figure_12.05_B18115.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – Linkerd pod 正在运行 ](img/Figure_12.05_B18115.jpg)'
- en: Figure 12.5 – The Linkerd pods are running
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – Linkerd pod 正在运行
- en: Now that the Linkerd add-on has been enabled, let’s deploy a sample Nginx application.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Linkerd 插件已经启用，让我们部署一个示例 Nginx 应用程序。
- en: Step 2 – Deploying the sample application
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2 – 部署示例应用程序
- en: In this step, we will be deploying a sample Nginx application from the Kubernetes
    sample repository.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们将从 Kubernetes 示例库部署一个示例 Nginx 应用程序。
- en: 'Use the following command to create a sample Nginx deployment:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建示例 Nginx 部署：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following output indicates that there is no error in the deployment. Now,
    we can ensure that the pods have been created:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表示部署过程中没有错误。现在，我们可以确保 pod 已被创建：
- en: '![Figure 12.6 – Sample Nginx application deployment ](img/Figure_12.06_B18115.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.6 – 示例 Nginx 应用程序部署 ](img/Figure_12.06_B18115.jpg)'
- en: Figure 12.6 – Sample Nginx application deployment
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 – 示例 Nginx 应用程序部署
- en: 'Now that the deployment is successful, let’s use the `kubectl` command to check
    if the pods are `Running`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在部署成功了，让我们使用 `kubectl` 命令来检查 pods 是否处于 `Running` 状态：
- en: '![Figure 12.7 – Sample application pods ](img/Figure_12.07_B18115.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.7 – 示例应用程序的 pods](img/Figure_12.07_B18115.jpg)'
- en: Figure 12.7 – Sample application pods
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7 – 示例应用程序的 pods
- en: Here, we can see that the sample application deployment has been successful
    and that all the pods are running. Our next step is to inject Linkerd into it
    by piping the `linkerd inject` and `kubectl apply` commands together. Without
    any downtime, Kubernetes will perform a rolling deployment and update each pod
    with the data plane’s proxies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到示例应用程序的部署已成功，并且所有 pods 都在运行。我们的下一步是通过将 `linkerd inject` 和 `kubectl
    apply` 命令串联起来，向其注入 Linkerd。在没有任何停机时间的情况下，Kubernetes 将执行滚动部署，并更新每个 pod 的数据平面代理。
- en: 'Use the following command to inject Linkerd into the sample application:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令将 Linkerd 注入到示例应用程序中：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following output confirms that the `linkerd inject` command has succeeded
    and that the sample application deployment has been reconfigured:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出确认了 `linkerd inject` 命令已成功，并且示例应用程序部署已重新配置：
- en: '![Figure 12.8 – Injecting Linkerd into the sample application ](img/Figure_12.08_B18115.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.8 – 将 Linkerd 注入示例应用程序](img/Figure_12.08_B18115.jpg)'
- en: Figure 12.8 – Injecting Linkerd into the sample application
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8 – 将 Linkerd 注入示例应用程序
- en: The `linkerd inject` command simply adds annotations to the pod spec instructing
    Linkerd to inject the proxy into pods when they are created.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`linkerd inject` 命令只是简单地向 pod 规范添加注释，指示 Linkerd 在创建时将代理注入到 pods 中。'
- en: Congratulations! Linkerd has now been added to the sample Nginx application!
    We added Linkerd to sample application services without touching the original
    YAML.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在已将 Linkerd 添加到示例 Nginx 应用程序中！我们在不改动原始 YAML 的情况下，向示例应用程序服务添加了 Linkerd。
- en: 'On the data plane side, it’s possible to double-check that everything is working
    properly. Examine the data plane with the following command:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据平面方面，可以通过以下命令双重检查一切是否正常运行。使用以下命令检查数据平面：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Linkerd CLI (`microk8s linkerd`) is the main interface for working with
    Linkerd. It can set up the control plane on the cluster, add the proxy to the
    service(s), and offer thorough performance metrics for the service(s).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd CLI (`microk8s linkerd`) 是与 Linkerd 一起工作的主要接口。它可以在集群上设置控制平面，向服务添加代理，并为服务提供详尽的性能指标。
- en: 'The following output confirms that the `linkerd check` command has started
    the checks for the data plane:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出确认了 `linkerd check` 命令已开始对数据平面进行检查：
- en: '![Figure 12.9 – Linkerd checks ](img/Figure_12.09_B18115.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.9 – Linkerd 检查](img/Figure_12.09_B18115.jpg)'
- en: Figure 12.9 – Linkerd checks
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9 – Linkerd 检查
- en: 'It will take some time to finish the data plane checks. The following output
    shows that the Linkerd checks have been completed:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面检查需要一些时间来完成。下面的输出显示 Linkerd 检查已完成：
- en: '![Figure 12.10 – Linkerd checks completed ](img/Figure_12.10_B18115.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.10 – Linkerd 检查完成](img/Figure_12.10_B18115.jpg)'
- en: Figure 12.10 – Linkerd checks completed
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10 – Linkerd 检查完成
- en: Now that the data plane checks have been completed, we can see if the Linkerd
    annotations have been added to the sample application deployment by using the
    `kubectl describe` command.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据平面检查已完成，我们可以使用 `kubectl describe` 命令查看是否已向示例应用程序部署添加了 Linkerd 注释。
- en: 'The following output confirms that Linkerd annotations have been added:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出确认了已添加 Linkerd 注释：
- en: '![Figure 12.11 – Linkerd annotations added ](img/Figure_12.11_B18115.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.11 – 添加了 Linkerd 注释](img/Figure_12.11_B18115.jpg)'
- en: Figure 12.11 – Linkerd annotations added
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11 – 添加了 Linkerd 注释
- en: Furthermore, we have injected Linkerd without having to write any special configurations
    or change the code of the application. If we can provide Linkerd with additional
    information, it will be able to impose a variety of restrictions, such as timeouts
    and retries. Then, it can provide stats for each route.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们已经成功注入了 Linkerd，无需编写任何特殊配置或更改应用程序的代码。如果可以提供给 Linkerd 附加信息，它将能够强加各种限制，如超时和重试。然后，它可以为每个路由提供统计信息。
- en: Next, we will start retrieving vital information about how each of the services
    of the sample Nginx deployment is performing. Since Linkerd has been injected
    into the application, we will look at various metrics and dashboards
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始检索关于示例 Nginx 部署中每个服务表现的重要信息。由于已向应用程序注入 Linkerd，我们将查看各种指标和仪表板。
- en: Step 3 – Exploring the Linkerd dashboard
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3 – 探索 Linkerd 仪表板
- en: Linkerd offers an on-cluster metrics stack that includes a web dashboard and
    pre-configured Grafana dashboards. In this step, we will learn how to launch the
    Linkerd and Grafana dashboards.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd提供了一个集群内度量堆栈，包括一个Web仪表板和预配置的Grafana仪表板。在这一步，我们将学习如何启动Linkerd和Grafana仪表板。
- en: 'Use the following command to launch the Linkerd dashboard:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启动Linkerd仪表板：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The following output indicates that the Linkerd dashboard has been launched
    and that it’s available on port `50750`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明Linkerd仪表板已启动，并且可以在端口`50750`上访问。
- en: 'To view those metrics, you can use the Grafana dashboard, which is available
    at `http://localhost:50750/grafana`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这些度量数据，您可以使用Grafana仪表板，Grafana仪表板可以通过`http://localhost:50750/grafana`访问：
- en: '![Figure 12.12 – Launching the Linkerd dashboard ](img/Figure_12.12_B18115.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.12 – 启动Linkerd仪表板](img/Figure_12.12_B18115.jpg)'
- en: Figure 12.12 – Launching the Linkerd dashboard
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12 – 启动Linkerd仪表板
- en: 'The Linkerd dashboard gives you a bird’s-eye view of what’s going on with the
    services in real time. It can be used to see metrics such as the success rate,
    requests per second, and latency, as well as visualize service dependencies and
    understand the health of certain service routes:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Linkerd仪表板为您提供了实时查看服务状态的鸟瞰图。它可以用来查看如成功率、每秒请求数和延迟等度量数据，并可视化服务依赖关系，了解某些服务路由的健康状况：
- en: '![Figure 12.13 – The Linkerd dashboard ](img/Figure_12.13_B18115.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.13 – Linkerd仪表板](img/Figure_12.13_B18115.jpg)'
- en: Figure 12.13 – The Linkerd dashboard
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13 – Linkerd仪表板
- en: 'While the Linkerd dashboard gives you a bird’s-eye view of what’s going on
    with the services in real-time, Grafana dashboards, which are also part of the
    Linkerd control plane, provide usable dashboards for the services out of the box.
    These can also be used to monitor the services. Even for pods, we can get high-level
    stats and dive into the details:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Linkerd仪表板为您提供了实时查看服务状态的鸟瞰图，但Grafana仪表板也是Linkerd控制面的一部分，提供了开箱即用的服务仪表板。这些仪表板还可以用来监控服务。即使是Pods，我们也能获取高层次的统计数据并深入了解细节：
- en: '![Figure 12\. 14 – Linkerd Top Line metrics dashboard ](img/Figure_12.14_B18115.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.14 – Linkerd顶级度量仪表板](img/Figure_12.14_B18115.jpg)'
- en: Figure 12\. 14 – Linkerd Top Line metrics dashboard
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14 – Linkerd顶级度量仪表板
- en: To summarize, we have enabled Linkerd on the MicroK8s Kubernetes cluster and
    used it to monitor the services of a sample Nginx application. We also gathered
    relevant telemetry data such as the success rate, throughput, and latency. After
    that, we looked into a few out-of-the-box Grafana dashboards to see high-level
    metrics and dig into the details.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已在MicroK8s Kubernetes集群上启用Linkerd，并使用它来监控示例Nginx应用程序的服务。我们还收集了相关的遥测数据，如成功率、吞吐量和延迟。之后，我们查看了一些开箱即用的Grafana仪表板，以查看高层次的度量数据并深入探讨细节。
- en: In the next section, we will look at Istio, another notable service mesh provider.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍Istio，另一个著名的服务网格提供商。
- en: Overview of the Istio service mesh
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Istio服务网格概述
- en: Istio is an open source platform-independent service mesh that manages traffic,
    enforces policies, and collects telemetry. It is a platform for managing communication
    between microservices and applications. It also provides automated baseline traffic
    resilience, service metrics collection, distributed tracing, traffic encryption,
    protocol upgrades, and advanced routing functionality for all service-to-service
    communication without requiring changes to the underlying services.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Istio是一个开源的、平台无关的服务网格，它管理流量、执行策略并收集遥测数据。它是一个管理微服务和应用程序之间通信的平台。它还为所有服务间通信提供自动化的基准流量弹性、服务度量收集、分布式跟踪、流量加密、协议升级和高级路由功能，而无需对底层服务进行任何更改。
- en: 'The following are some of the vital features of Istio:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Istio的一些重要功能：
- en: Secure *service-to-service* communication via TLS encryption, strong identity-based
    authentication, and authorization
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过TLS加密、基于强身份的认证和授权实现安全的*服务间*通信
- en: Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP、gRPC、WebSocket和TCP流量的自动负载均衡
- en: Fine-grained traffic control via rich routing rules, retries, failovers, and
    fault injection
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过丰富的路由规则、重试、故障转移和故障注入实现精细化流量控制
- en: A pluggable policy layer and configuration API that supports access controls,
    rate limits, and quotas
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可插拔的策略层和配置API，支持访问控制、速率限制和配额
- en: Automatic metrics, logs, and traces for all cluster traffic, including cluster
    ingress and egress
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化的度量、日志和跟踪，涵盖所有集群流量，包括集群的入口和出口流量
- en: 'An Istio service mesh is logically divided into two planes: a data plane and
    a control plane.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Istio 服务网格在逻辑上被划分为两个平面：数据平面和控制平面。
- en: The data plane is made up of a collection of intelligent proxies that are deployed
    as sidecars. All network communication between microservices is mediated and controlled
    by these proxies. In addition, they collect and report telemetry on all mesh traffic.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面由一组智能代理组成，这些代理作为边车部署。所有微服务之间的网络通信都由这些代理调解和控制。此外，它们收集并报告所有网格流量的遥测数据。
- en: The control plane is in charge of managing and configuring the proxies that
    are used to route traffic.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面负责管理和配置用于路由流量的代理。
- en: '![Figure 12.15 – Istio components ](img/Figure_12.15_B18115.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.15 – Istio 组件 ](img/Figure_12.15_B18115.jpg)'
- en: Figure 12.15 – Istio components
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.15 – Istio 组件
- en: 'Now that we’ve provided a high-level overview and looked at the architecture,
    let’s look at each component in more detail:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提供了高层次的概述并看过架构，让我们更详细地了解每个组件：
- en: '**Istiod**: This manages service discovery, configuration, and certificates.
    It translates high-level routing rules that govern traffic behavior into Envoy-specific
    configurations and propagates them to sidecars at runtime. The Pilot component
    abstracts platform-specific service discovery mechanisms and synthesizes them
    into a standard format that can be consumed by any Envoy API-compliant sidecar.
    Istio also supports discovery for a variety of environments, including Kubernetes
    and virtual machines.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Istiod**：它管理服务发现、配置和证书。它将高层的流量路由规则转换为 Envoy 特定的配置，并在运行时将其传播到边车。Pilot 组件将平台特定的服务发现机制进行抽象，并将其合成成标准格式，可以被任何遵循
    Envoy API 的边车使用。Istio 还支持多种环境下的发现，包括 Kubernetes 和虚拟机。'
- en: '**Envoy**: This is a high-performance proxy that mediates all inbound and outbound
    traffic for all services in the service mesh. The only Istio components that interact
    with data plane traffic are envoy proxies. Envoy proxies are deployed as service
    sidecars, logically augmenting the services with Envoy’s many built-in features,
    such as the following:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Envoy**：这是一款高性能的代理，调解所有进出服务网格的流量。唯一与数据平面流量交互的 Istio 组件是 Envoy 代理。Envoy 代理作为服务的边车部署，在逻辑上增强了服务，提供了许多内置功能，例如：'
- en: Dynamic service discovery
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态服务发现
- en: Load balancing
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡
- en: TLS termination
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLS 终止
- en: HTTP/2 and gRPC proxies
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP/2 和 gRPC 代理
- en: Circuit breakers
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熔断器
- en: Health checks
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康检查
- en: Staged rollouts with a percentage-based traffic split
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于百分比流量分配的分阶段发布
- en: Fault injection
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障注入
- en: Rich metrics
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丰富的指标
- en: 'The following are various components of the Istio system, as well as the abstractions
    that it employs:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Istio 系统的各种组件，以及它所使用的抽象：
- en: '**Traffic management**: The traffic routing rules provided by Istio allow you
    to easily control the flow of traffic and API calls between services. Istio makes
    it simple to configure service-level properties such as circuit breakers, timeouts,
    and retries, as well as important tasks such as A/B testing, canary rollouts,
    and staged rollouts with percentage-based traffic splits. It also includes out-of-the-box
    reliability features that aid in making the application more resilient to failures
    of dependent services or the network.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流量管理**：Istio 提供的流量路由规则使你可以轻松地控制服务之间流量和 API 调用的流向。Istio 简化了配置服务级属性（如熔断器、超时和重试）以及
    A/B 测试、金丝雀发布和基于百分比的分阶段发布等重要任务。它还包括开箱即用的可靠性功能，有助于提高应用程序对依赖服务或网络故障的容错能力。'
- en: '**Observability**: For all mesh service communications, Istio creates extensive
    telemetry. This telemetry lets users observe service behavior, allowing them to
    debug, maintain, and optimize their applications without putting additional strain
    on service developers. Users can acquire a comprehensive picture of how monitored
    services interact with one another and with Istio components.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**：对于所有网格服务通信，Istio 创建了广泛的遥测数据。这些遥测数据让用户能够观察服务行为，帮助他们调试、维护和优化应用程序，而无需给服务开发人员增加额外负担。用户可以全面了解被监控服务如何相互交互以及与
    Istio 组件的关系。'
- en: 'Istio creates the following kinds of telemetry to give total service mesh observability:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 创建以下几种遥测，以实现对整个服务网格的可观察性：
- en: '**Metrics**: Based on the four monitoring attributes, Istio creates a set of
    service metrics (latency, traffic, errors, and saturation). In addition, Istio
    provides extensive mesh control plane measurements. A basic set of mesh monitoring
    dashboards is supplied on top of these metrics.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标**：基于四个监控属性，Istio 创建了一组服务指标（延迟、流量、错误和饱和度）。此外，Istio 提供了广泛的网格控制平面度量。在这些指标基础上，提供了一组基础的网格监控仪表盘。'
- en: '**Distributed tracing**: Dispersed traces result in distributed trace spans
    for each service, providing users with a comprehensive view of call flows and
    the service relationships inside a mesh.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式追踪**：分散的追踪会生成每个服务的分布式追踪跨度，提供用户一个全面的调用流和服务关系视图，展示网格内部的调用关系。'
- en: '**Access logs**: As traffic flows into the service within a mesh, Istio generates
    a complete record of each request, including source and destination metadata.
    Users can utilize this information to examine service behavior down to individual
    workload instances.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问日志**：当流量进入网格中的服务时，Istio 会生成每个请求的完整记录，包括源和目标的元数据。用户可以利用这些信息检查服务行为，直到单个工作负载实例。'
- en: '**Security**: To protect hosted services as well as data, Istio security includes
    strong identity, powerful policy management, transparent TLS encryption, authentication,
    and audit tools.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：为了保护托管服务和数据，Istio 安全性包括强大的身份认证、强大的策略管理、透明的 TLS 加密、身份验证和审计工具。'
- en: Now that we have covered the fundamentals, we can proceed to the next step,
    which is to enable the Istio add-on and run a sample application.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了基础内容，可以继续下一步，即启用 Istio 插件并运行示例应用。
- en: Enabling the Istio add-on and running a sample application
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用 Istio 插件并运行示例应用
- en: In this section, you will enable the Istio add-on in your MicroK8s Kubernetes
    cluster. Then, you will launch a sample application to show off Istio’s capabilities.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将启用 MicroK8s Kubernetes 集群中的 Istio 插件。然后，您将启动示例应用来展示 Istio 的能力。
- en: Note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I’ll be using an Ubuntu virtual machine for this section. The instructions for
    setting up a MicroK8s cluster are the same as those in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Cluster*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将使用一台 Ubuntu 虚拟机。设置 MicroK8s 集群的指令与[*第 5 章*](B18115_05.xhtml#_idTextAnchor070)中的*创建和实现多节点
    Raspberry Pi Kubernetes 集群更新*相同。
- en: Step 1 – Enabling the Istio add-on
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 – 启用 Istio 插件
- en: 'Use the following command to enable the Istio add-on:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启用 Istio 插件：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following output indicates that the Istio add-on has been enabled:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表示 Istio 插件已成功启用：
- en: '![Figure 12.16 – Enabling the Istio add-on ](img/Figure_12.16_B18115.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.16 – 启用 Istio 插件](img/Figure_12.16_B18115.jpg)'
- en: Figure 12.16 – Enabling the Istio add-on
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.16 – 启用 Istio 插件
- en: 'It will take some time to finish activating the add-on. The following output
    shows that Istio has been successfully enabled:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 启用插件需要一些时间。以下输出显示 Istio 已成功启用：
- en: '![Figure 12.17 – Istio add-on enabled ](img/Figure_12.17_B18115.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.17 – Istio 插件已启用](img/Figure_12.17_B18115.jpg)'
- en: Figure 12.17 – Istio add-on enabled
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.17 – Istio 插件已启用
- en: 'Before we move on to the next step, let’s make sure that all of the Istio components
    are up and running by using the following command:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一步之前，让我们通过以下命令确保所有 Istio 组件都已启动并正常运行：
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following output indicates that all the components are `Running`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明所有组件都处于`Running`状态：
- en: '![Figure 12.18 – The Istio pods are running ](img/Figure_12.18_B18115.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.18 – Istio pod 正在运行](img/Figure_12.18_B18115.jpg)'
- en: Figure 12.18 – The Istio pods are running
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.18 – Istio pod 正在运行
- en: Now that the Istio add-on has been enabled, let’s deploy the sample application.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Istio 插件已经启用，我们可以部署示例应用了。
- en: Step 2 – Deploying the sample application
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 – 部署示例应用
- en: Before deploying the sample Nginx application, we need to label the namespace
    as `istio-injection=enabled` so that Istio can inject sidecars into the deployment’s
    pods.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署示例 Nginx 应用之前，我们需要将命名空间标记为`istio-injection=enabled`，这样 Istio 就可以将 sidecar
    注入到部署的 pod 中。
- en: 'Use the following command to label the namespace:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令标记命名空间：
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following output indicates that there is no error in the deployment. Now,
    we can deploy the sample application:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明部署没有错误。现在我们可以部署示例应用了：
- en: '![Figure 12.19 – Labeling the namespace ](img/Figure_12.19_B18115.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.19 – 标记命名空间](img/Figure_12.19_B18115.jpg)'
- en: Figure 12.19 – Labeling the namespace
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.19 – 标记命名空间
- en: 'Use the following command to create a sample Nginx deployment:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建一个示例 Nginx 部署：
- en: '[PRE9]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following output indicates that there is no error in the deployment. Now,
    we can ensure that Istio has been injected into pods:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表示部署中没有错误。现在，我们可以确保 Istio 已经被注入到 pods 中：
- en: '![Figure 12.20 – Sample application deployment ](img/Figure_12.20_B18115.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.20 – 示例应用部署](img/Figure_12.20_B18115.jpg)'
- en: Figure 12.20 – Sample application deployment
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.20 – 示例应用部署
- en: With the deployment completed, we can check if the Istio labels have been added
    to the sample application deployment by using the `kubectl describe` command.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，我们可以使用 `kubectl describe` 命令检查 Istio 标签是否已添加到示例应用部署中。
- en: 'The following output confirms that the Istio labels have been added:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出确认 Istio 标签已被添加：
- en: '![Figure 12.21 – Istio annotations added ](img/Figure_12.21_B18115.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.21 – 已添加 Istio 注解](img/Figure_12.21_B18115.jpg)'
- en: Figure 12.21 – Istio annotations added
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.21 – 已添加 Istio 注解
- en: 'We can also use the `istioctl` CLI command to get an overview of the Istio
    mesh:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `istioctl` 命令行工具获取 Istio 网格的概况：
- en: '[PRE10]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following output indicates that our sample Nginx deployment has been `SYNCED`
    with the Istiod control plane:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表示我们的示例 Nginx 部署已与 Istiod 控制平面 `SYNCED`：
- en: '![Figure 12.22 – Istio proxy status ](img/Figure_12.22_B18115.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.22 – Istio 代理状态](img/Figure_12.22_B18115.jpg)'
- en: Figure 12.22 – Istio proxy status
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.22 – Istio 代理状态
- en: If any of the sidecars aren’t receiving configuration or are out of sync, then
    you can use the `proxy-status` command.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些 sidecar 没有收到配置或不同步，可以使用 `proxy-status` 命令。
- en: 'If a proxy isn’t listed, it’s because it’s not currently linked to an Istiod
    instance:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有列出某个代理，说明它当前没有连接到 Istiod 实例：
- en: '`SYNCED` indicates that the Envoy proxy has acknowledged the most recent configuration
    that’s been supplied to it by Istiod.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SYNCED` 表示 Envoy 代理已确认接收到 Istiod 提供的最新配置。'
- en: '`NOT SENT` indicates that Istiod has not sent any messages to the Envoy proxy.
    This is frequently because Istiod has nothing to send.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NOT SENT` 表示 Istiod 尚未向 Envoy 代理发送任何消息。这通常是因为 Istiod 没有要发送的内容。'
- en: '`STALE` indicates that Istiod sent an update to the Envoy proxy but did not
    receive a response. This usually indicates a problem with networking between the
    Envoy proxy and Istiod, or a flaw with Istio itself.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STALE` 表示 Istiod 向 Envoy 代理发送了更新，但未收到响应。这通常表示 Envoy 代理与 Istiod 之间的网络存在问题，或
    Istio 本身存在缺陷。'
- en: Congratulations! You have added Istio proxies to the sample application! We
    added Istio to existing services without touching the original YAML.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已将 Istio 代理添加到示例应用中！我们将 Istio 添加到现有服务中，而没有修改原始的 YAML。
- en: Step 3 – Exploring the Istio service dashboard
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3 – 探索 Istio 服务仪表盘
- en: For all service communication within a mesh, Istio creates extensive telemetry.
    This telemetry allows service behavior to be observed, allowing service mesh users
    to troubleshoot, maintain, and optimize their applications without adding to the
    workload for service developers.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格中的所有服务通信中，Istio 创建了广泛的遥测数据。这些遥测数据可以帮助观察服务行为，使服务网格用户能够进行故障排除、维护和优化应用，而不会增加服务开发人员的工作负担。
- en: 'As we discussed previously, to enable overall service mesh observability, Istio
    creates the following forms of telemetry:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，为了启用整体服务网格可观测性，Istio 创建了以下几种形式的遥测数据：
- en: '**Metrics**: Based on monitoring performance, Istio generates a set of service
    metrics (latency, traffic, errors, and saturation). For the mesh control plane,
    Istio also provides detailed metrics. On top of these metrics, a default set of
    mesh monitoring dashboards is given:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标**：根据监控性能，Istio 会生成一组服务指标（延迟、流量、错误和饱和度）。对于网格控制平面，Istio 还提供详细的指标。在这些指标的基础上，提供了一套默认的网格监控仪表盘：'
- en: '![Figure 12.23 – The Istio service dashboard ](img/Figure_12.23_B18115.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.23 – Istio 服务仪表盘](img/Figure_12.23_B18115.jpg)'
- en: Figure 12.23 – The Istio service dashboard
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.23 – Istio 服务仪表盘
- en: 'The resource usage dashboard looks as follows. This is where we can get details
    about memory, CPU, and disk usage:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 资源使用仪表盘如下所示。在这里，我们可以查看关于内存、CPU 和磁盘使用的详细信息：
- en: '![Figure 12.24 – Istio Resource Usage dashboard ](img/Figure_12.24_B18115.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.24 – Istio 资源使用仪表盘](img/Figure_12.24_B18115.jpg)'
- en: Figure 12.24 – Istio Resource Usage dashboard
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.24 – Istio 资源使用仪表盘
- en: '**Distributed traces**: Istio creates distributed trace spans for each service,
    giving users a complete picture of the call flows and service dependencies in
    a mesh:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式追踪**：Istio为每个服务创建分布式追踪跨度，向用户展示完整的调用流和服务依赖关系图：'
- en: '![Figure 12.25 – Istio distributed traces ](img/Figure_12.25_B18115.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.25 – Istio 分布式追踪](img/Figure_12.25_B18115.jpg)'
- en: Figure 12.25 – Istio distributed traces
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.25 – Istio 分布式追踪
- en: '**Access logs**: Istio generates a full record of each request as traffic flows
    into a service within a mesh, including source and destination metadata. Users
    can audit service behavior down to the individual workload instance level using
    this data.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问日志**：Istio为每个请求生成完整的记录，当流量进入网格中的某个服务时，包括源和目的地的元数据。用户可以利用这些数据审计服务行为，直至单个工作负载实例级别。'
- en: To summarize, we deployed Istio on the MicroK8s Kubernetes cluster and used
    it to monitor a sample Nginx application’s services. We also had a look at the
    Istio service dashboard, which allows us to examine telemetry data to debug, maintain,
    and improve applications. Finally, we looked at how metrics, distributed traces,
    and access logs can be used to enable overall service mesh observability.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们在MicroK8s Kubernetes集群上部署了Istio，并使用它来监控一个示例Nginx应用程序的服务。我们还查看了Istio服务仪表盘，利用它可以检查遥测数据进行调试、维护和改进应用程序。最后，我们了解了如何使用度量、分布式追踪和访问日志来增强整体的服务网格可观察性。
- en: 'In short, a service mesh provides uniform discovery, security, tracing, monitoring,
    and failure management. So, if a Kubernetes cluster has a service mesh, you can
    have the following without changing the application code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，服务网格提供了统一的发现、安全、追踪、监控和故障管理。因此，如果Kubernetes集群中有一个服务网格，你可以在不改变应用代码的情况下实现以下功能：
- en: Automatic load balancing
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动负载均衡
- en: Fine-grained control of traffic behavior with routing rules, retries, failovers,
    and more
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过路由规则、重试、故障转移等实现流量行为的精细控制
- en: Pluggable policy layer
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可插拔的策略层
- en: A configuration API that supports access control, rate limits, and quotas
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个支持访问控制、速率限制和配额的配置API
- en: Service discovery
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现
- en: Service monitoring with automatic metrics, logs, and traces for all traffic
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对所有流量进行自动指标、日志和追踪的服务监控
- en: Secure service-to-service communication
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全的服务间通信
- en: In most implementations, the service mesh serves as the single pane of glass
    for a microservices architecture. It’s where you go to troubleshoot problems,
    enforce traffic policies, set rate limits, and test new code. It serves as your
    central point for monitoring, tracing, and controlling the interactions of all
    services – that is, how they are connected, performed, and secured. In the next
    section, we will look at some of the most common use cases.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实现中，服务网格作为微服务架构的单一视窗。它是你用于故障排查、执行流量策略、设置速率限制和测试新代码的地方。它作为你监控、追踪和控制所有服务交互的中心点——即它们是如何连接、执行和保障的。在接下来的部分中，我们将看看一些最常见的使用案例。
- en: Common use cases for a service mesh
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格的常见使用案例
- en: 'A service mesh is useful for any type of microservices architecture from an
    operations standpoint. This is because it allows you to control traffic, security,
    permissions, and observability. Here are some of the most common, standardized,
    and widely accepted use cases for service meshes today:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从运营角度来看，服务网格对任何类型的微服务架构都非常有用。这是因为它允许你控制流量、安全性、权限和可观察性。以下是当前服务网格最常见、标准化且广泛接受的使用案例：
- en: '**Improving the observability**: Through service-level visibility, tracing,
    and monitoring, we may improve the observability of distributed services. Some
    of the service mesh’s primary features boost visibility and your ability to troubleshoot
    and manage situations dramatically. For example, if one of the architecture’s
    services becomes a bottleneck, retrying is a frequent option, although this may
    exacerbate the bottleneck due to timeouts. With a service mesh, you can quickly
    break the circuit to failing services, disable non-functioning replicas, and maintain
    the API’s responsiveness.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升可观察性**：通过服务级别的可见性、追踪和监控，我们可以提升分布式服务的可观察性。服务网格的一些主要特性大幅提升了可见性，以及故障排查和管理的能力。例如，如果架构中的某个服务成为瓶颈，重试通常是一个选择，尽管这可能由于超时而加剧瓶颈问题。使用服务网格，你可以快速断开与故障服务的连接，禁用无法正常工作的副本，并保持API的响应性。'
- en: '**Blue/Green deployments**: A service mesh allows you to leverage Blue/Green
    deployments to successfully roll out new application upgrades without them affecting
    services due to its traffic control features. You begin by exposing the new version
    to a limited group of users, testing it, and then rolling it out to all production
    instances.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝绿部署**：服务网格使你能够利用蓝绿部署来成功推出新的应用升级，而不会因其流量控制功能而影响现有服务。你可以首先将新版本暴露给有限的用户群体，进行测试，之后再推广到所有生产实例。'
- en: '**Chaos monkey/production testing**: To improve deployment robustness, the
    ability to inject delays and errors is also available.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混沌猴子/生产环境测试**：为了提高部署的稳健性，还可以注入延迟和错误。'
- en: '**Modernizing your legacy applications**: You can utilize a service mesh as
    an enabler while decomposing your apps if you’re in the process of upgrading your
    old applications to Kubernetes-based microservices. You can register your existing
    applications as services in the service catalog and then migrate them to Kubernetes
    over time without changing the communication style between them.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现代化你的遗留应用**：如果你正在将旧应用升级为基于Kubernetes的微服务，可以利用服务网格作为支持工具来拆解应用。你可以将现有的应用注册为服务到服务目录中，并随着时间的推移将其迁移到Kubernetes上，而无需更改它们之间的通信方式。'
- en: '**The API Gateway technique**: With the help of a service mesh, you may leverage
    the API Gateway technique for service-to-service connectivity and complicated
    API management schemes within your clusters. A service mesh acts as superglue,
    dynamically connecting microservices with traffic controls, restrictions, and
    testing capabilities.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API网关技术**：借助服务网格，你可以利用API网关技术实现集群内服务间连接以及复杂的API管理方案。服务网格充当超级粘合剂，动态地将微服务连接在一起，并提供流量控制、限制和测试功能。'
- en: Many new and widely accepted use cases will join those listed previously as
    service meshes become more popular. Now, let’s look at the considerations for
    choosing a service mesh provider.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 随着服务网格的普及，许多新的、被广泛接受的使用案例将加入到之前列出的那些案例中。现在，让我们来看看选择服务网格提供商时需要考虑的因素。
- en: Guidelines on choosing a service mesh
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择服务网格的指南
- en: 'In this section, we will provide a brief comparison of the features offered
    by service mesh providers. Choosing one that meets your fundamental requirements
    boils down to whether or not you want more than just the essentials. Istio provides
    the most features and versatility, but keep in mind that flexibility equals complexity.
    Linkerd may be the best option for a basic strategy that only supports Kubernetes:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要比较各个服务网格提供商所提供的功能。选择一个满足基本要求的服务网格，关键在于你是否需要超出基础功能的更多特性。Istio提供了最多的功能和灵活性，但请记住，灵活性也意味着复杂性。Linkerd可能是支持Kubernetes的基本策略的最佳选择：
- en: '![](img/Table_01.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Table_01.jpg)'
- en: Table 12.1 – Comparison between the Istio and Linkerd service meshes
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1 – Istio与Linkerd服务网格的比较
- en: Now that we’ve seen some recommendations for selecting a service mesh, let’s
    look at the best practices for configuring one.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看过了一些选择服务网格的建议，接下来让我们看看配置服务网格的最佳实践。
- en: Best practices for configuring a service mesh
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置服务网格的最佳实践
- en: 'Although a service mesh is extremely beneficial to development teams, putting
    one in place requires some effort. A service mesh gives you a lot of flexibility
    and room to tailor it to your needs because it has so many moving pieces. Flexibility
    usually comes at the expense of complexity. While working with a service mesh,
    the following best practices will provide you with some useful guidelines:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管服务网格对开发团队非常有益，但实施一个服务网格需要一些努力。服务网格提供了极大的灵活性和定制空间，因为它有许多可调组件。灵活性通常伴随着复杂性。在使用服务网格时，以下最佳实践将为你提供一些有用的指导：
- en: '**Adopt a GitOps approach**: Traffic regulations, rate limits, and networking
    setup are all part of the service mesh’s configuration. The configuration can
    be used to install the service mesh from the ground up, update its versions, and
    migrate between clusters. As a result, it is recommended that the configuration
    be regarded as code and that GitOps be utilized in conjunction with a continuous
    deployment pipeline.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采用GitOps方法**：流量管制、速率限制和网络配置都是服务网格配置的一部分。该配置可以用来从零开始安装服务网格、更新其版本并在集群之间迁移。因此，建议将配置视为代码，并结合持续部署管道使用GitOps。'
- en: '**Use fewer clusters**: Fewer clusters with a big number of servers perform
    better than many clusters with fewer instances for service mesh products. As a
    result, it’s best to keep the number of redundant clusters as low as possible,
    allowing you to take advantage of your service mesh approach’s straightforward
    operation and centralized configuration.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用较少的集群**：较少的集群并且有大量服务器的表现优于多个集群和较少实例的服务网格产品。因此，最好将冗余集群的数量保持在最低限度，这样你就可以利用服务网格方法的简单操作和集中配置。'
- en: '**Use appropriate monitoring alerts and request tracing**: Service mesh apps
    are advanced applications that manage the traffic of increasingly complicated
    distributed applications. For system observability, metric collection, visualization,
    and dashboards are essential. Using Prometheus or Grafana, which will be offered
    by your service mesh, you can create alerts according to your requirements.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用适当的监控警报和请求追踪**：服务网格应用是管理日益复杂的分布式应用流量的高级应用。对于系统可观察性，指标收集、可视化和仪表板是至关重要的。通过你的服务网格提供的Prometheus或Grafana，你可以根据需要创建警报。'
- en: '**Focus on comprehensive security**: The majority of service mesh systems offer
    mutual TLS, certificate management, authentication, and authorization as security
    features. To limit communication across clustered apps, you can design and enforce
    network policies. However, it should be emphasized that designing network policies
    is not a simple operation. You must consider future scalability and cover all
    eventualities for currently running apps. As a result, using a service mesh to
    enforce network policies is inconvenient and prone to errors and security breaches.
    Who is transmitting or receiving data is unimportant to service mesh solutions.
    Any hostile or malfunctioning application can retrieve your sensitive data if
    network policies allow it. As a result, rather than depending exclusively on the
    security features of service mesh devices, it’s essential to think about the big
    picture.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注全面的安全性**：大多数服务网格系统提供互相TLS、证书管理、身份验证和授权等安全功能。为了限制集群应用之间的通信，你可以设计并执行网络策略。然而，应该强调的是，设计网络策略并非一项简单的操作。你必须考虑未来的可扩展性，并覆盖当前正在运行的应用的所有可能性。因此，使用服务网格来执行网络策略是不方便的，并且容易出现错误和安全漏洞。谁在传输或接收数据对服务网格解决方案来说并不重要。如果网络策略允许，任何恶意或故障的应用都可以获取你的敏感数据。因此，除了依赖服务网格设备的安全功能外，思考更广泛的整体安全措施是至关重要的。'
- en: In conclusion, a service mesh allows you to decouple the application’s business
    logic from observability, network, and security policies. You can connect to,
    secure, and monitor your microservices with it.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，服务网格使你能够将应用的业务逻辑与可观察性、网络和安全策略解耦。你可以使用它来连接、保护和监控你的微服务。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The number of services that make up an application grows dramatically as monolithic
    apps are split down into microservices. And managing a huge number of entities
    isn’t easy. By standardizing and automating communication between services, a
    Kubernetes native service mesh, such as Istio or Linkerd, tackles difficulties
    created by container and service sprawl in a microservices architecture. Security,
    service discovery, traffic routing, load balancing, service failure recovery,
    and observability are all standardized and automated by a service mesh.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 随着单体应用被拆分为微服务，构成应用的服务数量急剧增加。管理大量实体并不容易。通过标准化和自动化服务之间的通信，Kubernetes本地服务网格（如Istio或Linkerd）解决了微服务架构中容器和服务扩展带来的问题。服务网格对安全性、服务发现、流量路由、负载均衡、服务故障恢复和可观察性进行了标准化和自动化。
- en: In this chapter, we learned how to enable the Linkerd and Istio add-ons and
    inject sidecars into sample applications. Then, we examined the respective dashboards,
    which allowed us to examine telemetry data to debug, maintain, and improve applications.
    We also examined how metrics, distributed traces, and access logs can be used
    to improve overall service mesh observability.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何启用Linkerd和Istio插件，并将边车注入到示例应用中。然后，我们检查了各自的仪表板，这使我们能够查看遥测数据，以调试、维护和改进应用。我们还研究了如何使用指标、分布式跟踪和访问日志来提升整体服务网格的可观察性。
- en: After that, we looked at some of the most prevalent use cases for service meshes
    today, as well as some tips on how to pick the right service mesh. We also provided
    a list of service mesh configuration best practices.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们讨论了今天服务网格最常见的一些使用场景，并提供了一些关于如何选择合适的服务网格的建议。我们还列出了服务网格配置的最佳实践。
- en: In the next chapter, you will learn how to set up a highly available cluster.
    A highly available Kubernetes cluster can withstand a component failure and continue
    to serve workloads without interruption.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何设置一个高可用性集群。一个高可用的 Kubernetes 集群能够承受组件故障，并且在不中断的情况下继续为工作负载提供服务。
