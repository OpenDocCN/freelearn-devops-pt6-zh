- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Managing Application Resiliency
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理应用程序韧性
- en: Application resiliency is the ability of software applications to withstand
    faults and failures without any noticeable degradation of quality and level of
    service for its consumer. The move from monoliths to microservices has exacerbated
    the need for application resiliency in software design and architecture. In monolith
    applications, there is a single code base and single deployment whereas, in microservice-based
    architecture, there are many independent code bases each with its own deployment.
    When leveraging Kubernetes and other similar platforms, you also need to cater
    to deployment flexibility and the fact that multiple application instances are
    being deployed and scaled elastically; these dynamic instances need to not only
    coordinate with each other but also coordinate with all other microservices.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的韧性是指软件应用在面对故障和失败时，能够保持不明显的质量和服务水平下降的能力。从单体架构到微服务架构的转变，进一步加剧了软件设计和架构中对应用程序韧性的需求。在单体应用中，只有一个代码库和一个部署，而在基于微服务的架构中，则有多个独立的代码库，每个代码库都有自己的部署。当使用
    Kubernetes 和其他类似平台时，还需要考虑部署的灵活性，以及多个应用实例的弹性部署和扩展；这些动态实例不仅需要相互协调，还需要与所有其他微服务协调。
- en: In this chapter, we will read about how to make use of Istio to increase the
    application resiliency of microservices. As we go through each section, we will
    discuss various aspects of application resiliency and how they are addressed by
    Istio.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何利用 Istio 提高微服务的应用程序韧性。在我们深入每个部分时，我们将讨论应用程序韧性的各个方面，以及 Istio 如何解决这些问题。
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本章将涵盖以下主题：
- en: Application resiliency using fault injection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用故障注入提高应用程序的韧性
- en: Application resiliency using timeouts and retries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用超时和重试提高应用程序的韧性
- en: Building application resiliency using load balancing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用负载均衡构建应用程序的韧性
- en: Rate limiting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制请求速率
- en: Circuit breaker and outlier detection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熔断器和异常检测
- en: Important note
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The technical prerequisites for this chapter are the same as the previous chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术先决条件与上一章相同。
- en: Application resiliency using fault injection
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用故障注入提高应用程序的韧性
- en: Fault injections are used for testing the recovery capability of applications
    in case of any kind of failure. In principle, every microservice should be designed
    with in-built resiliency for internal and external failures but often, that is
    not the case. The most complex and difficult work of building resiliency is usually
    at design and test time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 故障注入用于测试应用程序在发生任何类型故障时的恢复能力。从原则上讲，每个微服务都应设计为具备内建的韧性，以应对内部和外部的故障，但实际上，往往并非如此。构建韧性最复杂和困难的工作通常是在设计和测试阶段。
- en: 'During design time, you must identify all known and unknown scenarios to which
    you need to cater. For example, you must address the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计阶段，必须识别所有已知和未知的场景，并为其提供解决方案。例如，必须解决以下问题：
- en: What kind of known and unknown errors might happen inside and outside of the
    microservice?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在微服务内部和外部，可能发生哪些已知和未知的错误？
- en: How should each of those errors be handled by the application code?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序代码应如何处理这些错误？
- en: 'During test time, you should be able to simulate these scenarios to validate
    the contingencies built into the application code:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，应该能够模拟这些场景，以验证应用程序代码中构建的应急处理措施：
- en: Mimic in real-time different failure scenarios in the behavior of other upstream
    services to test the overall application behavior
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时模拟其他上游服务的不同故障场景，以测试整体应用程序的行为
- en: Mimic not just application failures but also network failures such as delays,
    outages, etc., as well as infrastructure failure
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟不仅是应用程序故障，还包括网络故障（如延迟、停机等）以及基础设施故障
- en: Chaos engineering is the software engineering term used to refer to the discipline
    of testing software systems by introducing adverse conditions into software systems
    and their execution and communication environments and ecosystems. You can read
    more about Chaos engineering at [https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/](https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/).
    Various tools are available for generating chaos – as in, failures – mostly at
    the infrastructure level. One such popular tool is Chaos Monkey. It is available
    at [https://netflix.github.io/chaosmonkey/](https://netflix.github.io/chaosmonkey/).
    AWS also provides AWS Fault Injection Simulator for running fault injection simulations.
    You can read about AWS Fault Injection Simulator at [https://aws.amazon.com/fis/](https://aws.amazon.com/fis/).
    Another popular open source software is Litmus, which is a chaos engineering platform
    to identify weaknesses and potential outages in infrastructures by inducing chaos
    tests in a controlled way. You can read more about it [https://litmuschaos.io/](https://litmuschaos.io/).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 混沌工程是一个软件工程术语，用于指引通过在软件系统及其执行和通信环境及生态系统中引入不利条件来测试软件系统的学科。你可以在[https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/](https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/)了解更多关于混沌工程的内容。生成混沌（即故障）的各种工具大多数是在基础设施级别。一种流行的工具是
    Chaos Monkey。它可以在[https://netflix.github.io/chaosmonkey/](https://netflix.github.io/chaosmonkey/)找到。AWS
    还提供了 AWS 故障注入模拟器，用于运行故障注入模拟。你可以在[https://aws.amazon.com/fis/](https://aws.amazon.com/fis/)了解关于
    AWS 故障注入模拟器的更多信息。另一个流行的开源软件是 Litmus，它是一个混沌工程平台，通过以受控的方式引入混沌测试来识别基础设施中的弱点和潜在的停机。你可以在[https://litmuschaos.io/](https://litmuschaos.io/)了解更多信息。
- en: Istio provides fine-grained control for injecting failures because of its access
    and knowledge of application traffic. With Istio fault injection, you can use
    application-layer fault injection; this, combined with infrastructure-level fault
    injectors such as Chaos Monkey and AWS Fault Simulator, provides a very robust
    capability for testing application resiliency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 提供了细粒度的故障注入控制，因为它能够访问并了解应用程序流量。通过 Istio 故障注入，你可以使用应用层的故障注入；结合像 Chaos Monkey
    和 AWS 故障模拟器这样的基础设施级别故障注入工具，提供了非常强大的能力来测试应用程序的韧性。
- en: 'Istio supports the following types of fault injections:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 支持以下类型的故障注入：
- en: HTTP delay
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP 延迟
- en: HTTP abort
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP 中止
- en: In the following sections, we will discuss both fault injection types in more
    detail.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地讨论这两种故障注入类型。
- en: What is HTTP delay?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 HTTP 延迟？
- en: Delays are timing failures. They mimic increased request turnaround times caused
    either by network latency or an overloaded upstream service. These delays are
    injected via Istio VirtualServices.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是时间故障。它们模拟由于网络延迟或上游服务过载而导致的请求周转时间增加。这些延迟是通过 Istio VirtualServices 注入的。
- en: In our sock shop example, let’s inject an HTTP delay between the frontend service
    and catalog service and test how the frontend service behaves when it cannot get
    images from the catalog service. We will do this specifically for one image rather
    than all images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的袜子商店示例中，让我们在前端服务和目录服务之间注入一个 HTTP 延迟，并测试当前端服务无法从目录服务获取图片时，它的表现。我们将专门为一张图片而非所有图片进行此操作。
- en: You will find the VirtualService definition in `Chapter5/02-faultinjection_delay.yaml`
    on GitHub.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上的`Chapter5/02-faultinjection_delay.yaml`文件中找到 VirtualService 定义。
- en: '![Figure 5.1 – HTTP delay injection in catalogue VirtualService](img/B17989_05_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 目录 VirtualService 中的 HTTP 延迟注入](img/B17989_05_01.jpg)'
- en: Figure 5.1 – HTTP delay injection in catalogue VirtualService
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 目录 VirtualService 中的 HTTP 延迟注入
- en: 'The following is the snippet from the VirtualService definition:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自 VirtualService 定义的代码片段：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first thing to note is the `fault` definition, which is used to inject a
    delay and/or abort the fault before forwarding the request to the destination
    specified in the route. In this case, we are injecting a `delay` type of fault,
    which is used to emulate slow response times.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是`fault`的定义，它用于在将请求转发到路由中指定的目标之前，注入延迟和/或中止故障。在这种情况下，我们正在注入一种`delay`类型的故障，用于模拟慢响应时间。
- en: 'In the `delay` configuration, the following fields are defined:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在`delay`配置中，定义了以下字段：
- en: '`fixedDelay`: Specifies the delay duration, the value can be in hours, minutes,
    seconds, or milliseconds, specified by the `h`, `m`, `s`, or `ms` suffix, respectively'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fixedDelay`：指定延迟的持续时间，值可以是小时、分钟、秒或毫秒，分别由`h`、`m`、`s`或`ms`后缀指定。'
- en: '`percentage`: Specifies the percentage of requests for which the delay will
    be injected'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`percentage`：指定延迟将注入的请求百分比'
- en: The other thing to note is that the VirtualService is associated with the `mesh`
    gateway; you may have noticed that we did not define an Ingress or Egress gateway
    with `mesh`. So, you must be wondering where this came from, `mesh` is a reserved
    word used to refer all the sidecars in the mesh. This is also the default value
    for the gateway configuration, so if you don’t provide a value for the gateway,
    then the VirtualService by default associates itself with all the sidecars in
    the mesh.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另外需要注意的是，VirtualService 与 `mesh` 网关关联；你可能已经注意到我们没有为 `mesh` 定义 Ingress 或 Egress
    网关。那么，你一定在想这个是从哪里来的，`mesh` 是一个保留字，用来指代网格中的所有 sidecar。这也是网关配置的默认值，因此，如果你没有为网关提供值，VirtualService
    默认会与网格中的所有 sidecar 关联。
- en: So, let’s summarize what we have configured.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们总结一下我们配置的内容。
- en: The `sock-shop` VirtualService is associated with all sidecars in the mesh and
    is applied for requests destined for `catalogue.sock-shop.svc.cluster.local`,
    the VirtualService injects a delay of 10 seconds for all requests prefixed with
    `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b`, and it then forwards them to
    the `catalogue.sock-shop.svc.cluster.local` service. Requests that don’t have
    the `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b` prefix are forwarded as
    is to the `catalogue.sock-shop.svc.cluster.local` service.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`sock-shop` VirtualService 与网格中的所有 sidecar 关联，并且应用于目标为 `catalogue.sock-shop.svc.cluster.local`
    的请求。VirtualService 为所有以 `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b` 为前缀的请求注入了
    10 秒的延迟，然后将其转发到 `catalogue.sock-shop.svc.cluster.local` 服务。没有该前缀的请求则直接转发到 `catalogue.sock-shop.svc.cluster.local`
    服务。'
- en: 'Create a namespace for `Chapter5` with Istio injection enabled:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Istio 注入启用为 `Chapter5` 创建一个命名空间：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the Ingress gateway and VirtualService configuration for `sockshop.com`
    using the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下内容为 `sockshop.com` 创建 Ingress 网关和 VirtualService 配置：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After that, apply the VirtualService configuration for the catalog service:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，应用目录服务的 VirtualService 配置：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After this, open `sockshop.com` in your browser using the Ingress ELB and custom
    host headers. Enable the developer tools and search for requests with the `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b`
    prefix. You will notice those particular requests are taking more than 10 seconds
    to process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 Ingress ELB 和自定义主机头在浏览器中打开 `sockshop.com`。启用开发者工具并搜索以 `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b`
    为前缀的请求。你会注意到这些特定的请求处理时间超过了 10 秒。
- en: '![Figure 5.2 – HTTP delay causing requests to take more than 10 seconds](img/B17989_05_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – HTTP 延迟导致请求耗时超过 10 秒](img/B17989_05_02.jpg)'
- en: Figure 5.2 – HTTP delay causing requests to take more than 10 seconds
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – HTTP 延迟导致请求耗时超过 10 秒
- en: 'You can also check the sidecar injected in the `front-end` Pod to get the access
    logs for this request:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以检查注入到 `front-end` Pod 中的 sidecar，以获取此请求的访问日志：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Highlighted in the preceding code block is the request that, in this instance,
    took `10005` milliseconds to process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中突出显示的是在本实例中处理时耗时 `10005` 毫秒的请求。
- en: In this section, we injected the latency of 10 seconds, but you may have also
    noticed that the front-end web page functioned without any noticeable delays.
    All the images loaded asynchronously and any latency was limited to the catalogue
    section of the page. However, by configuring the delay, you are able to test the
    end-to-end behavior of the application in case of any unforeseen delays in the
    network or processing in the catalog service.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们注入了10秒的延迟，但你可能也注意到前端网页没有出现明显的延迟。所有图片都异步加载，任何延迟仅限于页面的目录部分。然而，通过配置延迟，你可以测试在网络或目录服务处理过程中出现任何不可预见的延迟时，应用程序的端到端行为。
- en: What is HTTP abort?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 HTTP 中止？
- en: HTTP abort is the second type of fault that can be injected using Istio. HTTP
    abort prematurely aborts the processing of the request; you can also specify the
    error code that needs to be returned downstream.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 中止是通过 Istio 注入的第二种故障类型。HTTP 中止会提前终止请求的处理；你还可以指定需要返回到下游的错误代码。
- en: 'The following is the snippet from the `VirtualService` definition with the
    `abort` configuration for the catalog service. The configuration is available
    in `Chapter5/03-faultinjection_abort.yaml`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `VirtualService` 定义中的片段，包含为目录服务配置的 `abort` 配置。该配置可在 `Chapter5/03-faultinjection_abort.yaml`
    文件中找到：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Under `fault`, there is another configuration called `abort` with the following
    parameters:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `fault` 下，还有一个名为 `abort` 的配置项，包含以下参数：
- en: '`httpStatus`: Specifies the HTTP status code that needs to be returned downstream'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`httpStatus`：指定需要下游返回的 HTTP 状态码'
- en: '`percentage`: Specifies the percentage of requests that need to be aborted'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`percentage`：指定需要中止的请求百分比'
- en: 'The following is a list of the additional configuration that can be applied
    to the gRPC request:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以应用于 gRPC 请求的额外配置列表：
- en: '`grpcStatus`: The gRPC status code that needs to be returned when aborting
    gRPC requests'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grpcStatus`：在中止 gRPC 请求时需要返回的 gRPC 状态码'
- en: '![Figure 5.3 – HTTP abort injection in catalog VirtualService](img/B17989_05_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 目录 VirtualService 中的 HTTP 中止注入](img/B17989_05_03.jpg)'
- en: Figure 5.3 – HTTP abort injection in catalog VirtualService
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 目录 VirtualService 中的 HTTP 中止注入
- en: In `Chapter5/03-faultinjection_abort.yaml`, we have configured a VirtualService
    rule for all calls originating from within the mesh to [http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b](http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b)
    to be aborted with an HTTP status code of `500`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Chapter5/03-faultinjection_abort.yaml` 文件中，我们为所有来自网格内部的调用配置了一个 VirtualService
    规则，要求它们访问 [http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b](http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b)
    时会被中止，并返回 HTTP 状态码 `500`。
- en: 'Apply the following configuration:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 应用以下配置：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When loading `sock-shop.com` from the browser, you will notice that one image
    doesn’t load. Looking at the `istio-proxy` access logs for the `front-end` Pod,
    you will find the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当从浏览器加载 `sock-shop.com` 时，你会注意到有一张图片未能加载。查看 `front-end` Pod 的 `istio-proxy`
    访问日志，你将看到以下内容：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This concludes fault injection and you have now practiced how to inject `delay`
    and `abort` into a Service Mesh.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束了故障注入的讲解，现在你已经练习了如何将 `delay` 和 `abort` 注入到服务网格中。
- en: Reminder
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒
- en: Please clean up `Chapter5/02-faultinjection_delay.yaml` and `Chapter5/03-faultinjection_abort.yaml`
    to avoid conflict with the upcoming exercises in this chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请清理 `Chapter5/02-faultinjection_delay.yaml` 和 `Chapter5/03-faultinjection_abort.yaml`
    文件，以避免与本章节后续练习的冲突。
- en: In this section, we read about how to inject faults into a mesh so that we can
    then test the microservices for their resiliency and design them to withstand
    any fault caused by latency and delays due to upstream service communications.
    In the following section, we will read about implementing timeouts and retries
    in the Service Mesh.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了如何向服务网格中注入故障，以便测试微服务的弹性，并设计它们以应对由于上游服务通信导致的延迟和故障。在接下来的章节中，我们将了解如何在服务网格中实现超时和重试机制。
- en: Application resiliency using timeouts and retries
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用超时和重试机制实现应用的弹性
- en: With communication between multiple microservices, several things can go wrong,
    network and infrastructure being the most common causes of service degradation
    and outages. A service too slow to respond can cause cascading failures across
    other services and have a ripple effect across the whole application. So, microservices
    design must be prepared for unexpected delays by setting **timeouts** when sending
    requests to other microservices.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个微服务之间的通信中，许多问题可能会发生，网络和基础设施是导致服务降级和中断的最常见原因。响应过慢的服务可能导致其他服务发生级联故障，并对整个应用产生连锁反应。因此，微服务的设计必须通过设置
    **超时** 来应对向其他微服务发送请求时可能发生的意外延迟。
- en: The timeout is the amount of time for which a service can wait for a response
    from other services; beyond the timeout duration, the response has no significance
    to the requestor. Once a timeout happens, the microservices will follow contingency
    methods, which may include servicing the response from the cache or letting the
    request gracefully fail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 超时是指服务等待其他服务响应的最大时间；超出超时期限，响应对请求方没有意义。一旦发生超时，微服务将按照应急方法进行处理，这可能包括从缓存提供响应或让请求优雅地失败。
- en: Sometimes, issues are transient, and it makes sense to make another attempt
    to get a response. This approach is called a **retry**, where a microservice can
    retry a request based on certain conditions. In this section, we will discuss
    how Istio enables service timeouts and retries without requiring any code changes
    for microservices. We will start with timeouts first.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，问题是短暂的，重新尝试获取响应是有意义的。这个方法称为 **重试**，即微服务可以基于某些条件重试请求。在本节中，我们将讨论如何在不需要修改微服务代码的情况下，使用
    Istio 实现服务超时和重试。我们将先从超时开始。
- en: Timeouts
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超时
- en: A timeout is the amount of time for which an `istio-proxy` sidecar should wait
    for replies from a given service. Timeouts help ensure that microservices are
    not waiting unreasonably long for replies and that calls succeed or fail within
    a predictable timeframe. Istio lets you easily adjust timeouts dynamically on
    a per-service basis using `VirtualServices` without having to edit your service
    code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 超时是 `istio-proxy` sidecar 应等待某个服务回复的时间。超时有助于确保微服务不会无故等待过长时间的回复，并且调用能够在可预测的时间框架内成功或失败。Istio
    让你可以轻松地通过 `VirtualServices` 动态调整每个服务的超时设置，而不需要修改服务代码。
- en: 'As an example, we will configure a timeout of 1 second on the `order` service
    and we will generate a `delay` of 10 seconds in the `payment` service. The `order`
    service calls the `payment` service during check out, so we are simulating a slow
    payment service and implementing resiliency in the frontend service by configuring
    a timeout during invocation of `order` service:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将在 `order` 服务上配置 `1` 秒的超时，并在 `payment` 服务中引入 `10` 秒的延迟。`order` 服务在结账时调用
    `payment` 服务，因此我们正在模拟一个缓慢的支付服务，并通过在调用 `order` 服务时配置超时来实现前端服务的弹性：
- en: '![Figure 5.4 – Timeout for order and delay fault in the payment service](img/B17989_05_04.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 订单超时与支付服务中的延迟故障](img/B17989_05_04.jpg)'
- en: Figure 5.4 – Timeout for order and delay fault in the payment service
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 订单超时与支付服务中的延迟故障
- en: 'We will start by first configuring a timeout in the `order` service, which
    also happens via the `VirtualService`. You can find the full configuration in
    `Chapter5/04-request-timeouts.yaml` on GitHub:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先在 `order` 服务中配置超时，这也是通过 `VirtualService` 实现的。你可以在 GitHub 上的 `Chapter5/04-request-timeouts.yaml`
    文件中找到完整的配置：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we have created a new VirtualService called `orders` and we have configured
    a `timeout` of `1` second to any request for `orders.sock-shop.svc.cluster.local`
    made from within the mesh. The timeout is part of the `http` route configuration.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个新的 VirtualService，名为 `orders`，并且为从网格内部发往 `orders.sock-shop.svc.cluster.local`
    的任何请求配置了 `1` 秒的超时。该超时是 `http` 路由配置的一部分。
- en: Following this, we are also injecting a delay of `10` seconds to all requests
    to the `payment` service. For details, you can refer to the `Chapter5/04-request-timeouts.yaml`
    file.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还将在所有请求到 `payment` 服务时注入 `10` 秒的延迟。详情请参见 `Chapter5/04-request-timeouts.yaml`
    文件。
- en: 'Go ahead and apply the changes:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 继续并应用更改：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: From the sockshop website, add any items to the cart and check out. Observe
    the behavior before and after applying the changes in this section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sockshop 网站添加任何商品到购物车并结账。观察在应用本节更改前后的行为。
- en: 'Check the logs for the sidecar in the order Pods:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 检查订单 Pod 中 sidecar 的日志：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice that the actual request to the `payment` Pod was processed in `2` milliseconds
    but the overall time taken was `10007` milliseconds due to the injection of the
    `delay` fault.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，实际请求到 `payment` Pod 的处理时间是 `2` 毫秒，但由于注入了 `delay` 故障，整体耗时为 `10007` 毫秒。
- en: 'Further, check the `istio-proxy` logs in the `front-end` Pods:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，检查 `front-end` Pods 中 `istio-proxy` 的日志：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we can see the request was returned after a little over `1` second with
    an HTTP status code of `504`. Although the underlying request for payment was
    processed in `10` seconds, the request to the `order` service timed out after
    `1` second.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到请求在大约 `1` 秒后返回，并且 HTTP 状态码为 `504`。尽管底层的支付请求在 `10` 秒内处理完毕，但对 `order`
    服务的请求在 `1` 秒后超时。
- en: In this scenario, we can see that the error was not gracefully handled by the
    website. Instead of returning a graceful message such as “`order` service.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以看到网站没有优雅地处理错误。它没有返回像“`order` 服务”的友好信息。
- en: Reminder
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒
- en: Don’t forget to clean up `Chapter5/04-request-timeouts.yaml` to avoid conflict
    with upcoming exercises.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了清理 `Chapter5/04-request-timeouts.yaml` 文件，以避免与后续练习发生冲突。
- en: Retries
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重试
- en: Timeouts are good firewalls to stop delays from cascading to other parts of
    an application, but the root cause of a delay is sometimes transient. In these
    scenarios, it may make sense to retry the requests at least a couple of times.
    The number of retries and the interval between them depends on the reason for
    the delay, which is determined by the error codes returned in the response.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 超时是防止延迟蔓延到应用程序其他部分的良好防火墙，但延迟的根本原因有时是暂时性的。在这些情况下，可能需要重试请求至少几次。重试次数和重试间隔取决于延迟的原因，该原因由响应中返回的错误代码确定。
- en: In this section, we will look at how to inject retries into the Service Mesh.
    To keep things simple so that we can focus on learning about the concepts in this
    section, we will make use of the `envoydummy` service we created in the previous
    chapter. Envoy has many filters to simulate various delays, which we will leverage
    to mimic application failures.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何将重试机制注入到服务网格中。为了简化操作，使我们能够专注于学习本节中的概念，我们将利用前一章中创建的 `envoydummy`
    服务。Envoy 提供了许多过滤器来模拟各种延迟，我们将利用这些过滤器来模拟应用程序故障。
- en: 'First, configure the `envoydummy` service:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，配置 `envoydummy` 服务：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After that deploy the `envoy` service and Pods:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后部署 `envoy` 服务和 Pods：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then deploy the gateway and VirtualService:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们部署网关和 VirtualService：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Test the service and see whether it is working OK.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 测试服务，查看它是否正常工作。
- en: After this, we will configure the `envoy` config to abort half of the calls
    and return an error code of `503`. Notice the similarity between the Istio config
    and the `envoy` config.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将配置 `envoy` 配置文件来中止一半的调用，并返回错误代码 `503`。注意 Istio 配置和 `envoy` 配置之间的相似性。
- en: 'Configure `envoydummy` to abort half of the API calls:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 `envoydummy` 以中止一半的 API 调用：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The complete file is available at `Chapter5/envoy-proxy-02-abort-02.yaml`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的文件可以在 `Chapter5/envoy-proxy-02-abort-02.yaml` 中找到。
- en: 'Apply the changes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 应用更改：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Conduct a few tests and you will notice that the API calls are working OK, although
    we have configured `envoydummy` to fail half of the calls. This is because Istio
    has already enabled default retries. Although the requests are aborted by `envoydummy`,
    the sidecar retries them for a default of two times and eventually gets a successful
    response. The interval between retries (over 25 ms) is variable and is determined
    automatically by Istio, preventing the called service from being overwhelmed by
    requests. This is made possible by the Envoy proxy in the sidecar, which uses
    a fully jittered exponential back-off algorithm for retries with a configurable
    base interval with a default value of 25 ms. If the base interval is C and N is
    the number of retry attempts, then the back-off for the retry is in the range
    of [0,(2^N−1)C). For example, for an interval of 25 ms and a retry attempt of
    2, then the first retry will be delayed randomly by 0-24 ms and the second by
    0-74 ms.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 进行几次测试后，你会注意到 API 调用正常工作，尽管我们已将`envoydummy`配置为使一半的调用失败。这是因为 Istio 已经启用了默认的重试机制。尽管请求被`envoydummy`中止，但
    sidecar 会默认重试两次，最终得到成功的响应。重试之间的间隔（超过 25 毫秒）是可变的，由 Istio 自动确定，从而防止被调用的服务被过多请求压垮。这得益于
    sidecar 中的 Envoy 代理，它使用完全抖动的指数回退算法进行重试，并提供可配置的基础间隔，默认值为 25 毫秒。如果基础间隔为 C，N 为重试次数，则重试的回退范围为
    [0, (2^N−1)C)。例如，间隔为 25 毫秒，重试次数为 2 时，第一个重试会随机延迟 0-24 毫秒，第二个重试会延迟 0-74 毫秒。
- en: 'To disable `retries`, make the following changes in the `mockshop` VirtualService:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用 `retries`，请在 `mockshop` VirtualService 中进行以下更改：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we have configured the number of `retries` to `0`. Apply the changes and
    this time, you will notice that half of the API calls return `503`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已将 `retries` 的次数配置为 `0`。应用更改后，你会注意到一半的 API 调用返回 `503`。
- en: 'We will be making the changes as depicted in the following illustration:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照下图所示进行更改：
- en: '![Figure 5.5 – Request retries](img/B17989_05_05.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 请求重试](img/B17989_05_05.jpg)'
- en: Figure 5.5 – Request retries
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 请求重试
- en: 'Make the following changes to the `retry` block. You can find the full configuration
    at `Chapter5/05-request-retry.yaml`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对 `retry` 块进行以下更改。你可以在 `Chapter5/05-request-retry.yaml` 中找到完整配置：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the `retry` block, we define the following configurations:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `retry` 块中，我们定义了以下配置：
- en: '`attempts`: The number of retries for a given request'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attempts`: 给定请求的重试次数'
- en: '`perTryTimeout`: The timeout for every attempt'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perTryTimeout`: 每次尝试的超时时间'
- en: '`retryOn`: The condition for which the request should be retried'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retryOn`: 请求应重试的条件'
- en: 'Apply the configuration and you will notice that the requests are working as
    usual:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 应用配置后，你会注意到请求按预期正常工作：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Reminder
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒
- en: Please clean up `Chapter5/05-request-retry.yaml` to avoid conflict with upcoming
    exercises.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请清理`Chapter5/05-request-retry.yaml`以避免与后续练习发生冲突。
- en: This concludes this section, where you have learned how to set timeouts and
    retries in the Service Mesh to improve application resiliency. In the next section,
    we will explore various strategies for load balancing.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本节到此结束，你已经学会了如何设置服务网格中的超时和重试机制，以提高应用程序的韧性。在下一节中，我们将探索不同的负载均衡策略。
- en: Building application resiliency using load balancing
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用负载均衡构建应用程序的韧性
- en: Load balancing is another technique to improve application resiliency. Istio
    load balancing policies help you maximize the availability of your application
    by distributing network traffic efficiently across the microservices or underlying
    services. Load balancing uses destination rules. Destination rules define the
    policy that controls how the traffic should be handled by the service after routing
    has occurred.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是提高应用程序韧性的另一种技术。Istio的负载均衡策略通过有效地将网络流量分配到微服务或底层服务，帮助你最大化应用程序的可用性。负载均衡使用目标规则，目标规则定义了在路由完成后，服务如何处理流量的策略。
- en: In the previous chapter, we used destination rules for traffic management purposes.
    In this section, we will go through various load balancing strategies provided
    by Istio and how to configure them using destination rules.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用了目标规则进行流量管理。在本节中，我们将介绍Istio提供的各种负载均衡策略，以及如何使用目标规则进行配置。
- en: 'Deploy another `envoydummy` Pod but with an additional label of `version:v2`
    this time and an output of `V2----------Bootstrap Service Mesh Implementation
    with Istio----------V2`. The config is available in `Chapter5/envoy-proxy-02.yaml`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 部署另一个`envoydummy` Pod，但这次添加一个标签`version:v2`，输出为`V2----------Bootstrap Service
    Mesh Implementation with Istio----------V2`。配置文件可在`Chapter5/envoy-proxy-02.yaml`中找到：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Istio supports the following load balancing strategies.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Istio支持以下负载均衡策略。
- en: Round-robins
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环轮询
- en: Round robins are one of the simplest ways of distributing loads, where requests
    are forwarded one by one to underlying backend services. Although simple to use,
    they don’t necessarily result in the most efficient distribution of traffic, because
    with round-robin load balancing, every upstream is treated the same, as if they
    are handling the same kind of traffic, are equally performant, and experience
    similar environmental constraints.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 循环轮询是最简单的负载分配方式之一，请求会依次转发到底层后端服务。虽然易于使用，但它们不一定能导致最有效的流量分配，因为在轮询负载均衡中，每个上游都被视为相同的流量类型，具有相同的性能，并且承受类似的环境约束。
- en: 'In `Chapter5/06-loadbalancing-roundrobbin.yaml`, we have created a destination
    rule with `trafficPolicy` as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Chapter5/06-loadbalancing-roundrobbin.yaml`中，我们已经创建了如下的目标规则，`trafficPolicy`如下：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In `DestinationRule`, you can define multiple parameters, which we will uncover
    one by one in this section and subsequent sections.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在`DestinationRule`中，你可以定义多个参数，我们将在本节及后续章节中逐一揭示。
- en: 'For round-robin load balancing, we have defined the following destination rules
    in `trafficPolicy`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于轮询负载均衡，我们在`trafficPolicy`中定义了如下的目标规则：
- en: '`simple`: Defines the load balancing algorithms to be used – the possible values
    are as follows:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`simple`：定义要使用的负载均衡算法—可能的值如下：'
- en: '`UNSPECIFIED`: Istio will select an appropriate default'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UNSPECIFIED`：Istio将选择一个合适的默认值'
- en: '`RANDOM`: Istio will select a healthy host at random.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RANDOM`：Istio将随机选择一个健康的主机。'
- en: '`PASSTHROUGH`: This option allows the client to ask for a specific upstream
    and the load balancing policy will forward the request to the requested upstream.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PASSTHROUGH`：此选项允许客户端请求特定的上游，并且负载均衡策略将把请求转发到请求的上游。'
- en: '`ROUND_ROBIN`: Istio will send requests in a round-robin fashion to upstream
    services.'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ROUND_ROBIN`：Istio将按轮询方式向上游服务发送请求。'
- en: '`LEAST_REQUEST`: This distributes the load across endpoints depending on how
    many requests are outstanding on each endpoint. This policy is the most efficient
    of all the load balancing policies.'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LEAST_REQUEST`：此策略根据每个端点上未处理的请求数量来分配负载。该策略是所有负载均衡策略中效率最高的。'
- en: 'Apply the configuration using this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令应用配置：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Test the endpoints and you will notice that you are receiving an equal amount
    of responses from both versions of `envoydummy`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 测试端点后，你会注意到从`envoydummy`的两个版本接收到了相等数量的响应。
- en: RANDOM
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RANDOM
- en: 'When using the `RANDOM` load balancing policy, Istio selects a destination
    host at random. You can find an example of the `RANDOM` load balancing policy
    in the `Chapter5/07-loadbalancing-random.yaml` file. The following is the destination
    rule with a `RANDOM` load balancing policy:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RANDOM`负载均衡策略时，Istio 会随机选择一个目标主机。你可以在`Chapter5/07-loadbalancing-random.yaml`文件中找到`RANDOM`负载均衡策略的示例。以下是使用`RANDOM`负载均衡策略的目标规则：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Apply the configuration:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 应用配置：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Make a few requests to the endpoints and you will notice that the response doesn’t
    have any predictable patterns.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 向端点发送几个请求，你会发现响应没有任何可预测的模式。
- en: LEAST_REQUEST
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LEAST_REQUEST
- en: As mentioned earlier, in the `LEAST_REQUEST` load balancing policy, Istio routes
    the traffic that has the least amount of outstanding requests upstream.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在`LEAST_REQUEST`负载均衡策略中，Istio 会将未完成请求最少的流量引导到上游。
- en: 'To mimic this scenario, we will create another service that specifically sends
    all requests to an `envoydummy` version 2 Pod. The configuration is available
    at `Chapter5/08-loadbalancing-leastrequest.yaml`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这种情况，我们将创建另一个服务，专门将所有请求发送到`envoydummy`版本 2 Pod。配置文件可以在`Chapter5/08-loadbalancing-leastrequest.yaml`找到：
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We have also made changes to `DestinationRule` to send the request to the host
    that has the fewest active connections:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还修改了`DestinationRule`，将请求发送到活动连接最少的主机：
- en: '[PRE26]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Apply the configuration:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 应用配置：
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Using `kubectl port-forward`, we can send a request to the `envoydummy2` service
    from `localhost`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl port-forward`，我们可以从`localhost`向`envoydummy2`服务发送请求：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After this, we will generate a request targeted for version 2 of the `envoydummy`
    service using the following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将使用以下命令生成一个目标为`envoydummy`版本 2 服务的请求：
- en: '[PRE29]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'While the load is in progress, access the request using the `mockshop` endpoint
    and you will notice the majority, if not all the requests, are served by version
    1 of the `envoydummy` Pods because of the `LEAST_REQUEST` load balancing policy:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当负载处理进行时，通过`mockshop`端点访问请求，你会发现大多数（如果不是所有）请求都是由`envoydummy` Pods 的版本 1 提供服务，因为使用了`LEAST_REQUEST`负载均衡策略：
- en: '[PRE30]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding example, you saw how Istio routes all requests for `mockshop`
    to version 1 of `envoydummy` because `v1` had the fewest active connections.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，你看到了 Istio 如何将所有对`mockshop`的请求引导到`envoydummy`版本 1，因为`v1`的活动连接最少。
- en: Defining multiple load balancing rules
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义多个负载均衡规则
- en: Istio has provisions to apply multiple load balancing rules for every subset.
    In the `Chapter5/09-loadbalancing-multirules.yaml` file, we are defining the default
    load balancing policy of `ROUND_ROBIN`, the `LEAST_REQUEST` policy for the `v1`
    subset, and `RANDOM` for the `v2` subset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 提供了为每个子集应用多个负载均衡规则的功能。在`Chapter5/09-loadbalancing-multirules.yaml`文件中，我们定义了默认的负载均衡策略为`ROUND_ROBIN`，为`v1`子集定义了`LEAST_REQUEST`策略，为`v2`子集定义了`RANDOM`策略。
- en: 'The following is the snippet from the configuration defined in `Chapter5/09-loadbalancing-multirules.yaml`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从`Chapter5/09-loadbalancing-multirules.yaml`配置文件中提取的配置片段：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code block, we have applied the `LEAST_REQUEST` load balancing
    policy to the `v1` subset, the `RANDOM` load balancing policy to the `v2` subset,
    and `ROUND_ROBBIN` to any other subset not specified in the code block.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们为`v1`子集应用了`LEAST_REQUEST`负载均衡策略，为`v2`子集应用了`RANDOM`负载均衡策略，而对代码块中未指定的其他子集应用了`ROUND_ROBIN`策略。
- en: Being able to define multiple load balancing rules for workloads enables you
    to apply fine-grained control on traffic distribution at the level of the destination
    rule subset. In the next section, we will go over another important aspect of
    application resiliency called **rate limiting** and how it can be implemented
    in Istio.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 能够为工作负载定义多个负载均衡规则，允许你在目标规则子集的级别上进行精细的流量分配控制。在下一节中，我们将介绍另一个关于应用程序弹性的重要方面——**限流**，以及它如何在
    Istio 中实现。
- en: Rate limiting
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限流
- en: 'Another important technique for application resiliency is rate limiting and
    circuit breaking. Rate limiting helps provide the following controls to handle
    traffic from consumers without breaking down the provider system:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的应用程序弹性技术是限流和断路器。限流有助于提供以下控制，来处理来自消费者的流量而不至于崩溃提供者系统：
- en: Surge protection to prevent a system from being overloaded by a sudden spike
    in traffic
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激增保护，防止系统因流量的突然激增而被超载
- en: Aligning the rate of incoming requests with the available capacity to process
    requests
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将传入请求的速率与可用的处理请求的能力对齐
- en: Protecting slow providers from fast consumers
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护慢速提供者免受快速消费者的影响
- en: 'Rate limiting is performed by configuring destination rules with a connection
    pool for connections to upstream services. Connection pool settings can be applied
    at the TCP level as well as the HTTP level, as described in the following configuration
    in `Chapter5/10-connection-pooling.yaml`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制通过配置目标规则与连接池来执行，以连接到上游服务。连接池设置可以应用于TCP层以及HTTP层，如在`Chapter5/10-connection-pooling.yaml`中的以下配置所示：
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following are the key attributes of the connection pool configuration:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是连接池配置的关键属性：
- en: '`http2MaxRequests`: The maximum number of active requests to a destination;
    the default value is `1024`.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http2MaxRequests`：每个目的地的最大活跃请求数；默认值为`1024`。'
- en: '`maxRequestsPerConnection`: The maximum number of requests per connection to
    upstream. A value of `1` disables keep-alive whereas `0` is unlimited.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxRequestsPerConnection`：每个连接到上游的最大请求数。值为`1`时禁用保持连接，而`0`表示无限制。'
- en: '`http1MaxPendingRequests`: The maximum number of requests that will be queued
    while waiting for a connection from the connection pool; the default value is
    `1024`.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http1MaxPendingRequests`：在等待来自连接池的连接时，将排队的最大请求数；默认值为`1024`。'
- en: We have configured a maximum of `1` request per connection to upstream, a maximum
    of `1` active connection at any point of time, and no queuing for connection requests
    allowed.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经配置了每个连接最多`1`个请求到上游，在任何时刻最多`1`个活跃连接，并且不允许连接请求排队。
- en: Testing the rate limit, circuit breakers, and outlier detection are not as straightforward
    as testing other features of application resiliency. Fortunately, there is a very
    handy load testing utility called `fortio` available at [https://github.com/fortio/fortio](https://github.com/fortio/fortio)
    and packaged in the Istio sample directory. We will use `fortio` for generating
    load and testing rate limit.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 测试速率限制、熔断器和异常值检测并不像测试应用程序弹性功能的其他特性那样直接。幸运的是，有一个非常方便的负载测试工具叫做`fortio`，它可以在[https://github.com/fortio/fortio](https://github.com/fortio/fortio)找到，并且已经打包在Istio示例目录中。我们将使用`fortio`来生成负载并测试速率限制。
- en: 'Deploy `fortio` from the Istio directory:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 从Istio目录部署`fortio`：
- en: '[PRE33]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Apply one of the load balancing policies to test normal behavior:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 应用其中一种负载均衡策略以测试正常行为：
- en: '[PRE34]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Generate a load using `fortio`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`fortio`生成负载：
- en: '[PRE35]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the previous request, we are configuring `folio` to generate a load test
    for `1` second with `2` parallel connections with a maximum query per second (`qps`)
    rate of `0`, meaning no waits/the maximum `qps` rate.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的请求中，我们配置了`fortio`以生成一个`1`秒的负载测试，使用`2`个并行连接，最大查询每秒（`qps`）为`0`，意味着没有等待/最大`qps`速率。
- en: 'In the output, you will notice that all requests were successfully processed:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，你会注意到所有请求都成功处理了：
- en: '[PRE36]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In this case, a total of `486 calls` were made with a 100% success rate. Next,
    we will apply the changes to enforce rate limiting:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，共进行了`486次调用`，成功率为100%。接下来，我们将应用更改以强制执行速率限制：
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Run the test again with `1` connection:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`1`个连接重新运行测试：
- en: '[PRE38]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Run the test again; this time, with two connections:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行测试，这次使用两个连接：
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You can see that `33.4%` of the calls failed with a 503 error code because the
    destination rule enforces the rate limiting rules.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，`33.4%`的调用因503错误代码失败，因为目标规则强制执行了速率限制规则。
- en: In this section, you saw an example of rate limiting, which, in turn, is also
    circuit breaking based on a rate limiting condition. In the next section, we will
    read about circuit breaking by detecting outliers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你看到了一种速率限制的示例，速率限制在某种程度上也是基于速率限制条件的熔断。在下一节中，我们将学习如何通过检测异常值来进行熔断。
- en: Circuit breakers and outlier detection
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熔断器和异常值检测
- en: In this section, we will look at outlier detection and circuit breaker patterns.
    **A circuit breaker** is a design pattern in which you continuously monitor the
    response processing behavior of upstream systems and when the behavior is unacceptable,
    you stop sending any further requests upstream until the behavior has become acceptable
    again.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论异常值检测和熔断器模式。**熔断器**是一种设计模式，它持续监控上游系统的响应处理行为，当行为不可接受时，停止向上游发送任何进一步的请求，直到该行为恢复到可接受状态。
- en: For example, you can monitor the average response time of the upstream system
    and when it crosses a certain threshold, you may decide to stop sending any further
    requests to the system; this is called tripping the circuit breaker. Once the
    circuit breaker has been tripped, you leave it that way for a certain duration
    so that the upstream service can heal. After the circuit breaker duration has
    elapsed, you can reset the circuit breaker to let the traffic pass through again.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以监控上游系统的平均响应时间，当其超过某个阈值时，您可以决定停止向该系统发送更多请求；这就叫做触发断路器。一旦断路器被触发，您可以保持这一状态一段时间，以便上游服务自我修复。在断路器持续时间过去后，您可以重置断路器，让流量重新通过。
- en: While circuit breaking is the part that handles the flow of traffic, **outlier
    detection** is a set of policies to identify the conditions when the circuit breaker
    should trip.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当断路器负责流量控制时，**异常检测**是一组策略，用于识别何时触发断路器。
- en: We will configure one of `envoy` Pods to return a `503` error at random. We
    will reuse `Chapter5/envoy-proxy-02-abort-02.yaml`, in which we configured a version
    of `envoydummy` to return a `503` error for 50% of the requests.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将配置一个`envoy` Pod，随机返回一个`503`错误。我们将重用`Chapter5/envoy-proxy-02-abort-02.yaml`，在其中我们配置了一个版本的`envoydummy`，以50%的概率返回`503`错误。
- en: To avoid any confusion, delete all previous deployments of `envoydummy` in the
    `utilities` namespace and any Istio config we have executed prior to this section.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为避免混淆，请删除`utilities`命名空间中所有之前部署的`envoydummy`以及在此部分之前执行的任何Istio配置。
- en: 'Perform the following in the same order as shown:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下顺序执行：
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: At this stage, we have two `envoydummy` Pods. For the Pod labeled `version:v1`,
    returning `V1----------Bootstrap Service Mesh Implementation with Istio----------V1`,
    we have modified it to abort 50% of the request and return `503`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有两个`envoydummy` Pods。对于标记为`version:v1`的Pod，返回`V1----------Bootstrap Service
    Mesh Implementation with Istio----------V1`，我们已将其修改为中止50%的请求并返回`503`。
- en: 'Perform the following command to disable the default automated retries by Istio:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令来禁用Istio的默认自动重试：
- en: '[PRE41]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Test the request and you will notice that the response is a mixed bag of `v1`,
    `v2`, and `503`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 测试请求，你会发现响应是`v1`、`v2`和`503`的混合结果。
- en: 'Now, the task at hand is to define an outlier detection policy to detect `v1`
    as the outlier because of its erroneous behavior of returning a `503` error code.
    We will do that via destination rules as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，任务是定义一个异常检测策略，将`v1`识别为异常，因为它返回了一个`503`错误码。我们将通过目标规则来实现这一点，如下所示：
- en: '[PRE42]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In `outlierDetection`, the following parameters are provided:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在`outlierDetection`中，提供了以下参数：
- en: '`baseEjectionTime`: The minimum ejection duration per ejection, which is then
    multiplied by the number of times an upstream is found to be unhealthy. For example,
    if a host is found to be an outlier five times, then it will be ejected from the
    connection pool for `baseEjectionTime*5`.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`baseEjectionTime`：每次弹出的最小弹出持续时间，然后根据发现上游不健康的次数进行乘法计算。例如，如果某个主机被判定为异常五次，那么它将在连接池中被弹出`baseEjectionTime*5`。'
- en: '`consecutive5xxErrors`: The number of 5xx errors that need to occur to qualify
    the upstream to be an outlier.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`consecutive5xxErrors`：需要发生的5xx错误次数，以判定上游为异常。'
- en: '`interval`: The time between the checks when Istio scans upstream for health
    status. The interval is specified in hours, mins, or seconds.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interval`：Istio扫描上游健康状态的检查间隔时间。间隔时间可以以小时、分钟或秒为单位。'
- en: '`maxEjectionPercent`: The maximum number of hosts in the connection pool that
    can be ejected.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxEjectionPercent`：连接池中可以被弹出的最大主机数。'
- en: In the destination rule, we have configured Istio to scan upstream at an interval
    of 1 second. If 1 or more 5xx errors are consecutively returned, then the upstream
    will be ejected from the connection pool for 5 minutes and if needed, all hosts
    can be ejected from the connection pool.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标规则中，我们配置了Istio以1秒的间隔扫描上游。如果连续返回1次或更多的5xx错误，则该上游将在5分钟内从连接池中弹出，如果需要，所有主机都可以从连接池中弹出。
- en: 'The following parameters can also be defined for outlier detection, but we
    have not used them in our example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以为异常检测定义以下参数，但我们在本例中未使用它们：
- en: '`splitExternalLocalOriginErrors`: This flag tells Istio whether it should consider
    the local service behavior to determine whether the upstream service is an outlier.
    For example, `404` may be returned, which is a valid response, but returning it
    too frequently can also mean that there may be a problem. Maybe the upstream service
    has an error but due to bad error handling, the upstream is returning `404`, which,
    in turn, makes the downstream service return a 5XX error. To summarize, this flag
    enables outlier detection not just based on the response codes returned upstream
    but also on how the downstream system perceived the response.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`splitExternalLocalOriginErrors`：此标志告诉 Istio 是否应考虑本地服务的行为来判断上游服务是否为异常节点。例如，可能会返回
    `404`，这是一个有效响应，但如果返回过于频繁，也可能意味着存在问题。也许上游服务出现了错误，但由于错误处理不当，导致上游返回 `404`，这进而使下游服务返回
    5XX 错误。总之，此标志使异常检测不仅基于上游返回的响应代码，还基于下游系统如何感知这些响应。'
- en: '`consecutiveLocalOriginFailures`: This is the number of consecutive local errors
    before the upstream service is ejected from the connection pool.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`consecutiveLocalOriginFailures`：这是在上游服务从连接池中被驱逐之前，发生的连续本地错误次数。'
- en: '`consecutiveGatewayErrors`: This is the number of gateway errors before an
    upstream service is ejected. This might be caused by unhealthy connections or
    misconfiguration between a gateway and the upstream service. When an upstream
    host is accessed over HTTP then, HTTP status codes of `502`, `503`, or `504` are
    usually returned due to communication issues between the gateway and upstream
    services.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`consecutiveGatewayErrors`：这是在上游服务被驱逐之前，发生的网关错误次数。通常由网关与上游服务之间的连接不健康或配置错误引起。当通过
    HTTP 访问上游主机时，由于网关和上游服务之间的通信问题，通常会返回 HTTP 状态码 `502`、`503` 或 `504`。'
- en: '`minHealthPercent`: This field defines the minimum number of healthy upstream
    systems available in the load balancing pool for outlier detection to be enabled.
    Once the number of healthy upstream systems drops below this level, outlier detection
    is disabled to maintain service availability.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minHealthPercent`：此字段定义了启用异常检测所需的负载均衡池中可用的最小健康上游系统数量。一旦健康上游系统的数量低于此阈值，异常检测将被禁用，以保持服务可用性。'
- en: The configuration defined in `Chapter5/12-outlier-detection.yaml` enables us
    to quickly observe the effects of outlier detection but when deploying this in
    a non-experimental scenario, the values need to be tuned and configured as per
    the resiliency requirements.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`Chapter5/12-outlier-detection.yaml` 中定义的配置使我们能够快速观察异常检测的效果，但在非实验场景下部署时，需要根据弹性要求调整和配置相应的值。'
- en: 'Apply the updated destination rule:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 应用更新后的目标规则：
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: After applying the changes, test the request a few times. you will notice that
    apart from just a few responses with `V1----------Bootstrap Service Mesh Implementation
    with Istio----------V1`, most of the response contains `V2----------Bootstrap
    Service Mesh Implementation with Istio----------V2` because Istio detected the
    `v1` Pod returning `503` and marked it as an outlier in the connection pool.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用更改后，测试请求几次。你会注意到，除了少数响应包含 `V1----------Bootstrap Service Mesh Implementation
    with Istio----------V1`，大部分响应包含 `V2----------Bootstrap Service Mesh Implementation
    with Istio----------V2`，这是因为 Istio 检测到 `v1` Pod 返回了 `503` 错误，并将其标记为连接池中的异常节点。
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we read about how Istio enables application resiliency and
    testing by providing options to inject delays and faults into request processing.
    Fault injection assists in validating an application’s resiliency when there is
    unexpected degradation in underlying services, as well as the network and infrastructure.
    After fault injection, we read about request timeouts and how they improve application
    resiliency. For transient failures, it might be a wise idea to make a few retries
    before giving up on the request and hence, we practiced configuring Istio to perform
    service retries. Fault injection, timeouts, and retries are properties of VirtualServices
    and are carried out before routing a request to upstream services.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了 Istio 如何通过提供注入延迟和故障的选项来实现应用程序的弹性和测试。故障注入有助于验证当底层服务、网络和基础设施出现意外故障时，应用程序的弹性。在故障注入之后，我们学习了请求超时以及它们如何改善应用程序的弹性。对于瞬时故障，在放弃请求之前进行几次重试可能是一个明智的选择，因此，我们实践了配置
    Istio 进行服务重试。故障注入、超时和重试是 VirtualServices 的属性，它们在将请求路由到上游服务之前执行。
- en: In the second part of the chapter, we read about various load balancing policies
    and how you can configure load balancing policies based on the dynamic behavior
    of an upstream service. Load balancing helps to distribute traffic to upstream
    services, with `LEAST_REQUEST` policy being the most effective policy for distributing
    traffic based on the number of requests being handled upstream at any point in
    time. Load balancing is configured in destination rules because it happens as
    part of the request routing to upstream services. After load balancing, we read
    about rate limiting and how it is based on the connection pool configuration in
    the destination rules. In the final part of the chapter, we read about how to
    configure destination rules to implement outlier detection.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们阅读了各种负载均衡策略，以及如何根据上游服务的动态行为配置负载均衡策略。负载均衡有助于将流量分配到上游服务，`LEAST_REQUEST`策略是最有效的流量分配策略，它基于任何时刻上游服务处理的请求数量进行流量分配。负载均衡在目标规则中配置，因为它是请求路由到上游服务的一部分。在负载均衡之后，我们阅读了速率限制以及它如何基于目标规则中的连接池配置。在本章的最后部分，我们阅读了如何配置目标规则来实现异常检测。
- en: The most noticeable element of everything that we read in this chapter was the
    ability to implement application resiliency via timeouts, retries, load balancing,
    circuit breaking, and outlier detection without ever needing to change the application
    code. Applications benefit from these resiliency strategies just by being part
    of the Service Mesh. Various types of software and utilities are used by software
    engineers to perform chaos engineering to understand the resiliency of an application
    suffering from failures. You can use these chaos engineering tools to test the
    application resiliency provided by the Service Mesh.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们阅读的最显著的内容是能够通过超时、重试、负载均衡、断路器和异常检测等方式实现应用程序的弹性，而无需更改应用程序代码。应用程序只需成为服务网格的一部分，就能从这些弹性策略中受益。软件工程师使用各种类型的软件和工具来执行混沌工程，以了解在遭遇故障时应用程序的弹性。你可以使用这些混沌工程工具来测试服务网格提供的应用程序弹性。
- en: The next chapter is very exciting and intense because we will read about how
    to make use of Istio to implement iron-clad security for applications running
    in the mesh.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章非常激动人心且紧张，因为我们将阅读如何利用Istio为运行在网格中的应用程序实现坚如磐石的安全性。
