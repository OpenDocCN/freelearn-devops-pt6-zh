- en: Managing Stateful Workloads
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理有状态的工作负载
- en: In [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started
    with Kubernetes,* we introduced the basic functions of Kubernetes. Once you start
    to deploy containers with Kubernetes, you'll need to consider the application's
    data life cycle and CPU/memory resource management.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 3 章](a5cf080a-372a-406e-bb48-019af313c676.xhtml)，《*Kubernetes 入门*》中，我们介绍了
    Kubernetes 的基本功能。一旦你开始使用 Kubernetes 部署容器，你就需要考虑应用程序的数据生命周期以及 CPU/内存资源管理。
- en: 'In this chapter, we will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: How a container behaves with volumes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器与卷的行为
- en: Introducing Kubernetes' volume functionalities
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Kubernetes 的卷功能
- en: Best practices and pitfalls of Kubernetes' persistent volume
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 持久卷的最佳实践与陷阱
- en: Submitting a short-lived application as a Kubernetes Job
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提交短生命周期的应用程序作为 Kubernetes 作业
- en: Kubernetes volume management
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 卷管理
- en: Kubernetes and Docker use a local disk by default. The Docker application may
    store and load any data onto the disk, for example, log data, temporary files,
    and application data. As long as the host has enough space and the application
    has the necessary permission, the data will exist as long as a container exists.
    In other words, when a container terminates, exits, crashes, or is reassigned
    to another host, the data will be lost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 和 Docker 默认使用本地磁盘。Docker 应用程序可能将任何数据存储到磁盘并加载，例如日志数据、临时文件和应用数据。只要主机有足够的空间，且应用程序具有必要的权限，数据将在容器存在期间保持存在。换句话说，当容器终止、退出、崩溃或重新分配到另一个主机时，数据将丢失。
- en: Container volume life cycle
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器卷生命周期
- en: 'In order to understand Kubernetes'' volume management, you''ll need to understand
    the Docker volume life cycle. The following example is how Docker behaves with
    a volume when a container restarts:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 Kubernetes 的卷管理，你需要了解 Docker 卷的生命周期。以下示例展示了当容器重新启动时 Docker 如何与卷交互：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In Kubernetes, you also need to take care of pod restart. In the case of a resource
    shortage, Kubernetes may stop a container and then restart a container on the
    same or another Kubernetes node.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，你还需要注意 pod 重启的情况。在资源不足时，Kubernetes 可能会停止一个容器，然后在同一个或另一个 Kubernetes
    节点上重启该容器。
- en: 'The following example shows how Kubernetes behaves when there is a resource
    shortage. One pod is killed and restarted when an out of memory error is received:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了当资源不足时 Kubernetes 的行为。收到内存不足错误时，一个 pod 会被终止并重新启动：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sharing volume between containers within a pod
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在同一 pod 中共享容器之间的卷
- en: '[Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started with
    Kubernetes,* stated that multiple containers within the same Kubernetes pod can
    share the same pod IP address, network port, and IPC. Therefore, applications
    can communicate with each other through a localhost network. However, the filesystem
    is segregated.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 3 章](a5cf080a-372a-406e-bb48-019af313c676.xhtml)，《*Kubernetes 入门*》中提到，同一
    Kubernetes pod 中的多个容器可以共享相同的 pod IP 地址、网络端口和 IPC。因此，应用程序可以通过 localhost 网络进行通信。然而，文件系统是隔离的。'
- en: 'The following diagram shows that **Tomcat** and **nginx** are in the same pod.
    Those applications can communicate with each other via localhost. However, they
    can''t access each other''s `config` files:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了 **Tomcat** 和 **nginx** 在同一个 pod 中。这些应用程序可以通过 localhost 进行互相通信。然而，它们无法访问彼此的
    `config` 文件：
- en: '![](img/495d3ce3-fbe2-4547-8a35-dec6534ab5d7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/495d3ce3-fbe2-4547-8a35-dec6534ab5d7.png)'
- en: Some applications won't affect these scenarios and behavior, but some applications
    may have some use cases that require them to use a shared directory or file. Consequently,
    developers and Kubernetes administrators need to be aware of the different types
    of stateless and stateful applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序不会影响这些场景和行为，但有些应用程序可能有一些使用案例需要它们使用共享目录或文件。因此，开发人员和 Kubernetes 管理员需要了解无状态和有状态应用程序的不同类型。
- en: Stateless and stateful applications
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无状态和有状态的应用程序
- en: Stateless applications don't need to preserve the application or user data on
    the disk volume. Although stateless applications may write the data to the filesystem
    while a container exists, it is not important in terms of the application's life
    cycle.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态应用程序不需要在磁盘卷上保留应用程序或用户数据。尽管无状态应用程序在容器存在期间可能会将数据写入文件系统，但从应用程序生命周期的角度来看，这些数据并不重要。
- en: For example, the `tomcat` container runs some web applications. It also writes
    an application log under `/usr/local/tomcat/logs/`, but it won't be affected if
    it loses a `log` file.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`tomcat`容器运行一些Web应用程序。它还会在`/usr/local/tomcat/logs/`下写入应用程序日志，但如果丢失`log`文件，它不会受到影响。
- en: But what if you need to persist an application log for analysis or auditing?
    In this scenario, Tomcat can still be stateless but share the `/usr/local/tomcat/logs`
    volume with another container such as Logstash ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)).
    Logstash will then send a log to the chosen analytic store, such as Elasticsearch
    ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你需要持久化一个应用程序日志用于分析或审计呢？在这种情况下，Tomcat仍然可以是无状态的，但可以与另一个容器（如Logstash）共享`/usr/local/tomcat/logs`存储卷（[https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)）。Logstash将把日志发送到选择的分析存储，如Elasticsearch（[https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)）。
- en: 'In this case, the `tomcat` container and `logstash` container must be in the
    same Kubernetes pod and share the `/usr/local/tomcat/logs` volume, as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`tomcat`容器和`logstash`容器必须在同一个Kubernetes Pod中，并共享`/usr/local/tomcat/logs`存储卷，如下所示：
- en: '![](img/e69e5d06-bcea-44d3-9c22-a784dddfa08c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e69e5d06-bcea-44d3-9c22-a784dddfa08c.png)'
- en: The preceding diagram shows how Tomcat and Logstash can share the `log` file
    using the Kubernetes `emptyDir` volume ([https://kubernetes.io/docs/concepts/storage/volumes/#emptydir](https://kubernetes.io/docs/concepts/storage/volumes/)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了Tomcat和Logstash如何使用Kubernetes的`emptyDir`存储卷共享`log`文件（[https://kubernetes.io/docs/concepts/storage/volumes/#emptydir](https://kubernetes.io/docs/concepts/storage/volumes/)）。
- en: 'Tomcat and Logstash didn''t use the network via localhost, but they did share
    the filesystem between `/usr/local/tomcat/logs` from the Tomcat container and
    `/mnt` from the `logstash` container, through Kubernetes'' `emptyDir` volume:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Tomcat和Logstash没有通过localhost使用网络，但它们通过Kubernetes的`emptyDir`卷共享了文件系统，Tomcat容器中的`/usr/local/tomcat/logs`与`logstash`容器中的`/mnt`共享：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s create `tomcat` and `logstash` pod, and then see whether Logstash can
    see the Tomcat application log under `/mnt`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建`tomcat`和`logstash` Pod，然后看看Logstash是否能在`/mnt`下看到Tomcat应用程序日志：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this scenario, Elasticsearch must be stateful in the final destination, meaning
    that it uses a persistent volume. The Elasticsearch container must preserve the
    data even if the container is restarted. In addition, you do not need to configure
    the Elasticsearch container within the same pod as Tomcat/Logstash. Because Elasticsearch
    should be a centralized log datastore, it can be separated from the Tomcat/Logstash
    pod and scaled independently.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Elasticsearch在最终目的地必须是有状态的，这意味着它使用持久化存储。即使容器被重启，Elasticsearch容器也必须保留数据。此外，你无需将Elasticsearch容器配置在与Tomcat/Logstash相同的Pod中。因为Elasticsearch应该是一个集中式日志数据存储，它可以与Tomcat/Logstash
    Pod分开，并独立扩展。
- en: Once you determine that your application needs a persistent volume, there are
    some different types of volumes and different ways to manage persistent volumes
    to look at.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定应用程序需要持久化存储，就可以查看不同类型的存储卷以及管理持久化存储的不同方法。
- en: Kubernetes' persistent volume and dynamic provisioning
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes的持久化存储和动态供给
- en: Kubernetes supports a variety of persistent volumes, for example, public cloud
    storage such as AWS EBS and Google persistent disks. It also supports network
    (distributed) filesystems such as NFS, GlusterFS, and Ceph. In addition, it can
    also support a block device such as iSCSI and Fibre Channel. Based on the environment
    and infrastructure, a Kubernetes administrator can choose the best matching types
    of persistent volume.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持多种持久化存储，例如，公共云存储，如AWS EBS和Google持久磁盘。它还支持网络（分布式）文件系统，如NFS、GlusterFS和Ceph。此外，它还可以支持块设备，如iSCSI和光纤通道。根据环境和基础设施，Kubernetes管理员可以选择最佳匹配的持久化存储类型。
- en: The following example is using a GCP persistent disk as a persistent volume.
    The first step is to create a GCP persistent disk and name it `gce-pd-1`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用GCP持久磁盘作为持久存储。第一步是创建一个GCP持久磁盘，并命名为`gce-pd-1`。
- en: If you use AWS EBS, Google persistent disk, or Azure disk storage, the Kubernetes
    node must be in the same cloud platform. In addition, Kubernetes has a limit on
    maximum volumes per node. Please look at the Kubernetes documentation at [https://kubernetes.io/docs/concepts/storage/storage-limits/](https://kubernetes.io/docs/concepts/storage/storage-limits/).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用 AWS EBS、Google 持久磁盘或 Azure 磁盘存储，Kubernetes 节点必须位于同一云平台上。此外，Kubernetes 对每个节点的最大卷数有限制。请参阅
    Kubernetes 文档 [https://kubernetes.io/docs/concepts/storage/storage-limits/](https://kubernetes.io/docs/concepts/storage/storage-limits/)。
- en: '![](img/8c4fd27c-12fe-4a90-94c8-a7f3290615f3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c4fd27c-12fe-4a90-94c8-a7f3290615f3.png)'
- en: 'Then, specify the name `gce-pd-1` in the `Deployment` definition:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 `Deployment` 定义中指定名称 `gce-pd-1`：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will mount the persistent disk from the GCE persistent disk to `/usr/local/tomcat/logs`,
    which can persist Tomcat application logs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把来自 GCE 持久磁盘的持久磁盘挂载到 `/usr/local/tomcat/logs`，从而保持 Tomcat 应用程序的日志持久化。
- en: Abstracting the volume layer with a persistent volume claim
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过持久卷声明抽象化卷层
- en: Directly specifying a persistent volume in a configuration file makes a tight
    couple with a particular infrastructure. For the preceding example (`tomcat-log`
    volume), `pdName` is `gce-pd-1` and Volume type is `gcePersistentDisk`. From a
    container management point of view, the pod definition shouldn't be locked into
    the specific environment because the infrastructure could be different based on
    the environment. The ideal pod definition should be flexible or abstract the actual
    infrastructure and specify only volume name and mount point.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件中直接指定持久卷会与特定基础设施紧密耦合。例如，在前面的示例（`tomcat-log` 卷）中，`pdName` 是 `gce-pd-1`，卷类型是
    `gcePersistentDisk`。从容器管理的角度来看，pod 定义不应该与特定环境绑定，因为基础设施可能会根据环境不同而有所变化。理想的 pod 定义应该是灵活的，或抽象出实际的基础设施，只指定卷名称和挂载点。
- en: 'Consequently, Kubernetes provides an abstraction layer that associates the
    pod with the persistent volume, which is called the **Persistent Volume Claim**
    (**PVC**). This allows us to decouple from the infrastructure. The Kubernetes
    administrator just needs to pre-allocate the size of the persistent volume in
    advance. Then Kubernetes will bind the persistent volume and the PVC as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Kubernetes 提供了一个抽象层，将 pod 与持久卷关联，这个抽象层被称为 **持久卷声明**（**PVC**）。这使我们能够与基础设施解耦。Kubernetes
    管理员只需提前预分配持久卷的大小。然后，Kubernetes 会将持久卷和 PVC 绑定如下：
- en: '![](img/fe65323a-e6db-489a-b13f-98f0f3c2be7c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe65323a-e6db-489a-b13f-98f0f3c2be7c.png)'
- en: 'The following example is a definition of a pod that uses a PVC; let''s reuse
    the previous example (`gce-pd-1`) to register with Kubernetes first:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例是使用 PVC 的 pod 定义；我们先使用之前的示例（`gce-pd-1`）在 Kubernetes 中进行注册：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that we assign `storageClassName` as `my-10g-pv-1`, as an identifier that
    the PVC can bind to by specifying the same name.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将 `storageClassName` 设置为 `my-10g-pv-1`，作为 PVC 可以通过指定相同名称来绑定的标识符。
- en: Next, we create a PVC that associates with the persistent volume (`pv-1`).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个与持久卷（`pv-1`）关联的 PVC。
- en: The `storageClassName` parameter lets Kubernetes use static provisioning. This
    is because some Kubernetes environments, such as **Google Container Engine** (**GKE**),
    are already set up with dynamic provisioning. If we don't specify `storageClassName`,
    Kubernetes will allocate a new `PersistentVolume` and then bind to `PersistentVolumeClaim`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`storageClassName` 参数允许 Kubernetes 使用静态供应。这是因为一些 Kubernetes 环境，如 **Google 容器引擎**（**GKE**），已经配置了动态供应。如果不指定
    `storageClassName`，Kubernetes 将分配一个新的 `PersistentVolume`，然后绑定到 `PersistentVolumeClaim`。'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now the `tomcat` setting has been decoupled from the GCE persistent volume,
    and bound to the abstracted volume, `pvc-1`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`tomcat` 设置已经与 GCE 持久卷解耦，并绑定到抽象卷 `pvc-1`：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Dynamic provisioning and StorageClass
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态供应和 StorageClass
- en: PVC a degree of flexibility for persistent volume management. However, pre-allocating
    some persistent volume pools might not be cost-efficient, especially in a public
    cloud.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: PVC 提供了持久卷管理的灵活性。然而，预分配一些持久卷池可能不是成本效益高，特别是在公共云环境中。
- en: 'Kubernetes also assists in this kind of situation by supporting dynamic provisioning
    for persistent volumes. The Kubernetes administrator defines the provisioner of
    the persistent volume, which is called `StorageClass`. Then, the PVC asks `StorageClass`
    to dynamically allocate a persistent volume, and then associates it with the PVC
    as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还通过支持持久卷的动态配置来帮助解决这种情况。Kubernetes管理员定义持久卷的供应者，称为`StorageClass`。然后，PVC请求`StorageClass`动态分配持久卷，并将其与PVC关联，如下所示：
- en: '![](img/bc67e1e5-9606-478f-867d-b539c1ae2f16.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc67e1e5-9606-478f-867d-b539c1ae2f16.png)'
- en: 'In the following example, AWS EBS is used as the `StorageClass`. When creating
    the PVC, the `StorageClass` dynamically creates an EBS then registers it as **Persistent
    Volume** (**PV**), and then attaches it to the PVC:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，AWS EBS作为`StorageClass`使用。在创建PVC时，`StorageClass`动态创建一个EBS，然后将其注册为**持久卷**(**PV**)，并将其附加到PVC：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once `StorageClass` has been successfully created, then, create a PVC without
    PV, but specify the `StorageClass` name. In this example, this would be `aws-sc`,
    as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`StorageClass`成功创建，接下来创建一个没有PV的PVC，但需要指定`StorageClass`名称。在这个示例中，名称应为`aws-sc`，如下所示：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following screenshot shows the EBS after submitting to StorageClass to
    create a PVC. AWS console shows a new EBS which is created by StorageClass:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了提交到StorageClass创建PVC后的EBS。AWS控制台显示一个由StorageClass创建的新EBS：
- en: '![](img/3cd3cac5-a5bd-4a17-901c-ad5687be8449.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cd3cac5-a5bd-4a17-901c-ad5687be8449.png)'
- en: 'Note that managed Kubernetes services such as Amazon EKS ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/)), Google
    Kubernetes Engine ([https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)),
    and Azure Kubernetes Service ([https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/))
    create `StorageClass` by default. For example, Google Kubernetes Engine sets up
    a default storage class as a Google Cloud persistent disk. For more information,
    please refer to [Chapter 10](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml), *Kubernetes
    on AWS*, [Chapter 11](d4de05e3-eb24-4e8e-bfd3-e68819b5e66c.xhtml)*, Kubernetes
    on* *GCP*, and [Chapter 12](89891610-4ca4-4216-9d76-2613d186421c.xhtml), *Kubernetes
    on Azure*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，像Amazon EKS ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))、Google
    Kubernetes Engine ([https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/))
    和 Azure Kubernetes Service ([https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/))
    这样的托管Kubernetes服务会默认创建`StorageClass`。例如，Google Kubernetes Engine将默认存储类设置为Google
    Cloud持久磁盘。有关更多信息，请参考[第10章](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml)，*Kubernetes在AWS上*，[第11章](d4de05e3-eb24-4e8e-bfd3-e68819b5e66c.xhtml)*，Kubernetes在*
    *GCP上*，以及[第12章](89891610-4ca4-4216-9d76-2613d186421c.xhtml)，*Kubernetes在Azure上*：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Problems with ephemeral and persistent volume settings
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于**短暂性**和**持久性**卷设置的问题
- en: You may determine your application as stateless because the `datastore` function
    is handled by another pod or system. However, there are some pitfalls in that
    sometimes; applications actually store important files that you aren't aware of.
    For example, Grafana ([https://grafana.com/grafana](https://grafana.com/grafana))
    connects time series datasources such as Graphite ([https://graphiteapp.org](https://graphiteapp.org))
    and InfluxDB ([https://www.influxdata.com/time-series-database/](https://www.influxdata.com/time-series-database/)),
    so people may misunderstand that Grafana is a stateless application.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会将你的应用程序视为无状态的，因为`datastore`功能由另一个Pod或系统处理。然而，这里面有一些陷阱，有时应用程序会存储一些重要文件，而你未曾意识到。例如，Grafana
    ([https://grafana.com/grafana](https://grafana.com/grafana)) 连接了时间序列数据源，如Graphite
    ([https://graphiteapp.org](https://graphiteapp.org)) 和 InfluxDB ([https://www.influxdata.com/time-series-database/](https://www.influxdata.com/time-series-database/))，所以人们可能误认为Grafana是一个无状态应用。
- en: Actually, Grafana itself also uses databases to store user, organization, and
    dashboard metadata. By default, Grafana uses SQLite3 components and stores the
    database as `/var/lib/grafana/grafana.db`. Therefore, when a container is restarted,
    the Grafana settings will all be reset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，Grafana本身也使用数据库来存储用户、组织和仪表板元数据。默认情况下，Grafana使用SQLite3组件，并将数据库存储为`/var/lib/grafana/grafana.db`。因此，当容器重新启动时，Grafana的设置将会全部被重置。
- en: 'The following example demonstrates how Grafana behaves with an ephemeral volume:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了Grafana在使用短暂性卷时的行为：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, navigate to the Grafana web console to create the Grafana `Organizations` named
    `kubernetes org`, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，进入Grafana web控制台创建名为`kubernetes org`的Grafana `Organizations`，如下所示：
- en: '![](img/b5eb22b7-fce0-4566-8f1a-15bad07ff121.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5eb22b7-fce0-4566-8f1a-15bad07ff121.png)'
- en: 'Then, look at the `Grafana` directory. Here, there is a database file (`/var/lib/grafana/grafana.db`)
    with a timestamp that has been updated after creating a Grafana `Organizations`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，查看 `Grafana` 目录。这里有一个数据库文件（`/var/lib/grafana/grafana.db`），其时间戳在创建 Grafana
    `Organizations` 后已更新：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When the pod is deleted, the `Deployment` will start a new pod and check whether
    a Grafana `Organizations` exists or not:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pod 被删除时，`Deployment` 将启动一个新 Pod，并检查是否存在 Grafana `Organizations`：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It looks like the `sessions` directory has disappeared and `grafana.db` has
    also been recreated by the Docker image again. If you access the web console,
    the Grafana `organization` will also disappear:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 `sessions` 目录消失了，`grafana.db` 也被 Docker 镜像重新创建了。如果你访问 Web 控制台，Grafana 的
    `organization` 也会消失：
- en: '![](img/b0906787-27fb-4d3b-b177-b42955fa24ef.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0906787-27fb-4d3b-b177-b42955fa24ef.png)'
- en: How about just attaching a persistent volume to Grafana? You'll soon find that
    mounting a persistent volume on a pod controlled by a `Deployment` doesn't scale
    properly as every new pod attempts to mount the same persistent volume. In most
    cases, only the first pod can mount the persistent volume. Other pods will try
    to mount the volume, and they will give up and freeze if they can't. This happens
    if the persistent volume is capable of only RWO (read write once; only one pod
    can write).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，直接将一个持久化卷挂载到 Grafana 上怎么样？你很快会发现，将持久化卷挂载到一个由 `Deployment` 控制的 Pod 上并不能很好地扩展，因为每个新创建的
    Pod 都会尝试挂载相同的持久化卷。在大多数情况下，只有第一个 Pod 能挂载持久化卷。其他 Pod 会尝试挂载该卷，如果无法挂载，它们将放弃并冻结。这种情况发生在持久化卷只支持
    RWO（只读写一次；仅一个 Pod 可以写入）时。
- en: 'In the following example, Grafana uses a persistent volume to mount `/var/lib/grafana`;
    however, it can''t scale because the Google persistent disk is RWO:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，Grafana 使用持久化卷挂载 `/var/lib/grafana`；然而，由于 Google 持久化磁盘是 RWO，导致无法扩展：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Even if the persistent volume has the RWX capability (read write many; many
    pods can mount to read and write simultaneously), such as NFS, it won''t complain
    if multiple pods try to bind the same volume. However, we still need to consider
    whether multiple application instances can use the same folder/file or not. For
    example, if it replicates Grafana to two or more pods, it will be conflicted,
    with multiple Grafana instances that try to write to the same `/var/lib/grafana/grafana.db`,
    and then the data could be corrupted, as shown in the following diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 即使持久化卷具有 RWX 能力（读写多次；多个 Pod 可以同时挂载进行读写），比如 NFS，当多个 Pod 尝试绑定相同的卷时，它也不会报错。然而，我们仍然需要考虑是否可以让多个应用实例使用相同的文件夹/文件。例如，如果将
    Grafana 复制到两个或更多 Pod，它们将发生冲突，因为多个 Grafana 实例尝试写入相同的 `/var/lib/grafana/grafana.db`，从而可能导致数据损坏，如下图所示：
- en: '![](img/eeb24510-e346-4bd1-941b-f4f4b1fe91e1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeb24510-e346-4bd1-941b-f4f4b1fe91e1.png)'
- en: 'In this scenario, Grafana must use backend databases such as MySQL or PostgreSQL,
    instead of SQLite3, as follows. It allows multiple Grafana instances to read/write
    Grafana metadata properly:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Grafana 必须使用像 MySQL 或 PostgreSQL 这样的后端数据库，而不是 SQLite3，如下所示。它允许多个 Grafana
    实例正确地读写 Grafana 元数据：
- en: '![](img/f7cb8468-f326-4dde-8750-2f76fad6baa3.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7cb8468-f326-4dde-8750-2f76fad6baa3.png)'
- en: Because RDBMS basically supports connecting with multiple application instances
    via a network, this scenario is perfectly suited to being used by multiple pods.
    Note that Grafana supports using RDBMS as a backend metadata store; however, not
    all applications support RDBMS.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RDBMS 基本上支持通过网络连接多个应用实例，这种场景非常适合多个 Pod 使用。请注意，Grafana 支持使用 RDBMS 作为后端元数据存储；然而，并不是所有应用程序都支持
    RDBMS。
- en: For the Grafana configuration that uses MySQL/PostgreSQL, please see the online
    documentation at
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用 MySQL/PostgreSQL 的 Grafana 配置，请参阅在线文档：
- en: '[http://docs.grafana.org/installation/configuration/#database](http://docs.grafana.org/installation/configuration/#database).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.grafana.org/installation/configuration/#database](http://docs.grafana.org/installation/configuration/#database)。'
- en: Therefore, the Kubernetes administrator needs to carefully monitor how an application
    behaves with volumes, and understand that in some use cases, just using a persistent
    volume may not help because of issues that might arise when scaling pods.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Kubernetes 管理员需要仔细监控应用程序与卷的交互行为，并理解在某些使用场景下，仅使用持久化卷可能无法解决问题，因为在扩展 Pod 时可能会出现问题。
- en: If multiple pods need to access the centralized volume, then consider using
    the database as previously shown, if applicable. On the other hand, if multiple
    pods need an individual volume, consider using `StatefulSet`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多个 Pod 需要访问集中存储卷，那么可以考虑使用前面所示的数据库（如果适用）。另一方面，如果多个 Pod 需要各自的存储卷，可以考虑使用 `StatefulSet`。
- en: Replicating pods with a persistent volume using StatefulSet
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 StatefulSet 复制带有持久化存储的 Pod
- en: '`StatefulSet` was introduced in Kubernetes 1.5; this consists of a bond between
    the pod and the persistent volume. When scaling a pod that increases or decreases,
    pod and persistent volume are created or deleted together.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`StatefulSet` 在 Kubernetes 1.5 中引入；它将 Pod 与持久化存储卷绑定。当扩展一个 Pod 时，无论是增加还是减少，Pod
    和持久化存储卷会一起创建或删除。'
- en: 'In addition, the pod-creation process is serial. For example, when requesting
    Kubernetes to scale two additional `StatefulSet`s, Kubernetes creates **Persistent
    Volume Claim 1** and **Pod 1** first, and then creates **Persistent Volume Claim
    2** and **Pod 2**, but not simultaneously. This helps the administrator if an
    application registers to a registry during the application bootstrap:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Pod 创建过程是串行的。例如，当请求 Kubernetes 扩展两个额外的 `StatefulSet` 时，Kubernetes 会首先创建 **持久化存储声明
    1** 和 **Pod 1**，然后再创建 **持久化存储声明 2** 和 **Pod 2**，但不会同时进行。这有助于管理员在应用程序启动时，如果应用程序需要注册到某个注册表：
- en: '![](img/871b9722-059b-4ee2-8a53-00e066f19cd2.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/871b9722-059b-4ee2-8a53-00e066f19cd2.png)'
- en: Even if one pod is dead, `StatefulSet` preserves the position of the pod (Kubernetes
    metadata, such as the pod name) and the persistent volume. Then it attempts to
    recreate a container that it reassigns to the same pod and mounts the same persistent
    volume.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 即使某个 Pod 宕机，`StatefulSet` 也会保留该 Pod 的位置（Kubernetes 元数据，如 Pod 名称）和持久化存储卷。然后，它会尝试重新创建一个容器，将其重新分配给相同的
    Pod，并挂载相同的持久化存储卷。
- en: If you run the headless service with `StatefulSet`, Kubernetes also assigns
    and preserves the FQDN for the pod as well.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 `StatefulSet` 运行无头服务，Kubernetes 还会为 Pod 分配并保留完全限定的域名（FQDN）。
- en: Headless services will be described in detail in [Chapter 6](fc67e008-b601-45a6-8297-d2fa28360b1f.xhtml)*,
    Kubernetes Network*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务将在 [第六章](fc67e008-b601-45a6-8297-d2fa28360b1f.xhtml)*Kubernetes 网络* 中详细描述。
- en: 'This helps to keep track of and maintain the number of pods/persistent volumes
    and the application remains online using the Kubernetes scheduler:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于通过 Kubernetes 调度器跟踪和维护 Pod/持久化存储卷的数量，确保应用程序保持在线：
- en: '![](img/ddea9f11-3327-4aee-abc7-42a522b8a9db.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ddea9f11-3327-4aee-abc7-42a522b8a9db.png)'
- en: '`StatefulSet` with a persistent volume requires dynamic provisioning and `StorageClass`
    because `StatefulSet` can be scalable. Kubernetes needs to know how to provision
    the persistent volume when adding more pods.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 带有持久化存储的 `StatefulSet` 需要动态提供和 `StorageClass`，因为 `StatefulSet` 可以进行扩展。Kubernetes
    需要知道如何在添加更多 Pod 时提供持久化存储。
- en: Submitting Jobs to Kubernetes
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向 Kubernetes 提交任务
- en: In general, the application is designed to be long-lived in the same way as
    the daemon process. A typical long-lived application opens the network port and
    keeps it running. It is required to keep running the application. If it fails,
    you will need to restart to recover the state. Therefore, using Kubernetes deployment
    is the best option for the long-lived application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，应用程序设计为长期运行，就像守护进程一样。典型的长期应用程序会打开网络端口并保持运行。它要求应用程序保持运行。如果失败，你需要重启以恢复状态。因此，使用
    Kubernetes 部署是长期应用程序的最佳选择。
- en: On the other hand, some applications are designed to be short-lived, such as
    a command script. It is expected to exit the application even if it is successful
    in order to finish the tasks. Therefore, a Kubernetes deployment is not the right
    choice, because a deployment tries to keep the process running.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一些应用程序设计为短生命周期的，例如命令脚本。即使成功执行，也预期应用程序会退出以完成任务。因此，Kubernetes 部署并不是合适的选择，因为部署会尝试保持进程运行。
- en: No worries; Kubernetes also supports short-lived applications. You can submit
    a container as a **Job** or **Scheduled Job**, and Kubernetes will dispatch it
    to an appropriate node and execute your container.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心，Kubernetes 也支持短生命周期的应用程序。你可以将容器提交为 **任务** 或 **定时任务**，Kubernetes 会将其调度到适当的节点并执行你的容器。
- en: 'Kubernetes supports several types of Jobs:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 支持几种类型的任务：
- en: Single Job
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一任务
- en: Repeatable Job
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复任务
- en: Parallel Job
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行任务
- en: Scheduled Job
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定时任务
- en: The last one is also called a **CronJob**. Kubernetes supports these different
    types of Jobs that are used differently to pods to utilize your resources.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个也被称为**CronJob**。Kubernetes 支持这些不同类型的 Jobs，它们与 pods 的使用方式不同，以便更好地利用你的资源。
- en: Submitting a single Job to Kubernetes
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向 Kubernetes 提交单个 Job
- en: A Job-like pod is suitable to run for batch programs such as collecting data,
    querying the database, generating a report, and so on. Although this is referred
    to as short-lived, it doesn't matter how long is spent on it. This may need to
    run for a few seconds, or perhaps a few days, in order to complete. It will eventually
    exit an application, which means it has an end state.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 Job 的 pod 适用于运行批处理程序，如收集数据、查询数据库、生成报告等。尽管这被称为短生命周期，但花费多少时间并不重要。这可能需要几秒钟，甚至几天才能完成。它最终会退出应用程序，这意味着它有一个结束状态。
- en: Kubernetes is capable of monitoring a short-lived application as a Job, and
    in the case of failure, Kubernetes will create a new pod for the Job that tries
    to accomplish your application to complete.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 能够将短生命周期的应用程序作为 Job 进行监控，在失败的情况下，Kubernetes 会为该 Job 创建一个新的 pod，尝试完成你的应用程序。
- en: 'In order to submit a Job to Kubernetes, you need to write a Job template that
    specifies the pod configuration. The following example demonstrates how to check
    the `dpkg` installed in Ubuntu Linux:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向 Kubernetes 提交 Job，你需要编写一个 Job 模板，指定 pod 配置。以下示例演示了如何检查 Ubuntu Linux 中安装的
    `dpkg`：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Job definition is similar to pod definition, but the important settings are
    `activeDeadlineSeconds` and `restartPolicy`. The `activeDeadlineSeconds `parameter sets
    the maximum timescale for the pod to run. If exceeded, the pod will be terminated.
    The `restartPolicy` parameter defines how Kubernetes behaves in the case of failure.
    For example, when the pod is crashed if you specify `Never`, Kubernetes doesn't
    restart; if you specify `OnFailure`, Kubernetes attempts to resubmit the Job until
    successfully completed.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Job 的定义与 pod 定义类似，但重要的设置是 `activeDeadlineSeconds` 和 `restartPolicy`。`activeDeadlineSeconds`
    参数设置 pod 运行的最大时间。如果超时，pod 将被终止。`restartPolicy` 参数定义了在失败情况下 Kubernetes 的行为。例如，当
    pod 崩溃时，如果你指定 `Never`，Kubernetes 不会重启；如果你指定 `OnFailure`，Kubernetes 会尝试重新提交 Job，直到成功完成。
- en: 'Use the `kubectl` command to submit a Job to see how Kubernetes manages the
    pod:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl`命令提交一个 Job，查看 Kubernetes 如何管理 pod：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Because this Job (the `dpkg-query -l` command) is short-lived, it will `exit()`
    eventually. Therefore, if the `dpkg-query` command completes gracefully, Kubernetes
    doesn''t attempt to restart, even if no active pod exists. So, when you type `kubectl
    get pods`, the pod status will be `completed` after finishing:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个 Job（`dpkg-query -l` 命令）是短生命周期的，它最终会`exit()`。因此，如果`dpkg-query`命令顺利完成，即使没有活动的
    pod，Kubernetes 也不会尝试重启。因此，当你输入`kubectl get pods`时，pod 状态将在完成后显示为 `completed`：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Although no active pod exists, you still have an opportunity to check the application
    log by typing the `kubectl logs` command as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有活动的 pod 存在，但你仍然可以通过输入 `kubectl logs` 命令来查看应用程序日志，如下所示：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Submitting a repeatable Job
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交一个可重复的 Job
- en: 'Users can also decide the number of tasks that should be finished in a single
    Job. This is helpful for solving some random sampling problems. Let''s reuse the
    previous template and add `spec.completions` to see the differences:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 用户还可以决定单个 Job 中应完成的任务数量。这对解决一些随机抽样问题非常有帮助。让我们重用之前的模板，并添加`spec.completions`来查看差异：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you can see, three pods are created to complete this Job. This is useful
    if you need to run your program repeatedly at particular times. However, as you
    may have noticed from the Age column in the preceding result, these pods ran sequentially
    one by one. In the preceding result, the ages are 7 seconds, 4 seconds, then 2
    seconds. This means that the second Job was started after the first Job was completed,
    and the third Job was started after the second Job was completed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，三个 pod 被创建来完成这个 Job。如果你需要在特定时间反复运行程序，这非常有用。然而，正如你从前面的结果中的 Age 列看到的，这些 pods
    是依次运行的。前面的结果显示年龄分别为 7 秒、4 秒和 2 秒。这意味着第二个 Job 是在第一个 Job 完成后启动的，第三个 Job 是在第二个 Job
    完成后启动的。
- en: If it is the case that a Job runs for a longer period (such as a few days),
    but if there is no correlation needs between the 1^(st), 2^(nd), and 3^(rd) Job,
    then it does not make sense to run them sequentially. In this case, use a parallel
    Job.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个 Job 运行时间较长（例如几天），但 1^(st)、2^(nd) 和 3^(rd) 个 Job 之间没有相关性，那么依次运行它们就没有意义了。在这种情况下，可以使用并行
    Job。
- en: Submitting a parallel Job
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交并行作业
- en: 'If your batch Job doesn''t have a state or dependency between Jobs, you may
    consider submitting Jobs in parallel. To do so, similar to the `spec.completions`
    parameter, the Job template has a `spec.parallelism` parameter to specify how
    many Jobs you want to run in parallel:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的批处理作业之间没有状态或依赖关系，你可以考虑并行提交作业。为此，类似于 `spec.completions` 参数，作业模板有一个 `spec.parallelism`
    参数，用于指定要并行运行的作业数量：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you see from the `AGE` column through the `kubectl get pods` command, it
    indicates that the three pods ran at the same time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `kubectl get pods` 命令的 `AGE` 列可以看出，表示这三个 Pod 是同时运行的。
- en: In this setting, Kubernetes can dispatch to an available node to run your application
    and that easily scales your Jobs. This is useful if you want to run something
    like a worker application to distribute a bunch of pods to different nodes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，Kubernetes 可以将任务调度到可用节点上运行应用程序，从而轻松扩展你的作业。如果你想运行像工人应用程序这样的东西，将多个 Pod
    分配到不同的节点，这会非常有用。
- en: 'Lastly, if you no longer need to check the Job''s results anymore, delete the
    resource by using the `kubectl delete` command as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你不再需要检查作业结果，可以使用 `kubectl delete` 命令删除资源，示例如下：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Scheduling running a Job using CronJob
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CronJob 调度运行作业
- en: If you are familiar with **UNIX CronJob** or **Java Quartz** ([http://www.quartz-scheduler.org](http://www.quartz-scheduler.org)),
    Kubernetes CronJob is a very straightforward tool that you can use to define a
    particular timing to run your Kubernetes Job repeatedly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 **UNIX CronJob** 或 **Java Quartz** ([http://www.quartz-scheduler.org](http://www.quartz-scheduler.org))，那么
    Kubernetes CronJob 是一个非常直观的工具，你可以用它定义一个特定的时间来重复运行 Kubernetes 作业。
- en: 'The scheduling format is very simple; it specifies the following five items:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 调度格式非常简单；它指定了以下五个项目：
- en: Minutes (0 to 59)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分钟（0 到 59）
- en: Hours (0 to 23)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小时（0 到 23）
- en: Days of the month (1 to 31)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月的日期（1 到 31）
- en: Months (1 to 12)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份（1 到 12）
- en: 'Days of the week (0: Sunday to 6: Saturday)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 星期几（0：星期日到 6：星期六）
- en: For example, if you only want to run your Job at 9:00 am on November 12th every
    year to send a birthday greeting to me, the schedule format would be `0 9 12 11
    *`
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你只想每年 11 月 12 日早上 9:00 运行你的作业，给我发送生日祝福，那么调度格式将是 `0 9 12 11 *`
- en: 'You may also use a slash (`/`) to specify a step value. Running the previous
    Job example at five-minute intervals would have the following schedule format:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用斜杠（`/`）来指定步长值。以五分钟为间隔运行前述作业的调度格式将是：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following template is using a `CronJob` to run the `package-check` command
    every five minutes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模板使用 `CronJob` 每五分钟运行 `package-check` 命令：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You may notice that the template format is slightly different from the Job
    template here. However, there is one parameter we need to pay attention to: `spec.concurrencyPolicy`.
    With this, you can specify a behavior if the previous Job is not finished but
    the next Job''s schedule is approaching. This determines how the next Job runs.
    You can set the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，模板格式与这里的作业模板略有不同。不过，我们需要关注一个参数：`spec.concurrencyPolicy`。通过这个参数，你可以指定如果上一个作业没有完成，而下一个作业的调度时间临近时的行为。这将决定下一个作业如何运行。你可以设置以下值：
- en: '**Allow**: Allow execution of the next Job'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许**：允许执行下一个作业'
- en: '**Forbid**: Skip execution of the next Job'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁止**：跳过下一个作业的执行'
- en: '**Replace**: Delete the current Job, then execute the next Job'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**替换**：删除当前作业，然后执行下一个作业'
- en: If you set `Allow`, there might be the potential risk of accumulating some unfinished
    Jobs in the Kubernetes cluster. Therefore, during the testing phase, you should
    set either `Forbid` or `Replace` to monitor Job execution and completion.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你设置了 `允许`，则可能会有积累一些未完成作业的风险。因此，在测试阶段，你应该设置 `禁止` 或 `替换` 来监控作业的执行和完成情况。
- en: 'After a few moments, the Job will be triggered by your desired timing—in this
    case, every five minutes. You may then see the Job entry with the `kubectl get
    jobs` and `kubectl get pods` commands, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，作业会在你期望的时间触发——在这种情况下是每五分钟一次。然后你可以使用 `kubectl get jobs` 和 `kubectl get pods`
    命令查看作业条目，如下所示：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`CronJob` will remain until you delete it. This means that, every five minutes,
    `CronJob` will create a new Job entry and related pods will also keep getting
    created. This will impact the consumption of Kubernetes resources. Therefore,
    by default, `CronJob` will keep up to three successful Jobs (with `spec.successfulJobsHistoryLimit`)
    and one failed Job (with `spec.failedJobsHistoryLimit`). You can change these
    parameters based on your requirements.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`CronJob` 将一直存在，直到你删除它。这意味着每五分钟，`CronJob` 将创建一个新的作业条目，相关的 pods 也会持续创建。这将影响
    Kubernetes 资源的消耗。因此，默认情况下，`CronJob` 会保留最多三个成功的作业（通过 `spec.successfulJobsHistoryLimit`）和一个失败的作业（通过
    `spec.failedJobsHistoryLimit`）。你可以根据需求更改这些参数。'
- en: Overall, `CronJob` allows Jobs to automatically run in your application with
    the desired timing. You can utilize `CronJob` to run report-generation Jobs, daily
    or weekly batch Jobs, and so on.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，`CronJob` 允许作业根据预定时间自动在你的应用程序中运行。你可以利用 `CronJob` 来运行报告生成作业、每日或每周批处理作业等。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered stateful applications that use persistent volumes.
    Compared to ephemeral volumes, they have some pitfalls when an application restarts
    or a pod scales. In addition, persistent volume management on Kubernetes has been
    enhanced to make it easier, as you can see from tools such as `StatefulSet` and
    dynamic provisioning.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了使用持久化卷的有状态应用程序。与短暂卷（ephemeral volumes）相比，在应用程序重启或 pod 扩展时，它们可能会遇到一些问题。此外，Kubernetes
    上的持久化卷管理也得到了改进，使其更加便捷，正如你可以从 `StatefulSet` 和动态供给等工具中看到的那样。
- en: Furthermore, Jobs and CronJobs are special utilities for pods. Compared to deployment/ReplicaSets,
    this has a desired number of pods running, which is against Job's ideal situation
    in which the pods should be deleted once they finish their tasks. This kind of
    short-lived application can be managed by Kubernetes as well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Jobs 和 CronJobs 是用于 pods 的特殊工具。与部署/副本集（deployment/ReplicaSets）相比，它们有一个期望的
    pod 数量在运行，而这与作业的理想情况相反，作业的理想状态是当 pods 完成任务后应该被删除。这种短生命周期的应用程序也可以由 Kubernetes 管理。
- en: In [Chapter 5](17a735e1-3810-4b77-a9ac-fac0120b0b90.xhtml), *Cluster Administration
    and Extension*, we will discuss the cluster administration such as authentication,
    authorization, and admission control. We will also introduce the **Custom Resource
    Definition** (**CRD**) that how to control Kubernetes object by your own code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](17a735e1-3810-4b77-a9ac-fac0120b0b90.xhtml)，*集群管理与扩展*，我们将讨论集群管理，例如身份验证、授权和准入控制。我们还将介绍**自定义资源定义**（**CRD**），以及如何通过你自己的代码控制
    Kubernetes 对象。
