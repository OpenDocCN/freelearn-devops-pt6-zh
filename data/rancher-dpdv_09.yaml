- en: '*Chapter 6*: Creating an RKE Cluster Using Rancher'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：使用 Rancher 创建 RKE 集群'
- en: 'One of the first things you''ll do after installing Rancher is to start building
    downstream clusters. There are three main types of clusters in Rancher: Rancher-managed
    **Rancher Kubernetes Engine** (**RKE**) clusters, Rancher-managed hosted clusters,
    and imported clusters.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Rancher 后，你首先要做的事情之一就是开始构建下游集群。Rancher 中有三种主要类型的集群：Rancher 管理的 **Rancher
    Kubernetes Engine** (**RKE**) 集群、Rancher 管理的托管集群和导入的集群。
- en: In this chapter, we'll be covering how to deploy a downstream cluster to use
    existing servers running Docker. We'll see how Rancher uses a set of agents to
    provide access to these servers for Rancher to create an RKE cluster. Then, we'll
    cover the requirements and limitations of this type of cluster. We will then cover
    the rules for designing a Rancher-managed RKE cluster, at which point, we'll go
    through the process of registering nodes in Rancher. Finally, we'll cover the
    maintenance tasks needed for the ongoing cluster management.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何部署一个下游集群，使用现有运行 Docker 的服务器。我们将看到 Rancher 如何使用一组代理为 Rancher 提供对这些服务器的访问，以便创建一个
    RKE 集群。接下来，我们将讨论这种类型集群的要求和限制。然后，我们将讨论设计 Rancher 管理的 RKE 集群的规则，届时我们将通过在 Rancher
    中注册节点的过程。最后，我们将讨论持续的集群管理所需的维护任务。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: What is a Rancher-managed cluster?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Rancher 管理的集群？
- en: Requirements and limitations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求和限制
- en: Rules for architecting a solution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决方案架构设计规则
- en: Preparing for nodes to join Rancher
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备节点加入 Rancher
- en: Prepping the infrastructure provider
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备基础设施提供商
- en: Steps for creating an RKE cluster using Rancher
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Rancher 创建 RKE 集群的步骤
- en: Deploying a cluster using node pools
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用节点池部署集群
- en: Ongoing maintenance tasks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续的维护任务
- en: What is a Rancher-managed cluster?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Rancher 管理的集群？
- en: Rancher can manage a cluster on behalf of end users. This can be done by using
    existing nodes or Rancher-created nodes. It is important to note that, at the
    time of writing, Rancher v2.6 has RKE2 support as a technical preview feature.
    But we will be talking about Rancher-managed clusters using RKE in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 可以代表最终用户管理集群。这可以通过使用现有节点或 Rancher 创建的节点来完成。需要注意的是，在写作时，Rancher v2.6
    将 RKE2 支持作为技术预览功能。但在本章中，我们将讨论使用 RKE 的 Rancher 管理集群。
- en: Where do Rancher-managed clusters come from?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rancher 管理的集群来自哪里？
- en: Since the beginning, Rancher has always used the technique of defining a cluster
    inside Rancher and then using the Rancher agents to provide access to the downstream
    nodes for cluster creation. In Rancher v1.6, this was used to deploy the `Cattle`
    clusters, and with Rancher v2.x, this same idea was advanced to deploy RKE clusters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Rancher 起步以来，它始终采用在 Rancher 内定义集群的技术，然后使用 Rancher 代理提供对下游节点的访问以创建集群。在 Rancher
    v1.6 中，这用于部署 `Cattle` 集群，而在 Rancher v2.x 中，采用相同的理念来部署 RKE 集群。
- en: How does Rancher manage nodes?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rancher 如何管理节点？
- en: In some environments, you don't want to manage the **Virtual Machines** (**VMs**).
    To solve this, Rancher has what are called **node drivers**. These drivers allow
    Rancher to launch and manage the VMs that Rancher will use to create the cluster.
    A node driver that Rancher uses is called the Rancher machine, which is based
    on Docker Machine. The main idea is that Docker Machine has several different
    **Software Development Kits** (**SDKs**) for most major infrastructure providers
    such as **Amazon Web Services** (**AWS**), Azure, DigitalOcean, and vSphere.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些环境中，你可能不想管理 **虚拟机** (**VMs**) 。为了解决这个问题，Rancher 提供了所谓的 **节点驱动程序**。这些驱动程序允许
    Rancher 启动和管理 Rancher 用来创建集群的虚拟机。Rancher 使用的一个节点驱动程序称为 Rancher 机器，它基于 Docker Machine。其主要思想是，Docker
    Machine 为大多数主要基础设施提供商（如 **Amazon Web Services** (**AWS**)、Azure、DigitalOcean 和
    vSphere）提供几个不同的 **软件开发工具包** (**SDKs**)。
- en: The basic process is that the cluster controller creates a machine object that
    defines the server being created. The machine controller takes over to handle
    calling the Rancher machine to start making the API calls to the infrastructure
    provider using the node templates defined in Rancher. As a part of the node creation
    process, Rancher creates an `cloud-init` to customize the base image and push
    the SSH keys to the server. It is important to note that the base image uses Ubuntu
    as default, but this image can be changed to any supported OS found at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
    The main requirement is that it supports `cloud-init` and Docker. Once `cloud-init`
    has been completed successfully, Rancher will SSH into the node to run the `docker
    run` command to handle pushing the Rancher agent to the node.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基本过程是集群控制器创建一个机器对象，用来定义正在创建的服务器。然后，机器控制器接管并调用Rancher机器，开始使用Rancher中定义的节点模板，通过基础设施提供者发起API调用。作为节点创建过程的一部分，Rancher会创建一个`cloud-init`来定制基础镜像，并将SSH密钥推送到服务器。需要注意的是，基础镜像默认使用Ubuntu，但可以更换为任何在[https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)上支持的操作系统。主要要求是支持`cloud-init`和Docker。一旦`cloud-init`成功完成，Rancher将通过SSH连接到节点，并运行`docker
    run`命令来处理将Rancher代理推送到节点的操作。
- en: How does Rancher manage a cluster?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rancher如何管理集群？
- en: 'In Rancher v2.x, once you have defined the cluster (which we''ll cover later
    in this chapter), Rancher will create a `docker run` command. Please see the example
    in the following figure. We''re now going to break this command down into its
    parts:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在Rancher v2.x中，一旦你定义了集群（我们将在本章后面详细讲解），Rancher将创建一个`docker run`命令。请参见下图中的示例。现在我们将逐部分解析这个命令：
- en: '![Figure 6.1 – The docker run command for joining a node to the cluster'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – 用于将节点加入集群的docker run命令'
- en: '](img/B18053_06_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_001.jpg)'
- en: Figure 6.1 – The docker run command for joining a node to the cluster
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 用于将节点加入集群的docker run命令
- en: First, the `docker run` command will create a new container. This is normally
    called the `-d` flag, which tells Docker to start this container in the background,
    with the next flag being `--privileged`. This flag is important because the Rancher
    agent will need access to the host and its resources to spin up the additional
    tools and containers needed by Rancher and RKE. The `--restart=unless-stopped`
    flag is to keep this container running even if it crashes. Then, the next flag,
    `--net=host`, tells Docker to use the host network for this container. This is
    needed to be able to get items such as the host's IP address and hostname.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`docker run`命令会创建一个新的容器。通常这被称为`-d`标志，它告诉Docker在后台启动该容器，下一个标志是`--privileged`。这个标志非常重要，因为Rancher代理需要访问主机及其资源，以便启动Rancher和RKE所需的附加工具和容器。`--restart=unless-stopped`标志用来确保容器即使崩溃也能继续运行。接下来，`--net=host`标志告诉Docker使用主机网络。这样做是为了能够获取主机的IP地址和主机名等信息。
- en: We then come to the `-v /etc/kubernetes:/etc/kubernetes` and `-v /var/run:/var/run`
    flags. These two flags will create a bind mount for the host filesystem in the
    bootstrap container. The first directory is used to store SSL certificates used
    by the RKE components and some `config` files. The second directory is used to
    provide access to several host-level commands. This includes the Docker **Command-Line
    Interface (CLI)** access, which Rancher uses for creating additional containers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着来看`-v /etc/kubernetes:/etc/kubernetes`和`-v /var/run:/var/run`这两个标志。这两个标志会在引导容器中为主机文件系统创建一个绑定挂载。第一个目录用于存储RKE组件使用的SSL证书和一些`config`文件。第二个目录则用于提供对多个主机级命令的访问，包括Docker
    **命令行界面（CLI）**访问，Rancher用来创建额外容器。
- en: The next section is the `image` tag. This will, of course, match the version
    of Rancher. The next section is the command-line options that are passed to the
    Rancher agent binary. The first option is `-server`, which is Rancher's API endpoint
    and should be used when it connects back to Rancher. It is important to note that
    this must be an `HTTPS` URL. The next option is `--token`, a special token used
    by Rancher to authenticate an agent and tie it to a cluster. It is important to
    note that this token will be the same for all nodes in the cluster. Also, this
    token should be treated like a password.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分是`image`标签。这个标签当然会与Rancher的版本匹配。接下来是传递给Rancher代理二进制文件的命令行选项。第一个选项是`-server`，它是Rancher的API端点，应该在连接回Rancher时使用。需要注意的是，这必须是一个`HTTPS`
    URL。下一个选项是`--token`，它是Rancher用来认证代理并将其与集群绑定的特殊令牌。需要注意的是，这个令牌在集群中的所有节点之间是相同的。同时，应该像对待密码一样对待这个令牌。
- en: The next option is `--ca-checksum`, which is a SHA256 checksum of the root certificate
    of Rancher's API endpoint. This is used because it is common for users to use
    self-signed or privately signed certificates for their Rancher servers, and because
    the root certificates that are inside the container might not be up to date. The
    Rancher agent will request the root certificate from the Rancher URL and compare
    that certificate's checksum to the `--ca-checksum` and assume they match. The
    agent will assume that the root certificate can be trusted. It is important to
    note that these only handle trusting the root certificate. The rest of the certificate
    must still be valid – that is, the certificate has not expired with the correct
    hostname. This is why it's important not to change the root CAs of your Rancher
    API endpoint. Officially, there is no support to change the Rancher API endpoint
    or the root CA, but Rancher support does have tools such as the **cluster agent**
    tool that can take care of this for you. The tool is located at [https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个选项是`--ca-checksum`，它是Rancher API端点根证书的SHA256校验和。之所以使用这个选项，是因为用户通常会为他们的Rancher服务器使用自签名或私签名证书，并且容器内的根证书可能不是最新的。Rancher代理会从Rancher
    URL请求根证书，并将该证书的校验和与`--ca-checksum`进行比较，假设它们匹配。代理将假设根证书可以被信任。需要注意的是，这些只处理信任根证书的问题。证书的其余部分仍然必须有效——即证书没有过期且具有正确的主机名。这也是为什么不应更改Rancher
    API端点的根CA的原因。官方上并不支持更改Rancher API端点或根CA，但Rancher支持提供了一些工具，例如**集群代理**工具，可以帮助你处理这些问题。该工具位于[https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool)。
- en: Finally, at the end of the command, we get to the section that will need to
    be customized based on the role and settings of the node. In the example shown
    in *Figure 6.2*, we have some of the standard agent options that users use, with
    the first being `--node-name`, which is an option that lets you override the hostname
    of the node. This is because, by default, the Rancher agent will use the short
    hostname of the server as the node name in both Rancher and Kubernetes. For some
    environments, this is fine, and the option can be skipped, but in cloud environments
    such as AWS, where a server hostname such as `ip-10-23-24-15` can be hard to read
    and doesn't match what the server is named in the console, it can be helpful to
    set the node name to something more user-friendly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在命令的末尾，我们到达了需要根据节点的角色和设置进行自定义的部分。在*图6.2*中所示的示例中，我们有一些用户常用的标准代理选项，第一个是`--node-name`，这是一个允许你覆盖节点主机名的选项。之所以使用这个选项，是因为默认情况下，Rancher代理会将服务器的短主机名作为节点名，在Rancher和Kubernetes中都使用它。对于某些环境来说，这样是可以的，且该选项可以跳过，但在像AWS这样的云环境中，服务器主机名如`ip-10-23-24-15`可能很难阅读，并且与控制台中的服务器名称不匹配，这时候设置一个更易读的节点名就变得很有帮助。
- en: It is important to note that Rancher and RKE do not use this hostname for networking
    communications, so the node name does not need to be a valid DNS record, but it
    is recommended that it be valid to help with future troubleshooting. Also, it
    is essential to remember that a hostname shouldn't be changed after a node is
    registered into Rancher, as the hostname is used as a key, and changing the hostname
    will cause Rancher to try registering it as a new node. This can break the cluster,
    as it is in an unknown state, so it is recommended that if you want to change
    the name of a node, remove it from the cluster, clean it using the cleanup script
    located at [https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh](https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh),
    and then rejoin the node as a new node to the cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Rancher 和 RKE 不使用此主机名进行网络通信，因此节点名称不需要是有效的 DNS 记录，但建议它是有效的，以便帮助未来的故障排除。此外，必须记住，节点在注册到
    Rancher 后不应更改主机名，因为主机名作为一个关键字，如果更改主机名，Rancher 会将其视为新节点进行注册。这可能会破坏集群，因为它处于未知状态，因此如果你想更改节点名称，建议先将其从集群中移除，使用
    [https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh](https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh)
    清理节点，然后将其作为新节点重新加入集群。
- en: The next option is `--address`, which sets the external IP address of the node.
    Usually, this is only needed when a node is behind a `--internal-address`, with
    this setting being used to set the IP address used by Kubernetes for inter-host
    communication. If a node has more than one **Network Interface Controller (NIC**),
    it is imperative that this setting is used to avoid the network being misrouted.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个选项是 `--address`，用于设置节点的外部 IP 地址。通常，只有当节点位于 `--internal-address` 后面时才需要此选项，该选项用于设置
    Kubernetes 用于主机间通信的 IP 地址。如果一个节点有多个 **网络接口卡（NIC）**，则必须使用此设置，以避免网络被错误路由。
- en: An example is you have 1 GB NIC for management and 10 GB NIC for data. We would
    want RKE/Kubernetes to use the 10 GB NIC IP address to improve speed. If this
    option is not set, the kubelet will try to auto-detect the correct IP for the
    node by using the default gateway and the DNS record for the node's hostname.
    It is recommended to set these manually if a node has more than one IP.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个用于管理的 1 GB 网络接口卡（NIC）和一个用于数据的 10 GB 网络接口卡。我们希望 RKE/Kubernetes 使用 10
    GB 网络接口卡的 IP 地址来提高速度。如果未设置此选项，kubelet 将尝试通过使用默认网关和节点主机名的 DNS 记录来自动检测节点的正确 IP。建议如果一个节点有多个
    IP 地址时，手动设置这些选项。
- en: There are additional flags that can be set at the agent level. For example,
    `--labels` will set the node's labels and `–taints` will set the node's taints
    at node creation, but it is important to note that these options are locked in
    at this point and can cause problems if they need to be changed at a later date.
    The rest of the agent options can be found at [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些可以在代理级别设置的额外标志。例如，`--labels` 将设置节点的标签，`–taints` 将在节点创建时设置节点的污点，但需要注意的是，这些选项在此时已被锁定，如果需要在稍后更改，可能会引发问题。其他代理选项可以在
    [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/)
    找到。
- en: 'At the very end of the command, we have the `role` options. These flags tell
    Rancher/RKE what role is assigned to this node, such as `--etcd`, `--controlplane`,
    and `--worker`. When the node is registering with Rancher for the first time,
    the `role` options are sent to Rancher and are used by it when generating the
    cluster configuration. It is important to note that these roles should not be
    changed after registering a node in Rancher. If you need to change a node''s role,
    it is recommended to remove the node, clean it, and rejoin it:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令的最后，我们有 `role` 选项。这些标志告诉 Rancher/RKE 此节点被分配了什么角色，例如 `--etcd`、`--controlplane`
    和 `--worker`。当节点第一次向 Rancher 注册时，`role` 选项会被发送给 Rancher，并在生成集群配置时被使用。需要注意的是，这些角色在节点注册到
    Rancher 后不应更改。如果需要更改节点的角色，建议将节点移除，进行清理，并重新加入：
- en: '![Figure 6.2 – A docker run command with node customizations'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – 带有节点自定义的 Docker 运行命令'
- en: '](img/B18053_06_002.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_002.jpg)'
- en: Figure 6.2 – A docker run command with node customizations
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 带有节点自定义的 Docker 运行命令
- en: What happens after a node has been registered? Once the Rancher agent has been
    successfully started, it will register the node in Rancher, and the node will
    go into the `Waiting to register with Kubernetes` state. At this point, the agent
    will create a WebSocket connection and wait. This triggers the cluster controller
    inside Rancher to update the cluster configuration. The object is equivalent to
    the `cluster.yaml` and `cluster.rkestate` files used by RKE but inside the Rancher
    container instead. This is because the cluster controller uses the same code as
    RKE. There are mostly minor differences, with the biggest one being the addition
    of a dialer to handle tunneling the Docker socket connection over WebSocket. The
    cluster controller will follow the same process as the RKE command.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 节点注册后会发生什么？一旦 Rancher 代理成功启动，它将注册该节点到 Rancher，节点将进入 `等待与 Kubernetes 注册` 状态。此时，代理将创建一个
    WebSocket 连接并等待。这会触发 Rancher 内部的集群控制器更新集群配置。该对象相当于 RKE 使用的 `cluster.yaml` 和 `cluster.rkestate`
    文件，但它是在 Rancher 容器内使用的。这是因为集群控制器使用与 RKE 相同的代码，差异主要较小，最大的不同是添加了一个拨号器来处理通过 WebSocket
    隧道传输 Docker 套接字连接。集群控制器将遵循与 RKE 命令相同的过程。
- en: Now that we understand what a Rancher-managed cluster is, let's look into the
    requirements and limitations of these types of clusters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了什么是 Rancher 托管的集群，让我们来看看这些类型集群的要求和限制。
- en: Requirements and limitations
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求和限制
- en: In this section, we'll be discussing the basic requirements of Rancher on various
    nodes along with its limitations and design considerations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 Rancher 在各种节点上的基本要求以及其限制和设计注意事项。
- en: Rancher-created managed nodes
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rancher 创建的托管节点
- en: 'These are the **basic requirements**:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是**基本要求**：
- en: A supported OS. The official supported OSes can be found at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的操作系统。官方支持的操作系统可以在[https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)找到。
- en: Rancher-created nodes have a special requirement that the Rancher servers must
    be able to SSH into the node.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 创建的节点有一个特殊要求，即 Rancher 服务器必须能够通过 SSH 连接到该节点。
- en: The required firewall rules and ports can be found at https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-node-pools.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的防火墙规则和端口可以在 https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-node-pools
    找到。
- en: Docker is not required to be already installed.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 不需要预先安装。
- en: 'These are the **design limitations and considerations**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是**设计限制和注意事项**：
- en: The base image used to create the nodes should be as small as possible and should
    be started in less than 10 minutes.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于创建节点的基础镜像应该尽可能小，并且启动时间应少于 10 分钟。
- en: Rancher does not have an IP address pool or integration with any **IP Address
    Management** (**IPAM**) solutions. Rancher relies on the infrastructure provider
    to handle assigning an IP address to nodes. If you are using **Dynamic Host Configuration
    Protocol** (**DHCP**), the IP addresses assigned to these nodes should have very
    long leases and be effectively static – that is, these IP addresses should not
    change.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 没有 IP 地址池，也没有与任何**IP 地址管理**（**IPAM**）解决方案的集成。Rancher 依赖基础设施提供商来分配 IP
    地址给节点。如果你使用的是**动态主机配置协议**（**DHCP**），则分配给这些节点的 IP 地址应具有非常长的租期，并且实际上是静态的——即这些 IP
    地址不应发生变化。
- en: The hostname of the nodes is defined at the node pool level, with the node names
    being sequential by adding a number to the end of the template name and incrementing
    by one each time a node is created.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的主机名在节点池级别定义，节点名称通过在模板名称末尾添加一个数字并在每次创建节点时递增来依次生成。
- en: Important Note
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Rancher will reuse old hostnames that have been successfully reclaimed.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Rancher 将重用已成功回收的旧主机名。
- en: If the nodes are being deployed in an air-gapped environment, Rancher will require
    a proxy server to be configured in `cloud-init`, or the package manager should
    be able to pull packages such as curl and Docker from its repository. Even if
    these packages are already installed, Rancher will still run either the `yum install
    curl` or `apt install curl` commands.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果节点部署在隔离网络环境中，Rancher 将要求在 `cloud-init` 中配置代理服务器，或者包管理器应该能够从其仓库中拉取如 curl 和
    Docker 等软件包。即使这些软件包已经安装，Rancher 仍会运行 `yum install curl` 或 `apt install curl` 命令。
- en: Auto should be set to `0` for node pools with etcd and controlplane nodes.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于包含etcd和controlplane节点的节点池，Auto应设置为`0`。
- en: Existing nodes
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现有节点
- en: 'These are the **basic requirements**:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是**基本要求**：
- en: A supported OS. The official supported OSes can be found at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个受支持的操作系统。官方支持的操作系统可以在[https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)找到。
- en: The required firewall rules and ports can be found at [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-custom-nodes](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-custom-nodes).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的防火墙规则和端口可以在[https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-custom-nodes](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-custom-nodes)找到。
- en: Docker should already be installed on the node(s), and we recommend using the
    installation script located at [https://github.com/rancher/install-Docker](https://github.com/rancher/install-Docker).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点上应已安装Docker，建议使用位于[https://github.com/rancher/install-Docker](https://github.com/rancher/install-Docker)的安装脚本。
- en: If you are using auto-scaling groups, it's essential to ensure that only one
    etcd node or controlplane node is taken offline at once. You want to ensure that
    you don't lose a quorum for etcd or get stuck in a cluster update because multiple
    controlplane nodes are down.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用自动扩展组，必须确保每次只有一个etcd节点或controlplane节点下线。您需要确保不会丢失etcd的法定人数，或者在多个controlplane节点宕机时，集群更新不会卡住。
- en: 'These are the **design limitations and considerations**:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是**设计限制和考虑事项**：
- en: When registering nodes in a new cluster, Rancher requires at least one node
    with each of the roles, such as etcd, controlplane, and worker. This can be a
    single node or separate nodes for each role or a mix of any roles.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注册新集群的节点时，Rancher要求每个角色（如etcd、controlplane和worker）至少有一个节点。可以是单个节点，也可以是为每个角色单独设置节点，或者混合使用不同角色的节点。
- en: When adding nodes to a cluster, it is crucial to ensure that new etcd and controlplane
    nodes are added one at a time. You can technically add them all at once, but you
    can run into stability issues with new clusters.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向集群中添加节点时，至关重要的是确保每次只添加一个新的etcd和controlplane节点。技术上可以一次性添加所有节点，但这样可能会导致新集群出现稳定性问题。
- en: If you are using a private registry for hosting the Docker images used by Rancher,
    you should configure the registry setting in the cluster using the steps listed
    at [https://rancher.com/docs/rke/latest/en/config-options/private-registries/](https://rancher.com/docs/rke/latest/en/config-options/private-registries/).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用私有注册表托管Rancher使用的Docker镜像，您应该按照[https://rancher.com/docs/rke/latest/en/config-options/private-registries/](https://rancher.com/docs/rke/latest/en/config-options/private-registries/)中的步骤配置集群中的注册表设置。
- en: We now understand the requirements and limitations. In the next section, we
    are going to use this knowledge, along with additional rules and example designs,
    to help us architect a solution that meets our needs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了要求和限制。在下一部分，我们将结合这些知识以及额外的规则和示例设计，帮助我们架构出满足需求的解决方案。
- en: Rules for architecting a solution
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构解决方案的规则
- en: In this section, we'll cover some standard designs and the pros and cons of
    each. It is important to note that each environment is unique and will require
    tuning for the best performance and experience. It's also important to note that
    all CPU, memory, and storage sizes are recommended starting points and may need
    to be increased or decreased by your workloads and deployment processes. Also,
    we'll be covering designs for the major infrastructure providers (AWS and **Google
    Cloud Platform** (**GCP**)), but you should be able to translate the core concepts
    for other infrastructure providers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些标准设计及其优缺点。需要注意的是，每个环境都是独特的，需要调整以获得最佳性能和体验。还需要注意的是，所有的CPU、内存和存储大小都是推荐的起始点，可能需要根据工作负载和部署过程进行调整。此外，我们将介绍适用于主要基础设施提供商（AWS和**谷歌云平台**（**GCP**））的设计，但您应该能够将核心概念转化为其他基础设施提供商的设计。
- en: 'Before designing a solution, you should be able to answer the following questions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计解决方案之前，您应该能够回答以下问题：
- en: Will multiple environments be sharing the same cluster?
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有多个环境共享同一个集群？
- en: Will production and non-production workloads be on the same cluster?
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产和非生产工作负载是否会在同一个集群中运行？
- en: What level of availability does this cluster require?
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个集群需要什么级别的可用性？
- en: Will this cluster be spanning multiple data centers in a metro cluster environment?
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个集群是否会跨多个数据中心，在一个城域集群环境中运行？
- en: How much latency will there be between nodes in the cluster?
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中节点之间的延迟是多少？
- en: How many pods will be hosted in the cluster?
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中将托管多少个 Pods？
- en: What are the average and maximum sizes of pods deployed in the cluster?
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中部署的 Pods 的平均大小和最大大小是多少？
- en: Will you need GPU support for some of your applications?
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否需要为某些应用程序提供 GPU 支持？
- en: Will you need to provide storage to your applications?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要为应用程序提供存储吗？
- en: If you need storage, do you need only **Read Write Once** (**RWO**), or will
    you need **Read Write Many** (**RWX**)?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要存储，你是只需要 **一次读写**（**RWO**），还是需要 **多次读写**（**RWX**）？
- en: Note
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Rancher's official server sizing guide can be found at [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes).
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Rancher 官方的服务器规格指南可以在 [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes)
    找到。
- en: AWS
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS
- en: In this design, we will be deploying a standard size cluster on AWS using the
    Rancher EC2 node driver, using a very similar design to the one that we created
    in [*Chapter 4*](B18053_04_Epub.xhtml#_idTextAnchor052), *Creating an RKE and
    RKE2 Cluster*, for the medium-size RKE cluster. The basic idea is to try and balance
    **High Availability** (**HA**) with cost and use the fact that AWS intro-zone
    network speed and latency are so good that we can treat it like a single data
    center.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们将使用 Rancher EC2 节点驱动程序在 AWS 上部署一个标准大小的集群，设计与我们在[*第4章*](B18053_04_Epub.xhtml#_idTextAnchor052)中为中型
    RKE 集群创建的设计非常相似，*创建 RKE 和 RKE2 集群*。基本思路是尝试在 **高可用性**（**HA**）和成本之间进行平衡，并利用 AWS
    的内网区域网络速度和延迟非常优秀这一事实，来将其视为单个数据中心。
- en: Note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This needs to be tested because some regions are slower than others, and some
    users have reported much higher latency.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要进行测试，因为某些区域的速度比其他区域慢，一些用户报告了更高的延迟。
- en: This design works for 2 to 50 worker nodes in the clusters. This is higher than
    a medium RKE cluster because the **Non-Volatile Memory** (**NVM**) storage in
    AWS can handle more throughput than most on-premises storage.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计适用于集群中的 2 到 50 个工作节点。这比中型 RKE 集群要大，因为 AWS 中的 **非易失性存储**（**NVM**）能够处理比大多数本地存储更多的吞吐量。
- en: Note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You might need to scale up the management nodes, depending on the environment.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据环境的不同，你可能需要扩展管理节点。
- en: '![Figure 6.3 – A Rancher-managed AWS cluster across zones'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 – 一个跨区域的 Rancher 管理的 AWS 集群](img/B18053_06_003.jpg)'
- en: '](img/B18053_06_003.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[图示](img/B18053_06_003.jpg)'
- en: Figure 6.3 – A Rancher-managed AWS cluster across zones
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 一个跨区域的 Rancher 管理的 AWS 集群
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/AWS/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/AWS/README.md)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图示： [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/AWS/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/AWS/README.md)
- en: 'The **pros** are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: Node-level redundancy – you can lose a worker without an application outage.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点级冗余 – 你可以失去一个工作节点而不发生应用程序宕机。
- en: Full HA – you can lose any one of the management nodes (etcd and controlplane)
    in the cluster and still have complete cluster management.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的 HA – 你可以失去集群中的任何一个管理节点（etcd 和控制平面），仍然能够进行完整的集群管理。
- en: User workloads and management services run on different nodes, stopping runaway
    applications from taking down the cluster.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户工作负载和管理服务运行在不同的节点上，从而防止失控的应用程序导致集群宕机。
- en: Availability zone redundancy – you can lose a whole Availability Zone without
    an outage.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用区冗余 – 你可以失去整个可用区而不发生宕机。
- en: Safer patching and upgrades for the master nodes because the node pools are
    across zones. So, we can simply scale up all three node pools from one node to
    two in parallel, and then scale down each pool one at a time.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更安全的主节点修补和升级，因为节点池跨区域部署。因此，我们可以简单地将三个节点池中的每一个从一个节点扩展到两个节点并行运行，然后逐个缩减每个池。
- en: 'Uses zone anti-affinity rules to make sure applications are being spread across
    different zones using Pod topology spread constraints, which you can learn more
    about here: [https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用区域反亲和规则，确保通过 Pod 拓扑分布约束将应用程序分布在不同的可用区，您可以在此了解更多：[https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/)。
- en: 'The **cons** are as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: Additional cost for the additional worker node.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的工作节点成本。
- en: Additional complexity during setup because each Availability Zone has its own
    node group.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置过程中由于每个可用区都有自己的节点组而增加了额外的复杂性。
- en: Additional complexity with the NLB because it must have an interface in each
    Availability Zone.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 NLB 必须在每个可用区都有一个接口，因此增加了额外的复杂性。
- en: Additional complexity during an upgrade as each availability node group needs
    to upgrade on its own.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级过程中每个可用节点组需要单独升级，增加了额外的复杂性。
- en: AWS doesn't support **Elastic Block Storage** (**EBS**) volumes in different
    zones, so if you plan to use AWS's storage class, you'll need to ensure that application
    data is stored redundantly across Availability Zones. You can use AWS's EFS, but
    the cost can be very prohibitive.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 不支持跨可用区的**弹性块存储**（**EBS**）卷，因此如果计划使用 AWS 的存储类，您需要确保应用数据在可用区之间冗余存储。您可以使用
    AWS 的 EFS，但成本可能非常高昂。
- en: 'The **node sizing** requirements are as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点大小**要求如下：'
- en: 'Servers(s): three EC2 instances'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器：三台 EC2 实例
- en: 'CPU: eight cores per server'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每个服务器八个核心
- en: 'Memory: 8-16 GB'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：8-16 GB
- en: 'Storage: **Solid-State Drive** (**SSD**) or **Non-Volatile Memory Express**
    (**NVMe**) 10-15 GB'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储：**固态硬盘**（**SSD**）或**非易失性内存快速接口**（**NVMe**）10-15 GB
- en: Important Note
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: The latency is the most important metric we want to monitor when it comes to
    etcd.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 延迟是我们在监控 etcd 时最重要的指标。
- en: Worker node sizing should be based on your workload and its requirements.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工作节点大小应基于您的工作负载及其要求。
- en: GCP
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GCP
- en: 'In this design, we''ll deploy a standard size cluster on GCP as we did with
    AWS:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本设计中，我们将像在 AWS 上一样，在 GCP 上部署一个标准大小的集群：
- en: '![Figure 6.4 – A Rancher-managed AWS cluster across zones'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4 – 跨可用区的 Rancher 管理 AWS 集群'
- en: '](img/B18053_06_004.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_004.jpg)'
- en: Figure 6.4 – A Rancher-managed AWS cluster across zones
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 跨可用区的 Rancher 管理 AWS 集群
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/GCP/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/GCP/README.md)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图表：[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/GCP/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/GCP/README.md)
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The pros, cons, and node sizing requirements for GCP are exactly the same as
    that of AWS. You can refer to the *Amazon's AWS* section for more details on this.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 的优缺点和节点大小要求与 AWS 完全相同。您可以参考*Amazon 的 AWS*部分以获取更多详情。
- en: Now we have the design for our cluster created. In the next section, we are
    going to start the process of creating the cluster, with the first step being
    to prepare the nodes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了集群设计。接下来的部分，我们将开始创建集群的过程，第一步是准备节点。
- en: Preparing for nodes to join Rancher
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备节点加入 Rancher
- en: Before creating the cluster, we need to prepare the nodes/images that we'll
    use to create the cluster. This section will assume that you are using an Ubuntu
    or Red Hat-/CentOS-based image/node, as these are the two most common ones used.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群之前，我们需要准备将用于创建集群的节点/镜像。本节假设您使用的是基于 Ubuntu 或 Red Hat/CentOS 的镜像/节点，因为这两者是最常用的。
- en: 'For Rancher-managed nodes, we need to create a base image cloned and deployed
    in the environment. There are two main ways to create this image:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Rancher 管理的节点，我们需要创建一个基础镜像，并在环境中进行克隆和部署。创建此镜像有两种主要方式：
- en: The first is to start from a public image such as the Ubuntu ServerW cloud images,
    which can be found at [https://cloud-images.ubuntu.com/focal/current/](https://cloud-images.ubuntu.com/focal/current/).
    Note that this image must come from a trusted source directly from the Ubuntu
    site or your infrastructure provider's official images. These images are designed
    to be small and lightweight, as they only have the essential tools and pre-installed
    packages. And for most people, that is the end of the process, as most of the
    customization you will want to make can be done through the `cloud-init` tool.
    If you need to install any additional tools or require changes to settings, you
    should refer to your infrastructure provider's documentation for opening that
    image and customizing it.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，从一个公有镜像开始，比如可以在 [https://cloud-images.ubuntu.com/focal/current/](https://cloud-images.ubuntu.com/focal/current/)
    找到的 Ubuntu ServerW 云镜像。请注意，此镜像必须来自一个受信任的源，直接来自 Ubuntu 网站或您的基础设施提供商的官方镜像。这些镜像设计为小巧轻量，因为它们只包含必要的工具和预安装的包。对于大多数人来说，这就是整个过程的结束，因为大多数您想要进行的定制都可以通过
    `cloud-init` 工具来完成。如果您需要安装任何额外的工具或更改设置，应参考您的基础设施提供商的文档，以打开该镜像并进行定制。
- en: We usually recommend changing as little as possible and not installing tools
    such as Puppet, Chef, or backup clients because these servers are designed to
    be disposable and easily replaced. Also, we usually recommend patching the base
    image for minor updates. Still, we would recommend going back to the official
    image source and pulling down the new version for major upgrades instead. Finally,
    we recommend not updating/upgrading the node in the `cloud-init` file, as we want
    all nodes deployed from that image to be the same. In addition, Rancher has a
    10-minute timeout during the node creation process, and updating/patching can
    cause the node to exceed that window.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常建议尽量减少更改，不安装像 Puppet、Chef 或备份客户端这样的工具，因为这些服务器设计为可丢弃并容易替换。此外，我们通常建议对基础镜像进行补丁更新，以便进行小规模更新。然而，对于重大升级，我们更推荐返回官方镜像源并拉取新版本。最后，我们建议不要在
    `cloud-init` 文件中更新/升级节点，因为我们希望从该镜像部署的所有节点都保持一致。此外，Rancher 在节点创建过程中有 10 分钟的超时时间，更新/修补可能导致节点超过此时间限制。
- en: The second is to start from a golden image that you or your team already use
    in your environment for other applications. For example, if your Linux team already
    has a Red Hat-based image with all the customization needed for your environment,
    there is no sense in reinventing the wheel, and you can simply use that existing
    image. Note that you might need to do additional testing and tuning of that image
    to ensure it is fully supported.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方式是从一个黄金镜像开始，该镜像是您或您的团队已经在环境中用于其他应用程序的镜像。例如，如果您的 Linux 团队已经有一个基于 Red Hat
    的镜像，并且已经针对您的环境做了所有定制，那么没有必要重新发明轮子，您可以直接使用该现有镜像。请注意，您可能需要对该镜像进行额外的测试和调整，以确保它完全受支持。
- en: Note
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You should still follow the same recommendations as listed under the public
    image option as far as the `cloud-init` settings.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就 `cloud-init` 设置而言，您仍然应该遵循公有镜像选项下列出的相同建议。
- en: In addition, we should make sure that any tools for automating patching are
    disabled because we don't want the node to change after its creation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还应确保任何自动化修补工具都被禁用，因为我们不希望节点在创建后发生变化。
- en: The process is much different for custom nodes because Rancher has nothing to
    do with the node creation process or the OS. In this case, you or your Linux team
    are responsible for creating the server, configuring it, installing Docker, and
    registering it with Rancher. This has the upside of giving you a great deal of
    control over the server. You should still follow the same recommendations as listed
    under the public image option. The difference is that tools such as Puppet or
    Chef are supported because Rancher is not managing the OS.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自定义节点，过程与公有节点大不相同，因为 Rancher 与节点创建过程或操作系统无关。在这种情况下，您或您的 Linux 团队需要负责创建服务器、配置服务器、安装
    Docker，并将其注册到 Rancher 中。这一方式的优点是您可以对服务器拥有更大的控制权。您仍然应该遵循公有镜像选项下列出的相同建议。不同之处在于，由于
    Rancher 不管理操作系统，像 Puppet 或 Chef 这样的工具是被支持的。
- en: At this point, you should have your nodes built and ready to go if you are planning
    to bring your own nodes to Rancher. In the next section, we'll be covering the
    steps if we want Rancher to build the nodes for us.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，如果您计划将自有节点带入 Rancher，您应该已经构建并准备好节点。在下一节中，我们将介绍如果希望 Rancher 为我们构建节点时的步骤。
- en: Prepping the infrastructure provider
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备基础设施提供商
- en: Now that we have the node image created, we have to configure that image in
    Rancher.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了节点镜像，接下来需要在Rancher中配置该镜像。
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This is only applicable to Rancher-managed clusters. If you are using existing
    nodes, then this section can be skipped.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅适用于Rancher管理的集群。如果您正在使用现有节点，则可以跳过此部分。
- en: The first step is to create a service account in the infrastructure provider
    that has the permissions that Rancher needs to create, manage, and delete the
    nodes. For security reasons, we recommend this to be a dedicated account not shared
    with other applications or users and that the permissions for this account be
    limited to only what is needed. Details for the permissions of each of the different
    infrastructure providers can be found at [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/cloud-providers/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/cloud-providers/).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在基础设施提供商中创建一个服务账户，该账户需要具备Rancher用来创建、管理和删除节点的权限。出于安全考虑，我们建议此账户应为专用账户，不与其他应用程序或用户共享，并且该账户的权限应仅限于所需的权限。有关不同基础设施提供商的权限详情，可以参考[https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/cloud-providers/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/cloud-providers/)。
- en: It is important to remember that Rancher and the infrastructure provider are
    still evolving, so these permissions might change over time. Once you have that
    account created, you'll need to log into the Rancher v2.6.x UI and go to the **Cluster
    management** page and select the **Cloud Credential** page. This brings up a setup
    wizard, as shown in *Figure 6.5*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，Rancher和基础设施提供商仍在不断发展，因此这些权限可能会随时间变化。一旦创建了该账户，您需要登录Rancher v2.6.x UI，进入**集群管理**页面并选择**云凭证**页面。这将弹出一个设置向导，如*图6.5*所示。
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Rancher UI will test that the credentials are correct but will not validate
    that the account has all the permissions that Rancher will need.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher UI会测试凭证是否正确，但不会验证该账户是否具备Rancher所需的所有权限。
- en: '![Figure 6.5 – The Cloud Credential setup wizard for Amazon'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 – 亚马逊云凭证设置向导'
- en: '](img/B18053_06_005.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_005.jpg)'
- en: Figure 6.5 – The Cloud Credential setup wizard for Amazon
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 亚马逊云凭证设置向导
- en: For more details about the cloud credentials, please go to [https://rancher.com/docs/rancher/v2.5/en/user-settings/cloud-credentials/](https://rancher.com/docs/rancher/v2.5/en/user-settings/cloud-credentials/).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有关云凭证的更多详细信息，请访问[https://rancher.com/docs/rancher/v2.5/en/user-settings/cloud-credentials/](https://rancher.com/docs/rancher/v2.5/en/user-settings/cloud-credentials/)。
- en: The next step is to create the node template. This is how we define a node configuration.
    This includes selecting the image, location, and any other infrastructure settings.
    We do this by going to the **Cluster Management** page, expanding **RKE1 Configuration**,
    and then choosing **Node templates**. This will bring up a setup wizard, as shown
    in the following screenshot.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建节点模板。这是我们定义节点配置的方式。包括选择镜像、位置以及其他任何基础设施设置。我们通过进入**集群管理**页面，展开**RKE1配置**，然后选择**节点模板**来完成。这将会弹出一个设置向导，如下图所示。
- en: Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Rancher UI will dynamically query the infrastructure provider as you click
    through different pages.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher UI将在您点击不同页面时动态查询基础设施提供商。
- en: '![Figure 6.6 – The node template wizard for Amazon – step one'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – 亚马逊节点模板向导 – 步骤一'
- en: '](img/B18053_06_006.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_006.jpg)'
- en: Figure 6.6 – The node template wizard for Amazon – step one
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 亚马逊节点模板向导 – 步骤一
- en: The following two pages are different, based on the infrastructure provider.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两页根据基础设施提供商有所不同。
- en: '![Figure 6.7 – The node template wizard for Amazon – Instance settings'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – 亚马逊节点模板向导 – 实例设置'
- en: '](img/B18053_06_007.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_007.jpg)'
- en: Figure 6.7 – The node template wizard for Amazon – Instance settings
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 亚马逊节点模板向导 – 实例设置
- en: 'The final page of the setup wizard is where you''ll do most of the node customization
    – for example, setting the server size, root disk size, and tags. Most of these
    settings can be left to the default values; the only setting I usually recommend
    changing is **Root Disk Size**, which defaults to 16 GB. This is great for a lab/sandbox,
    but for actual production nodes, I would recommend going with 30-40 GB. Also,
    the **Name** field is usually not changed, so I recommend using a very descriptive
    name. There also is a **Description** field for entering notes. Finally, the **Labels**
    field can be a little confusing (refer to *Figure 6.8*). The bottom section of
    the page is for setting the Docker/Kubernetes labels, taints, and engine options:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 设置向导的最后一页是你进行大部分节点自定义的地方——例如，设置服务器大小、根磁盘大小和标签。大部分这些设置可以保留默认值；我通常推荐更改的唯一设置是**根磁盘大小**，默认值为
    16 GB。这对于实验室/沙盒环境来说足够了，但对于实际的生产节点，我建议设置为 30-40 GB。另外，**名称**字段通常不会更改，所以我建议使用一个非常具描述性的名称。还有一个**描述**字段可以用于输入备注。最后，**标签**字段可能有些让人困惑（参见*图
    6.8*）。页面的底部部分是用于设置 Docker/Kubernetes 标签、污点和引擎选项：
- en: '![Figure 6.8 – The node template wizard for Amazon – the node settings'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8 – 亚马逊的节点模板向导 – 节点设置'
- en: '](img/B18053_06_008.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_008.jpg)'
- en: Figure 6.8 – The node template wizard for Amazon – the node settings
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 亚马逊的节点模板向导 – 节点设置
- en: For more details about the node templates, please go to [https://rancher.com/docs/rancher/v2.5/en/user-settings/node-templates/](https://rancher.com/docs/rancher/v2.5/en/user-settings/node-templates/).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有关节点模板的更多详细信息，请访问[https://rancher.com/docs/rancher/v2.5/en/user-settings/node-templates/](https://rancher.com/docs/rancher/v2.5/en/user-settings/node-templates/)。
- en: At this point, we have done all the preparation work that is needed for Rancher
    to create and manage our nodes for us. In the next section, we'll be starting
    the process of actually creating the cluster in Rancher.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了 Rancher 为我们创建和管理节点所需的所有准备工作。在下一部分，我们将开始在 Rancher 中实际创建集群的过程。
- en: The steps for creating an RKE cluster using Rancher
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Rancher 创建 RKE 集群的步骤
- en: In this section, we're going to create a custom cluster mainly using default
    settings. In the next section, we'll cover creating an RKE cluster using an infrastructure
    provider.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用默认设置创建一个自定义集群。在下一部分，我们将介绍如何使用基础设施提供商创建 RKE 集群。
- en: 'The first step is to go to the Rancher UI and the **Cluster Management** page.
    From there, go to the **Clusters** page and click the **Create** button in the
    top right corner of the page. This brings you to a page that shows you all the
    major cluster types. Please see the following figure for an example. From this
    page, we are going to click the **Custom** button:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是进入 Rancher 用户界面（UI）并访问**集群管理**页面。在此页面中，进入**集群**页面，并点击页面右上角的**创建**按钮。这样会带你到一个展示所有主要集群类型的页面。请参见下面的示例图。我们将在此页面中点击**自定义**按钮：
- en: '![Figure 6.9 – The cluster creation page'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.9 – 集群创建页面'
- en: '](img/B18053_06_009.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_009.jpg)'
- en: Figure 6.9 – The cluster creation page
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 集群创建页面
- en: 'The next page is where you can define the cluster. The first field that you''ll
    fill out is **Cluster Name**. The cluster name is limited to a maximum of 253
    characters, all lower-case and alphanumeric, with dots and dashes. For more details
    about the rest of the other settings on this page, refer to [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '下一页是你可以定义集群的地方。你需要填写的第一个字段是**集群名称**。集群名称最多限制为 253 个字符，且必须是全小写字母、数字，支持点号和破折号。关于此页面上其他设置的详细信息，请参考[https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/):'
- en: '![Figure 6.10 – The cluster settings page'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.10 – 集群设置页面'
- en: '](img/B18053_06_010.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_010.jpg)'
- en: Figure 6.10 – The cluster settings page
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 集群设置页面
- en: 'Now that we have a cluster, we need to start adding nodes to the cluster. Because
    we''re creating a custom cluster, the next page will be the `docker run` commands
    that we''ll need to join the different kinds of nodes. This page can be retrieved
    later by going to the **Cluster Management** page and selecting **Cluster** from
    the list:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个集群，接下来需要为集群添加节点。由于我们正在创建一个自定义集群，下一页将显示我们需要用来加入不同类型节点的`docker run`命令。稍后你可以通过进入**集群管理**页面并从列表中选择**集群**来检索此页面：
- en: '![Figure 6.11 – The Customize Node Run Command wizard'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.11 – 自定义节点运行命令向导'
- en: '](img/B18053_06_011.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_011.jpg)'
- en: Figure 6.11 – The Customize Node Run Command wizard
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 自定义节点运行命令向导
- en: At this point, Rancher should be creating our new cluster. You can monitor the
    process via the Rancher UI by clicking on the cluster dashboard or the **Nodes**
    tab. At the end of this process, you will have a Kubernetes cluster. In the next
    section, we'll cover creating a cluster using node pools.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，Rancher 应该正在创建我们的新集群。你可以通过点击集群仪表板或**节点**标签来通过 Rancher UI 监控该过程。在此过程结束时，你将拥有一个
    Kubernetes 集群。在下一节中，我们将介绍如何使用节点池创建集群。
- en: Deploying a cluster using node pools
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用节点池部署集群
- en: Now that we have a custom cluster, we will follow the same steps for creating
    a cluster with node pools. To start with, instead of selecting `docker run` command
    wizard.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个自定义集群，我们将按照相同的步骤创建一个包含节点池的集群。首先，选择 `docker run` 命令向导，而不是选择其他选项。
- en: 'With this wizard, you''ll add a node pool for each different type of node you
    want to configure, with the important field being **Name Prefix**, which is used
    to set the hostnames on the nodes in this pool. It is essential that these names
    are meaningful and do not overlap. The other main fields are the roles'' checkboxes.
    The UI will warn you about the minimum number of nodes required for each type
    of node. If this is not met, Rancher will not allow you to create the cluster:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个向导，你可以为每种不同类型的节点添加一个节点池，关键字段是**名称前缀**，用于设置该池中节点的主机名。确保这些名称具有意义且不会重复非常重要。其他主要字段是角色的复选框。UI
    会提醒你每种节点类型所需的最小节点数量。如果未满足要求，Rancher 将不允许你创建集群：
- en: '![Figure 6.12 – The node pool creation wizard'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12 – 节点池创建向导'
- en: '](img/B18053_06_012.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_06_012.jpg)'
- en: Figure 6.12 – The node pool creation wizard
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 节点池创建向导
- en: At this point, Rancher will take over to start the process of creating servers
    and provisioning RKE on top of them. You can monitor the creation of the nodes
    by going to the **Nodes** tab and watching status messages for each node. The
    same applies to the status of the cluster as a whole at the top of the page. At
    the end of this process, you'll have a Kubernetes cluster ready to start deploying
    applications to. In the next section, we'll be covering some of the tasks that
    we'll need to do in order to keep the cluster healthy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，Rancher 将接管，开始创建服务器并在其上部署 RKE。你可以通过进入**节点**标签来监控节点的创建，并查看每个节点的状态消息。集群的整体状态也会显示在页面顶部。在此过程结束时，你将拥有一个可以开始部署应用程序的
    Kubernetes 集群。在下一节中，我们将介绍一些需要执行的任务，以保持集群的健康。
- en: Ongoing maintenance tasks
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续维护任务
- en: After creating a cluster, a few ongoing maintenance tasks need to be done to
    keep the cluster running in a healthy state.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 创建集群后，需要进行一些持续的维护任务，以保持集群处于健康状态。
- en: The first task that I recommend setting up is the scheduled etcd backups, which
    in Rancher v2.4 and beyond are enabled by default. The default behavior is to
    have each etcd node take an etcd snapshot and store it locally in `/opt/rke/etcd-snapshots`.
    The etcd backup is a point-in-time snapshot of the etcd database that stores the
    configuration of the cluster. This backup is critical when recovering from a failure.
    So, it is pretty common to configure the backup to the S3 option, as we don't
    want to store the backups on the same server that is being backed up. You can
    find a detailed list of the S3 settings at [https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service](https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我推荐的第一个任务是设置定期的 etcd 备份，在 Rancher v2.4 及更高版本中，该功能默认启用。默认行为是每个 etcd 节点会拍摄一个 etcd
    快照，并将其存储在 `/opt/rke/etcd-snapshots` 本地。etcd 备份是一个时点快照，存储了集群的配置。在发生故障时，这个备份非常重要，因此，通常会将备份配置为
    S3 选项，因为我们不希望将备份存储在与被备份的服务器相同的位置。你可以在 [https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service](https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service)
    找到详细的 S3 设置列表。
- en: Note
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Rancher/RKE supports any S3-compatible storage. So, for on-premises environments,
    you can use a tool such as MinIO. If you already have an enterprise storage solution,
    you might want to review it and see whether it has S3 support, as several newer
    enterprise storage subsystems provide S3 out of the box.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher/RKE支持任何与S3兼容的存储。因此，对于本地环境，您可以使用诸如MinIO之类的工具。如果您已经拥有企业存储解决方案，可能需要审查一下，看看它是否支持S3，因为一些较新的企业存储子系统提供了开箱即用的S3支持。
- en: 'The second task that I recommend testing and documenting is how you will patch/upgrade
    the nodes in the cluster. The two main ways are to patch in place or replace the
    node. The most common way for custom clusters is to patch in place, with the high-level
    process being the creation of a script walk-through of all nodes in the cluster
    and using the following steps:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议测试和记录的第二项任务是如何对集群中的节点进行补丁/升级。主要的两种方式是在原地进行补丁或替换节点。自定义集群的最常见方式是在原地进行补丁，具体过程是创建一个脚本，逐个浏览集群中的所有节点，并使用以下步骤：
- en: Drain and cordon the node.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排空并隔离节点。
- en: Then, apply any patches/upgrades/reboots that are needed on the node.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在节点上应用任何所需的补丁/升级/重启。
- en: Once all tasks have been completed, the node is un-cordoned, and the next node
    is processed. An example script can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/cluster_patching/rolling_reboot.sh](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/cluster_patching/rolling_reboot.sh).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有任务完成，节点就会取消隔离，并处理下一个节点。可以在[https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/cluster_patching/rolling_reboot.sh](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/cluster_patching/rolling_reboot.sh)找到示例脚本。
- en: Note
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: This script is designed to have a lot of sleep periods, as it was intended to
    be run in unattended mode. For clusters with node pools, you'll typically replace
    the nodes instead of changing the existing nodes. This is done by scaling up the
    node pool and then removing the old nodes one at a time and replacing them.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该脚本设计为具有大量的休眠周期，因为它旨在无人参与模式下运行。对于具有节点池的集群，通常会替换节点而不是更改现有节点。这通过扩展节点池，然后逐个删除旧节点并替换它们来完成。
- en: The third task that I recommend testing and documenting is how to upgrade Kubernetes.
    The basic process is to review the release notes for the new version. Then, when
    it comes to upgrading the cluster, you'll want to take an etcd snapshot, as this
    is the only way to roll back an upgrade. The rules and process for this upgrade
    can be found at https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/rancher-k8s-upgrades#rke-upgrade--prep-work
    along with a masterclass that does a deep dive into the subject.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议测试和记录的第三项任务是如何升级Kubernetes。基本过程是查看新版本的发布说明。然后，在升级集群时，您需要进行etcd快照，因为这是回滚升级的唯一方式。有关此升级的规则和过程，可以在https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/rancher-k8s-upgrades#rke-upgrade--prep-work找到，以及进行深入研究的大师课程。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the different types of Rancher-managed clusters,
    including the requirements and limitations of each. We then covered the rules
    of architecting each type of cluster, including some example designs and the pros
    and cons of each solution. Finally, we went into detail about the steps for creating
    each type of cluster using the design we made earlier. We ended the chapter by
    going over the major maintenance tasks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了Rancher管理的不同类型的集群，包括每种类型的要求和限制。然后，我们讨论了为每种类型的集群架构规则，包括一些示例设计以及每种解决方案的优缺点。最后，我们详细介绍了使用我们早期制定的设计创建每种类型集群的步骤。我们通过讨论主要的维护任务结束了本章。
- en: The next chapter will cover how to create a hosted cluster in Rancher – that
    is, a downstream cluster. We will cover how Rancher creates these clusters and
    what the limitations are.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍如何在Rancher中创建托管集群，即下游集群。我们将讨论Rancher如何创建这些集群以及其限制。
