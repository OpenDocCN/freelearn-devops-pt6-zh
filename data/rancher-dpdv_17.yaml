- en: '*Chapter 13*: Scaling in Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：Kubernetes中的扩展'
- en: 'In this chapter, we''ll be covering scaling your Kubernetes cluster. We''ll
    go over the three main ways of doing so: with the **Horizontal Pod Autoscaler**
    (**HPA**), **Vertical Pod Autoscaler** (**VPA**), and Cluster Autoscaler. We will
    cover the pros and cons and some examples of each of these ways, as well as diving
    into the best practices for each method.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何扩展Kubernetes集群。我们将介绍三种主要的扩展方式：**水平Pod自动扩展器**（**HPA**）、**垂直Pod自动扩展器**（**VPA**）和集群自动扩展器。我们将介绍每种方式的优缺点，并提供一些示例，同时深入探讨每种方法的最佳实践。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要话题：
- en: What is an HPA?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是HPA？
- en: What is a VPA?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是VPA？
- en: What is Kubernetes Cluster Autoscaler?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Kubernetes集群自动扩展器？
- en: What is an HPA?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是HPA？
- en: An HPA is a controller within a controller manager. An HPA automatically scales
    pods in replication controllers, deployments, replica sets, or stateful sets based
    on CPU usage (or custom metrics). Objects that cannot be scaled, such as DaemonSets,
    are not affected by horizontal pod autoscaling. With a default value of `15` seconds,
    the `horizontal-pod-autoscaler-sync-period` flag in the controller manager determines
    how often the HPA runs. Every cycle, the controller manager checks resource utilization
    on the workload in question. The controller uses a custom metrics endpoint along
    with the metrics server to gather its statistics.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: HPA是控制器管理器中的一个控制器。HPA根据CPU使用率（或自定义指标）自动扩展复制控制器、部署、副本集或有状态集中的Pods。无法扩展的对象（例如DaemonSets）不会受到水平Pod自动扩展的影响。在控制器管理器中，`horizontal-pod-autoscaler-sync-period`标志的默认值为`15`秒，决定了HPA运行的频率。每个周期中，控制器管理器会检查相关工作负载的资源利用率。控制器使用自定义指标端点和度量服务器来收集其统计数据。
- en: 'In essence, the HPA monitors current and desired metric values, and if they
    do not match the specification, it takes action. The HPA follows the following
    algorithm:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，HPA监控当前和期望的指标值，如果它们与规范不匹配，HPA会采取行动。HPA遵循以下算法：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For example, let's say you have an application to keep the CPU usage at 50%.
    Currently, this deployment has a CPU request of 1000 m (millicores), which equals
    one core on the node. It is important to note that the HPA uses the CPU and memory
    request metrics and not the limits. The HPA computes the `currentMetricValue`
    object type by dividing the request value by the current usage, which outputs
    a percentage. HPA does this calculation for every pod in the deployment, then
    averages them to create a `currentMetricValue` object type for the deployment.
    The HPA then compares the `currentMetricValue` object type to the `desiredMetricValue`
    object type, which is 50% for both, so that the HPA won't make a change. But if
    the ratio is too high (1.0 is the target), it will trigger a scale-up event, which
    will add more pods. It is important to note that *not ready* or *terminating*
    pods are not counted. By default, the metrics for the first 30 seconds of a pod's
    life are ignored as defined by the `horizontal-pod-autoscaler-initial-readiness-delay`
    flag. Also, the HPA can scale by more than one pod at a time, but usually, this
    requires a significant ratio difference, so most of the time, the workload is
    only scaled by one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设你有一个应用程序需要保持CPU使用率在50%。目前，这个部署的CPU请求为1000m（毫核心），相当于节点上的一个核心。需要注意的是，HPA使用的是CPU和内存请求的指标，而不是限制值。HPA通过将请求值除以当前使用值来计算`currentMetricValue`对象类型，得到一个百分比。HPA会对部署中的每个Pod进行此计算，然后对它们进行平均，以创建一个部署的`currentMetricValue`对象类型。然后，HPA将`currentMetricValue`对象类型与`desiredMetricValue`对象类型进行比较，若两者都是50%，则HPA不会进行任何改变。但如果比率过高（目标是1.0），它会触发扩展事件，增加更多的Pods。需要注意的是，*未准备好*或*正在终止*的Pods不会被计入。默认情况下，Pod生命周期的前30秒内的度量值会被忽略，这一行为由`horizontal-pod-autoscaler-initial-readiness-delay`标志定义。此外，HPA一次可以扩展多个Pods，但通常这需要显著的比率差异，因此大多数情况下，工作负载仅会扩展一个Pod。
- en: At this point, we know how HPAs work, including how the HPA makes its decisions
    to scale up and down workloads. Of course, the next question we need to ask is
    when you should and shouldn't use HPAs for your workload, and in the next section,
    we will dive into that topic.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了HPA的工作原理，包括HPA如何决定何时扩展和缩减工作负载。当然，接下来的问题是何时应该使用HPA，何时不应该使用HPA来处理你的工作负载，在接下来的部分，我们将深入探讨这个话题。
- en: When should you use an HPA?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候应该使用HPA？
- en: Now that we know how an HPA works, let's dive into when you should use one.
    An example application of an HPA is a web server. The question is, why is it a
    great example? The answer is that, traditionally, even before Kubernetes, web
    servers were designed in a way that you could remove or add them at any time without
    impacting your application.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了HPA是如何工作的，接下来我们来看看什么时候应该使用HPA。一个使用HPA的典型应用程序是Web服务器。那么，为什么Web服务器是一个很好的例子呢？答案是，传统上，甚至在Kubernetes之前，Web服务器的设计方式就是能够随时添加或删除，而不会影响到应用程序。
- en: 'The following is a list of characteristics of an application that it would
    make sense to use with an HPA:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是适合使用HPA的应用程序特征列表：
- en: '**Stateless** – The application must be able to be added and removed at any
    time.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无状态** – 应用程序必须能够随时添加或删除。'
- en: '**Quick startup** – Generally, your pods should start up and be ready for requests
    within 30 seconds.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速启动** – 通常情况下，Pod应该在30秒内启动并准备好处理请求。'
- en: '**HTTP-based** – Most applications that will use HPAs are HTTP-based because
    we want to leverage the built-in load balancing capabilities that come with an
    ingress controller.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于HTTP的** – 大多数会使用HPA的应用程序是基于HTTP的，因为我们希望利用Ingress控制器内置的负载均衡能力。'
- en: '**Batch jobs** – If you have batch jobs that can be run in parallel, you can
    use an HPA to scale up and down the number of worker pods based on the load, for
    example, having a pod that grabs an item out of a queue, processes the data, then
    publishes the output. Assuming multiple jobs can be run at once, you should set
    up an HPA based on the length of the queue to scale up and down the number of
    workers, that is, deployment.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理作业** – 如果你有可以并行运行的批处理作业，你可以使用HPA根据负载动态扩展或缩减工作Pod的数量。例如，一个Pod从队列中获取任务，处理数据，然后发布输出。假设可以同时运行多个作业，你应该根据队列的长度设置HPA，以扩展或缩减工作Pod的数量，也就是部署。'
- en: Next, let's learn about when to not use an HPA.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来学习一下什么情况下不应该使用HPA。
- en: When should you not use an HPA?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候不应该使用HPA？
- en: 'Of course, it doesn''t make sense to have an HPA on all applications, and having
    an HPA can break them, for example, a database cluster, where you do not want
    pods being added and removed all the time because it could cause application errors.
    It is important to note that HPAs support scaling StatefulSets, but you should
    be careful as most applications that require StatefulSets do not like being scaled
    up and down a lot. There, of course, are other reasons why you might not want
    to use an HPA with your applications and the following are a few of the most common
    reasons:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于所有应用程序都使用HPA是没有意义的，实际上使用HPA可能会导致应用程序出现问题，例如数据库集群，你不希望在集群中频繁添加和删除Pod，因为这可能导致应用程序错误。需要注意的是，HPA支持扩展StatefulSets，但你应该小心，因为大多数需要StatefulSets的应用程序并不喜欢频繁的扩缩容。还有其他一些原因，可能会导致你不希望在应用程序中使用HPA，以下是一些最常见的原因：
- en: '**StatefulSets** – Applications such as databases that require storage and
    orderly scaling tend to not be compatible with an HPA adding and removing pods
    as it sees fit.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**StatefulSets** – 需要存储和有序扩展的应用程序，例如数据库，通常与HPA不兼容，因为HPA会根据需求随意添加和删除Pod。'
- en: '**Pods that require storage** – Applications that require a **PersistentVolumeClaim**
    (**PVC**) are generally not recommended for the fact that provisioning and attaching
    storage can require a reasonable amount of time to complete and can break over
    time.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要存储的Pod** – 需要**PersistentVolumeClaim**（**PVC**）的应用程序通常不建议使用HPA，因为提供和连接存储需要一定的时间，且随着时间推移可能会导致问题。'
- en: '**Applications that require reconfiguration when scale changes** – A great
    example of this kind of workload is a Java app that uses database connection pooling
    with an external database. This is because any time a pod is created, Java will
    need to develop several new connections, which can be a heavy load on the database
    server and cause connection exhaustion as the database runs out of connections,
    which will cause the pod to fail. This, in turn, will generate a new pod to be
    created and at the same time, the load on the pods can go up due to the database
    running slowly, causing more scaling. The problem just runs away, creating more
    and more pods, which eventually causes a cluster outage.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要在规模变化时重新配置的应用程序** – 这种工作负载的一个典型例子是使用外部数据库的 Java 应用程序，它使用数据库连接池。这是因为每次创建
    pod 时，Java 都需要开发多个新的连接，这可能会给数据库服务器带来较大的负载，并导致连接耗尽，当数据库连接耗尽时，pod 将失败。这反过来会导致创建一个新的
    pod，同时，由于数据库运行缓慢，pods 上的负载可能会增加，导致更多的扩展。问题会变得越来越严重，产生越来越多的 pods，最终导致集群停机。'
- en: '**Workloads that burst** – If you have an application that sits at very low
    utilization most of the time and then jumps to high utilization for a short time,
    it doesn''t make sense to use an HPA because the HPA will scale down the deployment
    until it needs the resources. The issue happens during short-lived bursts as by
    the time the HPA reacts and spins up new pods, the event has already passed, making
    the HPA worthless.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**突发工作负载** – 如果你有一个应用程序大多数时间处于非常低的利用率，然后在短时间内突然跳到高利用率，使用 HPA 是没有意义的，因为 HPA
    会缩减部署，直到资源需要时再扩展。问题出现在短暂的突发事件中，因为当 HPA 反应过来并启动新的 pods 时，事件已经过去，使得 HPA 变得毫无价值。'
- en: Note
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can set a crazily high minimum scale but at that point, the question needs
    to be asked, why do you need an HPA if it's never going to scale up or down?
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以设置一个极高的最小规模，但在这种情况下，需要问自己一个问题：如果 HPA 永远不会扩展或缩小，为什么还需要它？
- en: We have covered the pros and cons of an HPA in this section. It is important
    to note that every application is different, and you should work with the application
    developer to decide whether adding an HPA could help. Let's look at an example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经讨论了 HPA 的优缺点。需要注意的是，每个应用程序都是不同的，你应该与应用程序开发人员合作，决定是否添加 HPA 会有帮助。我们来看一个例子。
- en: Example – simple web server with CPU utilization
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 – 简单的 Web 服务器与 CPU 利用率
- en: 'To deploy this example, please run the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署此示例，请运行以下命令：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command will deploy a namespace called `hpa-example-simple` with a test
    app called `hello-world` and an HPA that will trigger a CPU utilization of 50%.
    We can test the HPA using the `load-generator` deployment, which is set to a scale
    of `0` by default.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将部署一个名为 `hpa-example-simple` 的命名空间，其中包含一个名为 `hello-world` 的测试应用程序和一个将在 CPU
    利用率达到 50% 时触发的 HPA。我们可以通过 `load-generator` 部署来测试 HPA，默认情况下其规模为 `0`。
- en: 'To create a load on the `hello-world` app, simply run the following command
    to turn on the load:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要给 `hello-world` 应用程序制造负载，只需运行以下命令启动负载：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run the following command to turn it back off:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令将其关闭：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you run the `kubectl -n hpa-example-simple describe hpa hello-world` command,
    you can see the following events and actions taken by the HPA:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行 `kubectl -n hpa-example-simple describe hpa hello-world` 命令，你可以看到以下事件以及
    HPA 执行的操作：
- en: '![Figure 13.1 – HPA events'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.1 – HPA 事件'
- en: '](img/B18053_13_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_13_01.jpg)'
- en: Figure 13.1 – HPA events
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – HPA 事件
- en: In this section, we covered scaling our workload in the horizontal direction,
    that is, adding and removing pods. In the next section, we will go in the other
    direction, vertically.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了水平扩展工作负载，也就是添加和移除 pods。在下一节中，我们将讨论垂直方向的扩展。
- en: What is a VPA?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 VPA？
- en: If there is an HPA, is there a VPA? Yes, there is. A VPA is similar to an HPA,
    but instead of scaling the pod count up and down, the VPA automatically sets the
    resource request and limit values based on the actual CPU usage. The main goal
    of a VPA is to reduce the maintenance overhead associated with managing resource
    requests and limits for containers and to improve cluster utilization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有 HPA，那是否也有 VPA？是的，有。VPA 与 HPA 类似，但不是通过增加或减少 pod 数量来进行扩展，而是根据实际的 CPU 使用情况自动设置资源请求和限制值。VPA
    的主要目标是减少管理容器资源请求和限制的维护开销，并提高集群的利用率。
- en: Even though a VPA is similar to an HPA, it's important to know that VPAs have
    a different way of working, which we'll be covering in the next section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 VPA 与 HPA 类似，但重要的是要知道 VPA 有不同的工作方式，我们将在下一节中讨论这一点。
- en: How does a VPA work?
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VPA是如何工作的？
- en: 'There are three different components that make up a VPA:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: VPA由三个不同的组件组成：
- en: '**VPA admission hook** – Each pod submitted to the cluster is examined with
    this webhook to see whether its parent object references the pod (a replication
    set, a deployment, and so on).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VPA准入钩子** – 每个提交到集群的Pod都会通过这个Webhook进行检查，看看它的父对象是否引用了该Pod（例如副本集、部署等）。'
- en: '**VPA recommender** – Connections to the metrics-server application give recommendations
    for scaling up or down the requests and limits of the pods based on historical
    and current usage data (CPU and memory) for each pod with VPA enabled.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VPA推荐器** – 通过与metrics-server应用程序的连接，基于每个启用VPA的Pod的历史和当前使用数据（CPU和内存），提供有关扩展或缩减请求和限制的建议。'
- en: '**VPA updater** – Each minute the VPA updater runs, it will evict the running
    version of a pod that is not in the recommended range, so the pod can restart
    and go through the VPA admission webhook to adjust the CPU and memory settings
    before starting.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VPA更新器** – 每分钟运行一次VPA更新器，它将驱逐那些不在推荐范围内的正在运行的Pod版本，以便Pod可以重启并通过VPA准入Webhook在启动前调整CPU和内存设置。'
- en: This means that if you are running something such as Argo CD, the VPA updater
    will not detect any changes in the deployment, and the two won't be fighting to
    adjust the specifications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果你运行的是像Argo CD这样的应用，VPA更新器将不会检测到部署中的任何变化，两个组件也不会互相争夺调整规格。
- en: Next, let's learn why we need a VPA.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解为什么我们需要VPA。
- en: Why do you need a VPA?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么需要VPA？
- en: We need to cover the resource request and limit before diving deeper into VPAs.
    When kube-scheduler assigns a pod to a node, it does not know how much memory
    and CPU it will need. There is only 16 GB of free RAM on the node, but the pod
    needs 64 GB. As soon as the pod starts, it runs out of memory, evicting pods.
    A cluster node with the correct amount of memory might support that pod. Here
    is where resource requests are put in play, where we specify that kube-scheduler
    should give this pod X amount of CPU and memory on a specific node. By adding
    this intelligence into the scheduling process, kube-scheduler can make a more
    informed decision about where to schedule pods. We also have resource limits,
    which act as hard limits for throttling or killing pods if they exceed their limits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解VPA之前，我们需要先了解资源请求和限制。当kube-scheduler将Pod分配到一个节点时，它并不知道该Pod需要多少内存和CPU。节点上只有16
    GB的空闲内存，但Pod需要64 GB。一旦Pod启动，就会内存不足，驱逐其他Pods。一个具有正确内存大小的集群节点可能会支持该Pod。此时，资源请求起作用，我们可以指定kube-scheduler在特定节点上为这个Pod分配多少CPU和内存。通过将这种智能加入调度过程中，kube-scheduler可以做出更明智的决定，选择将Pod调度到哪里。我们还可以设置资源限制，当Pod超过其限制时，资源限制将作为硬限制进行限制或终止Pod。
- en: Of course, setting the resource request and limits can require a lot of work
    because you need to load test your application and review the performance data,
    and set your requests and limits. Then you have to keep monitoring this over time
    to tune these settings. This is why we see many environments where everything
    is set to unlimited, and the cluster administrator will just throw hardware at
    the problem. This is where a VPA comes into the picture by setting our requests
    for us and fine-tuning them over time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，设置资源请求和限制可能需要大量工作，因为你需要对应用程序进行负载测试、审查性能数据，并设置请求和限制。然后你必须不断监控这些设置，以便进行调整。这就是为什么我们看到许多环境中，所有设置都是无限制的，集群管理员只会把硬件投向问题的原因。这时，VPA就发挥作用了，它为我们设置资源请求并随着时间推移进行微调。
- en: 'For example, we can build a pod with the following settings:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下设置构建一个Pod：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The VPA recommender determines that you will need 120 MB of CPU power and 300
    MB of memory to make your pod function properly. The recommended settings are
    as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: VPA推荐器会判断你需要120 MB的CPU和300 MB的内存来使Pod正常运行。推荐的设置如下：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There will also be an increase in the limits because the VPA will scale them
    proportionally. Therefore, it is imperative to set your limits to something realistic
    rather than something crazy such as 1 TB of memory if your nodes are only equipped
    with 128 GB each. As a starting point, set your limits by doubling your request
    size, for example, if your request is 100 MB, your limit should be 200 MB. But
    don't forget that your limits are meaningless because the scheduling decision
    (and therefore, resource contention) will always be based on the requests. Limits
    are helpful only when there is resource contention or to avoid uncontrollable
    memory leaks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于VPA会按比例扩展它们，限制也会随之增加。因此，务必将限制设置为实际的数字，而不是一些不切实际的值，比如1TB的内存，尤其是在节点只有128GB内存的情况下。作为起点，可以将你的限制设置为请求大小的两倍，例如，如果请求是100MB，那么你的限制应该是200MB。但不要忘记，限制本身没有多大意义，因为调度决策（也就是资源争用）总是基于请求来做的。限制只有在资源争用时才有帮助，或者用于避免无法控制的内存泄漏。
- en: How to write VPA manifests
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何编写VPA清单
- en: You should never define more than one VPA for the same Pod/ReplicaSet/Deployment/StatefulSet
    – the behavior becomes unpredictable in such cases. A VPA should not be used on
    the same pod as an HPA.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你永远不应该为同一个Pod/ReplicaSet/Deployment/StatefulSet定义多个VPA——在这种情况下，行为将变得不可预测。VPA不应该与HPA在同一个Pod上使用。
- en: 'You first need to create a VPA object with `updateMode: off` for the target
    application, which puts the VPA in a dry run mode (which is the recommendation
    mode).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '你首先需要为目标应用创建一个`updateMode: off`的VPA对象，这将使VPA进入干运行模式（即推荐模式）。'
- en: 'The following is an example VPA with the minimum required settings:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是具有最小所需设置的VPA示例：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After about 5 minutes, you will be able to query the data and start to see
    some of the recommendations:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 大约5分钟后，你将能够查询数据并开始看到一些建议：
- en: '[PRE27]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As you can see in the following screenshot, the `status` section gives us some
    recommendations for our targets, and following the screenshot is a complete breakdown
    of what each of these sections means:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，`status`部分为我们的目标提供了一些建议，接下来的屏幕截图将详细说明这些部分的含义：
- en: '![Figure 13.2 – Example VPA description'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.2 – 示例VPA描述'
- en: '](img/B18053_13_02.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_13_02.jpg)'
- en: Figure 13.2 – Example VPA description
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – 示例VPA描述
- en: 'You can find the complete output at [https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch13/vpa/describe-vpa.yaml](https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch13/vpa/describe-vpa.yaml).
    To break down this output, you''ll see the following sections:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch13/vpa/describe-vpa.yaml](https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch13/vpa/describe-vpa.yaml)找到完整的输出。为了分解这个输出，你将看到以下几个部分：
- en: '`Uncapped Target`: When upper limits are not configured in the VPA definition,
    the resource request on your pod will be uncapped.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`未限制的目标`：当VPA定义中没有配置上限时，Pod上的资源请求将不受限制。'
- en: '`Target`: This is the amount that will be configured on the subsequent execution
    of the admission webhook. If it already has this configuration, no changes will
    happen (your pod won''t be in a restart/evict loop). Otherwise, the pod will be
    evicted and restarted using this target setting.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`目标`：这是在后续执行入场Webhook时将配置的量。如果已经有这个配置，则不会发生变化（你的Pod不会进入重启/驱逐循环）。否则，Pod将被驱逐并使用这个目标设置重新启动。'
- en: '`Lower Bound`: When your pod goes below this usage, it will be evicted and
    downscaled.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`下限`：当你的Pod的使用量低于这个值时，它将被驱逐并缩减规模。'
- en: '`Upper Bound`: When your pod goes above this usage, it will be evicted and
    upscaled.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`上限`：当你的Pod的使用量高于这个值时，它将被驱逐并扩展规模。'
- en: At this point, you can just use this information to create and set the request
    limits of your deployments. But if you want to use automatic predictions, you
    need to change the `updateMode` value to `Auto`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可以使用这些信息来创建并设置你的部署请求限制。但如果你想使用自动预测，你需要将`updateMode`的值改为`Auto`。
- en: 'Now, if you want to set minimum and maximum limits for the VPA, you can add
    the following section to your VPA config:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想为VPA设置最小和最大限制，可以在VPA配置中添加以下部分：
- en: '[PRE28]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So far, we have focused on scaling pods, but the other half of the picture is
    scaling the nodes. In the next section, we are going to dive into node autoscaling.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注的是伸缩Pod，但图像的另一半是伸缩节点。在下一部分中，我们将深入了解节点自动伸缩。
- en: What is Kubernetes Node Autoscaler?
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Kubernetes节点自动伸缩？
- en: As new workloads and pods are deployed, all the cluster worker nodes' resources
    can be exhausted. This will result in pods not being scheduled on existing workers.
    In some cases, pods can sit in a pending state, awaiting resource allocation and
    possibly causing an outage. Manually adding or removing worker nodes can, of course,
    solve this problem, as Cluster Autoscaler increases or decreases the size of a
    Kubernetes cluster based on pending pods and node utilization metrics.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着新工作负载和 Pod 的部署，所有集群工作节点的资源可能会被耗尽。这将导致 Pod 无法在现有的工作节点上调度。在某些情况下，Pod 会处于等待资源分配的挂起状态，可能会导致服务中断。当然，手动添加或移除工作节点可以解决这个问题，因为集群自动扩展器会根据挂起的
    Pod 和节点使用情况指标增加或减少 Kubernetes 集群的规模。
- en: Now that we know what a Node Autoscaler is, it's essential to know when it should
    or shouldn't be used, which we'll cover in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了节点自动扩展器是什么，了解它应该何时使用或不使用至关重要，我们将在下一部分详细讨论。
- en: When should you use a Kubernetes Node Autoscaler?
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候应该使用 Kubernetes 节点自动扩展器？
- en: 'There are two main reasons to set up a Node Autoscaler in Rancher/Kubernetes
    environments:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Rancher/Kubernetes 环境中，设置节点自动扩展器有两个主要原因：
- en: '**Cost controls/efficiency** – When moving workloads into the cloud, a big
    mistake many people make is to treat cloud VMs just like they treated on-prem
    VMs. What I mean by this is on-prem, if you provision an eight-core VM but are
    only really using four cores of resources, the cost in physical hardware is only
    the four cores that are actually in use. But in the cloud, for example, if you
    provision an eight-core VM in AWS, your bill will be the same if you use 100%
    of the CPU or zero. Because of this, we want to keep our nodes as close to 100%
    utilization as possible without impacting applications. The general rule of thumb
    is 80% CPU and 90% memory. This is because these are the default node pressure
    limits. Node autoscaling can allow you to add just enough nodes to your cluster
    to meet your needs when you need them. This is very helpful for workloads that
    vary throughout the day. For example, your application might just be very busy
    between 8 A.M. and 5 P.M. from Monday to Friday but have a very low utilization
    after hours. So autoscaling and spinning up your nodes early in the morning and
    spinning them down at night will help to cut costs.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本控制/效率** – 当将工作负载迁移到云端时，许多人常犯的一个大错误是将云端虚拟机（VM）当作本地虚拟机（VM）来对待。我的意思是，本地环境中，如果你配置了一个八核虚拟机，但实际上只使用了四个核心的资源，那么物理硬件的成本仅仅是实际使用的四个核心。但是在云端，例如，如果你在
    AWS 中配置了一个八核虚拟机，不管你是否使用了 100% 的 CPU，账单的费用都是一样的。因此，我们希望将节点的使用率保持尽可能接近 100%，同时不影响应用程序。一般的经验法则是
    CPU 使用率保持在 80%，内存使用率保持在 90%。这是因为这些是默认的节点压力限制。节点自动扩展能够在需要时，向集群中添加足够的节点以满足需求。这对全天变化的工作负载非常有帮助。例如，你的应用程序可能在周一至周五的
    8 点到 17 点非常繁忙，但在非工作时间的利用率非常低。因此，自动扩展和在早晨启动节点、晚上关闭节点将有助于减少成本。'
- en: '**Node patching/upgrading** – Node rehydration is one of the side effects of
    autoscaling your nodes. You have to create a process to easily add and remove
    nodes from your cluster, instead of patching/upgrading your nodes in place, which
    means you need to drain and cordon your nodes one at a time, then apply any OS
    patches and upgrades, then finally reboot the node. You need to wait for the node
    to come back online and uncordon the node. Then repeat this process for each node
    in the cluster. This, of course, requires scripting and automation along with
    checks and tests. With autoscaling, you just need to update the base VM image
    and verify its health. Then, just trigger a rolling update on the node pool. At
    this point, the autoscaling takes over.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点修补/升级** – 节点重启是自动扩展节点时的副作用之一。你必须创建一个流程来轻松地将节点添加到集群或从集群中移除，而不是在原地修补或升级节点。这意味着你需要一个一个地排空并隔离节点，然后应用操作系统补丁和升级，最后重启节点。你还需要等待节点重新上线并解除隔离。接着，重复这一过程，直到集群中的每个节点都完成。这当然需要脚本和自动化，以及检查和测试。使用自动扩展时，你只需要更新基础虚拟机镜像并验证其健康状况。然后，触发节点池的滚动更新。在此时，自动扩展将接管工作。'
- en: Next, let's learn about when to not use a Kubernetes Node Autoscaler.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解何时不应该使用 Kubernetes 节点自动扩展器。
- en: When should you not use a Kubernetes Node Autoscaler?
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候不应该使用 Kubernetes 节点自动扩展器？
- en: 'Node Autoscaler has some practical limitations and best practices that can
    cause instability in your cluster if not followed:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 节点自动扩展器有一些实际限制和最佳实践，如果不遵循，可能会导致集群不稳定：
- en: '**Kubernetes and OS matching** – Kubernetes is an ever-evolving platform, with
    new features and releases being released regularly. To ensure the best performance,
    ensure that you deploy the Kubernetes Cluster Autoscaler with the recommended
    version. To keep Kubernetes in lockstep with your OSs, you have to upgrade them
    regularly. For a list of recommended versions, visit Rancher''s support matrix
    at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 和操作系统匹配** – Kubernetes 是一个不断发展的平台，定期发布新功能和版本。为了确保最佳性能，请确保使用建议版本部署
    Kubernetes 集群自动扩展器。为了使 Kubernetes 与您的操作系统保持同步，您必须定期升级它们。有关建议版本的列表，请访问 Rancher
    的支持矩阵，网址为 [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)。'
- en: '**Your nodes must be the right size** – The Cluster Autoscaler will only function
    properly if your node pools have nodes of the same capacity. Among the reasons
    is the underlying assumption of the Cluster Autoscaler that each node in the node
    group has the same CPU and memory capacity. Autoscaling decisions are made based
    on the template nodes for each node group. Therefore, the best practice is to
    ensure that all nodes and instances in the instance group being autoscaled are
    the same type. This might not be the best approach for public cloud providers
    such as AWS, as diversification and availability factors dictate having multiple
    instance types.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您的节点必须是正确的大小** – 只有当节点池中的节点具有相同的容量时，集群自动扩展器才能正常运行。其中一个原因是集群自动扩展器的基本假设是节点组中的每个节点具有相同的
    CPU 和内存容量。自动缩放决策是基于每个节点组的模板节点进行的。因此，最佳做法是确保自动缩放的节点组中的所有节点和实例都是相同类型的。这对于像 AWS 这样的公共云提供商可能不是最佳方法，因为多样化和可用性因素决定了需要多种实例类型。'
- en: '`PodDisruptionBudget` flag prevents them from being drained. You can specify
    the disruption budgets by setting the `.spec.minAvailable` and `.spec.maxUnavailable`
    fields. As an absolute or a percentage value, `.spec.minAvailable` specifies the
    minimum number of pods available after the eviction. In the same way, `.spec.maxUnavailable`
    specifies the number of pods that will be unavailable after eviction, either as
    an absolute number or as a percentage.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PodDisruptionBudget` 标志阻止它们被排空。您可以通过设置 `.spec.minAvailable` 和 `.spec.maxUnavailable`
    字段来指定中断预算。`.spec.minAvailable` 作为绝对值或百分比值指定了驱逐后的最小可用 pod 数。同样，`.spec.maxUnavailable`
    指定了驱逐后将不可用的 pod 数，可以作为绝对数或百分比。'
- en: At this point, we have covered what node autoscaling is and why you would want
    to use it. In the next section, we will cover how to set up node autoscaling in
    Rancher.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了什么是节点自动缩放以及为什么要使用它。在下一节中，我们将介绍如何在 Rancher 中设置节点自动缩放。
- en: How to set up autoscaling with Rancher-managed clusters
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在 Rancher 管理的集群中设置自动扩展
- en: At present, Rancher supports only AWS Auto Scaling groups. The details can be
    found in Rancher's official documentation located at [https://rancher.com/docs/rancher/v2.5/en/cluster-admin/cluster-autoscaler/amazon/](https://rancher.com/docs/rancher/v2.5/en/cluster-admin/cluster-autoscaler/amazon/).
    It is crucial to note that autoscaling etcd and control plane nodes can be risky
    as removing and adding multiple management nodes simultaneously can cause cluster
    outages. Also, you must configure etcd S3 backups because the etcd backups are
    stored locally on the etcd nodes by default. This can cause you to lose your backups
    if you recycle your etcd nodes. Details on configuring S3 backups can be found
    at [https://rancher.com/docs/rancher/v2.5/en/cluster-admin/backing-up-etcd/](https://rancher.com/docs/rancher/v2.5/en/cluster-admin/backing-up-etcd/).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Rancher 仅支持 AWS 自动扩展组。详细信息可在 Rancher 的官方文档中找到，位于 [https://rancher.com/docs/rancher/v2.5/zh/cluster-admin/cluster-autoscaler/amazon/](https://rancher.com/docs/rancher/v2.5/zh/cluster-admin/cluster-autoscaler/amazon/)。需要注意的是，自动扩展
    etcd 和控制平面节点可能存在风险，因为同时删除和添加多个管理节点可能会导致集群故障。此外，必须配置 etcd 的 S3 备份，因为默认情况下 etcd
    备份存储在本地 etcd 节点上，这可能导致在回收 etcd 节点时丢失备份。有关配置 S3 备份的详细信息，请访问 [https://rancher.com/docs/rancher/v2.5/zh/cluster-admin/backing-up-etcd/](https://rancher.com/docs/rancher/v2.5/zh/cluster-admin/backing-up-etcd/)。
- en: How to set up autoscaling with hosted clusters
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在托管集群中设置自动扩展
- en: Rancher does not focus on autoscaling clusters for hosted clusters, but all
    Kubernetes providers support Cluster Autoscaler. Visit [https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment)
    for the list of supported providers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 并不专注于托管集群的自动扩展，但所有 Kubernetes 提供商都支持 Cluster Autoscaler。请访问 [https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment)
    查看支持的提供商列表。
- en: At this point, you should be able to autoscale your Rancher-managed and hosted
    clusters, allowing you to add and remove nodes from your cluster with ease.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你应该能够对你的 Rancher 管理和托管的集群进行自动扩展，从而轻松地向集群中添加和移除节点。
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter went over the three main ways of scaling your Kubernetes cluster:
    with HPA, VPA, and Cluster Autoscaler. For the HPA, we dove into how it works
    and when it should be used to scale your workloads by adding and removing pods.
    We then covered how VPAs are like HPAs but are used to add and remove resources
    from pods, closing out the chapter by covering Cluster Autoscalers for adding
    and removing nodes from the cluster, including the different autoscalers and when
    it makes sense to use them.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了扩展 Kubernetes 集群的三种主要方式：使用 HPA、VPA 和 Cluster Autoscaler。对于 HPA，我们深入探讨了它的工作原理以及在何时应使用它来通过添加和移除
    Pods 来扩展工作负载。接着我们讲解了 VPA，它类似于 HPA，但用于向 Pods 添加和移除资源，最后通过讲解 Cluster Autoscalers
    来总结本章内容，Cluster Autoscalers 用于向集群添加和移除节点，介绍了不同的自动扩展器以及何时使用它们。
- en: In the next chapter, we'll cover the topics of load balancers and SSL certificates,
    which are very important for publishing our applications to the outside world.
    And in that chapter, we'll be covering the different technicalities for accomplishing
    this task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论负载均衡器和 SSL 证书，这对于将我们的应用程序发布到外部世界非常重要。在那一章中，我们将介绍实现这一任务的不同技术细节。
