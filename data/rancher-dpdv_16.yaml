- en: '*Chapter 12*: Security and Compliance Using OPA Gatekeeper'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：使用 OPA Gatekeeper 进行安全性和合规性管理'
- en: In this chapter, we'll cover bringing security and compliance to our Kubernetes
    clusters using **OPA Gatekeeper** and why it is needed to manage a cluster at
    scale. (OPA stands for **Open Policy Agent**.) With so many different teams deploying
    their applications on your clusters, enforcing standards in your environment (for
    example, blocking public image registries and blocking deployments that don't
    follow the rules, such as setting CPU and memory limits on Pods) becomes extremely
    hard. We'll also cover Rancher's **Center for Internet Security** (**CIS**) scanner,
    which is required to scan a Kubernetes cluster for known vulnerabilities, along
    with Rancher's hardening guides applying changes to RKE and RKE2 clusters that
    enforce extra security standards as defined in the CIS benchmark. We'll also look
    at how to maintain the cluster on an ongoing basis, and enterprise solutions such
    as NeuVector.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用**OPA Gatekeeper**将安全性和合规性引入我们的 Kubernetes 集群，并探讨它为何在大规模管理集群时至关重要。（OPA
    代表**开放策略代理**。）由于有这么多不同的团队在你的集群上部署应用程序，在你的环境中强制执行标准（例如，阻止公共镜像仓库、阻止不符合规则的部署，例如设置
    Pod 的 CPU 和内存限制）变得极其困难。我们还将讨论 Rancher 的**互联网安全中心**（**CIS**）扫描工具，该工具用于扫描 Kubernetes
    集群中的已知漏洞，以及 Rancher 的硬化指南，帮助应用对 RKE 和 RKE2 集群的更改，以执行 CIS 基准中定义的额外安全标准。我们还将探讨如何持续维护集群，以及像
    NeuVector 这样的企业级解决方案。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Why should I care about security in Kubernetes?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我需要关注 Kubernetes 中的安全问题？
- en: How do I enforce standards and security policies in Kubernetes?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何在 Kubernetes 中强制执行标准和安全策略？
- en: What is OPA Gatekeeper?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 OPA Gatekeeper？
- en: How do I install OPA Gatekeeper from the marketplace?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何从市场中安装 OPA Gatekeeper？
- en: Best practices and standard policies.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳实践和标准政策。
- en: How do I scan my cluster for security issues?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何扫描我的集群以检查安全问题？
- en: How do I lock down my cluster?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何锁定我的集群？
- en: Deploying the Rancher CIS scan.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署 Rancher CIS 扫描。
- en: Additional security tools for protecting a cluster.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于保护集群的额外安全工具。
- en: Why should I care about security in Kubernetes?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我需要关注 Kubernetes 中的安全问题？
- en: One of the questions I get asked a lot is *Why should I care about security?
    Doesn't containerization fix this?* The short answer is no. Containerization does
    not solve every IT security problem, but it does change the game if we look back
    at how we traditionally handled security with servers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常被问到一个问题：*为什么我需要关注安全问题？容器化不是已经解决了这个问题吗？* 简短的回答是，不是的。容器化并不能解决所有 IT 安全问题，但如果回顾我们传统上如何处理服务器安全问题，它确实改变了游戏规则。
- en: First, we would deploy **antivirus** (**AV**) software on all our servers to
    detect and block malware, worms, and Trojan horses, for example, from attacking
    our servers and stealing our data. Now, containerization throws a big wrench into
    the works because we virtualize our applications inside another server, and most
    AV software does not understand nor support running on a server with Docker. This
    is even discussed in the official Docker documentation at [https://docs.docker.com/engine/security/antivirus/](https://docs.docker.com/engine/security/antivirus/)
    that recommends excluding the Docker process and its directories from being scanned.
    In addition, most of the popular AV vendors, such as Symantec Endpoint Protection,
    block Docker from running altogether. It is important to note that security software,
    such as Aqua, supports running AV scans at the host level. We'll talk more about
    this topic later in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在所有服务器上部署**杀毒软件**（**AV**），以检测和阻止恶意软件、蠕虫和特洛伊木马等攻击我们的服务器并窃取数据。现在，容器化给这一过程带来了很大挑战，因为我们在另一个服务器中虚拟化了应用程序，而大多数杀毒软件无法理解或支持在
    Docker 环境中运行。官方的 Docker 文档中也讨论了这一点，指出推荐将 Docker 进程及其目录排除在扫描之外，网址是[https://docs.docker.com/engine/security/antivirus/](https://docs.docker.com/engine/security/antivirus/)。此外，大多数流行的杀毒软件供应商，例如赛门铁克（Symantec
    Endpoint Protection），会完全阻止 Docker 的运行。值得注意的是，像 Aqua 这样的安全软件支持在主机级别运行 AV 扫描。我们将在本章后续部分深入讨论这一话题。
- en: Second, we would create firewall rules between our servers, giving only the
    bare minimum access. For example, you might only allow a **Secure Shell** (**SSH**)
    protocol from a limited number of jump servers so that if someone compromised
    a public-facing web server and gained remote access, they wouldn't be able to
    SSH into other servers, such as the database server. Kubernetes changed this because
    most **Container Network Interface** (**CNI**) providers, such as Canal, Calico,
    and Weave, are open by default, meaning any pod in the cluster can directly connect
    to any other pod in the cluster. This means if someone compromised the pod running
    your web server, they could now start directly attacking other pods in the same
    cluster. It is essential to note that Kubernetes has NetworkPolicies that allow
    you to bring firewall-like rules into your cluster. However, not all CNI providers
    support NetworkPolicies. You can find a list of the different CNIs that Rancher
    supports located at https://rancher.com/docs/rancher/v2.6/en/faq/networking/cni-providers/#cni-features-by-provider.
    This table includes the ones that support the NetworkPolicies resource type as
    not all CNI providers support this feature.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们会在服务器之间创建防火墙规则，只提供最基本的访问权限。例如，你可能只允许从有限数量的跳板服务器访问**安全外壳**（**SSH**）协议，这样，如果某人攻破了一个面向公众的
    Web 服务器并获得了远程访问权限，他们将无法通过 SSH 进入其他服务器，比如数据库服务器。Kubernetes 改变了这一点，因为大多数**容器网络接口**（**CNI**）提供商，如
    Canal、Calico 和 Weave，默认是开放的，这意味着集群中的任何 Pod 都可以直接连接到集群中的任何其他 Pod。这意味着，如果某人攻破了运行
    Web 服务器的 Pod，他们现在可以直接攻击同一集群中的其他 Pod。需要注意的是，Kubernetes 有网络策略（NetworkPolicies），允许你将类似防火墙的规则引入集群。然而，并不是所有的
    CNI 提供商都支持网络策略。你可以在 https://rancher.com/docs/rancher/v2.6/en/faq/networking/cni-providers/#cni-features-by-provider
    查找 Rancher 支持的不同 CNI 提供商的列表。此表格包括支持 NetworkPolicies 资源类型的提供商，因为并非所有 CNI 提供商都支持此功能。
- en: Third is control over what software is allowed to be installed in your environment.
    Most enterprise environments do not allow application teams to directly access
    production servers. They require application teams to document how to install
    their application and how this process is being reviewed by outside teams such
    as security and compliance. This was mainly to prevent application teams from
    making changes to the server that could be a security issue. For example, without
    controls in place, an application team might just disable the AV software instead
    of going through the process of whitelisting their processes and fixing security
    issues with their application. In addition, a team might be using outdated software,
    such as Java, that has known vulnerabilities, with most security teams blocking
    this. Containerization flips the script because an application team builds their
    images; that is, if needed, they can install the software and libraries they want,
    including outdated and vulnerable software.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第三是对环境中允许安装的软件的控制。大多数企业环境不允许应用团队直接访问生产服务器。他们要求应用团队记录如何安装他们的应用程序，以及这一过程如何经过外部团队（如安全和合规团队）的审查。这主要是为了防止应用团队对服务器进行可能引发安全问题的更改。例如，如果没有控制措施，应用团队可能会直接禁用防病毒软件，而不是通过将他们的进程列入白名单并解决应用程序的安全问题来解决。除此之外，某些团队可能会使用过时的软件，例如有已知漏洞的
    Java，且大多数安全团队会对此进行拦截。容器化改变了这种情况，因为应用团队自己构建镜像；也就是说，如果需要，他们可以安装自己想要的软件和库，包括过时和有漏洞的软件。
- en: Fourth, we come to patching. Most translational server environments have regular
    patching schedules, such as applying OS patches every month. Because of this,
    known vulnerabilities are regularly removed from your environment. Containerization,
    by default, makes a task such as monthly patching not a thing. One of the core
    concepts with containers is that your images should be static and can only change
    with a re-deployment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，我们来谈谈补丁管理。大多数企业服务器环境都有定期的补丁计划，例如每月应用操作系统补丁。因此，已知的漏洞会定期从你的环境中移除。而容器化默认情况下使得像每月打补丁这样的任务不再是必要的。容器的一个核心概念是，你的镜像应该是静态的，只有在重新部署时才能更改。
- en: For example, an application team deploys a pod running a currently patched Ubuntu-based
    image. They then leave it untouched for six months. Now, the Ubuntu-based image
    is out of date and needs to be updated. You can, of course, connect to the pod
    and run `apply patches` by running `apt upgrade` but your changes will be wiped
    out as soon as that pod gets rescheduled. You need to rely on application teams
    to keep the images up to date by redeploying the application on a set schedule,
    and we all know how that will work out, as they don't want to change anything
    if it's working. It's important to note that this problem is solved by adding
    image scanning to your pipeline, and we'll be covering this topic later in this
    chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个应用团队部署了一个运行当前补丁的基于 Ubuntu 的镜像的 pod。然后他们将其保持不变长达六个月。现在，基于 Ubuntu 的镜像已过时，需要更新。当然，你可以连接到
    pod 并运行 `apt upgrade` 来应用补丁，但只要该 pod 被重新调度，你的更改就会被清除。你需要依赖应用团队通过按计划重新部署应用来保持镜像的更新，我们都知道这将如何进行，因为如果一切正常，他们不愿意更改任何内容。重要的是要注意，通过将图像扫描添加到您的流水线中，可以解决这个问题，我们将在本章后面讨论这个主题。
- en: Last, but not least, is limiting access in a traditional enterprise environment.
    Access to production servers is limited, with application teams not being allowed
    access. But, with Kubernetes, it is typical for organizations to get started on
    their Kubernetes journey to give developers access to the production clusters
    for speed and efficiency.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，在传统企业环境中限制访问。生产服务器的访问受到限制，应用团队不被允许访问。但是，对于 Kubernetes 来说，组织通常会开始其
    Kubernetes 之旅，以便为开发人员提供对生产集群的访问权限，以提高速度和效率。
- en: In the next section, we'll dive into how we start solving these issues, mainly
    focusing on OPA Gatekeeper and Rancher CIS scans.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将深入探讨如何开始解决这些问题，主要集中在 OPA Gatekeeper 和 Rancher CIS 扫描上。
- en: How do I enforce standards and security policies in Kubernetes?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我如何在 Kubernetes 中强制执行标准和安全策略？
- en: Of course, now that we know about all these security issues/limitations, we
    need to ask what can we do about them?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，既然我们现在知道所有这些安全问题和限制，我们需要问的是我们能做什么？
- en: First is the AV software issue, with the question being, why do we need AV software
    in the first place? We need it because we need to detect when rogue software runs
    in our environment. For example, a hacker compromises a web server by finding
    a vulnerability that gives them **Remote Code Execution** (**RCE**), also known
    as **Arbitrary Code Execution** (**ACE**). Then, the hacker will start installing
    a **Remote Access Tool** (**RAT**) to start moving laterally to other servers,
    trying to gain more access. Containerization addresses this issue by shrinking
    the attack surface. For example, it's tough to get a remote shell on a server
    if it doesn't have a shell. This is why it's expected that most container images
    contain only the absolute minimum software packages and libraries. Kubernetes
    also addresses this issue by running containers with non-privileged accounts.
    For example, we might run Apache as a non-root user. So, even if someone gains
    access to the pod, they wouldn't have permission to install additional software
    and libraries. This also includes if they were to find a way to break out of the
    container, they would still be running as an account with little to no permissions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是 AV 软件问题，问题是，为什么我们首先需要 AV 软件？我们需要它是因为我们需要检测恶意软件在我们的环境中运行的情况。例如，黑客通过找到一个漏洞来获取远程代码执行（**RCE**，也称为**ACE**），然后黑客将开始安装远程访问工具（**RAT**）以开始横向移动到其他服务器，试图获取更多访问权限。容器化通过缩小攻击面来解决这个问题。例如，如果服务器没有
    shell，那么很难在其上获取远程 shell。这就是为什么大多数容器镜像只包含绝对必要的软件包和库。Kubernetes 也通过以非特权帐户运行容器来解决此问题。例如，我们可能将
    Apache 作为非 root 用户运行。因此，即使有人获取了对 pod 的访问权限，他们也没有权限安装额外的软件和库。这也包括如果他们找到一种方法来打破容器的限制，他们仍然将作为几乎没有权限的帐户运行。
- en: The second is firewall rules, as discussed in the previous section. Kubernetes,
    by default, is open between pods but because you need to expose ports and services
    to the outside world explicitly, you get the benefit of being secure to the outside
    world by default. For example, if we are running an Apache server as a pod in
    our cluster, by default, we are not opening that pod directly to the world. But
    still, you are exposing it via an ingress-controller where you can enable security
    settings such as **ModSecurity**, which is a **Web Application Firewall** (**WAF**)
    that can protect you from cross-site scripting, SQL injections, and sessions.
    You can find out more about ModSecurity by visiting [https://github.com/SpiderLabs/ModSecurity](https://github.com/SpiderLabs/ModSecurity),
    and you can find the details around enabling ModSecurity with Ingress NGINX located
    at https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#modsecurity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是防火墙规则，正如前一节所讨论的。Kubernetes默认情况下在Pod之间是开放的，但因为你需要显式地将端口和服务暴露给外部，所以你默认能够获得对外部世界的安全保护。例如，如果我们在集群中以Pod的形式运行一个Apache服务器，默认情况下我们并没有直接将该Pod暴露给外部世界。但仍然，通过Ingress控制器，你可以启用安全设置，比如**ModSecurity**，这是一种**Web应用防火墙**（**WAF**），可以保护你免受跨站脚本、SQL注入和会话攻击等威胁。你可以通过访问[https://github.com/SpiderLabs/ModSecurity](https://github.com/SpiderLabs/ModSecurity)了解更多关于ModSecurity的信息，关于如何在Ingress
    NGINX中启用ModSecurity的详细信息，请访问https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#modsecurity。
- en: The third issue is control of the deployed software. We would manage the software
    on the endpoints, or servers, in translational environments. Containerization
    moved this process from the endpoint into your build pipeline. This is done by
    integrating software such as Clair into your build scripts. The idea with Clair
    is that you run your `docker build` command, then pass the image over to Clair,
    where it will be downloaded and scanned for known vulnerabilities inside the image.
    Clair will create a report listing the vulnerabilities, **CVE** (**Common Vulnerabilities
    and Exposures**) number, and severity level. Depending on your **continuous integration/continuous
    deployment** (**CI/CD**) software, you can choose to block a build if the image
    has too many vulnerabilities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个问题是已部署软件的控制。我们将管理端点或服务器上的软件，特别是在过渡环境中。容器化将这个过程从端点转移到了构建管道中。这是通过将诸如Clair之类的软件集成到构建脚本中实现的。Clair的想法是，在你运行`docker
    build`命令后，将镜像传递给Clair，Clair会下载并扫描镜像中的已知漏洞。Clair会生成一份报告，列出漏洞、**CVE**（**公共漏洞和暴露**）编号以及严重性等级。根据你的**持续集成/持续部署**（**CI/CD**）软件，你可以选择在镜像有过多漏洞时阻止构建。
- en: The fourth issue is application patching. Unfortunately, there isn't an easy
    way to solve this problem; the proper way is to regularly update your containers
    as part of your ongoing development process. But, as we know, regular patching
    of images can fall behind. So, what some people do is set up scheduled builds
    in their pipeline. For example, you might create a scheduled task to rebuild your
    application image every month using the current code versions and automatically
    deploy and test it in your environment. By doing this, you are *patching* your
    containers every month, just like we do with our servers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个问题是应用程序的补丁更新。不幸的是，这个问题没有简单的解决办法；正确的方法是定期更新你的容器，作为持续开发过程的一部分。但正如我们所知，定期修补镜像可能会滞后。因此，一些人做法是设置定期构建任务。例如，你可以创建一个定期任务，每月重新构建你的应用镜像，使用当前的代码版本并自动在你的环境中部署和测试。通过这种方式，你每个月都会对容器进行*补丁*更新，就像我们对服务器进行更新一样。
- en: Finally, we have the issue of access. The easy way is to give everyone access
    to deploy into production and make any changes they want. But, this will provide
    you with unwanted problems and security, because it's always easier to just turn
    off protection when it's in your way than to fix the reason it's being blocked.
    To prevent this behavior, it is recommended to force all changes via a CI/CD pipeline
    to be tracked and controlled using tools such as GitHub/GitLab **pull request**
    (**PR**) approvals and not give anyone access to production outside cluster administrators.
    Rancher can address this issue using its Fleet product, which you can learn more
    about by visiting [https://fleet.rancher.io](https://fleet.rancher.io). In addition,
    as of Rancher v2.6, Fleet is built into Rancher itself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们面临访问问题。最简单的方式是给每个人访问权限，让他们可以部署到生产环境并随意修改。但这种做法会带来不必要的问题和安全隐患，因为当保护措施挡在路上时，人们通常更容易选择关掉保护，而不是解决被阻挡的原因。为了防止这种行为，建议强制通过
    CI/CD 管道进行所有更改，并使用如 GitHub/GitLab **拉取请求**（**PR**）审批来跟踪和控制，且不允许非集群管理员访问生产环境。Rancher
    可以通过其 Fleet 产品解决此问题，您可以通过访问 [https://fleet.rancher.io](https://fleet.rancher.io)
    了解更多信息。此外，从 Rancher v2.6 起，Fleet 已集成到 Rancher 本身中。
- en: At this point, we should know about the basic concepts around translating traditional
    IT security requirements into their Kubernetes counterparts. In the next section,
    we'll be diving into OPA Gatekeeper to enforce our clusters' security standards.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该了解如何将传统 IT 安全需求转化为 Kubernetes 对应项的基本概念。在下一部分，我们将深入了解 OPA Gatekeeper，以强制执行我们集群的安全标准。
- en: What is OPA Gatekeeper?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 OPA Gatekeeper？
- en: OPA is an open source policy engine that has been built for the cloud. In addition,
    OPA is a **Cloud Native Computing Foundation** (**CNCF**) graduated project like
    a number of the other tools we covered. OPA uses declarative language to enforce
    policies across your environment. The basic idea is everything should call OPA
    and ask, *Hey can I do XYZ?* At which point, OPA will evaluate the request against
    its policies to approve or reject the request. It is important to note that OPA
    is designed to be generic to be integrated with Kubernetes and other systems,
    such as Terraform, Docker, and SSH.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: OPA 是一个为云环境构建的开源策略引擎。此外，OPA 是一个 **云原生计算基金会**（**CNCF**）毕业项目，就像我们讨论过的其他工具一样。OPA
    使用声明式语言在环境中强制执行策略。基本思路是，所有内容都应调用 OPA 并询问，*嘿，我能做 XYZ 吗？* 这时，OPA 会根据其策略评估请求，决定是否批准或拒绝该请求。值得注意的是，OPA
    设计上是通用的，旨在与 Kubernetes 和其他系统（如 Terraform、Docker 和 SSH）集成。
- en: Of course, the question comes up, what is Gatekeeper then? In short, OPA Gatekeeper
    is a Kubernetes controller that allows you to define your OPA policies as Kubernetes
    objects. This mainly includes constraints and constraint templates. This enables
    users to define their policies as YAML and apply them to their cluster just like
    another Kubernetes deployment. For the rest of this chapter, we'll focus on OPA
    Gatekeeper.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，问题接踵而至，那么 Gatekeeper 是什么呢？简而言之，OPA Gatekeeper 是一个 Kubernetes 控制器，它允许你将 OPA
    策略定义为 Kubernetes 对象。这主要包括约束和约束模板。通过这种方式，用户可以将其策略定义为 YAML，并像其他 Kubernetes 部署一样将其应用于集群。接下来，我们将重点讨论
    OPA Gatekeeper。
- en: Now that we know what OPA Gatekeeper is, the next question to answer is *How
    does it work?* The best way to answer that question is to follow a request through
    the process. Please see the following diagram to understand this process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是 OPA Gatekeeper，接下来的问题是*它是如何工作的？* 最好的回答方式是通过一个请求流程来进行讲解。请参阅下图来理解这一过程。
- en: '![Figure 12.1 – OPA Gatekeeper request flow diagram'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.1 – OPA Gatekeeper 请求流程图'
- en: '](img/B18053_12_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_12_01.jpg)'
- en: Figure 12.1 – OPA Gatekeeper request flow diagram
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – OPA Gatekeeper 请求流程图
- en: 'Let''s look at these steps in detail next:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们详细看看这些步骤：
- en: '**Step 1**: A developer creates a request, and in this case, the request is
    for a new Deployment called **webserver** in the **testing** namespace.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第1步**：开发者创建请求，在这个例子中，请求是针对 **testing** 命名空间中的新部署 **webserver**。'
- en: '**Step 2**: The developer submits the request to **Kube-Apiserver** as a YAML
    or JSON file.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2步**：开发者将请求以 YAML 或 JSON 文件的形式提交给 **Kube-Apiserver**。'
- en: '**Step 3**: **Kube-Apiserver** receives the request and starts processing by
    verifying if the user has valid credentials, the request has valid syntax, and
    the user has permissions to access the requested resources. At this point, usually,
    **Kube-Apiserver** would respond to the developer accepting or denying the request.
    But, because we deployed OPA Gatekeeper to this cluster, the request follows a
    different path.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第 3 步**：**Kube-Apiserver** 接收请求并开始处理，验证用户是否具有有效凭据、请求语法是否有效，并且用户是否有权限访问请求的资源。在这一点上，通常
    **Kube-Apiserver** 会响应开发者，接受或拒绝请求。但由于我们已将 OPA Gatekeeper 部署到该集群，请求将走不同的路径。'
- en: '`ValidatingAdmissionWebhook` to route some requests to a defined webhook. It
    is important to note that not all requests are forwarded to the webhook. For example,
    a `kubectl get pods` request is a read-only request, so it would not be forwarded
    to the webhook. But, a request for creating a pod would be because it''s a change
    to the environment. In this case, OPA Gatekeeper has added `ValidatingAdmissionWebhook`
    into **Kube-Apiserver**.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ValidatingAdmissionWebhook` 用于将某些请求路由到定义好的 webhook。需要注意的是，并不是所有请求都会被转发到 webhook。例如，`kubectl
    get pods` 请求是只读请求，因此不会被转发到 webhook。但创建 pod 的请求会被转发，因为它是对环境的更改。在这种情况下，OPA Gatekeeper
    已将 `ValidatingAdmissionWebhook` 添加到 **Kube-Apiserver** 中。'
- en: '`billing-code`, which you can use for charge-back to your application teams.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`billing-code`，您可以用它向您的应用程序团队进行费用分摊。'
- en: '`billing-code` label, the creation will be blocked, and the rejection will
    be forwarded to the next step. But, assuming that the request was approved, OPA
    Gatekeeper will respond with an HTTP success code of `200`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`billing-code` 标签，创建将被阻止，拒绝将被转发到下一步骤。但假设请求已被批准，OPA Gatekeeper 将响应一个 HTTP 成功代码
    `200`。'
- en: '`fail close by default` by default. What this means is if the request is anything
    but `200`, it is assumed to be a denial, and the request will be rejected. Even
    valid deployments will be rejected if the OPA Gatekeeper pods are down or unavailable.
    This means your cluster is blocked, and no pod will spin up. So, Rancher''s OPA
    Gatekeeper deployment switches the default policy to `failurePolicy: Ignore`,
    which states if the webhook request receives an error such as a timeout. The Admission
    Controller will fail to open, which means that the controller will assume the
    request would have been approved if OPA Gatekeeper is ever offline. You should
    review this setting with your security team and confirm if availability is more
    important than potentially allowing a deployment that doesn''t meet the standards
    during an outage.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '默认情况下，`fail close by default`。这意味着如果请求的响应不是 `200`，则假定为拒绝，请求将被拒绝。即使是有效的部署，如果
    OPA Gatekeeper pod 宕机或不可用，也会被拒绝。这意味着您的集群被阻塞，无法启动任何 pod。所以，Rancher 的 OPA Gatekeeper
    部署将默认策略切换为 `failurePolicy: Ignore`，表示如果 webhook 请求收到错误（如超时），Admission Controller
    将无法打开，这意味着控制器会假设请求在 OPA Gatekeeper 离线时已被批准。您应该与您的安全团队一起审查此设置，并确认在故障期间是否更重要的是可用性，而不是可能允许不符合标准的部署。'
- en: Finally, at this point, the end user will receive a response to their request.
    Because of the steps listed previously, the user might see additional latency
    during a deployment. But, this usually is so quick that it becomes background
    noise for most environments. It is also important to note that this process applies
    to both end users and internal requests from other controllers inside the cluster,
    such as kube-scheduler.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此时，最终用户将收到其请求的响应。由于前面列出的步骤，用户在部署过程中可能会看到额外的延迟。但通常这非常快速，以至于大多数环境中它成为了背景噪音。还需要注意的是，这个过程适用于终端用户以及来自集群内部其他控制器的内部请求，例如
    kube-scheduler。
- en: At this point, you should understand how a request flows through kube-apiserver
    into the Admission Controller, then forwarded on to OPA Gatekeeper, then finally
    back to the developer via kube-apiserver. In the next section, we are going to
    dive into the process of installing OPA Gatekeeper in your cluster.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您应该理解请求是如何通过 kube-apiserver 流向 Admission Controller，然后转发到 OPA Gatekeeper，最后通过
    kube-apiserver 返回给开发者的。在下一节中，我们将深入探讨在集群中安装 OPA Gatekeeper 的过程。
- en: How to install OPA Gatekeeper from the marketplace
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从市场安装 OPA Gatekeeper
- en: With Rancher, there are two main ways to deploy OPA Gatekeeper, these being
    via the App marketplace in Rancher and via the Rancher Helm Chart. Of course,
    you can deploy upstream OPA Gatekeeper directly without Rancher. It is typically
    recommended to deploy OPA Gatekeeper via the App marketplace simply for convenience.
    The steps for installing OPA Gatekeeper are listed in this section for each of
    the major Rancher releases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Rancher 有两种主要方式可以部署 OPA Gatekeeper，分别是通过 Rancher 中的应用市场和通过 Rancher Helm Chart。当然，你也可以在没有
    Rancher 的情况下直接部署上游的 OPA Gatekeeper。通常建议通过应用市场来部署 OPA Gatekeeper，主要是为了方便。安装 OPA
    Gatekeeper 的步骤在本节中针对每个主要的 Rancher 版本进行列出。
- en: 'Before we discuss the installation steps, here''s a list of the prerequisites
    to have ready for the installation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论安装步骤之前，先列出一些安装所需的先决条件：
- en: You should have the global role Administrator or Cluster Owner permissions to
    the cluster.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该具有全局角色管理员或集群所有者权限。
- en: OPA Gatekeeper requires Kubernetes v1.16 or higher.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OPA Gatekeeper 需要 Kubernetes v1.16 或更高版本。
- en: You should review the official support matrix at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)
    to confirm that you are deploying a fully compatible and validated system solution.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该查看官方支持矩阵，网址是[https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)，以确认你部署的是完全兼容且验证过的系统解决方案。
- en: 'To install OPA Gatekeeper in Rancher v2.4, follow these steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Rancher v2.4 中安装 OPA Gatekeeper，请按照以下步骤操作：
- en: In the Rancher UI, go to the **Cluster** dashboard.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Rancher UI 中，进入**Cluster**仪表盘。
- en: 'Go to the **Tools** menu and select **OPA Gatekeeper** from the drop-down menu;
    refer to *Figure 12.2*:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入**Tools**菜单，并从下拉菜单中选择**OPA Gatekeeper**；参见*图 12.2*：
- en: '![Figure 12.2 – Rancher v2.4 installing OPA Gatekeeper from the Tools menu'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.2 – Rancher v2.4 从工具菜单安装 OPA Gatekeeper](img/B18053_12_06.jpg)'
- en: '](img/B18053_12_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_12_02.jpg)'
- en: Figure 12.2 – Rancher v2.4 installing OPA Gatekeeper from the Tools menu
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – Rancher v2.4 从工具菜单安装 OPA Gatekeeper
- en: It is normally recommended to use the default settings, which can be found at
    [https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/opa-gatekeeper/](https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/opa-gatekeeper/).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常建议使用默认设置，相关设置可以在[https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/opa-gatekeeper/](https://rancher.com/docs/rancher/v2.0-v2.4/en/cluster-admin/tools/opa-gatekeeper/)找到。
- en: '![Figure 12.3 – Rancher v2.4 OPA Gatekeeper install wizard'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.3 – Rancher v2.4 OPA Gatekeeper 安装向导](img/B18053_12_03.jpg)'
- en: '](img/B18053_12_03.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_12_03.jpg)'
- en: Figure 12.3 – Rancher v2.4 OPA Gatekeeper install wizard
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – Rancher v2.4 OPA Gatekeeper 安装向导
- en: 'Next, let''s look at the installation steps for Rancher v2.5:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看 Rancher v2.5 的安装步骤：
- en: In the Rancher UI, go to **Cluster Explorer**.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Rancher UI 中，进入**Cluster Explorer**。
- en: Click on the **Apps & Marketplace** option.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**Apps & Marketplace**选项。
- en: Select the **OPA Gatekeeper** chart.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**OPA Gatekeeper**图表。
- en: '![Figure 12.4 – Rancher v2.5 installing OPA Gatekeeper from Apps & Marketplace'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.4 – Rancher v2.5 从应用市场安装 OPA Gatekeeper](img/B18053_12_04.jpg)'
- en: '](img/B18053_12_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_12_04.jpg)'
- en: Figure 12.4 – Rancher v2.5 installing OPA Gatekeeper from Apps & Marketplace
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – Rancher v2.5 从应用市场安装 OPA Gatekeeper
- en: It is normally recommended to use the default settings, which can be found at
    [https://rancher.com/docs/rancher/v2.5/en/opa-gatekeper/](https://rancher.com/docs/rancher/v2.5/en/opa-gatekeper/).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常建议使用默认设置，相关设置可以在[https://rancher.com/docs/rancher/v2.5/en/opa-gatekeper/](https://rancher.com/docs/rancher/v2.5/en/opa-gatekeper/)找到。
- en: 'Now, we''ll discuss the installation for Rancher v2.6:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论 Rancher v2.6 的安装：
- en: In the Rancher UI, go to **Cluster Management**.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Rancher UI 中，进入**Cluster Management**。
- en: On the **Clusters** page, go to the cluster where you want to enable OPA Gatekeeper
    and click **Explore**.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**Clusters**页面，进入你希望启用 OPA Gatekeeper 的集群，并点击**Explore**。
- en: In the left navigation bar, click **Apps & Marketplace**.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航栏中，点击**Apps & Marketplace**。
- en: '![Figure 12.5 – Rancher v2.6 installing OPA Gatekeeper from Apps & Marketplace'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.5 – Rancher v2.6 从应用市场安装 OPA Gatekeeper](img/B18053_12_05.jpg)'
- en: '](img/B18053_12_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_12_05.jpg)'
- en: Figure 12.5 – Rancher v2.6 installing OPA Gatekeeper from Apps & Marketplace
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – Rancher v2.6 从应用市场安装 OPA Gatekeeper
- en: Click **Charts**, then click **OPA Gatekeeper** and click **Install**.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**Charts**，然后点击**OPA Gatekeeper**，再点击**Install**。
- en: '![Figure 12.6 – OPA Gatekeeper Install page'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.6 – OPA Gatekeeper 安装页面](img/B18053_12_05.jpg)'
- en: '](img/B18053_12_06.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_12_06.jpg)'
- en: Figure 12.6 – OPA Gatekeeper Install page
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 – OPA Gatekeeper 安装页面
- en: It is usually recommended to use the default settings, which can be found at
    [https://rancher.com/docs/rancher/v2.6/en/opa-gatekeper/](https://rancher.com/docs/rancher/v2.6/en/opa-gatekeper/).
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常建议使用默认设置，可以在 [https://rancher.com/docs/rancher/v2.6/en/opa-gatekeper/](https://rancher.com/docs/rancher/v2.6/en/opa-gatekeper/)
    找到。
- en: '![Figure 12.7 – Rancher v2.6 OPA Gatekeeper Install options'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.7 – Rancher v2.6 OPA Gatekeeper 安装选项'
- en: '](img/B18053_12_07.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_12_07.jpg)'
- en: Figure 12.7 – Rancher v2.6 OPA Gatekeeper Install options
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7 – Rancher v2.6 OPA Gatekeeper 安装选项
- en: At this point, we should have OPA Gatekeeper installed in our environment but
    without constraints, it won't do much. In the next section, we are going to cover
    some standard constraint templates that you can use to get started.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们应该已经在环境中安装了 OPA Gatekeeper，但没有约束，它并不会发挥太大作用。在接下来的部分，我们将介绍一些你可以用来入门的标准约束模板。
- en: Best practices and standard policies
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳实践和标准策略
- en: Now that we have OPA Gatekeeper installed, it's time to start creating templates
    and applying them to your cluster. In this section, we are going to cover some
    of the most popular templates. It is important to note that most of these templates
    come from upstream OPA Gatekeeper.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 OPA Gatekeeper，是时候开始创建模板并将其应用到你的集群中了。在本节中，我们将讨论一些最受欢迎的模板。需要注意的是，这些模板大多数来自上游的
    OPA Gatekeeper。
- en: I like to deploy the first one called `containerlimits`, a template rule that
    ensures that all pods have CPU and memory limits set. You can find the template
    at [https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/containerlimits](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/containerlimits).
    The main idea here is that, by default, all pods are given unlimited CPU and memory,
    meaning a single pod can steal all the resources on a node forcing out other pods
    and even causing nodes to lock up and then crash. This causes what I call a *runaway
    app* that will consume a node, cause it to crash, then move to another node and
    repeat the process until you run out of nodes. You can find an example of this
    kind of application at [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢部署第一个叫做`containerlimits`的模板规则，它确保所有的 pod 都设置了 CPU 和内存限制。你可以在 [https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/containerlimits](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/containerlimits)
    找到该模板。这里的主要思想是，默认情况下，所有 pod 都没有 CPU 和内存限制，这意味着单个 pod 可以占用节点上所有的资源，强行将其他 pod 挤出，甚至导致节点卡死然后崩溃。这就造成了我所说的
    *失控应用*，它会消耗一个节点，导致该节点崩溃，然后转移到另一个节点，重复这个过程，直到节点用尽。你可以在 [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app)
    找到这种应用的示例。
- en: 'This link also includes how to resolve a runaway app issue. This template makes
    sure that it''s set to a valid value. It could be 1 MB of memory, meaning the
    pods will never be scheduled, or it could be 100 TB, meaning the pod will never
    be scheduled because there is no node big enough for it to start. This also has
    the benefit of giving you insight into the capacity required for your clusters,
    because if you see that your cluster is reporting 100% allocation, it means one
    of two things:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该链接还包括如何解决“失控应用”问题。这个模板确保它被设置为一个有效的值。它可能是 1 MB 的内存，意味着 pod 永远不会被调度，或者它可能是 100
    TB，意味着 pod 永远无法被调度，因为没有足够大的节点可以启动它。这个方法还有一个好处，就是能够让你洞察集群所需的容量，因为如果你看到集群报告了 100%
    的资源分配，意味着两种情况中的一种：
- en: Your limits are being set too high, and you are wasting resources.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的限制设置得太高，浪费了资源。
- en: You need to start adding nodes, because you will run out of resources.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要开始添加节点，因为你将用完资源。
- en: OPA Gatekeeper can protect us from runaway apps by blocking pods/deployments
    that are missing CPU and memory limits, which means a runaway app can never make
    it into the cluster in the first place.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: OPA Gatekeeper 可以通过阻止缺少 CPU 和内存限制的 pod/deployment 来保护我们免受失控应用的影响，这意味着失控应用根本无法进入集群。
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We'll be covering scaling in the next chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章讨论扩展问题。
- en: 'To apply this template, simply run the following command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这个模板，只需运行以下命令：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But, make sure you test this in a non-production cluster first.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请确保首先在非生产集群中进行测试。
- en: Another template I like to use is called `requiredlabels`, which is a template
    that allows you to force all namespaces to have a label. For example, you might
    want to move all namespaces to have a billing code so you can do show-back and
    charge-back. Or maybe, you want to force the namespaces to have a technical contact
    listed to make it easier to contact the application team when deployments are
    misbehaving.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我喜欢使用的模板叫做`requiredlabels`，它是一个强制所有命名空间都有标签的模板。例如，你可能希望将所有命名空间都设置一个账单代码，以便进行展现费用和收费回退。或者，也许你希望强制命名空间中列出技术联系人，以便在部署出现问题时，方便联系应用团队。
- en: 'To apply this template, simply run the following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这个模板，只需运行以下命令：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This will only apply to newly created namespaces, so you will have to go back
    and set the label for existing namespaces.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这只会应用于新创建的命名空间，因此你需要回去为现有命名空间设置标签。
- en: Full details about this template can be found at [https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/requiredlabels](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/requiredlabels).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此模板的完整详细信息，请访问[https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/requiredlabels](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/requiredlabels)。
- en: Finally, the template that I see many security teams asking for is `httpsonly`,
    which forces all ingresses to have the `kubernetes.io/ingress.allow-http = false`
    annotation, meaning all ingresses must have an SSL certificated defined in the
    ingress configuration under the **TLS** (**Transport Layer Security**) section.
    Many security teams require all traffic, including backend services, to be HTTPS
    only.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我看到许多安全团队要求使用的模板是`httpsonly`，它强制所有 ingress 必须具有`kubernetes.io/ingress.allow-http
    = false`注解，这意味着所有 ingress 必须在 ingress 配置中的**TLS**（**传输层安全性**）部分定义 SSL 证书。许多安全团队要求所有流量，包括后端服务，必须仅支持
    HTTPS。
- en: 'To apply this template, simply run the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这个模板，只需运行以下命令：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Full details about this template can be found at [https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/httpsonly](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/httpsonly).
    It is important to note that, by default, Rancher clusters come with a self-signed
    certificate for ingresses to default to when another certificate is not defined.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此模板的完整详细信息，请访问[https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/httpsonly](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/httpsonly)。需要注意的是，默认情况下，Rancher
    集群会为 ingress 配置自签名证书，作为在未定义其他证书时的默认证书。
- en: There are, of course, more templates that the open source community has created,
    which can be found at [https://github.com/open-policy-agent/gatekeeper-library](https://github.com/open-policy-agent/gatekeeper-library).
    I also recommend using these templates as a base for creating your custom templates.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，开源社区还创建了更多模板，可以在[https://github.com/open-policy-agent/gatekeeper-library](https://github.com/open-policy-agent/gatekeeper-library)找到。我还建议使用这些模板作为创建自定义模板的基础。
- en: At this point, you should have OPA Gatekeeper installed in your cluster and
    some rules defined, allowing you to enforce your standards. In the next section,
    we'll be diving into taking the next step of locking down your cluster to prevent
    security-related issues.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你应该已经在集群中安装了 OPA Gatekeeper，并定义了一些规则，允许你执行标准。在接下来的部分，我们将深入探讨如何采取下一步措施，通过锁定集群来防止与安全相关的问题。
- en: How do I scan my cluster for security issues?
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我如何扫描集群中的安全问题？
- en: Rancher has a Rancher CIS scan or Rancher-cis-benchmark tool built on top of
    kube-bench, an open source software from Aqua Security. kube-bench is designed
    to scan a Kubernetes cluster and review the settings on the components of Kubernetes
    such as kube-apiserver, etcd, and kubelet. kube-bench uses the CIS Benchmark report
    from the non-profit organization CIS, which creates a standard list of best practice
    settings for protecting your Kubernetes cluster. These reports are made after
    the most significant changes to Kubernetes and are designed to be as vendor-neutral
    as possible. You can learn more about CIS and its reports by going to [https://learn.cisecurity.org](https://learn.cisecurity.org).
    But, because this report is intended to be vendor-neutral, some settings don't
    apply to Rancher and its cluster. So, Rancher publishes a self-assessment and
    hardening guide that addresses all the items in the report. This assessment is
    designed so that it can be handed over to your security team/auditors when they
    start asking questions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher有一个基于kube-bench的Rancher CIS扫描或Rancher-cis-benchmark工具，kube-bench是Aqua
    Security提供的开源软件。kube-bench旨在扫描Kubernetes集群，并检查Kubernetes组件（如kube-apiserver、etcd和kubelet）的设置。kube-bench使用非营利组织CIS的CIS基准报告，CIS创建了一份最佳实践设置的标准列表，用于保护你的Kubernetes集群。这些报告是在Kubernetes进行重大更改后发布的，旨在尽可能保持厂商中立。你可以通过访问[https://learn.cisecurity.org](https://learn.cisecurity.org)来了解更多关于CIS及其报告的信息。但是，由于该报告旨在保持厂商中立，某些设置不适用于Rancher及其集群。因此，Rancher发布了一份自我评估和加固指南，解决报告中的所有项目。此评估设计为可以交给你的安全团队/审计员，当他们开始提问时使用。
- en: But, it is important to note that, by default, Rancher and its cluster will
    not pass the CIS report without making changes to your environment and nodes,
    with some of these steps requiring manual steps. In the next section, we'll be
    diving into how to make Rancher pass the report.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，重要的是要注意，默认情况下，Rancher及其集群将无法通过CIS报告，除非对你的环境和节点进行更改，其中一些步骤需要手动操作。在下一节中，我们将深入探讨如何使Rancher通过报告。
- en: You can find Rancher's reports and assessment at [https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/](https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/).
    It is important to note that these guides change over time as Kubernetes and Rancher
    are upgraded, and additionally, as security issues are found; thus, it is recommended
    to review these assessments on a scheduled basis.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/](https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/)找到Rancher的报告和评估。需要注意的是，随着Kubernetes和Rancher的升级，以及发现安全问题，这些指南会不断变化，因此建议定期审查这些评估。
- en: How do I lock down my cluster?
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何锁定我的集群？
- en: 'In the previous section, we talked about the CIS scan and Rancher''s self-assessments,
    but of course, the question of *What can I do about this report?* comes up, and
    Rancher''s answer to this question is what Rancher calls its **hardening guides**.
    These guides cover the three Kubernetes distributions that Rancher owns: RKE,
    RKE2, and k3s. We won''t be going into too much detail in this section, as Rancher
    already has this process documented. Here, we''ll be linking to guides for each
    cluster type.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了CIS扫描和Rancher的自我评估，但当然，*我可以做些什么来处理这个报告？*这个问题会随之而来，Rancher对此问题的回答就是Rancher所称的**加固指南**。这些指南涵盖了Rancher拥有的三种Kubernetes发行版：RKE、RKE2和k3s。我们在本节中不会深入细节，因为Rancher已经有了相关文档。在这里，我们将提供每种集群类型的指南链接。
- en: 'For RKE clusters, the hardening guide is tied to the Rancher server and Kubernetes
    version. The following are the high-level steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RKE集群，加固指南与Rancher服务器和Kubernetes版本相关联。以下是高层次的步骤：
- en: Configuring the Linux kernel to safely handle **Out-Of-Memory** (**OOM**) and
    kernel panics.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置Linux内核以安全处理**内存不足**（**OOM**）和内核崩溃。
- en: Creating a local user and group to be used by etcd to isolate the database from
    other processes to protect the data.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个本地用户和组供etcd使用，以将数据库与其他进程隔离，从而保护数据。
- en: Disabling the default service accounts being mounted to every pod.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用默认的服务帐户挂载到每个pod上。
- en: Enabling a `NetworkPolicy` default to limit all pod-to-pod traffic, requiring
    that you open rules as you need them.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`NetworkPolicy`默认限制所有pod之间的流量，要求你根据需要打开规则。
- en: Turning on secret encryption wherein, by default, secrets are stored in etcd
    in plain text, but with etcd encryption enabled, the secrets will be encrypted
    at rest.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开启秘密加密功能，默认情况下，秘密以明文存储在etcd中，但启用etcd加密后，秘密将被加密存储。
- en: Finally, ending by turning on `PodSecurityPolicy` to limit pod security settings,
    such as blocking pods from running as root.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过启用`PodSecurityPolicy`来限制Pod的安全设置，例如禁止Pod以root身份运行。
- en: The complete guide can be found at [https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/1.6-hardening-2.5/](https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/1.6-hardening-2.5/).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的指南可以在[https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/1.6-hardening-2.5/](https://rancher.com/docs/rancher/v2.5/en/security/rancher-2.5/1.6-hardening-2.5/)中找到。
- en: 'This process is much easier for RKE2 clusters, as RKE2 is secure by default,
    but you still need to apply a few changes, which are listed here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RKE2集群，这个过程要容易得多，因为RKE2默认是安全的，但您仍然需要应用一些更改，具体如下：
- en: Setting the same Linux kernel parameters as RKE.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与RKE相同的Linux内核参数。
- en: Creating an etcd user.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个etcd用户。
- en: 'Finally, we just need to turn on the CIS profile that we would like on the
    master servers by adding the `profile: cis-1.5/6` option.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最后，我们只需要通过添加`profile: cis-1.5/6`选项，在主服务器上启用我们想要的CIS配置文件。'
- en: The complete guide can be found at [https://docs.rke2.io/security/hardening_guide/](https://docs.rke2.io/security/hardening_guide/).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的指南可以在[https://docs.rke2.io/security/hardening_guide/](https://docs.rke2.io/security/hardening_guide/)中找到。
- en: 'The hardening process for k3s clusters is similar to RKE2 but still requires
    a few extra steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于k3s集群的硬化过程与RKE2类似，但仍然需要一些额外的步骤：
- en: Setting the same Linux kernel parameters as RKE2
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置与RKE2相同的Linux内核参数
- en: Skipping the etcd steps, as k3s doesn't use etcd by default
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过etcd步骤，因为k3s默认不使用etcd
- en: Turning on `PodSecurityPolicy` to limit pod security settings, such as blocking
    pods from running as root
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`PodSecurityPolicy`来限制Pod的安全设置，例如禁止Pod以root身份运行
- en: It is important to note that k3s doesn't have the CIS profiles like RKE2 and
    is not secure by default as it's designed to be lightweight and fast.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，k3s不像RKE2那样具有CIS配置文件，并且默认情况下不安全，因为它被设计为轻量级且快速。
- en: The complete guide can be found at [https://rancher.com/docs/k3s/latest/en/security/hardening_guide/](https://rancher.com/docs/k3s/latest/en/security/hardening_guide/).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的指南可以在[https://rancher.com/docs/k3s/latest/en/security/hardening_guide/](https://rancher.com/docs/k3s/latest/en/security/hardening_guide/)中找到。
- en: At this point, assuming you have followed the guides listed previously, you
    should be able to move to install the Rancher CIS scan and be able to pass it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，假设您已按照前面列出的指南操作，您应该能够开始安装Rancher CIS扫描并顺利通过它。
- en: Deploying Rancher CIS scan
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Rancher CIS扫描
- en: 'The only recommended way of installing Rancher CIS scan is via **Apps & Marketplace**
    inside Rancher. But first, let''s cover some of the basic requirements:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Rancher CIS扫描的唯一推荐方式是通过Rancher中的**应用和市场**。但首先，让我们了解一些基本要求：
- en: Rancher v2.4 or greater.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher v2.4或更高版本。
- en: Cluster owner permissions to the downstream cluster(s) or global role Administrator
    permissions.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群所有者权限到下游集群（或全局角色管理员权限）。
- en: The downstream cluster must be an RKE, RKE2, EKS, or GKE cluster for full support.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下游集群必须是RKE、RKE2、EKS或GKE集群才能完全支持。
- en: Note
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: A generic profile can be used for other clusters, but there might be false positives
    or other permissions issues.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于其他集群，可以使用通用配置文件，但可能会出现误报或其他权限问题。
- en: 'Here is how to install `rancher-cis-benchmark` with Rancher v2.4.x and v2.5.x
    via the Cluster Explorer:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在Rancher v2.4.x和v2.5.x中通过集群浏览器安装`rancher-cis-benchmark`的方法：
- en: In the Rancher UI, go to **Cluster Explorer**.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Rancher UI中，转到**集群浏览器**。
- en: Click **Apps**.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**应用**。
- en: Click **rancher-cis-benchmark**.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**rancher-cis-benchmark**。
- en: Click **Install**.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**安装**。
- en: 'To install the CIS Benchmark with Rancher v2.6.x via the **Cluster Management**
    pane, do the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**集群管理**面板在Rancher v2.6.x中安装CIS基准，操作如下：
- en: In the Rancher UI, go to **Cluster Management**.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Rancher UI中，转到**集群管理**。
- en: Navigate to the cluster where you will install the CIS Benchmark.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到您将安装CIS基准的集群。
- en: In the left navigation bar, click **Apps & Marketplace** | **Charts**.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航栏中，点击**应用和市场** | **Charts**。
- en: Click **CIS Benchmark**.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**CIS基准**。
- en: Click **Install**.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**安装**。
- en: Note
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For Rancher-created clusters, you should use the *default* settings. For generic
    clusters, please review the example settings at [https://github.com/rancher/cis-operator/tree/master/examples](https://github.com/rancher/cis-operator/tree/master/examples).
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于Rancher创建的集群，您应该使用*默认*设置。对于通用集群，请查看[https://github.com/rancher/cis-operator/tree/master/examples](https://github.com/rancher/cis-operator/tree/master/examples)中的示例设置。
- en: At this point, you should have your cluster locked down, and Rancher's CIS scan
    running in your cluster to confirm that the cluster stays locked over time. In
    the next section, we're going to dive into some additional tools for protecting
    your cluster, including some paid solutions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到此时，你应该已经锁定了集群，并且Rancher的CIS扫描正在你的集群中运行，以确保集群能够随着时间的推移保持安全。在接下来的部分，我们将深入探讨一些额外的工具来保护你的集群，包括一些付费解决方案。
- en: Additional security tools for protecting a cluster
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护集群的额外安全工具
- en: We have mostly been talking about OPA Gatekeeper in this chapter, as it is one
    of the most popular open source solutions. But, of course, there are other paid
    solutions, such as NeuVector, which provides image and cluster scanning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们主要讨论了OPA Gatekeeper，因为它是最受欢迎的开源解决方案之一。当然，也有其他付费解决方案，如NeuVector，它提供镜像和集群扫描。
- en: Note
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'With **SUSE** (German: **Software- und System-Entwicklung**) buying NeuVector,
    it has been announced that NeuVector''s container runtime security platform will
    be released to the public under the Apache 2.0 license on GitHub at [https://github.com/neuvector/neuvector](https://github.com/neuvector/neuvector).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 随着**SUSE**（德语：**Software- und System-Entwicklung**）收购NeuVector，已宣布NeuVector的容器运行时安全平台将在GitHub上以Apache
    2.0许可证公开发布，网址为[https://github.com/neuvector/neuvector](https://github.com/neuvector/neuvector)。
- en: In addition, the Aqua Platform is ubiquitous for paying customers, as it is
    a closed source product that requires licensing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Aqua平台对于付费客户来说无处不在，因为它是一个闭源产品，需要授权。
- en: For NeuVector, I recommend watching the master class *PCI Compliance and Vulnerability
    Management for Kubernetes* on YouTube ([https://www.youtube.com/watch?v=kSkX5MRmEkE](https://www.youtube.com/watch?v=kSkX5MRmEkE))
    to learn more about NeuVector and how to integrate it into Rancher.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NeuVector，我建议在YouTube上观看大师班*PCI合规性和Kubernetes漏洞管理*（[https://www.youtube.com/watch?v=kSkX5MRmEkE](https://www.youtube.com/watch?v=kSkX5MRmEkE)），以了解更多关于NeuVector的信息，并学习如何将其集成到Rancher中。
- en: 'I also recommend watching the master class *Kubernetes Master Class Preventive
    Security for Kubernetes Enterprise Deployments* for Aqua, found here: [https://www.youtube.com/watch?v=2Phk7yyWezU](https://www.youtube.com/watch?v=2Phk7yyWezU).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议观看关于Aqua的*Kubernetes大师班：Kubernetes企业部署的预防性安全*，视频地址为：[https://www.youtube.com/watch?v=2Phk7yyWezU](https://www.youtube.com/watch?v=2Phk7yyWezU)。
- en: Note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both the YouTube videos are hosted on Rancher's YouTube channel and include
    links to their slides and scripts.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这两部YouTube视频都托管在Rancher的YouTube频道上，并附有它们的幻灯片和脚本链接。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter taught us about some of the security and compliance issues that
    containers and Kubernetes have addressed. We went over what OPA Gatekeeper is
    and how it works. We then dove into some of the best practices and standard templates.
    We learned how OPA Gatekeeper enforces different rules for your cluster by deploying
    these templates. This is important because this skill is necessary to create your
    own rules that suit your environment and its requirements. We then covered how
    to lock down your cluster and ensure it stays locked down using Rancher CIS scans.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本章教会了我们容器和Kubernetes所解决的一些安全性和合规性问题。我们了解了什么是OPA Gatekeeper以及它如何工作。然后，我们深入探讨了一些最佳实践和标准模板。我们学习了OPA
    Gatekeeper如何通过部署这些模板来强制执行集群的不同规则。这一点非常重要，因为掌握这一技能是创建适合自己环境及其需求的规则所必需的。接着，我们介绍了如何锁定集群并确保通过Rancher
    CIS扫描保持锁定状态。
- en: The next chapter will cover how to bring scaling to your clusters for both pods
    and nodes, including all the limitations and concerns with scaling your clusters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍如何为集群的Pods和节点提供扩展功能，包括扩展集群时的所有限制和关注点。
