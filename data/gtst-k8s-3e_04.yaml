- en: Implementing Reliable Container-Native Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现可靠的容器原生应用程序
- en: This chapter will cover the various types of workloads that Kubernetes supports.
    We will cover deployments for applications that are regularly updated and long-running.
    We will also revisit the topics of application updates and gradual rollouts using
    Deployments. In addition, we will look at jobs used for short-running tasks. We
    will look at DaemonSets, which allow programs to be run on every node in our Kubernetes
    cluster. In case you noticed, we won't look into StatefulSets yet in this chapter
    but we'll investigate them in the next, when we look at store and how K8s helps
    you manage storage and stateful applications on your cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 Kubernetes 支持的各种类型的工作负载。我们将讨论适用于经常更新且长期运行的应用程序的部署。我们还将重新探讨应用程序更新和使用部署进行渐进式发布的话题。此外，我们将介绍用于短期任务的作业。我们还将研究
    DaemonSets，它允许程序在 Kubernetes 集群的每个节点上运行。如果你注意到的话，本章不会涉及 StatefulSets，但我们将在下一章讨论它们，届时我们将探讨存储以及
    Kubernetes 如何帮助你管理集群上的存储和有状态应用程序。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Deployments
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署
- en: Application scaling with Deployments
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用部署进行应用程序扩展
- en: Application updates with Deployments
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用部署进行应用程序更新
- en: Jobs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业
- en: DaemonSets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSets
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You'll need a running Kubernetes cluster like the one we created in the previous
    chapters. You'll also need access to deploy to that cluster through the `kubectl` command.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个正在运行的 Kubernetes 集群，就像我们在前几章中创建的那样。你还需要通过 `kubectl` 命令访问该集群并进行部署。
- en: Here's the GitHub repository for this chapter: [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的 GitHub 仓库链接：[https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04)
- en: How Kubernetes manages state
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 如何管理状态
- en: 'As discussed previously, we know that Kubernetes makes an effort to enforce
    the desired state of the operator in a given cluster. Deployments give operators
    the ability to define an end state and the mechanisms to effect change at a controlled
    rate of stateless services, such as microservices. Since Kubernetes is a control
    and data plane that manages the metadata, current status, and specification of
    a set of objects, Deployments provide a deeper level of control for your applications.
    There are a few archetypal deployment patterns that are available: recreate, rolling
    update, blue/green via selector, canary via replicas, and A/B via HTTP headers.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们知道 Kubernetes 努力在给定的集群中强制执行操作员期望的状态。部署使操作员能够定义一个最终状态，并通过控制速率的机制对无状态服务（如微服务）进行更改。由于
    Kubernetes 是一个管理元数据、当前状态和一组对象规范的控制平面和数据平面，部署为你的应用程序提供了更深层次的控制。可以使用几种典型的部署模式：重建、滚动更新、通过选择器的蓝绿发布、通过副本的金丝雀发布和通过
    HTTP 头的 A/B 测试。
- en: Deployments
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: In the previous chapter, we explored some of the core concepts for application
    updates using the old rolling-update method. Starting with version 1.2, Kubernetes
    added the **Deployment** construct, which improves on the basic mechanisms of
    rolling-update and **ReplicationControllers**. As the name suggests, it gives
    us finer control over the code deployment itself. Deployments allow us to pause
    and resume application rollouts via declarative definitions and updates to pods
    and **ReplicaSets. **Additionally, they keep a history of past deployments and
    allow the user to easily roll back to previous versions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了使用旧的滚动更新方法进行应用程序更新的一些核心概念。从版本 1.2 开始，Kubernetes 添加了 **Deployment**
    构造，它改进了滚动更新和 **ReplicationControllers** 的基本机制。顾名思义，它使我们能够对代码部署本身进行更精细的控制。部署允许我们通过声明性定义和对
    Pod 和 **ReplicaSets** 的更新来暂停和恢复应用程序的发布。此外，它们还会保留过去部署的历史记录，允许用户轻松回滚到先前的版本。
- en: It is no longer recommended to use ReplicationControllers. Instead, use a Deployment
    that configures a ReplicaSet in order to set up application availability for your
    stateless services or applications. Furthermore, do not directly manage the ReplicaSets
    that are created by your deployments; only do so through the Deployment API.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不再推荐使用 ReplicationControllers，而是使用配置 ReplicaSet 的 Deployment 来为无状态服务或应用程序设置应用可用性。此外，不要直接管理由部署创建的
    ReplicaSets；应仅通过 Deployment API 进行管理。
- en: Deployment use cases
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署的使用案例
- en: 'We''ll explore a number of typical scenarios for deployments in more detail:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地探讨一些部署的典型场景：
- en: Roll out a ReplicaSet
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署一个 ReplicaSet
- en: Update the state of a set of Pods
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新一组 Pod 的状态
- en: Roll back to an earlier version of a Deployment
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚到先前版本的部署
- en: Scale up to accommodate cluster load
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展以适应集群负载
- en: Pause and use Deployment status in order to make changes or indicate a stuck
    deployment
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暂停并使用 Deployment 状态来进行更改或指示一个卡住的部署
- en: Clean up a deployment
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理一个部署
- en: 'In the following code of the `node-js-deploy.yaml` file, we can see that the
    definition is very similar to a ReplicationController. The main difference is
    that we now have an ability to make changes and updates to the deployment objects
    and let Kubernetes manage updating the underlying pods and replicas for us:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下的 `node-js-deploy.yaml` 文件代码中，我们可以看到定义与 ReplicationController 非常相似。主要区别在于，我们现在可以对部署对象进行更改和更新，并让
    Kubernetes 管理底层 Pod 和副本的更新：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we've created a Deployment named `node-js-deploy` via the `name`
    field under `metadata`. We're creating a single pod that will be managed by the
    `selector` field, which is going to help the Deployment understand which pods
    to manage. The `spec` tells the pod to run the `jobbaier/pod-scaling` container
    and directs traffic through port `80` via the `containerPort`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们通过 `metadata` 下的 `name` 字段创建了一个名为 `node-js-deploy` 的 Deployment。我们正在创建一个由
    `selector` 字段管理的单个 Pod，`selector` 字段将帮助 Deployment 确定管理哪些 Pod。`spec` 指示该 Pod 运行
    `jobbaier/pod-scaling` 容器，并通过 `containerPort` 将流量引导到端口 `80`。
- en: We can run the familiar `create` command with the optional `--record` flag so
    that the creation of the Deployment is recorded in the rollout history. Otherwise,
    we will only see subsequent changes in the rollout history using the `$ kubectl
    create -f node-js-deploy.yaml --record` command.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用熟悉的 `create` 命令，并附加可选的 `--record` 标志，这样 Deployment 的创建过程将记录在回滚历史中。否则，我们只能通过
    `$ kubectl create -f node-js-deploy.yaml --record` 命令查看随后的更改记录。
- en: You may need to add `--validate=false` if this beta type is not enabled on your
    cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群没有启用此 beta 类型，您可能需要添加 `--validate=false`。
- en: 'We should see a message about the deployment being successfully created. After
    a few moments, it will finish creating our pod, which we can check for ourselves
    with a `get pods` command. We add the `-l` flag to only see the pods relevant
    to this deployment:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该会看到部署成功创建的消息。几秒钟后，它将完成 Pod 的创建，我们可以使用 `get pods` 命令自行检查。我们添加 `-l` 标志只查看与此部署相关的
    Pods：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you''d like to get the state of the deployment, you can issue the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想查看部署的状态，可以执行以下命令：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can also see the state of a rollout, which will be more useful in the future
    when we update our Deployments. You can use `kubectl rollout status deployment/node-js-deploy`
    to see what's going on.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到部署状态，这在将来更新我们的部署时会更加有用。你可以使用 `kubectl rollout status deployment/node-js-deploy`
    来查看当前的状态。
- en: 'We create a service just as we did with ReplicationControllers. The following
    is a `Service` definition for the Deployment we just created. Notice that it is
    almost identical to the `Services` we created in the past. Save the following
    code in `node-js-deploy-service.yaml` file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个服务，就像之前用 ReplicationControllers 时那样。以下是我们刚刚创建的 Deployment 的 `Service`
    定义。请注意，它几乎与我们过去创建的 `Services` 完全相同。将以下代码保存为 `node-js-deploy-service.yaml` 文件：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once this service is created using `kubectl`, you'll be able to access the deployment
    pods through the service IP or the service name if you are inside a pod on this
    namespace.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过 `kubectl` 创建了这个服务，您就可以通过服务 IP 或服务名称访问部署的 Pods，前提是您位于该命名空间的某个 Pod 内。
- en: Scaling
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: 'The `scale` command works the same way as it did in our ReplicationController.
    To scale up, we simply use the deployment name and specify the new number of replicas,
    as shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`scale` 命令的工作方式与我们在 ReplicationController 中一样。要扩展副本数，我们只需要使用部署名称并指定新的副本数，如下所示：'
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If all goes well, we'll simply see a message about the deployment being scaled in
    the output of our Terminal window. We can check the number of running pods using
    the `get pods` command from earlier. In the latest versions of Kubernetes, you're
    also able to set up pod scaling for your cluster, which allows you to do horizontal
    autoscaling so you can scale up pods based on the CPU utilization of your cluster.
    You'll need to set a maximum and minimum number of pods in order to get this going.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，我们将在终端窗口的输出中看到一个关于部署被扩展的消息。我们可以使用之前的`get pods`命令检查当前运行的pod数量。在最新版本的Kubernetes中，你还可以为集群设置pod扩展，这样你就可以根据集群的CPU利用率进行水平自动扩展，从而扩展pods。你需要设置pod的最大和最小数量才能启动这个功能。
- en: 'Here''s what that command would look like with this example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该命令在这个示例中的样子：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Read more about horizontal pod scaling in this walkthrough: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于水平pod扩展的信息，请参阅此教程：[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)。
- en: There's also a concept of proportional scaling, which allows you to run multiple
    version of your application at the same time. This implementation would be useful
    when incrementing a backward-compatible version of an API-based microservice,
    for example. When doing this type of deployment, you'll use `.spec.strategy.rollingUpdate.maxUnavailable`
    and `.spec.strategy.rollingUpdate.maxSurge` to limit the maximum number of pods
    that can be down during an update to the deployment, or the maximum number of
    pods that can be created that exceed the desired number of pods, respectively.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个比例扩展的概念，它允许你同时运行多个版本的应用程序。例如，当你增量更新一个向后兼容的API微服务版本时，这种实现会非常有用。在进行这种类型的部署时，你将使用`.spec.strategy.rollingUpdate.maxUnavailable`和`.spec.strategy.rollingUpdate.maxSurge`来限制更新期间部署中不可用的最大pod数量，或限制超过所需pod数量的最大创建pod数量。
- en: Updates and rollouts
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新与滚动发布
- en: Deployments allow for updating in a few different ways. First, there is the `kubectl
    set` command, which allows us to change the deployment configuration without redeploying
    manually. Currently, it only allows for updating the image, but as new versions
    of our application or container image are processed, we will need to do this quite
    often.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 部署允许通过几种不同的方式进行更新。首先是`kubectl set`命令，它允许我们更改部署配置而无需手动重新部署。目前，它只允许更新镜像，但随着应用程序或容器镜像的新版本的处理，我们将需要频繁地执行此操作。
- en: 'Let''s take a look using our deployment from the previous section. We should
    have three replicas running right now. Verify this by running the `get pods` command
    with a filter for our deployment:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下前一节中的部署。我们现在应该有三个副本在运行。通过运行`get pods`命令并加上部署筛选器来验证这一点：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We should see three pods similar to those listed in the following screenshot:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到三个pod，类似于下面截图中的内容：
- en: '![](img/683596db-0c26-4df3-99a7-3357eec3c2b6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/683596db-0c26-4df3-99a7-3357eec3c2b6.png)'
- en: Deployment pod listing
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 部署pod列表
- en: 'Take one of the pods listed on our setup, replace it in the following command
    where it says `{POD_NAME_FROM_YOUR_LISTING}`, and run this command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 选择我们设置中的一个pod，将其替换为以下命令中的`{POD_NAME_FROM_YOUR_LISTING}`，并运行此命令：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We should see an output like the following screenshot with the current image
    version of `0.1`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似下面截图的输出，当前镜像版本为`0.1`：
- en: '![](img/ed7b1939-9e63-4206-bc41-f5c172fa08d0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed7b1939-9e63-4206-bc41-f5c172fa08d0.png)'
- en: Current pod image
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当前pod镜像
- en: 'Now that we know what our current deployment is running, let''s try to update
    to the next version. This can be achieved easily using the `kubectl set` command
    and specifying the new version, as shown here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道当前的部署在运行什么版本，接下来我们试着更新到下一个版本。这可以通过使用`kubectl set`命令并指定新版本来轻松实现，如下所示：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If all goes well, we should see text that says `deployment "node-js-deploy"
    image updated` displayed on the screen.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，我们应该在屏幕上看到显示`deployment "node-js-deploy" image updated`的文本。
- en: 'We can double–check the status using the following `rollout status` command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下`rollout status`命令再次检查状态：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, we can directly edit the deployment in an editor window with
    `kubectl edit deployment/node-js-deploy` and change `.spec.template.spec.containers[0].image`
    from `jonbaier/pod-scaling:0.1` to `jonbaier/pod-scaling:0.2`. Either of these
    methods will work to update your deployment, and as a reminder you can check the
    status of your update with the `kubectl status` command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们也可以在编辑器窗口中直接编辑部署，使用`kubectl edit deployment/node-js-deploy`，并将`.spec.template.spec.containers[0].image`从`jonbaier/pod-scaling:0.1`更改为`jonbaier/pod-scaling:0.2`。这两种方法都可以更新你的部署，提醒一下，你可以使用`kubectl
    status`命令检查更新状态：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We should see some text saying that the deployment successfully rolled out.
    If you see any text about waiting for the rollout to finish, you may need to wait
    a moment for it to finish, or alternatively check the logs for issues.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一些文本，说明部署已经成功推出。如果你看到任何关于等待推出完成的文本，可能需要等一下它完成，或者检查日志寻找问题。
- en: 'Once it''s finished, run the `get pods` command as earlier. This time, we will
    see new pods listed:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，像之前一样运行`get pods`命令。这一次，我们将看到列出了新的 pods：
- en: '![](img/e49ee7d9-21e2-43a5-954d-8c5184b3a02f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e49ee7d9-21e2-43a5-954d-8c5184b3a02f.png)'
- en: Deployment pod listing after update
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的部署 pod 列表
- en: Once again, plug one of your pod names into the `describe` command we ran earlier.
    This time, we should see the image has been updated to 0.2.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，将你其中一个 pod 的名称输入到我们之前运行的`describe`命令中。这次，我们应该看到镜像已经更新为0.2。
- en: What happened behind the scenes is that Kubernetes has *rolled out* a new version
    for us. It basically creates a new ReplicaSet with the new version. Once this
    pod is online and healthy, it kills one of the older versions. It continues this
    behavior, scaling out the new version and scaling down the old versions, until
    only the new pods are left. Another way to observe this behavior indirectly is
    to investigate the ReplicaSet that the Deployment object is using to update your
    desired application state.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 背后发生的事情是 Kubernetes 已经为我们*推出*了一个新版本。它基本上创建了一个新的 ReplicaSet 并使用新版本。一旦这个 pod 上线并且健康，它就会终止一个旧版本的
    pod。它会继续这种行为，扩展新版本并缩减旧版本，直到只剩下新的 pod。另一种间接观察此行为的方法是查看 Deployment 对象用于更新所需应用状态的
    ReplicaSet。
- en: 'Remember, you don''t interact directly with ReplicaSet, but rather give Kubernetes
    directives in the form of Deployment elements and let Kubernetes make the required
    changes to the cluster object store and state. Take a look at the ReplicaSets
    quickly after running your `image update` command, and you''ll see how multiple
    ReplicaSets are used to effect the image change without application downtime:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你不是直接与 ReplicaSet 交互，而是通过 Deployment 元素向 Kubernetes 提供指令，并让 Kubernetes 根据这些指令对集群对象存储和状态进行必要的更改。快速查看你运行`image
    update`命令后创建的 ReplicaSets，你将看到如何通过多个 ReplicaSets 实现镜像更改而不发生应用停机：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following diagram describes the workflow for your reference:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了工作流，供你参考：
- en: '![](img/2de57da2-e018-4396-902e-b2f53c4c3254.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2de57da2-e018-4396-902e-b2f53c4c3254.png)'
- en: Deployment life cycle
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 部署生命周期
- en: It's worth noting that the rollback definition allows us to control the pod
    replace method in our deployment definition. There is a `strategy.type` field
    that defaults to `RollingUpdate` and the preceding behavior. Optionally, we can
    also specify `Recreate` as the replacement strategy and it will kill all the old
    pods first before creating the new versions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，回滚定义允许我们在部署定义中控制 pod 替换方法。有一个`strategy.type`字段，默认为`RollingUpdate`和之前的行为。我们还可以选择将`Recreate`作为替代策略，先终止所有旧的
    pods，然后再创建新版本的 pods。
- en: History and rollbacks
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 历史和回滚
- en: 'One of the useful features of the rollout API is the ability to track the deployment
    history. Let''s do one more update before we check the history. Run the `kubectl
    set` command once more and specify version 0.3:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: rollout API 的一个有用功能是能够跟踪部署历史。在查看历史之前，让我们再进行一次更新。再次运行`kubectl set`命令，并指定版本0.3：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once again, we''ll see text that says `deployment "node-js-deploy" image updated` displayed
    on the screen. Now, run the `get pods` command once more:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次会看到屏幕上显示`deployment "node-js-deploy" image updated`的文本。现在，再次运行`get pods`命令：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s also take a look at our deployment history. Run the `rollout history` command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也来看看我们的部署历史。运行`rollout history`命令：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We should see an output similar to the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似以下的输出：
- en: '![](img/93d9e5ae-cf4d-4886-ab7f-9eb2169b63d3.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93d9e5ae-cf4d-4886-ab7f-9eb2169b63d3.png)'
- en: Rollout history
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 部署历史
- en: As we can see, the history shows us the initial deployment creation, our first
    update to 0.2, and then our final update to 0.3\. In addition to status and history,
    the `rollout` command also supports the `pause`, `resume`, and `undo` sub-commands.
    The `rollout pause` command allows us to pause a command while the rollout is
    still in progress. This can be useful for troubleshooting and also helpful for
    canary-type launches, where we wish to do final testing of the new version before
    rolling out to the entire user base. When we are ready to continue the rollout,
    we can simply use the `rollout resume` command.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，历史记录显示了初始的发布创建、我们第一次更新到0.2以及最后更新到0.3。除了状态和历史记录，`rollout`命令还支持`pause`、`resume`和`undo`子命令。`rollout
    pause`命令允许我们在发布过程中暂停命令。这在故障排除时非常有用，也适用于金丝雀发布类型的部署，我们希望在发布到整个用户群之前做最后的版本测试。当我们准备继续发布时，可以简单地使用`rollout
    resume`命令。
- en: 'But what if something goes wrong? That is where the `rollout undo` command
    and the rollout history itself are really handy. Let''s simulate this by trying
    to update to a version of our pod that is not yet available. We will set the image
    to version 42.0, which does not exist:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果出现问题怎么办？这时候，`rollout undo`命令和发布历史记录就非常有用了。我们通过尝试更新到一个尚未可用的Pod版本来模拟这个问题。我们将镜像设置为42.0版本，但该版本并不存在：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We should still see the text that says `deployment "node-js-deploy" image updated`
    displayed on the screen. But if we check the status, we will see that it is still
    waiting:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然应该看到屏幕上显示`deployment "node-js-deploy" image updated`的文本。但是如果检查状态，我们会看到它仍在等待：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we see that the deployment has been paused after updating two of the
    three pods, but Kubernetes knows enough to stop there in order to prevent the
    entire application from going offline due to the mistake in the container image
    name. We can press *Ctrl* + *C* to kill the `status` command and then run the `get
    pods` command once more:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到发布在更新了三个Pod中的两个后已暂停，但Kubernetes足够智能，知道在此停止，以防止由于容器镜像名称错误导致整个应用程序下线。我们可以按*Ctrl*
    + *C*来终止`status`命令，然后再次运行`get pods`命令：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We should now see an `ErrImagePull`, as in the following screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该看到`ErrImagePull`，如下图所示：
- en: '![](img/c8c13fb1-d8d9-4a95-aba3-5cfc710cf2ee.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8c13fb1-d8d9-4a95-aba3-5cfc710cf2ee.png)'
- en: Image pull error
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像拉取错误
- en: As we expected, it can't pull the 42.0 version of the image because it doesn't
    exist. This error refers to a container that's stuck in an image pull loop, which
    is noted as `ImagePullBackoff` in the latest versions of Kubernetes. We may also
    have issues with deployments if we run out of resources on the cluster or hit
    limits that are set for our namespace. Additionally, the deployment can fail for
    a number of application-related causes, such as health check failure, permission
    issues, and application bugs, of course.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，它无法拉取42.0版本的镜像，因为该版本并不存在。这个错误是指容器陷入了镜像拉取循环，最新版本的Kubernetes中标记为`ImagePullBackoff`。如果我们在集群中资源耗尽或达到命名空间设置的限制，也可能会遇到发布问题。当然，发布失败还可能是由于一些应用相关的原因，比如健康检查失败、权限问题和应用程序缺陷等。
- en: It's entirely possible to create deployments that are wholly unavailable if
    you don't change `maxUnavailable` and `spec.replicas` to different numbers, as
    the default for each is `1`!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不改变`maxUnavailable`和`spec.replicas`的值，完全有可能创建一个完全不可用的发布，因为默认值都是`1`！
- en: 'Whenever a failure to roll out happens, we can easily roll back to a previous
    version using the `rollout undo` command. This command will take our deployment
    back to the previous version:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 每当发布失败时，我们可以轻松地通过`rollout undo`命令回滚到之前的版本。此命令会将我们的发布回滚到上一个版本：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After that, we can run a `rollout status` command once more and we should see
    everything rolled out successfully. Run the `kubectl rollout history deployment/node-js-deploy` command
    again and we''ll see both our attempt to roll out version 42.0 and revert to 0.3:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以再运行一次`rollout status`命令，应该会看到所有内容都成功发布。再次运行`kubectl rollout history deployment/node-js-deploy`命令，我们将看到我们尝试发布版本42.0并回滚到0.3的记录：
- en: '![](img/88b2c148-4001-45d7-9fb8-8976c2207624.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88b2c148-4001-45d7-9fb8-8976c2207624.png)'
- en: Rollout history after rollback
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚后的发布历史
- en: We can also specify the `--to-revision` flag when running an undo to roll back
    to a specific version. This can be handy for times when our rollout succeeds,
    but we discover logical errors down the road.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在运行撤销操作时指定`--to-revision`标志，以回滚到特定版本。这在我们的发布成功，但在后期发现逻辑错误时非常有用。
- en: Autoscaling
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动伸缩
- en: As you can see, Deployments are a great improvement over ReplicationControllers,
    allowing us to seamlessly update our applications, while integrating with the
    other resources of Kubernetes in much the same way.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Deployment比ReplicationController有了很大的改进，使我们能够无缝地更新应用程序，同时与Kubernetes的其他资源集成，方式类似。
- en: 'Another area that we saw in the previous chapter, and also supported for Deployments,
    is **Horizontal Pod Autoscalers** (**HPAs**). HPAs help you manage cluster utilization
    by scaling the number of  pods based on CPU utilization. There are three objects
    that can scale using HPAs, DaemonSets not included:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们在上一章看到的领域，也是支持Deployment的，是**水平Pod自动扩缩**（**HPAs**）。HPA帮助你通过根据CPU利用率自动调整pod数量来管理集群资源利用率。通过HPA可以扩缩的对象有三个，不包括DaemonSets：
- en: Deployment (the recommended method)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deployment（推荐方法）
- en: ReplicaSet
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaSet
- en: ReplicationController (not recommended)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicationController（不推荐）
- en: The HPA is implemented as a control loop similar to other controllers that we've
    discussed, and you can adjust the sensitivity of the controller manager by adjusting
    its sync period via `--horizontal-pod-autoscaler-sync-period` (default 30 seconds).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: HPA作为一个控制循环实现，类似于我们之前讨论过的其他控制器，你可以通过调整其同步周期来调节控制器管理器的灵敏度，方法是使用`--horizontal-pod-autoscaler-sync-period`（默认值为30秒）。
- en: 'We will walk through a quick remake of the HPAs from the previous chapter,
    this time using the Deployments we have created so far. Save the following code
    in `node-js-deploy-hpa.yaml` file:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过快速重建上一章的HPAs，这次使用我们目前创建的Deployments。将以下代码保存到`node-js-deploy-hpa.yaml`文件中：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The API is changing quickly with these tools as they're in beta, so take careful
    note of the `apiVersion` element, which used to be `autoscaling/v1`, but is now
    `autoscalingv2beta1`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些工具仍处于测试版，因此API正在迅速变化，请注意`apiVersion`元素，它以前是`autoscaling/v1`，但现在是`autoscalingv2beta1`。
- en: 'We have lowered the CPU threshold to 10% and changed our minimum and maximum
    pods to `3` and `6`, respectively. Create the preceding HPA with our trusty `kubectl
    create -f` command. After this is completed, we can check that it''s available
    with the `kubectl get hpa` command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将CPU阈值降低到10%，并将最小和最大pod数量分别更改为`3`和`6`。使用我们可靠的`kubectl create -f`命令创建上述HPA。完成后，我们可以通过`kubectl
    get hpa`命令检查它是否可用：
- en: '![](img/b2a2e54d-698c-4e2b-b36a-cbda1a284ef2.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2a2e54d-698c-4e2b-b36a-cbda1a284ef2.png)'
- en: Horizontal pod autoscaler
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 水平Pod自动扩缩器
- en: 'We can also check that we have only `3` pods running with the `kubectl get
    deploy` command. Now, let''s add some load to trigger the autoscaler:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过`kubectl get deploy`命令检查是否只有`3`个pod在运行。现在，让我们增加一些负载来触发自动扩缩器：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create `boomload-deploy.yaml` file as usual. Now, monitor the HPA with the
    alternating `kubectl get hpa` and `kubectl get deploy` commands. After a few moments,
    we should see the load jump above `10%`. After a few more moments, we should also
    see the number of pods increase all the way up to `6` replicas:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例创建`boomload-deploy.yaml`文件。现在，使用交替的`kubectl get hpa`和`kubectl get deploy`命令来监控HPA。几分钟后，我们应该看到负载跃升超过`10%`。再过一会儿，我们还应该看到pod的数量增加，直到达到`6`个副本：
- en: '![](img/efd80b34-393d-476a-bf48-33c97b7b838a.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efd80b34-393d-476a-bf48-33c97b7b838a.png)'
- en: HPA increase and pod scale up
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: HPA增加并且pod扩展
- en: 'Again, we can clean this up by removing our load generation pod and waiting
    a few moments:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过删除负载生成pod并等待片刻来清理这个过程：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Again, if we watch the HPA, we'll start to see the CPU usage drop. After a few
    minutes, we will go back down to `0%` CPU load and then the Deployment will scale
    back to `3` replicas.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果我们监视HPA，开始会看到CPU使用率下降。几分钟后，我们将回到`0%`的CPU负载，然后Deployment将缩放回`3`个副本。
- en: Jobs
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jobs
- en: Deployments and ReplicationControllers are a great way to ensure long-running
    applications are always up and able to tolerate a wide array of infrastructure
    failures. However, there are some use cases this does not address, specifically
    short-running, run once tasks, as well as regularly scheduled tasks. In both cases,
    we need the tasks to run until completion, but then terminate and start again
    at the next scheduled interval.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment和ReplicationController是确保长期运行的应用程序始终保持在线并能够容忍各种基础设施故障的好方法。然而，仍有一些用例不适用，特别是短期运行的、仅执行一次的任务以及定期计划任务。在这两种情况下，我们需要任务运行直到完成，然后终止，并在下一个计划的时间间隔重新启动。
- en: 'To address this type of workload, Kubernetes has added a `batch` API, which
    includes the `Job` type. This type will create 1 to n pods and ensure that they
    all run to completion with a successful exit. Based on `restartPolicy`, we can
    either allow pods to simply fail without retry (`restartPolicy: Never`) or retry
    when a pods exits without successful completion (`restartPolicy: OnFailure`).
    In this example, we will use the latter technique as shown in the listing `longtask.yaml`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对这种类型的工作负载，Kubernetes 增加了 `batch` API，其中包括 `Job` 类型。该类型将创建 1 到 n 个 Pod，并确保它们都成功完成并退出。根据
    `restartPolicy`，我们可以选择让 Pod 在失败后不重试（`restartPolicy: Never`），或在 Pod 未成功完成时进行重试（`restartPolicy:
    OnFailure`）。在这个示例中，我们将使用后者技术，如 `longtask.yaml` 列表所示：'
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s go ahead and run this with the following command:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令来运行它：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If all goes well, you'll see `job "long-task" created` printed on the screen.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你将看到屏幕上打印出 `job "long-task" created`。
- en: 'This tells us the job was created, but doesn''t tell us if it completed successfully.
    To check that, we need to query the job status with the following command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们作业已经创建，但并没有告诉我们是否成功完成。要检查这一点，我们需要使用以下命令查询作业状态：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/0f780c70-10b1-40df-b24c-4c20c68e05f0.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f780c70-10b1-40df-b24c-4c20c68e05f0.png)'
- en: Job status
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 作业状态
- en: You should see that we had `1` task that succeeded, and in the `Events` logs,
    we have a `SuccessfulCreate` message. If we use the `kubectl get pods` command,
    we won't see our `long-task` pods in the list, but we may notice the message at
    the bottom in the listing states that there are completed jobs that are not shown.
    We will need to run the command again with the `-a` or `--show-all` flag to see
    the `long-task` pod and the completed job status.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到有 `1` 个任务成功，并且在 `Events` 日志中，我们有一条 `SuccessfulCreate` 消息。如果我们使用 `kubectl
    get pods` 命令，我们将看不到 `long-task` Pod 在列表中，但可能会注意到列表底部有一条消息，说明有已完成的作业未显示。我们需要再次运行命令，并加上
    `-a` 或 `--show-all` 标志，才能看到 `long-task` Pod 和已完成的作业状态。
- en: Let's dig a little deeper to prove to ourselves the work was completed successfully.
    We could use the `logs` command to look at the pod logs. However, we can also
    use the UI for this task. Open a browser and go to the following UI URL: `https://<your
    master ip>/ui/`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入一点，验证任务是否成功完成。我们可以使用 `logs` 命令查看 Pod 日志。但是，我们也可以使用 UI 来完成这项任务。打开浏览器并访问以下
    UI URL：`https://<your master ip>/ui/`。
- en: 'Click on Jobs and then long-task from the list, so we can see the details.
    Then, in the Pods section, click on the pod listed there. This will give us the
    Pod details page. At the bottom of the details, click on View Logs and we will
    see the log output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 “Jobs”，然后从列表中选择 long-task，这样我们就可以查看详细信息。接着，在 Pods 部分，点击列出的 Pod。这将带我们进入 Pod
    详细信息页面。在页面底部，点击 “View Logs”，我们将看到日志输出：
- en: '![](img/71e6c777-5aff-4487-aad0-21c41f28d0cf.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71e6c777-5aff-4487-aad0-21c41f28d0cf.png)'
- en: Job log
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 作业日志
- en: As you can see in the preceding screenshot, the whalesay container is complete
    with the ASCII art and our custom message from the runtime parameters in the example.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的截图中看到的，whalesay 容器已经完成，并且显示了 ASCII 艺术和我们在示例中通过运行时参数传递的自定义消息。
- en: Other types of jobs
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他类型的作业
- en: While this example provides a basic introduction to short-running jobs, it only
    addresses the use case of once and done tasks. In reality, batch work is often
    done in parallel or as part of a regularly occurring task.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个示例提供了短期运行任务的基础介绍，但它仅处理一次性任务的使用案例。实际上，批量任务通常是并行执行的，或者作为定期任务的一部分。
- en: Parallel jobs
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行作业
- en: Using parallel jobs, we may be grabbing tasks from an ongoing queue or simply
    running a set number of tasks that are not dependent on each other. In the case
    of jobs pulling from a queue, our application must be aware of the dependencies
    and have the logic to decide how tasks are processed and what to work on next.
    Kubernetes is simply scheduling the jobs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用并行作业时，我们可能会从一个正在进行的队列中获取任务，或者仅运行一组相互独立的任务。在从队列中提取任务的情况下，我们的应用程序必须了解任务之间的依赖关系，并具有逻辑来决定如何处理任务以及接下来要处理的任务。Kubernetes
    只是调度这些作业。
- en: You can learn more about parallel jobs from the Kubernetes documentation and
    batch API reference.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 Kubernetes 文档和批处理 API 参考了解更多关于并行作业的内容。
- en: Scheduled jobs
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定时作业
- en: 'For tasks that need to run periodically, Kubernetes has also released a `CronJob`
    type in alpha. As we might expect, this type of job uses the underlying cron formatting
    to specify a schedule for the task we wish to run. By default, our cluster will
    not have the alpha batch features enabled, but we can look at an example `CronJob`
    listing to learn how these types of workloads will work going forward. Save the
    following code in `longtask-cron.yaml` file:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要定期运行的任务，Kubernetes 还发布了一个 alpha 版本的 `CronJob` 类型。正如我们预期的那样，这种类型的任务使用底层的
    cron 格式来指定我们希望运行任务的时间表。默认情况下，我们的集群将不会启用 alpha 批处理功能，但我们可以查看一个示例 `CronJob` 列表，了解这些类型的工作负载将如何在未来运行。将以下代码保存为
    `longtask-cron.yaml` 文件：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, the schedule portion reflects a crontab with the following format: *minute
    hour day-of-month month day-of-week*.  In this example, 15 10 * * 6 creates a
    task that will run every Saturday at 10:15 am.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，调度部分反映了一个 cron 表，格式为：*分钟 小时 月日 月份 星期几*。在此示例中，`15 10 * * 6` 创建了一个每周六上午 10:15
    运行的任务。
- en: DaemonSets
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DaemonSets
- en: While ReplicationControllers and Deployments are great at making sure that a
    specific number of application instances are running, they do so in the context
    of the best fit. This means that the scheduler looks for nodes that meet resource
    requirements (available CPU, particular storage volumes, and so on) and tries
    to spread across the nodes and zones.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ReplicationControllers 和 Deployments 很擅长确保特定数量的应用实例在运行，但它们是在最佳匹配的上下文中进行工作的。这意味着调度器会寻找满足资源需求（如可用
    CPU、特定存储卷等）的节点，并尝试将负载分配到不同的节点和区域。
- en: This works well for creating highly available and fault tolerant applications,
    but what about cases where we need an agent to run on every single node in the
    cluster? While the default spread does attempt to use different nodes, it does
    not guarantee that every node will have a replica and, indeed, will only fill a
    number of nodes equivalent to the quantity specified in the ReplicationController
    or Deployment specification.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式非常适合创建高可用和容错的应用程序，但如果我们需要一个代理在集群中的每个节点上都运行，该怎么办呢？虽然默认的调度会尝试使用不同的节点，但它并不能保证每个节点都有一个副本，实际上，它只会填充与
    ReplicationController 或 Deployment 规格中指定的数量等量的节点。
- en: To ease this burden, Kubernetes introduced `DaemonSet`, which simply defines
    a pod to run on every single node in the cluster or a defined subset of those
    nodes. This can be very useful for a number of production–related activities,
    such as monitoring and logging agents, security agents, and filesystem daemons.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这一负担，Kubernetes 引入了 `DaemonSet`，它简单地定义了一个 Pod，在集群中的每个节点或这些节点的定义子集上运行。这对于一些生产相关的活动非常有用，例如监控和日志代理、安全代理以及文件系统守护进程。
- en: In Kubernetes version 1.6, `RollingUpdate` was added as an update strategy for
    the `DaemonSet` object. This functionality allows you to perform serial updates
    to your pods based on updates to `spec.template`. In the next version, 1.7, history
    was added so that operators could roll back an update based on a history of revisions
    to `spec.template`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 1.6 版本中，`RollingUpdate` 被添加为 `DaemonSet` 对象的更新策略。此功能允许你根据 `spec.template`
    的更新对 Pod 执行串行更新。在下一个版本 1.7 中，添加了历史记录功能，操作员可以根据 `spec.template` 的修订历史回滚更新。
- en: 'You would roll back a rollout with the following `kubectl` example command:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下 `kubectl` 示例命令回滚部署：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In fact, Kubernetes already uses these capabilities for some of its core system
    components. If we recall from [Chapter 1](446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml),
    *Introduction to Kubernetes*, we saw `node-problem-detector` running on the nodes.
    This pod is actually running on every node in the cluster as `DaemonSet`. We can
    see this by querying DaemonSets in the `kube-system` namespace:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Kubernetes 已经在其一些核心系统组件中使用了这些功能。如果我们回顾 [第1章](446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml)，*Kubernetes简介*，我们看到
    `node-problem-detector` 正在节点上运行。这个 Pod 实际上是在集群中的每个节点上作为 `DaemonSet` 运行的。我们可以通过查询
    `kube-system` 命名空间中的 DaemonSets 来看到这一点：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/001d9a67-2327-450d-b6e3-41763648fbc9.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/001d9a67-2327-450d-b6e3-41763648fbc9.png)'
- en: kube-system DaemonSets
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: kube-system DaemonSets
- en: 'You can find more information about `node-problem-detector`, as well as `yaml`,
    in the following `node-problem-detector definition` listing at [http://kubernetes.io/docs/admin/node-problem/#node-problem-detector](http://kubernetes.io/docs/admin/node-problem/#node-problem-detector):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下 `node-problem-detector 定义` 列表中找到关于 `node-problem-detector` 和 `yaml` 的更多信息：[http://kubernetes.io/docs/admin/node-problem/#node-problem-detector](http://kubernetes.io/docs/admin/node-problem/#node-problem-detector)：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Node selection
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点选择
- en: As mentioned previously, we can schedule DaemonSets to run on a subset of nodes
    as well. This can be achieved using something called **nodeSelectors**. Theseallow
    us to constrain the nodes a pod runs on, by looking for specific labels and metadata.
    They simply match key-value pairs on the labels for each node. We can add our
    own labels or use those that are assigned by default.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们也可以安排DaemonSets在节点的一个子集上运行。这可以通过称为**nodeSelectors**的东西来实现。它们允许我们通过查找特定的标签和元数据来约束pod运行的节点。它们只是简单地匹配每个节点标签上的键值对。
- en: 'The default labels are listed in the following table:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 默认标签列在以下表格中：
- en: '| **Default node labels** | **Description** |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **默认节点标签** | **描述** |'
- en: '| `kubernetes.io/hostname` | This shows the hostname of the underlying instance
    or machine |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `kubernetes.io/hostname` | 这显示底层实例或机器的主机名 |'
- en: '| `beta.kubernetes.io/os` | This shows the underlying operating system as a
    report in the Go language |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `beta.kubernetes.io/os` | 这显示基础操作系统作为Go语言报告 |'
- en: '| `beta.kubernetes.io/arch` | This shows the underlying processor architecture
    as a report in the Go language |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `beta.kubernetes.io/arch` | 这显示基础处理器架构作为Go语言报告 |'
- en: '| `beta.kubernetes.io/instance-type` | This is the instance type of the underlying
    cloud provider (cloud-only)  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| `beta.kubernetes.io/instance-type` | 这是底层云提供商的实例类型（仅限云） |'
- en: '| `failure-domain.beta.kubernetes.io/region` | This is the region of the underlying
    cloud provider (cloud-only)  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `failure-domain.beta.kubernetes.io/region` | 这是底层云提供商的区域（仅限云） |'
- en: '| `failure-domain.beta.kubernetes.io/zone` | This is the fault-tolerance zone of
    the underlying cloud provider (cloud-only) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `failure-domain.beta.kubernetes.io/zone` | 这是底层云提供商的容错区域（仅限云） |'
- en: '*Table 5.1 - Kubernetes default node* labels'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.1 - Kubernetes默认节点标签*'
- en: We are not limited to DaemonSets, as nodeSelectors actually work with pod definitions
    as well. Let's take a closer look at a job example (a slight modification of our
    preceding long-task example).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于DaemonSets，因为nodeSelectors实际上也适用于pod定义。让我们更详细地看一下作业示例（稍微修改了我们之前的长期任务示例）。
- en: 'First, we can see these on the nodes themselves. Let''s get the names of our
    nodes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以在节点本身看到这些。让我们获取我们节点的名称：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use a name from the output of the previous command and plug it into this one:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一个命令的输出中的名称，并将其插入到此命令中：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/bdcf22e3-815c-4443-af84-f44c341d4f19.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdcf22e3-815c-4443-af84-f44c341d4f19.png)'
- en: Excerpt from node describe
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从节点描述中摘录
- en: 'Let''s now add a nickname label to this node:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为此节点添加一个昵称标签：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we run the `kubectl describe node` command again, we will see this label
    listed next to the defaults. Now, we can schedule workloads and specify this specific
    node. The following listing `longtask-nodeselector.yaml` is a modification of
    our earlier long-running task with `nodeSelector` added:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次运行`kubectl describe node`命令，我们将看到此标签列在默认标签旁边。现在，我们可以安排工作负载并指定此特定节点。以下列表`longtask-nodeselector.yaml`是我们之前长时间运行任务的修改版本，添加了`nodeSelector`：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Create the job from this listing with `kubectl create -f`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl create -f`从此列表创建作业。
- en: 'Once that succeeds, it will create a pod based on the preceding specification.
    Since we have defined `nodeSelector`, it will try to run the pod on nodes that
    have matching labels and fail if it finds no candidates. We can find the pod by
    specifying the job name in our query, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功，它将根据前述规范创建一个pod。由于我们定义了`nodeSelector`，它将尝试在具有匹配标签的节点上运行pod，并在找不到候选者时失败。我们可以通过在查询中指定作业名称来找到该pod，如下所示：
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We use the `-a` flag to show all pods. Jobs are short lived and once they enter
    the completed state, they will not show up in a basic `kubectl get pods` query.
    We also use the `-l` flag to specify pods with the `job-name=long-task-ns` label.
    This will give us the pod name, which we can push into the following command:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`-a`标志显示所有pod。作业的生命周期很短，一旦进入完成状态，它们将不会在基本的`kubectl get pods`查询中显示。我们还使用`-l`标志指定带有`job-name=long-task-ns`标签的pod。这将给出pod名称，我们可以将其推入以下命令：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The result should show the name of the node this pod was run on. If all has
    gone well, it should match the node we labeled a few steps earlier with the `trusty-steve`
    label.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应显示此pod运行在的节点名称。如果一切顺利，它应与我们之前使用`trusty-steve`标签标记的节点匹配。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Now, you should have a good foundation of the core constructs in Kubernetes.
    We explored the new Deployment abstraction and how it improves on the basic ReplicationController,
    allowing for smooth updates and solid integration with services and autoscaling.
    We also looked at other types of workload in jobs and DaemonSets. You learned
    how to run short-running or batch tasks, as well as how to run agents on every
    node in our cluster. Finally, we took a brief look at node selection and how that
    can be used to filter the nodes in the cluster used for our workloads.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该已经掌握了 Kubernetes 核心构件的基础。我们探讨了新的部署抽象及其如何改进基本的 ReplicationController，支持平滑更新，并与服务和自动伸缩实现紧密集成。我们还研究了其他类型的工作负载，如作业（Jobs）和
    DaemonSets。你学会了如何运行短时或批处理任务，以及如何在集群中的每个节点上运行代理。最后，我们简要了解了节点选择以及如何使用它来过滤集群中用于工作负载的节点。
- en: We will build on what you learned in this chapter and look at stateful applications
    in the next chapter, exploring both critical application components and the data
    itself.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章所学的基础上，进入下一章探讨有状态应用程序，深入了解关键应用组件以及数据本身。
- en: Questions
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Name four use cases for Kubernetes deployments
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请列举四种 Kubernetes 部署的使用场景。
- en: Which element of a deployment definition tells the deployment which pod to manage?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署定义中的哪个元素告诉部署管理哪个 Pod？
- en: Which flag do you need to activate in order to see the history of your changes?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要激活哪个标志才能查看你的更改历史？
- en: Which underlying mechanism (a Kubernetes object, in fact) does a Deployment
    use in order to update your container images?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署使用什么底层机制（实际上是一个 Kubernetes 对象）来更新你的容器镜像？
- en: What's the name of the technology that lets your pods scale up and down according
    to CPU load?
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使 Pod 根据 CPU 负载进行伸缩的技术名称是什么？
- en: Which type of workload should you run for an ephemeral, short-lived task?
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个短暂、短生命周期的任务，你应该运行哪种类型的工作负载？
- en: What's the purpose of a `DaemonSet`?
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DaemonSet` 的目的是什么？'
