- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: The Modern Data Stack
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代数据架构
- en: In this chapter, we will explore the modern data architecture that has emerged
    for building scalable and flexible data platforms. Specifically, we will cover
    the Lambda architecture pattern and how it enables real-time data processing along
    with batch data analytics. You will learn about the key components of the Lambda
    architecture, including the batch processing layer for historical data, the speed
    processing layer for real-time data, and the serving layer for unified queries.
    We will discuss how technologies such as Apache Spark, Apache Kafka, and Apache
    Airflow can be used to implement these layers at scale.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨为构建可扩展和灵活的数据平台而出现的现代数据架构。具体来说，我们将介绍 Lambda 架构模式，并讨论它如何结合批量数据分析实现实时数据处理。你将了解
    Lambda 架构的关键组件，包括用于历史数据的批处理层、用于实时数据的速度处理层，以及用于统一查询的服务层。我们还将讨论如何利用 Apache Spark、Apache
    Kafka 和 Apache Airflow 等技术，在大规模环境中实现这些层。
- en: By the end of the chapter, you will understand the core design principles and
    technology choices for building a modern data lake. You will be able to explain
    the benefits of the Lambda architecture over traditional data warehouse designs.
    Most importantly, you will have the conceptual foundation to start architecting
    your own modern data platform.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将理解构建现代数据湖的核心设计原则和技术选择。你将能够解释 Lambda 架构相比传统数据仓库设计的优势。最重要的是，你将拥有开始构建自己现代数据平台的概念基础。
- en: The concepts covered will allow you to process streaming data at low latency
    while also performing complex analytical workloads on historical data. You will
    gain practical knowledge on leveraging open source big data technologies to build
    scalable and flexible data pipelines. Whether you need real-time analytics, machine
    learning model training, or ad-hoc analysis, modern data stack patterns empower
    you to support diverse data needs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 所涵盖的概念将帮助你以低延迟处理流数据，同时在历史数据上执行复杂的分析工作负载。你将获得利用开源大数据技术构建可扩展、灵活的数据管道的实践经验。无论你需要实时分析、机器学习模型训练，还是临时分析，现代数据架构模式都能帮助你支持多样化的数据需求。
- en: This chapter provides the blueprint for transitioning from legacy data warehouses
    to next-generation data lakes. The lessons equip you with the key architectural
    principles, components, and technologies to build modern data platforms on Kubernetes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了从传统数据仓库过渡到下一代数据湖的蓝图。通过这些课程，你将掌握构建现代数据平台所需的关键架构原则、组件和技术，以便在 Kubernetes 上实现。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Data architectures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据架构
- en: Data lake design for big data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据数据湖设计
- en: Implementing the lakehouse architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现湖屋架构
- en: Data architectures
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据架构
- en: Modern data architectures have evolved significantly over the past decade to
    enable organizations to harness the power of big data and drive advanced analytics.
    Two key architectural patterns that have emerged are the Lambda and Kappa architectures.
    In this section, we will have a look at both of them and understand how they can
    provide a useful framework for structuring our big data environment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现代数据架构在过去十年中经历了显著的发展，使得组织能够利用大数据的力量并推动先进的数据分析。两种关键的架构模式是 Lambda 架构和 Kappa 架构。在本节中，我们将探讨这两种架构，并了解它们如何为构建我们的大数据环境提供有用的框架。
- en: The Lambda architecture
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda 架构
- en: The Lambda architecture is a big data processing architecture pattern that balances
    batch and real-time processing methods. Its name comes from the Lambda calculus
    model of computation. The Lambda architecture became popular in the early 2010s
    as a way to handle large volumes of data in a cost-effective and flexible manner.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 架构是一种大数据处理架构模式，平衡了批处理和实时处理方法。其名称源自 Lambda 计算模型。Lambda 架构在 2010 年代初期变得流行，成为一种以经济高效和灵活的方式处理大规模数据的方法。
- en: 'The core components of the Lambda architecture include the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 架构的核心组件包括以下内容：
- en: '**Batch layer**: Responsible for managing the master dataset. This layer ingests
    and processes data in bulk at regular intervals, typically every 24 hours. Once
    processed, the batch views are considered immutable and stored.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理层**：负责管理主数据集。该层在固定间隔内批量摄取和处理数据，通常为每 24 小时一次。数据处理完成后，批处理视图被视为不可变的，并被存储。'
- en: '**Speed layer**: Responsible for recent data that has not yet been processed
    by the batch layer. This layer processes data in real time as it arrives to provide
    low-latency views.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度层**：负责处理尚未由批处理层处理的最新数据。此层在数据到达时实时处理数据，以提供低延迟视图。'
- en: '**Serving layer**: Responsible for responding to queries by merging views from
    both the batch and speed layers.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务层**：负责通过合并批处理层和速度层的视图来响应查询。'
- en: 'Those components are presented in the architecture diagram in *Figure 4**.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件在*图 4.1*的架构图中有所展示：
- en: '![Figure 4.1 – Lambda architecture design](img/B21927_04_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – Lambda 架构设计](img/B21927_04_01.jpg)'
- en: Figure 4.1 – Lambda architecture design
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – Lambda 架构设计
- en: The key benefit of the Lambda architecture is that it provides a hybrid approach
    that combines historical views of large data volumes (batch layer) with up-to-date
    views of recent data (speed layer). This enables analysts to query both recent
    and historical data in a unified way to gain quick insights.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 架构的主要优点是它提供了一种混合方法，将大量历史数据视图（批处理层）与最新数据的视图（速度层）结合在一起。这使得分析师可以以统一的方式查询最新和历史数据，从而获得快速的洞察。
- en: The batch layer is optimized for throughput and efficiency while the speed layer
    is optimized for low latency. By separating the responsibilities, the architecture
    avoids having to run large-scale, long-running batch jobs for every query. Instead,
    queries can leverage pre-computed batch views and augment them with up-to-date
    data from the speed layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理层针对吞吐量和效率进行了优化，而速度层则针对低延迟进行了优化。通过分离各自的职责，该架构避免了每次查询都需要运行大规模、长期运行的批处理任务。相反，查询可以利用预计算的批处理视图，并通过速度层的最新数据进行增强。
- en: In a modern data lake built on cloud infrastructure, the Lambda architecture
    provides a flexible blueprint. The cloud storage layer serves as the foundational
    data lake where data is landed. The batch layer leverages distributed data processing
    engines such as Apache Spark to produce batch views. The speed layer streams and
    processes the most recent data, and the serving layer runs performant query engines
    such as Trino to analyze data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建于云基础设施上的现代数据湖中，Lambda 架构提供了一个灵活的蓝图。云存储层作为基础数据湖，用于存储数据。批处理层利用分布式数据处理引擎，如 Apache
    Spark，生成批处理视图。速度层流式处理并处理最新数据，服务层运行高效的查询引擎，如 Trino，用于分析数据。
- en: The Kappa architecture
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kappa 架构
- en: The Kappa architecture emerged more recently as an alternative approach from
    primarily the same creators of the Lambda architecture. The main difference in
    the Kappa architecture is that it aims to simplify the Lambda model by eliminating
    the separate batch and speed layers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kappa 架构作为一种替代方法，最近由 Lambda 架构的主要创始人提出。Kappa 架构的主要区别在于，它旨在通过消除单独的批处理层和速度层来简化
    Lambda 模型。
- en: 'Instead, the Kappa architecture handles all data processing through a single
    stream processing pathway. The key components include the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Kappa 架构通过单一的流处理路径来处理所有数据。其关键组件包括以下内容：
- en: '**Stream processing layer**: Responsible for ingesting and processing all data
    as streams. This layer handles both historical data (via replay of logs/files)
    as well as new incoming data.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流处理层**：负责将所有数据作为流进行摄取和处理。此层处理历史数据（通过重放日志/文件）以及新来的数据。'
- en: '**Serving layer**: Responsible for responding to queries by accessing views
    produced by the stream processing layer.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务层**：负责通过访问流处理层生成的视图来响应查询。'
- en: 'We can see a visual representation in *Figure 4**.2*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 4.2*中看到一个可视化表示：
- en: '![Figure 4.2 – Kappa architecture design](img/B21927_04_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – Kappa 架构设计](img/B21927_04_02.jpg)'
- en: Figure 4.2 – Kappa architecture design
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – Kappa 架构设计
- en: At the core of Kappa is an immutable, append-only log for all data-using tools
    such as Kafka and event-sourcing paradigms. Streaming data is ingested directly
    into the log instead of separate pipelines. The log ensures ordered, tamper-proof
    data with automatic replayability – key enablers for both stream and batch processing.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Kappa 架构的核心是一个不可变的、仅附加的日志，供所有数据使用工具（如 Kafka 和事件溯源范式）使用。流数据直接被摄取到日志中，而不是通过独立的管道。该日志确保数据有序、无法篡改，并支持自动重放——这些是流处理和批处理的关键支持功能。
- en: The benefit of the Kappa architecture is its design simplicity. By having a
    single processing pathway, there is no need to manage separate batch and real-time
    systems. All data is handled through stream processing, which also enables flexible
    reprocessing and analysis of historical data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kappa 架构的优势在于其设计的简单性。通过单一的处理路径，不需要管理独立的批处理和实时系统。所有数据都通过流处理进行处理，这也使得历史数据的重新处理和分析变得更加灵活。
- en: The trade-off is that stream processing engines may not offer the same scale
    and throughput as the most advanced batch engines (although modern stream processors
    have continued to evolve to handle very large workloads). Also, while Kappa design
    can be simpler, the architecture itself can be much harder to implement and maintain
    than Lambda.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其权衡之处在于，流处理引擎可能无法提供与最先进的批处理引擎相同的规模和吞吐量（尽管现代流处理器已不断发展，以处理非常大的工作负载）。另外，尽管 Kappa
    设计可能更简单，但其架构本身可能比 Lambda 更难实现和维护。
- en: For data lakes, the Kappa architecture aligns well with the nature of large
    volumes of landing data. The cloud storage layer serves as the raw data backbone.
    Then, stream processors such as Apache Kafka and Apache Flink ingest, process,
    and produce analysis-ready views of the data. The serving layer leverages technologies
    such as Elasticsearch and MongoDB to power analytics and dashboards.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据湖，Kappa 架构与大量原始数据的特性非常契合。云存储层充当原始数据的骨干。然后，像 Apache Kafka 和 Apache Flink
    这样的流处理器接收、处理并生成分析准备好的数据视图。服务层利用 Elasticsearch 和 MongoDB 等技术来推动分析和仪表板的展示。
- en: Comparing Lambda and Kappa
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较 Lambda 和 Kappa
- en: 'The Lambda and Kappa architectures take different approaches but solve similar
    needs in preparing, processing, and analyzing large datasets. Key differences
    are listed in *Table 4.1*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 和 Kappa 架构采取不同的方法，但在准备、处理和分析大数据集方面解决了相似的问题。其主要区别列于*表 4.1*：
- en: '|  | **Lambda** | **Kappa** |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | **Lambda** | **Kappa** |'
- en: '| --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Complexity | Manages separate batch and real-time systems | Consolidates
    processing through streams |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 复杂度 | 管理独立的批处理和实时系统 | 通过流合并处理 |'
- en: '| Reprocessing | Reprocesses historical batches | Relies on stream replay and
    algorithms |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 重新处理 | 重新处理历史批次 | 依赖于流重放和算法 |'
- en: '| Latency | Lower latencies for recent data in the speed layer | Same latency
    for all data |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 延迟 | 在速度层中对最近数据有较低的延迟 | 所有数据的延迟相同 |'
- en: '| Throughput | Leverages batch engines optimized for throughput | Processes
    all data as streams |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 吞吐量 | 利用优化吞吐量的批处理引擎 | 将所有数据作为流处理 |'
- en: Table 4.1 – Lambda and Kappa architecture main differences
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 – Lambda 和 Kappa 架构的主要区别
- en: In practice, modern data architectures often blend these approaches. For example,
    a batch layer on Lambda may run only weekly or monthly while real-time streams
    fill the gap. Kappa may leverage small batches within its streams to optimize
    throughput. The core ideas around balancing latency, throughput, and reprocessing
    are shared.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，现代数据架构通常将这些方法结合使用。例如，Lambda 的批处理层可能每周或每月运行一次，而实时流则填补空白。Kappa 可能会在流中利用小批次来优化吞吐量。平衡延迟、吞吐量和重新处理的核心思想是相同的。
- en: For data lakes, Lambda provides a proven blueprint while Kappa offers a powerful
    alternative. While some may argue that Kappa offers a simpler operation, it is
    hard to implement and its costs can grow rapidly with scale. Another advantage
    of Lambda is that it is fully adaptable. We can implement only the batch layer
    if no data streaming is necessary (or financially viable).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据湖，Lambda 提供了一个经过验证的蓝图，而 Kappa 提供了一个强大的替代方案。虽然有人可能认为 Kappa 提供了更简化的操作，但它难以实现，并且随着规模的扩大，其成本可能快速增长。Lambda
    的另一个优势是它完全可以适应需求。如果没有必要进行数据流处理（或者没有经济可行性），我们可以仅实现批处理层。
- en: Data lake builders should understand the key principles of each to craft optimal
    architectures aligned to their businesses, analytics, and operational needs. By
    leveraging the scale and agility of cloud infrastructure, modern data lakes can
    implement these patterns to handle today’s data volumes and power advanced analytics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖构建者应该理解每种架构的关键原则，以便根据他们的业务、分析和运营需求设计最佳架构。通过利用云基础设施的规模和灵活性，现代数据湖可以实施这些模式，以处理当今的数据量并推动高级分析。
- en: In the next section, we will dive deeper into the Lambda architecture approach
    and how it can be applied to creating performant, scalable data lakes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节，我们将深入探讨 Lambda 架构方法以及它如何应用于创建高性能、可扩展的数据湖。
- en: Data lake design for big data
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据的数据湖设计
- en: 'In this section, we will contrast data lakes with traditional data warehouses
    and cover core design patterns. This will set the stage for the hands-on tools
    and implementation coverage in the final “How to” section. Let’s start with the
    baseline for the modern data architecture: the data warehouse.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对比数据湖与传统数据仓库，并涵盖核心设计模式。这将为后面“如何做”的工具和实施部分奠定基础。让我们从现代数据架构的基线——数据仓库开始。
- en: Data warehouses
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库
- en: Data warehouses have been the backbone of business intelligence and analytics
    for decades. A data warehouse is a repository of integrated data from multiple
    sources, organized and optimized for reporting and analysis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库几十年来一直是商业智能和分析的支柱。数据仓库是一个集成多个来源的数据存储库，经过组织和优化，旨在用于报告和分析。
- en: 'The key aspects of the traditional data warehouse architecture are as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 传统数据仓库架构的关键方面如下：
- en: '**Structured data**: Data warehouses typically only store structured data such
    as transaction data from databases and CRM systems. Unstructured data from documents,
    images, social media, and so on are not included.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化数据**：数据仓库通常只存储结构化数据，例如来自数据库和CRM系统的交易数据。不包括来自文档、图片、社交媒体等的非结构化数据。'
- en: '**Schema-on-write**: The data structure and schema are defined upfront during
    data warehouse design. This means adding new data sources and changing business
    requirements can be difficult.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**写时模式**：数据结构和模式在数据仓库设计时就已定义。这意味着添加新的数据源和改变业务需求可能会很困难。'
- en: '**Batch processing**: Data is **extracted, transformed, and loaded** (**ETL**)
    from source systems in batches according to a schedule, often daily or weekly.
    This introduces latency when accessing up-to-date data.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理**：数据**提取、转换和加载**（**ETL**）是按计划批量从源系统提取的，通常是每日或每周。这会引入访问最新数据时的延迟。'
- en: '**Separate from source systems**: The data warehouse acts as a separate store
    of data optimized for analytics, independent from the source transactional systems.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与源系统分离**：数据仓库作为一个独立的数据存储，用于优化分析，与源事务系统分开。'
- en: The growth of data volumes, variety, and velocity in the era of big data exposed
    some limitations with the traditional data warehouse architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据时代，数据量、种类和速度的增长暴露了传统数据仓库架构的一些局限性。
- en: It could not cost-effectively store and process huge volumes of unstructured
    and semi-structured data from new sources such as websites, mobile apps, IoT devices,
    and social media. Also, it lacked flexibility – adding new data sources required
    changes to schemas and ETL, which made adaptations slow and expensive. Finally,
    batch processing couldn’t deliver insights quickly enough for emerging requirements
    such as real-time personalization and fraud detection.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它无法以具有成本效益的方式存储和处理来自新来源（如网站、移动应用、物联网设备和社交媒体）的庞大非结构化和半结构化数据。此外，它缺乏灵活性——添加新的数据源需要更改模式和ETL，这使得适应变得缓慢且昂贵。最后，批处理无法快速提供足够的洞察力，以满足实时个性化和欺诈检测等新兴需求。
- en: This gave rise to the data lake architecture in response, which we will see
    in detail next.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了数据湖架构的产生，作为应对，我们将在接下来的部分详细讨论。
- en: The rise of big data and data lakes
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据和数据湖的崛起
- en: In response to the aforementioned challenges, the new data lake approach made
    it possible to deal with huge storage of any type of data at scale, using affordable
    distributed storage such as Hadoop HDFS or cloud object storage. Data lakes operate
    in a schema-on-read way instead of an upfront schema. Data is stored in native
    formats, and only the schema is interpreted at the time of reading. It includes
    the capture, storage, and access of real-time streaming data via tools such as
    Apache Kafka. There is also a big open source ecosystem for scalable processing
    including MapReduce, Spark, and other tools.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对上述挑战，新的数据湖方法使得能够大规模处理任何类型的数据存储，使用诸如Hadoop HDFS或云对象存储等负担得起的分布式存储。数据湖以读取时模式运行，而不是预先定义的模式。数据以原生格式存储，只有在读取时才会解释模式。它包括通过诸如Apache
    Kafka等工具捕获、存储和访问实时流数据。此外，还有一个庞大的开源生态系统，支持可扩展的处理，包括MapReduce、Spark和其他工具。
- en: A data lake is a centralized data repository. It is designed to store data in
    its raw format as-is. This provides the flexibility to analyze different types
    of data on demand (tables, images, text, videos, etc.), instead of needing to
    predetermine how it will be used. Because it is implemented on top of object storages,
    it can store a huge amount of data coming from anywhere in different intervals
    (some data can come daily, some hourly, and some in near real time). Data lakes
    also separate storage and processing technology (different from the data warehouse,
    where storage and processing happen in a whole unique structure). Usually, data
    processing involves a distributed compute engine (such as Spark) for terabyte-scale
    processing.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是一个集中式的数据存储库。它旨在以原始格式存储数据，这样可以灵活地按需分析不同类型的数据（表格、图像、文本、视频等），而无需预先确定数据的使用方式。由于它是基于对象存储实现的，因此可以存储来自不同来源、在不同时间间隔下的数据（有些数据可能每天更新，有些每小时更新，甚至有些是接近实时的数据）。数据湖还将存储和处理技术分开（与数据仓库不同，数据仓库将存储和处理整合在一个独特的结构中）。通常，数据处理涉及分布式计算引擎（如Spark），用于处理TB级别的数据。
- en: 'Data lakes provided a way to cost-effectively store the huge and diverse data
    volumes that organizations were grappling with and perform analytics. However,
    they also had some challenges:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖提供了一种以成本效益的方式存储组织面临的庞大而多样化数据量并进行分析的方式。然而，它们也面临一些挑战：
- en: Without governance, data lakes risked becoming inaccessible data *swamps*. Data
    needed to be cataloged with context for discoverability.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有治理，数据湖有可能变成无法访问的数据*沼泽*。数据需要进行分类并提供上下文，以便发现。
- en: Preparing raw data for analysis still involved complex data wrangling across
    disparate siloed tools.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备原始数据进行分析仍然涉及在分散的孤立工具中进行复杂的数据处理。
- en: Most analytics still required data to be modeled, cleansed, and transformed
    first – such as a data warehouse. This duplicated efforts.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数分析仍然需要先对数据进行建模、清洗和转换——就像数据仓库一样。这种做法导致了重复劳动。
- en: The object-based storage systems used in data lakes did not allow to perform
    line-level modifications. Whenever a line in a table needed to be modified, the
    whole file would be rewritten, causing a big impact on processing performance.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖中使用的基于对象存储的系统无法执行行级修改。每当表格中的一行需要修改时，整个文件都必须重写，这会大大影响处理性能。
- en: In data lakes, there is not an efficient schema control. While the schema-on-read
    approach makes it easier for new data sources, there is no guarantee that tables
    will not change their structure because of a failed ingestion.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据湖中，没有高效的模式控制。虽然按需读取模式使得新的数据源更容易加入，但无法保证因为数据摄取失败，表格的结构不会发生变化。
- en: In recent years, there has been a major effort to overcome these new challenges
    by joining the best of both worlds, which is now known as the data lakehouse.
    Let’s dive into this concept.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，为了克服这些新挑战，业界进行了大量努力，将两者的优势结合在一起，这就是现在所称的数据湖仓（data lakehouse）。让我们深入了解这一概念。
- en: The rise of the data lakehouse
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据湖仓的兴起
- en: 'In the 2010s, the term “lakehouse” gained attention because of new open source
    technologies such as Delta Lake, Apache Hudi, and Apache Iceberg. The lakehouse
    architecture aims to combine the best aspects of data warehouses and data lakes:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代，随着Delta Lake、Apache Hudi和Apache Iceberg等新兴开源技术的出现，“湖仓”（lakehouse）这一术语开始受到关注。湖仓架构旨在结合数据仓库和数据湖的最佳特性：
- en: Supporting diverse structured and unstructured data at any scale like a data
    lake
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持像数据湖一样在任何规模上处理多种结构化和非结构化数据
- en: Providing performant SQL analytics across raw and refined data like a data warehouse
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供像数据仓库一样对原始数据和精炼数据进行高效的SQL分析
- en: '**ACID** (**atomic, c****onsistent, isolated, and durable**) transactions on
    large datasets'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对大规模数据集的**ACID**（**原子性、一致性、隔离性和持久性**）事务
- en: 'Data lakehouses allow storing, updating, and querying data simultaneously in
    open formats while ensuring correctness and reliability at scale. This enables
    features such as the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖仓允许以开放格式同时存储、更新和查询数据，同时确保数据在大规模环境下的正确性和可靠性。它支持以下功能：
- en: Schema enforcement, evolution, and management
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式强制执行、演化和管理
- en: Line-level *upserts* (updates + inserts) and deletes for performant mutability
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行级*更新插入*（更新+插入）和删除以实现高效的可变性
- en: Point-in-time consistency views across historic data
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按时间点一致性视图跨历史数据
- en: Using the lakehouse architecture, the entire analytics life cycle, from raw
    data to cleaned and modeled data to curated data products, is directly accessible
    in one place for both batch and real-time use cases. This drives greater agility,
    reduces duplication of efforts, and enables easier reuse and repurposing of data
    through the life cycle.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用湖屋架构，整个分析生命周期——从原始数据到清洗和建模后的数据，再到策划的数据产品——都可以在一个地方直接访问，适用于批处理和实时用例。这提升了敏捷性，减少了重复劳动，并通过生命周期使数据的重用和再利用更加容易。
- en: Next, we will look at how data is structured within this architecture concept.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨数据在这一架构概念中的结构化方式。
- en: The lakehouse storage layers
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 湖屋存储层
- en: 'Like the data lake architecture, the lakehouse is also built on cloud object
    storage, and it is commonly divided into three main layers: bronze, silver, and
    gold. This approach became known as the “medallion” design.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据湖架构类似，湖屋（Lakehouse）也建立在云对象存储之上，通常分为三个主要层次：铜层、银层和金层。这种方法被称为“奖章”设计。
- en: The bronze layer is the raw ingested data layer. It contains the original raw
    data from various sources, stored exactly as it was received. The data formats
    can be structured, semi-structured, or unstructured. Examples include log files,
    CSV files, JSON documents, images, audio files, and so on.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 铜层是原始数据摄取层。它包含来自各种来源的原始数据，存储方式与接收到时完全相同。数据格式可以是结构化的、半结构化的或非结构化的。例如，日志文件、CSV
    文件、JSON 文档、图片、音频文件等。
- en: The purpose of this layer is to store the data in its most complete and original
    format, acting as the version of truth for analytical purposes. No transformations
    or aggregations happen at this layer. It serves as the source for building curated
    and aggregated datasets in the higher layers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的目的是以最完整和最原始的格式存储数据，作为分析用途的真相版本。在这一层不会进行任何转换或聚合。它作为构建更高层次的策划和聚合数据集的源数据。
- en: The silver layer contains curated, refined, and standardized datasets that are
    enriched, cleaned, integrated, and conformed to business standards. The data has
    consistent schemas and is queryable for analytics.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 银层包含精心策划、精炼和标准化的数据集，这些数据集经过丰富、清洗、集成，并符合业务标准。数据具有一致的模式，能够进行查询以支持分析。
- en: The purpose of this layer is to prepare high-quality, analysis-ready datasets
    that can feed into downstream analytics and machine learning models. This involves
    data wrangling, standardization, deduplication, joining disparate data sources,
    and so on.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的目的是准备高质量的、可分析的数据集，这些数据集可以为下游分析和机器学习模型提供支持。这涉及到数据整理、标准化、去重、联合不同数据源等。
- en: The structure can be tables, views, or files optimized for querying. Examples
    include Parquet, Delta Lake tables, materialized views, and so on. Metadata is
    added to enable data discovery.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该结构可以是表格、视图或优化查询的文件。例如，Parquet 文件、Delta Lake 表、物化视图等。元数据被添加以启用数据发现。
- en: The gold layer contains aggregated data models, metrics, KPIs, and other derivative
    datasets that power business intelligence and analytics dashboards.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 金层包含聚合后的数据模型、指标、关键绩效指标（KPI）和其他衍生数据集，支持商业智能和分析仪表板。
- en: The purpose of this layer is to serve ready-to-use curated data models to business
    users for reporting and visualization. This involves pre-computing metrics, aggregations,
    business logic, and so on to optimize for analytical workloads.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的目的是为业务用户提供现成的策划数据模型，用于报告和可视化。这涉及到预计算指标、聚合、业务逻辑等，以优化分析工作负载。
- en: The structure optimizes analytics through columnar storage, indexing, partitioning,
    and so on. Examples include aggregates, cubes, dashboards, and ML models. Metadata
    ties this to upstream data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该结构通过列存储、索引、分区等优化分析。示例包括聚合、数据立方体、仪表板和机器学习模型。元数据将其与上游数据连接。
- en: Sometimes, it is common to have an extra layer – a landing zone before the bronze
    layer. In this case, the landing zone receives raw data, and all the cleansing
    and structuring are done in the bronze layer.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在铜层之前，通常会有一个额外的层——着陆区。在这种情况下，着陆区接收原始数据，所有清洗和结构化工作都在铜层完成。
- en: In the next section, we will see how to operationalize the data lakehouse design
    with modern data engineering tools.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到如何使用现代数据工程工具将数据湖屋设计付诸实践。
- en: Implementing the lakehouse architecture
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施湖屋架构
- en: '*Figure 4**.3* shows a possible implementation of a data lakehouse architecture
    in a Lambda design. The diagram shows the common lakehouse layers and the technologies
    used to implement this on Kubernetes. The first group on the left represents the
    possible data sources to work with this architecture. One of the key advantages
    of this approach is its ability to ingest and store data from a wide variety of
    sources and in diverse formats. As shown in the diagram, the data lake can connect
    to and integrate structured data from databases as well as unstructured data such
    as API responses, images, videos, XML, and text files. This schema-on-read approach
    allows the raw data to be loaded quickly without needing upfront modeling, making
    the architecture highly scalable. When analysis is required, the lakehouse layer
    enables querying across all these datasets in one place using schema-on-query.
    This makes it simpler to integrate data from disparate sources to gain new insights.
    The separation of loading from analysis also enables iterative analytics as a
    new understanding of the data emerges. Overall, the modern data lakehouse is optimized
    for rapidly landing multi-structured and multi-sourced data while also empowering
    users to analyze it flexibly.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.3* 显示了在 Lambda 设计中实现数据湖仓架构的可能方式。该图显示了常见的数据湖仓层以及在 Kubernetes 上实现这些层所使用的技术。左侧的第一组代表了与此架构一起工作的可能数据源。此方法的一个关键优势是它能够摄取和存储来自多种来源和格式的数据。如图所示，数据湖可以连接并整合来自数据库的结构化数据，以及来自
    API 响应、图像、视频、XML 和文本文件等的非结构化数据。这种按需模式（schema-on-read）允许原始数据快速加载，而无需提前建模，从而使架构具有高度的可扩展性。当需要分析时，数据湖仓层可以使用按查询模式（schema-on-query）在一个地方查询所有这些数据集。这使得从不同来源整合数据以获得新见解变得更加简便。加载与分析的分离还使得随着对数据的新理解的出现，能够进行迭代分析。总体而言，现代数据湖仓旨在快速接纳多结构和多源数据，同时使用户能够灵活地分析这些数据。'
- en: First, we will take a closer look at the batch layer shown at the top of *Figure
    4**.3*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将仔细查看*图 4.3*顶部显示的批量层。
- en: Batch ingestion
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量摄取
- en: The first layer of the design refers to the batch ingestion process. For all
    the unstructured data, customized Python processes are the way to go. It is possible
    to develop custom code to query data from API endpoints, to read XML structures,
    and to process text and images. For structured data in databases, we have two
    options for data ingestion. First, Kafka and Kafka Connect provide a way of simply
    configuring data migration jobs and connecting to a large set of databases. Apache
    Kafka is a distributed streaming platform that allows publishing and subscribing
    to streams of records. At its core, Kafka is a durable message broker built on
    a publish-subscribe model. Kafka Connect is a tool included with Kafka that provides
    a generic way to move data into and out of Kafka. It offers reusable connectors
    that can help connect Kafka topics to external systems such as databases, key-value
    stores, search indexes, and filesystems. Kafka Connect features connector plugins
    for many common data sources and sinks such as JDBC, MongoDB, Elasticsearch, and
    so on. These connectors move data from the external system into Kafka topics and
    vice versa.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 设计的第一层是批量摄取过程。对于所有的非结构化数据，定制的 Python 处理过程是首选。可以开发自定义代码来从 API 端点查询数据，读取 XML 结构，处理文本和图像。对于数据库中的结构化数据，我们有两种数据摄取选择。首先，Kafka
    和 Kafka Connect 提供了一种简单配置数据迁移任务并连接到大量数据库的方法。Apache Kafka 是一个分布式流处理平台，允许发布和订阅记录流。在其核心，Kafka
    是一个基于发布-订阅模型构建的持久化消息代理。Kafka Connect 是 Kafka 附带的工具，提供了一种通用的方式将数据进出 Kafka。它提供了可重用的连接器，帮助将
    Kafka 主题连接到外部系统，如数据库、键值存储、搜索索引和文件系统等。Kafka Connect 为许多常见的数据源和接收器提供了连接器插件，如 JDBC、MongoDB、Elasticsearch
    等。这些连接器将外部系统中的数据移动到 Kafka 主题中，反之亦然。
- en: '![Figure 4.3 – Data lakehouse in Kubernetes](img/B21927_04_03.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – Kubernetes 中的数据湖仓](img/B21927_04_03.jpg)'
- en: Figure 4.3 – Data lakehouse in Kubernetes
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – Kubernetes 中的数据湖仓
- en: These connectors are reusable and configurable. For example, the JDBC connector
    can be configured to capture changes from a PostgreSQL database and write them
    to Kafka topics. Kafka Connect handles translating the data formats, distributed
    coordination, fault tolerance, and so on, and it supports stream processing by
    tracking data changes in the source connectors (e.g., database **change data capture**
    (**CDC**) connectors) and piping the change stream into Kafka topics. This simplifies
    the process of getting data in and out of Kafka. Although Kafka is a well-known
    tool for streaming data, its use alongside Kafka Connect has proven extremely
    efficient for batch data migration from databases.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些连接器是可重用且可配置的。例如，JDBC 连接器可以配置为捕获来自 PostgreSQL 数据库的变更并将其写入 Kafka 主题。Kafka Connect
    负责处理数据格式转换、分布式协调、容错等，并支持通过跟踪源连接器（例如数据库 **变更数据捕获**（**CDC**）连接器）中的数据变更，将变更流管道化到
    Kafka 主题中，从而简化了数据的进出 Kafka 的过程。虽然 Kafka 是一个广为人知的流数据工具，但与 Kafka Connect 一起使用已证明在数据库的批量数据迁移方面非常高效。
- en: Sometimes, when managing a Kafka cluster for data migration is not viable (we
    will talk about some of these cases later), it is possible to ingest data from
    structured sources with Apache Spark. Apache Spark provides a versatile tool for
    ingesting data from various structured data sources into a data lake built on
    cloud object storage such as Amazon S3 or Azure Data Lake Storage. The Spark DataFrame
    API allows querying data from relational databases, NoSQL data stores, and other
    structured data sources. While convenient, reading from JDBC data sources in Spark
    can be inefficient. Spark will read the table as a single partition, so all processing
    will occur in a single task. For large tables, this can slow down ingestion and
    subsequent querying (more details in [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092)).
    To optimize, we need to manually partition the reading from the source database.
    The main drawback with Spark data ingestion is handling these partitioning and
    optimization concerns yourself. Other tools can help by managing parallel ingestion
    jobs for you, but Spark gives the flexibility to connect and process many data
    sources out of the box.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，当管理 Kafka 集群进行数据迁移不可行时（我们稍后会讨论其中的一些情况），可以通过 Apache Spark 从结构化数据源摄取数据。Apache
    Spark 提供了一种多功能的工具，可以从各种结构化数据源摄取数据到基于云对象存储的数据湖中，例如 Amazon S3 或 Azure Data Lake
    Storage。Spark 的 DataFrame API 允许从关系数据库、NoSQL 数据存储以及其他结构化数据源查询数据。尽管很方便，但从 JDBC
    数据源读取数据在 Spark 中可能效率低下。Spark 会将表作为单个分区读取，因此所有处理将在单个任务中进行。对于大型表格，这可能会减慢数据摄取和后续查询的速度（更多细节请参见
    [*第 5 章*](B21927_05.xhtml#_idTextAnchor092)）。为了优化，我们需要手动对源数据库的读取进行分区。使用 Spark
    进行数据摄取的主要缺点是需要自己处理这些分区和优化问题。其他工具可以通过为你管理并行摄取任务来提供帮助，但 Spark 提供了连接和处理许多数据源的灵活性，开箱即用。
- en: Now, let’s take a look at the storage layer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下存储层。
- en: Storage
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: Next, in the middle of the diagram, we have the **storage** layer. This is the
    only one I do not recommend moving to Kubernetes. Cloud-based object storage services
    now have plenty of features that optimize scalability and reliability, making
    it simple to operate and with great retrieval performance. Although there are
    some great tools for building a data lake storage layer in Kubernetes (e.g., [https://min.io/](https://min.io/)),
    it is not worth the effort, since you would have to take care of scalability and
    reliability yourself. For the purposes of this book, we will work with all the
    lakehouse layers in Kubernetes except the storage layer.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在图表的中间部分，我们有 **存储** 层。这是我不建议迁移到 Kubernetes 的唯一一层。基于云的对象存储服务现在有许多功能，能够优化可扩展性和可靠性，使其操作简单且具有很好的检索性能。尽管有一些很棒的工具可以在
    Kubernetes 中构建数据湖存储层（例如 [https://min.io/](https://min.io/)），但这样做不值得，因为你必须自己处理可扩展性和可靠性。对于本书的目的，我们将在
    Kubernetes 中处理所有数据湖仓层，除了存储层。
- en: Batch processing
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理
- en: Now, we will talk about the **batch processing** layer. Apache Spark has become
    the de facto standard for large-scale batch data processing in the big data ecosystem.
    Unlike traditional MapReduce jobs that write intermediate data to disk, Spark
    processes data in memory, making it much faster for iterative algorithms and interactive
    data analysis. Spark utilizes a cluster manager to coordinate job execution across
    a group of worker nodes. This allows it to process very large datasets by distributing
    the data across the cluster and parallelizing the processing. Spark can efficiently
    handle terabytes of data stored in distributed filesystems such as HDFS and cloud
    object stores.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论**批处理处理**层。Apache Spark 已成为大数据生态系统中大规模批处理数据处理的事实标准。与传统的 MapReduce 作业不同，传统作业将中间数据写入磁盘，而
    Spark 在内存中处理数据，这使得其在迭代算法和交互式数据分析上更快。Spark 使用集群管理器来协调作业在多个工作节点上的执行。这使得它能够通过将数据分布到集群中并并行处理，来高效处理非常大的数据集。Spark
    能够高效地处理存储在分布式文件系统（如 HDFS）和云对象存储中的 TB 级数据。
- en: One of the key advantages of Spark is the unified API it provides for both SQL
    and complex analytics. Data engineers and scientists can use the Python DataFrame
    API to process and analyze batch datasets. The same DataFrames can then be queried
    through Spark SQL, providing familiarity and interactivity. This makes Spark very
    simple to operate for a wide range of users. By leveraging in-memory processing
    and providing easy-to-use APIs, Apache Spark has become the go-to solution for
    scalable batch data analytics. Companies with large volumes of log files, sensor
    data, or other records can rely on Spark to efficiently process these huge datasets
    in parallel. This has cemented its place as a foundational technology in modern
    data architectures.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的一个关键优势是它为 SQL 和复杂分析提供的统一 API。数据工程师和科学家可以使用 Python DataFrame API 来处理和分析批量数据集。然后，可以通过
    Spark SQL 查询相同的 DataFrame，提供熟悉性和交互性。这使得 Spark 对于各种用户来说非常易于操作。通过利用内存处理并提供易于使用的
    API，Apache Spark 成为可扩展批量数据分析的首选解决方案。拥有大量日志文件、传感器数据或其他记录的公司可以依赖 Spark 高效地并行处理这些庞大的数据集。这也巩固了它在现代数据架构中的基础技术地位。
- en: Next, we will discuss the orchestration layer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论编排层。
- en: Orchestration
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编排
- en: Above the storage layer and the batch processing layer, in *Figure 4**.3*, we
    find an **orchestration** layer. As we build more complex data pipelines that
    chain together multiple processing steps, we need a way to reliably manage the
    execution of these pipelines. This is where orchestration frameworks come in.
    Here, we chose to work with Airflow. Airflow is an open source workflow orchestration
    platform originally developed at Airbnb to author, schedule, and monitor data
    pipelines. It has since become one of the most popular orchestration tools for
    data pipelines.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储层和批处理层之上，在*图 4.3*中，我们找到了一个**编排**层。当我们构建更复杂的数据管道，将多个处理步骤连接在一起时，我们需要一种可靠的方式来管理这些管道的执行。这就是编排框架的作用。在这里，我们选择使用
    Airflow。Airflow 是一个开源的工作流编排平台，最初由 Airbnb 开发，用于编写、调度和监控数据管道。此后，它已成为数据管道中最流行的编排工具之一。
- en: 'The key reasons why using Airflow is important for batch data pipelines are
    as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Airflow 对于批处理数据管道的重要原因如下：
- en: '**Scheduling**: Airflow allows you to schedule batch jobs to run periodically
    (hourly, daily, weekly, etc.). This removes the need to manually kick off jobs
    and ensures they run reliably.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度**：Airflow 允许你定期调度批处理作业（每小时、每天、每周等）。这样就不需要手动启动作业，确保它们可靠地运行。'
- en: '**Dependency management**: Jobs often need to run sequentially or wait for
    other jobs to complete. Airflow provides an easy way to set up these dependencies
    in a **directed acyclic** **graph** (**DAG**).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖管理**：作业通常需要按顺序运行，或者等待其他作业完成。Airflow 提供了一种简单的方法来设置这些依赖关系，在**有向无环图**（**DAG**）中进行管理。'
- en: '**Monitoring**: Airflow has a built-in dashboard to monitor the status of jobs.
    You get visibility of what has succeeded, failed, is currently running, and so
    on. It also keeps logs and history for later debugging.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：Airflow 具有内置的仪表板，用于监控作业的状态。你可以看到哪些作业已成功、失败、正在运行等状态。它还会保留日志和历史记录，以供后续调试。'
- en: '**Flexibility**: New data sources, transformations, and outputs can be added
    by modifying the DAG without impacting other non-related jobs. Airflow DAGs provide
    high configurability.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：可以通过修改 DAG 来添加新的数据源、转换和输出，而不会影响其他不相关的作业。Airflow 的 DAG 提供了高度的可配置性。'
- en: '**Abstraction**: Airflow DAGs allow pipeline developers to focus on the business
    logic rather than application orchestration. The underlying Airflow platform handles
    the workflow scheduling, status monitoring, and so on.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象**：Airflow DAG 允许管道开发人员专注于业务逻辑，而非应用程序编排。底层的 Airflow 平台处理工作流调度、状态监控等事务。'
- en: Now, we will move on to the serving layer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进入服务层的介绍。
- en: Batch serving
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理服务
- en: For the **batch serving** layer in Kubernetes, we have chosen to work with Trino.
    Trino (formerly known as PrestoSQL) is an open source, distributed SQL query engine
    built for executing interactive analytic queries against a variety of data sources.
    Trino can be used to run queries up to a petabyte scale. With Trino, you can query
    multiple data sources in parallel. When a SQL query is submitted to Trino, it
    is parsed and planned to create a distributed execution plan. This execution plan
    is then submitted to worker nodes that process the query in parallel and return
    the results to the coordinator node. It supports ANSI SQL (one of the most common
    patterns for SQL) and it can connect to a variety of data sources, including all
    the main cloud-based object storage services. By leveraging Trino, data teams
    can enable self-service SQL analytics directly on their cloud data lakes. This
    eliminates costly and slow data movement just for analytics while still providing
    interactive response times.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Kubernetes 中的 **批处理服务** 层，我们选择了 Trino。Trino（前身为 PrestoSQL）是一个开源的分布式 SQL 查询引擎，旨在对多种数据源执行交互式分析查询。Trino
    可以处理高达 PB 级别的数据查询。使用 Trino，你可以并行查询多个数据源。当 SQL 查询提交给 Trino 时，它会被解析和规划，生成分布式执行计划。该执行计划随后会提交给工作节点，工作节点并行处理查询并将结果返回给协调节点。它支持
    ANSI SQL（最常见的 SQL 标准之一），并且可以连接多种数据源，包括所有主要的云端对象存储服务。通过利用 Trino，数据团队可以直接在云数据湖中实现自助
    SQL 分析，避免了仅为分析而进行的数据移动，且仍能提供交互式的响应时间。
- en: Next, we will take a look at the tools chosen for data visualization.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下为数据可视化选择的工具。
- en: Data visualization
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可视化
- en: For data visualization and analytics, we chose to work with Apache Superset.
    Although there are many great tools for this on the market, we find Superset easy
    to deploy, easy to run, easy to use, and extremely easy to integrate. Superset,
    an open source data exploration and visualization application, enables users to
    build interactive dashboards, charts, and graphs with ease. Superset originated
    at Airbnb in 2015 as an internal tool for its analysts and data scientists. As
    Airbnb’s usage and contributions grew, it decided to open source Superset in 2016
    under the Apache license and donated it to the Apache Software Foundation. Since
    then, Superset has been adopted by many other companies and has an active open
    source community contributing to its development. It has an intuitive graphical
    interface to visualize and explore data through rich dashboards, charts, and graphs
    that support many complex visualization types out of the box. It has a SQL Lab
    editor that allows you to write SQL queries against different databases and visualize
    results. It provides secure access and role management that allows granular control
    over data access and modification. It can connect to a great variety of data sources,
    including relational databases, data warehouses, and SQL engines such as Trino.
    Superset can be conveniently deployed on Kubernetes using Helm charts that are
    provided. The Helm chart provisions all the required Kubernetes objects – deployments,
    services, ingress, and so on, to run Superset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据可视化和分析，我们选择了使用 Apache Superset。尽管市场上有许多优秀的工具，我们发现 Superset 易于部署、易于运行、易于使用，并且极易集成。Superset
    是一个开源的数据探索和可视化应用，能够让用户轻松构建交互式仪表板、图表和图形。Superset 最早于 2015 年在 Airbnb 内部作为分析师和数据科学家的工具开发。随着
    Airbnb 对其使用和贡献的增加，它决定在 2016 年将 Superset 开源，并以 Apache 许可证发布，将其捐赠给 Apache 软件基金会。从那时起，Superset
    被许多其他公司采用，并拥有一个活跃的开源社区为其发展做出贡献。它具有直观的图形界面，可以通过丰富的仪表板、图表和图形可视化和探索数据，支持多种复杂的可视化类型。它拥有一个
    SQL Lab 编辑器，可以让你编写 SQL 查询，连接不同的数据库并可视化结果。它提供了安全访问和角色管理，允许对数据访问和修改进行细粒度的控制。它可以连接多种数据源，包括关系数据库、数据仓库和
    SQL 引擎，如 Trino。Superset 可以通过提供的 Helm charts 方便地在 Kubernetes 上部署。Helm chart 会配置所有必需的
    Kubernetes 对象——如部署、服务、入口等——以运行 Superset。
- en: With rich visualization capabilities, the flexibility to work with diverse data
    sources, and Kubernetes’ deployment support, Apache Superset is a valuable addition
    to the modern data stack on Kubernetes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于具备丰富的可视化功能、处理多样数据源的灵活性以及 Kubernetes 部署支持，Apache Superset 是现代数据栈中在 Kubernetes
    上的一个重要补充。
- en: Now, let’s move on to the bottom part of the diagram in *Figure 4**.3*, the
    real-time layer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续查看图表下方的部分，*图 4.3*，即实时层。
- en: Real-time ingestion
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时摄取
- en: In batch data ingestion, data is loaded in larger chunks or batches on a regular
    schedule. For example, batch jobs may run every hour, day, or week to load new
    data from source systems. On the other hand, in real-time data ingestion, data
    is streamed into the system continuously as it is generated. This enables a true,
    near-real-time flow of data into the data lake. Real-time data ingestion is *event-driven*
    – as events occur, they generate data that flows into the system. This could include
    things such as user clicks, IoT sensor readings, financial transactions, and so
    on. The system reacts to and processes each event as it arrives. Apache Kafka
    is one of the most popular open source tools that provide a scalable, fault-tolerant
    platform for handling real-time data streams. It can be used with Kafka Connect
    for streaming data from databases and other structured data sources or with customized
    data producers developed in Python, for instance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量数据摄取中，数据按照定期的时间表以较大的块或批次加载。例如，批量作业可能每小时、每天或每周运行一次，以从源系统加载新数据。另一方面，在实时数据摄取中，数据随着其生成持续不断地流入系统。这使得数据能够真实地、接近实时地流入数据湖。实时数据摄取是*事件驱动*的——当事件发生时，它们会生成数据并流入系统。这可能包括用户点击、物联网传感器读数、金融交易等内容。系统会对每个到达的事件做出反应并进行处理。Apache
    Kafka 是最流行的开源工具之一，它提供了一个可扩展、容错的平台来处理实时数据流。它可以与 Kafka Connect 配合使用，用于从数据库和其他结构化数据源流式传输数据，或者与用
    Python 等语言开发的定制数据生产者一起使用。
- en: Data ingested in real time on Kafka is usually also “synced” to the storage
    layer for later historical analysis and backup. It is not recommended that we
    use Kafka as our only real-time data storage. Instead, we apply the best practice
    of erasing data from it to save storage space after a defined period. The default
    period for this is seven days but we can configure it for any period. Nevertheless,
    real-time data processing is not done on top of the storage layer but by reading
    directly from Kafka. That is what we’re going to see next.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 实时摄取到 Kafka 的数据通常也会“同步”到存储层，以便进行后续的历史分析和备份。我们不建议仅仅使用 Kafka 作为实时数据存储。相反，我们遵循最佳实践，在定义的时间段后从
    Kafka 中清除数据，以节省存储空间。默认的时间段为七天，但我们可以根据需要进行配置。然而，实时数据处理并不是基于存储层进行的，而是通过直接从 Kafka
    中读取数据来完成的。这就是接下来要讨论的内容。
- en: Real-time processing
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时处理
- en: 'There is a variety of great tools for real-time data processing: Apache Flink,
    Apache Storm, and KSQLDB (which is part of the Kafka family). Nevertheless, we
    chose to work with Spark because of its great performance and ease of use.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多优秀的实时数据处理工具：Apache Flink、Apache Storm 和 KSQLDB（它是 Kafka 家族的一部分）。然而，我们选择使用
    Spark，因为它具有出色的性能和易用性。
- en: Spark Structured Streaming is a Spark module that we can use to process streaming
    data. The key idea is that Structured Streaming conceptually turns a live data
    stream into a table to which data is continuously appended. Internally, it works
    by breaking the live stream into tiny batches of data, which are then processed
    by Spark SQL as if they were tables.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Structured Streaming 是一个 Spark 模块，我们可以用它来处理流式数据。其核心思想是，Structured Streaming
    从概念上将实时数据流转换为一个表，数据会持续不断地附加到这个表中。内部机制是将实时数据流拆分成小批量的数据，然后通过 Spark SQL 处理这些数据，就像它们是表一样。
- en: After data from the live stream is broken up into micro-batches of a few milliseconds,
    each micro-batch is treated as a table that is appended to a logical table. Spark
    SQL queries are then executed on the batches as they arrive to generate the final
    stream of results. This micro-batch architecture provides scalability as it can
    leverage Spark’s distributed computation model to parallelize across data batches.
    More machines can be added to scale to higher data volumes. The micro-batch approach
    also provides fault tolerance guarantees. Structured Streaming uses checkpointing
    where the state of the computation is periodically snapshotted. If a failure occurs,
    streaming can be restarted from the last checkpoint to continue where it left
    off rather than recomputing all data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时流数据被拆分成几毫秒的小批次后，每个小批次被视为一个表，并附加到逻辑表中。然后，在每个批次到达时，Spark SQL 查询会在这些批次上执行，以生成最终的结果流。这种小批次架构提供了可扩展性，因为它可以利用
    Spark 的分布式计算模型在数据批次之间并行处理。可以通过增加更多机器来扩展以处理更大的数据量。小批次方法还提供了容错保障。结构化流使用检查点机制，在计算状态被定期快照。如果发生故障，流处理可以从最后一个检查点重新启动，而不是重新计算所有数据。
- en: Usually, Spark Structured Streaming queries read data directly from Kafka topics
    (using the necessary external libraries), process and make the necessary calculations
    internally, and write them to a real-time data serving engine. The real-time serving
    layer is our next topic.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Spark Structured Streaming 查询直接从 Kafka 主题读取数据（使用必要的外部库），内部处理并进行必要的计算后，将数据写入实时数据服务引擎。实时服务层是我们接下来的话题。
- en: Real-time serving
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时数据服务
- en: To serve data in real time, we need technologies that are able to make fast
    data queries and also return data with low latency. Two of the most used technologies
    for this are MongoDB and Elasticsearch.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实时提供数据，我们需要能够快速查询数据并以低延迟返回数据的技术。MongoDB 和 Elasticsearch 是两种最常用的技术。
- en: MongoDB is a popular open source, document-oriented NoSQL database. Instead
    of using tables and rows like traditional relational databases, MongoDB stores
    data in flexible, JSON-like documents that can vary in structure. MongoDB is designed
    for scalability, high availability, and performance. It uses an efficient storage
    format, index optimization, and other techniques to provide low-latency reads
    and writes. The document model and distributed capabilities allow MongoDB to handle
    the writes and reads of real-time data very efficiently. Queries, data aggregation,
    and analytics can be performed at scale on real-time data as it accumulates.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 是一个流行的开源文档型 NoSQL 数据库。与传统的关系型数据库使用表和行不同，MongoDB 以灵活的 JSON 类文档存储数据，这些文档的结构可以不同。MongoDB
    设计用于可扩展性、高可用性和高性能。它使用高效的存储格式、索引优化以及其他技术，以提供低延迟的读写操作。文档模型和分布式能力使 MongoDB 能够非常高效地处理实时数据的写入和读取。查询、数据聚合和分析可以在实时数据累积过程中进行大规模操作。
- en: 'Elasticsearch is an open source search and analytics engine that is built on
    Apache Lucene. It provides a distributed, multitenant-capable full-text search
    engine with an HTTP web interface and schema-free JSON documents. Some key capabilities
    and use cases of Elasticsearch include the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是一个开源的搜索和分析引擎，基于 Apache Lucene 构建。它提供了一个分布式、多租户支持的全文搜索引擎，具备 HTTP
    Web 接口和无模式的 JSON 文档。一些 Elasticsearch 的关键功能和应用场景包括：
- en: '**Real-time analytics and insights**: Elasticsearch allows you to analyze and
    explore unstructured data in real time. As the data is ingested, Elasticsearch
    indexes the data and makes it searchable immediately. This enables real-time monitoring
    and analysis of data streams.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时分析与洞察**：Elasticsearch 允许你实时分析和探索非结构化数据。当数据被接收时，Elasticsearch 会立即对其进行索引并使其可搜索。这使得数据流的实时监控和分析成为可能。'
- en: '**Log analysis**: Elasticsearch is commonly used to ingest, analyze, visualize,
    and monitor log data in real time from various sources such as application logs,
    network logs, web server logs, and so on. This enables real-time monitoring and
    troubleshooting.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志分析**：Elasticsearch 通常用于从各种源（如应用程序日志、网络日志、Web 服务器日志等）实时接收、分析、可视化和监控日志数据。这使得实时监控和故障排除成为可能。'
- en: '**Application monitoring and performance analytics**: By ingesting and indexing
    application metrics, Elasticsearch can be used to monitor and analyze application
    performance in real time. Metrics such as request rates, response times, error
    rates, and so on can be analyzed.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用监控和性能分析**：通过摄取和索引应用程序指标，Elasticsearch 可用于实时监控和分析应用程序性能。可以分析诸如请求速率、响应时间、错误率等指标。'
- en: '**Real-time web analytics**: Elasticsearch can ingest and process analytics
    data from website traffic in real time to enable features such as auto-suggest,
    real-time tracking of user behavior, and so on.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时网站分析**：Elasticsearch 可以实时摄取和处理来自网站流量的分析数据，以启用自动建议、用户行为实时追踪等功能。'
- en: '**Internet of Things (IoT) and sensor data**: For time-series IoT and sensor
    data, Elasticsearch provides functionality such as aggregation of data over time,
    anomaly detection, and so on that can enable real-time monitoring and analytics
    for IoT platforms.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物联网（IoT）和传感器数据**：对于时间序列的物联网和传感器数据，Elasticsearch 提供诸如数据聚合、异常检测等功能，能够实现物联网平台的实时监控和分析。'
- en: Because of its low latency and speed of data querying, Elasticsearch is a great
    tool for real-time data consumption. Also, the Elastic family has Kibana, which
    allows for real-time data visualization, which we will explore next.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其低延迟和数据查询速度，Elasticsearch 是一个非常适合实时数据消费的工具。此外，Elastic 系列产品中还有 Kibana，它可以实现实时数据可视化，接下来我们将进一步探讨它。
- en: Real-time data visualization
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时数据可视化
- en: Kibana is an open source data visualization and exploration tool that is designed
    to operate specifically with Elasticsearch. Kibana provides easy-to-use dashboards
    and visualizations that allow you to explore and analyze data indexed in Elasticsearch
    clusters. Kibana connects directly to an Elasticsearch cluster and indexes metadata
    about the cluster that it uses to present visualizations and dashboards about
    the data. It provides pre-built and customizable dashboards that allow for data
    exploration through visualizations such as histograms, line graphs, pie charts,
    heatmaps, and more. These make it easy to understand trends and patterns. With
    Kibana, users can create and share their own dashboards and visualizations to
    fit their specific data analysis needs. It has specialized tools for working with
    time-series log data, making it well-suited to monitoring IT infrastructure, applications,
    IoT devices, and so on, and it allows for the powerful ad-hoc filtering of data
    to drill down into specifics very quickly.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 是一个开源的数据可视化和探索工具，专门设计用于与 Elasticsearch 配合使用。Kibana 提供易于使用的仪表板和可视化功能，允许你探索和分析已在
    Elasticsearch 集群中索引的数据。Kibana 直接连接到 Elasticsearch 集群，并索引关于集群的元数据，利用这些元数据呈现关于数据的可视化和仪表板。它提供预构建的和可定制的仪表板，允许通过直方图、折线图、饼图、热力图等可视化方式进行数据探索。这些可视化方式使得理解趋势和模式变得容易。通过
    Kibana，用户可以创建并分享自己的仪表板和可视化，以满足特定的数据分析需求。它拥有专门的工具来处理时间序列日志数据，非常适合用于监控 IT 基础设施、应用程序、物联网设备等，并且能够快速进行强大的临时数据过滤，深入挖掘具体细节。
- en: A major reason that Kibana is great for real-time data is that Elasticsearch
    was designed for log analysis and full-text search – both of which require fast
    and near-real-time data ingestion and analysis. As data is streamed into Elasticsearch,
    Kibana visualizations update in real time to reflect the current state of the
    data. This makes it possible to monitor systems, detect anomalies, set alerts,
    and more based on live data feeds. The combination of scalability in Elasticsearch
    and interactive dashboards in Kibana makes an extremely powerful solution for
    real-time data visualization and exploration in large-scale systems.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 适用于实时数据的一个重要原因是 Elasticsearch 是为日志分析和全文搜索而设计的，这两者都需要快速和接近实时的数据摄取和分析。当数据流入
    Elasticsearch 时，Kibana 的可视化会实时更新，以反映数据的当前状态。这使得根据实时数据流监控系统、检测异常、设置警报等成为可能。Elasticsearch
    的可扩展性与 Kibana 的交互式仪表板相结合，为大规模系统中的实时数据可视化和探索提供了一个极为强大的解决方案。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the evolution of modern data architectures and key
    design patterns, such as the Lambda architecture that enables building scalable
    and flexible data platforms. We learned how the Lambda approach combines both
    batch and real-time data processing to provide historical analytics while also
    powering low-latency applications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了现代数据架构的演变和关键设计模式，例如 Lambda 架构，它支持构建可扩展和灵活的数据平台。我们学习了 Lambda 方法如何结合批处理和实时数据处理，提供历史数据分析的同时，还能支持低延迟的应用程序。
- en: We discussed the transition from traditional data warehouses to next-generation
    data lakes and lakehouses. You now understand how these modern data platforms
    based on cloud object storage provide schema flexibility, cost efficiency at scale,
    and unification of batch and streaming data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了从传统数据仓库到下一代数据湖和湖仓的过渡。现在你已经理解了这些基于云对象存储的现代数据平台如何提供架构灵活性、按需扩展的成本效益，并统一批处理和流式数据。
- en: We also did a deep dive into the components and technologies that make up the
    modern data stack. This included data ingestion tools such as Kafka and Spark,
    distributed processing engines such as Spark Structured Streaming for streams
    and Spark SQL for batch data, orchestrators such as Apache Airflow, storage on
    cloud object stores, and serving layers with Trino, Elasticsearch, and visualization
    tools such as Superset and Kibana.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还深入研究了构成现代数据技术栈的组件和技术。这包括数据摄取工具，如 Kafka 和 Spark，分布式处理引擎，如用于流式处理的 Spark Structured
    Streaming 和用于批数据的 Spark SQL，调度器，如 Apache Airflow，存储在云对象存储中的数据，以及使用 Trino、Elasticsearch
    和可视化工具（如 Superset 和 Kibana）的服务层。
- en: Whether your use cases demand ETL and analytics on historical data or real-time
    data applications, this modern data stack provides a blueprint. The lessons here
    form the foundation you need to ingest, process, store, analyze, and serve data
    to empower advanced analytics and power data-driven decisions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的使用场景是需要对历史数据进行 ETL 和分析，还是对实时数据应用程序进行处理，这个现代数据技术栈都提供了一个蓝图。这里的课程为你提供了必要的基础，帮助你摄取、处理、存储、分析并服务数据，从而支持高级分析并推动数据驱动的决策。
- en: In the next chapter, we are going to take a deep dive into Apache Spark, how
    it works, its internal architecture, and the basic commands to run data processing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨 Apache Spark，了解它的工作原理、内部架构，以及执行数据处理的基本命令。
