- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Building Multitenant Clusters with vClusters
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用vClusters构建多租户集群
- en: We’ve alluded to multitenancy in previous chapters, but this is the first chapter
    where we will focus on the challenges of multitenancy in Kubernetes and how to
    approach them with a relatively new technology called “virtual clusters.” In this
    chapter, we’ll explore the use cases for virtual clusters, how they’re implemented,
    how to deploy them in an automated way, and how to interact with external services
    with your `Pod's` identity. We’ll finish the chapter by building and deploying
    a self-service multitenant portal for Kubernetes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中提到了多租户，但这是第一章，我们将重点讨论Kubernetes中的多租户挑战，以及如何通过一种相对较新的技术“虚拟集群”来应对这些挑战。在本章中，我们将探索虚拟集群的使用案例，它们是如何实现的，如何以自动化的方式部署它们，以及如何通过`Pod`的身份与外部服务进行交互。最后，我们将通过构建和部署一个自助式多租户门户来结束本章内容。
- en: 'In this chapter, we’ll cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下内容：
- en: The benefits and challenges of multitenancy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多租户的好处与挑战
- en: Using vClusters for tenants
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用vClusters作为租户
- en: Building a multitenant cluster with self service
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个具有自助服务的多租户集群
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter will involve a larger workload than previous chapters, so a more
    powerful cluster will be needed. This chapter has the following technical requirements:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及比前几章更大的工作量，因此需要一个更强大的集群。 本章有以下技术要求：
- en: An Ubuntu 22.04+ server running Docker with a minimum of 8 GB of RAM, though
    16 GB is suggested
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台运行Docker的Ubuntu 22.04+服务器，最低需要8 GB内存，建议16 GB
- en: 'Scripts from the `chapter9` folder from the repo, which you can access by going
    to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自`chapter9`文件夹的脚本，可以通过访问本书的GitHub仓库获取：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)
- en: Getting Help
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取帮助
- en: We do our best to test everything, but there are sometimes half a dozen systems
    or more in our integration labs. Given the fluid nature of technology, sometimes
    things that work in our environment don’t work in yours. Don’t worry, we’re here
    to help! Open an issue on our GitHub repo at [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues)
    and we’ll be happy to help you out!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尽力测试所有内容，但有时我们的集成实验室中可能有六个或更多的系统。鉴于技术的不断变化，有时在我们的环境中能正常工作，而在你的环境中却不能。别担心，我们在这里为你提供帮助！请在我们的GitHub仓库上打开一个问题：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues)，我们很乐意帮助你！
- en: The Benefits and Challenges of Multitenancy
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多租户的好处与挑战
- en: Before we dive into virtual clusters and the **vCluster** project, let’s first
    explore what makes multitenant Kubernetes valuable and so difficult to implement.
    So far, we’ve alluded to challenges with multitenancy, but our focus has been
    on configuring and building a single cluster. This is the first chapter where
    we’re going to directly address multitenancy and how to implement it. The first
    topic we will explore is what multitenant Kubernetes is, and why you should consider
    using it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨虚拟集群和**vCluster**项目之前，让我们首先探讨一下是什么使得多租户Kubernetes如此有价值，却又如此难以实现。到目前为止，我们提到了多租户的挑战，但我们的重点一直是配置和构建单一集群。这是我们第一次直接解决多租户及其实现方法。本章的第一个主题是多租户Kubernetes是什么，以及为什么你应该考虑使用它。
- en: Exploring the Benefits of Multitenancy
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索多租户的好处
- en: 'Kubernetes orchestrates the allocation of resources for workloads through an
    API and a database. These workloads are typically comprised of Linux processes
    that require specific computing resources and memory. In the initial five to six
    years of Kubernetes’ evolution, a prevailing trend was to have a dedicated cluster
    for each “application.” It’s important to note that when we say “application,”
    it could refer to a single monolithic application, a collection of microservices,
    or several interrelated monolithic applications. This approach is notably inefficient
    and results in the proliferation of management complexities and potential wasted
    resources. To understand this better, let’s examine all of the elements within
    a single cluster:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 通过 API 和数据库协调工作负载的资源分配。这些工作负载通常由需要特定计算资源和内存的 Linux 进程组成。在 Kubernetes
    发展初期的五到六年里，主流的趋势是为每个“应用程序”分配一个专用集群。需要注意的是，当我们提到“应用程序”时，它可以指一个单体应用程序、一组微服务或几个相互关联的单体应用程序。这种方法明显效率低下，导致管理复杂性激增并可能浪费资源。为了更好地理解这一点，让我们来看看单个集群中的所有元素：
- en: '**etcd Database**: You should always have an odd number of `etcd` instances;at
    least three instances of `etcd` are needed to maintain high availability.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd 数据库**：你应该始终拥有奇数个 `etcd` 实例；至少需要三个 `etcd` 实例以保持高可用性。'
- en: '**Control Plane Nodes**: Similar to `etcd`, you’ll want at least two control
    plane nodes, but more likely three for high availability.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面节点**：与 `etcd` 类似，你至少需要两个控制平面节点，但为了高可用性，通常需要三个节点。'
- en: '**Worker Nodes**: You’ll want a minimum of two nodes, regardless of the load
    you’re putting on your infrastructure.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：无论基础设施上的负载如何，你至少需要两个节点。'
- en: Your control plane requires resources, so even though most Kubernetes distributions
    don’t require dedicated control plane nodes anymore, it’s still additional components
    to manage. Looking at your worker nodes, how much utilization are the nodes using?
    If you have a heavily used system, you will get the most out of the hardware,
    but are all your applications always that heavily utilized? Allowing multiple
    “applications” to use a single infrastructure can vastly reduce your hardware
    utilization, requiring fewer clusters, whether you’re running on pre-paid infrastructure
    or pay-as-you-go infrastructure. Over-provisioning resources will increase power,
    cooling, rack space, etc., all of which will add additional costs. Fewer servers
    also means reducing the maintenance requirements for hardware, further reducing
    costs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面需要资源，因此即使大多数 Kubernetes 发行版不再需要专门的控制平面节点，它仍然是额外的组件需要管理。看看你的工作节点，这些节点的利用率是多少？如果你有一个高负载的系统，你将能充分利用硬件，但你的所有应用程序是否总是如此高负载？让多个“应用程序”共享单一基础设施可以大大降低硬件利用率，减少所需集群数量，无论你是在预付费基础设施还是按需付费基础设施上运行。过度配置资源将增加功耗、冷却需求、机架空间等，这些都会增加额外的成本。更少的服务器还意味着减少硬件的维护需求，进一步降低成本。
- en: In addition to hardware costs, there’s a human cost to the cluster-per-application
    approach. Kubernetes skills are very expensive and difficult to find in the market.
    It’s probably why you’re reading this book! If each application group needs to
    maintain its own Kubernetes expertise, that means duplicating skills that are
    difficult to obtain, also increasing costs. By pooling infrastructure resources,
    it becomes possible to establish a centralized team responsible for overseeing
    Kubernetes deployments, eliminating the necessity for other teams, whose primary
    focus lies outside infrastructure development, to duplicate these skills.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件成本，集群每应用程序的方法还带来了人力成本。Kubernetes 技能非常昂贵且难以在市场上找到。这可能就是你正在阅读这本书的原因！如果每个应用程序组都需要维护自己的
    Kubernetes 专业知识，那么就意味着要重复获取那些难以获得的技能，也增加了成本。通过集中基础设施资源，可以建立一个专门负责 Kubernetes 部署的集中团队，消除其他团队（其主要关注点不在基础设施开发）重复这些技能的必要性。
- en: There are major security benefits to multitenancy as well. With common infrastructure,
    it’s easier to centralize enforcement of security requirements. Taking authentication
    as an example, if every application gets its own cluster, how will you enforce
    common authentication requirements? How will you onboard new clusters in an automated
    way? If you’re using OIDC, are you going to integrate a provider for each cluster?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户还有重要的安全优势。通过共享基础设施，更容易集中执行安全要求。以身份验证为例，如果每个应用都有自己的集群，如何执行统一的身份验证要求？如何以自动化的方式添加新集群？如果你使用
    OIDC，是不是要为每个集群集成一个身份提供者？
- en: Another example of the benefits of centralized infrastructure is secret management.
    If you have a centralized Vault deployment, do you want to integrate a new cluster
    for each application? In *Chapter 8*, we integrated a cluster with Vault; if you
    have massive cluster sprawl, the same integration needs to be done to each individual
    cluster – how will that be automated and managed?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 集中基础设施的另一个好处是密钥管理。如果你有一个集中式 Vault 部署，你是否希望为每个应用集成一个新集群？在*第 8 章*中，我们将一个集群与 Vault
    集成；如果你有大量集群扩展，相同的集成需要在每个单独的集群上完成——这如何实现自动化和管理？
- en: Moving to a multitenant architecture reduces your long-term costs by reducing
    the amount of infrastructure needed to run your workload. It also cuts down on
    the number of administrators needed to manage infrastructure and makes it easier
    to centralize security and policy enforcement.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移到多租户架构可以通过减少运行工作负载所需的基础设施数量，从而降低长期成本。它还减少了管理基础设施所需的管理员人数，并使得集中安全和策略执行变得更加容易。
- en: While multitenancy provides a significant advantage, it does come with some
    challenges. Next, we’ll explore the challenges of implementing multitenant Kubernetes
    clusters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然多租户提供了显著的优势，但它也带来了一些挑战。接下来，我们将探讨实现多租户 Kubernetes 集群的挑战。
- en: The Challenges of Multitenant Kubernetes
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多租户 Kubernetes 的挑战
- en: We explored the benefits of multitenant Kubernetes, and while the adoption is
    growing for multitenancy, why isn’t it as common as the cluster-per-application
    approach?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了多租户 Kubernetes 的好处，尽管多租户的采用正在增长，但为什么它不像每应用一个集群的方法那样普遍？
- en: Creating a multitenant Kubernetes cluster requires several considerations for
    security, usability, and management. Implementing solutions for these challenges
    is often very implementation-specific and requires integration with third-party
    tools, which are outside the scope of most distributions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个多租户 Kubernetes 集群需要考虑安全性、可用性和管理等多个方面。实施这些挑战的解决方案通常是非常具体的，并且需要与第三方工具集成，这些工具往往不在大多数发行版的范围内。
- en: Most enterprises add an additional layer of complexity based on their management
    silos. Application owners are judged based on their own criteria and by their
    own management. Anyone whose pay is dependent on certain criteria will want to
    make sure they have as much control of those criteria as possible, even if it’s
    for the wrong reasons. This silo effect can have an adversely negative impact
    on any centralization effort that doesn’t afford appropriate control to application
    owners. Since these silos are unique to each enterprise, it’s impossible for a
    single distribution to account for them in a way that is easily marketable. Rather
    than deal with the additional complexities, it’s much easier for a vendor to market
    a cluster-per-application approach.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业在管理孤岛的基础上增加了额外的复杂性。应用程序所有者根据自己的标准和管理进行评判。任何收入与某些标准挂钩的人都会想确保尽可能多地控制这些标准，即使这是出于错误的原因。这种孤岛效应可能对任何没有给予应用程序所有者适当控制的集中化努力产生负面影响。由于这些孤岛对每个企业来说都是独特的，因此单一发行版无法以易于推广的方式考虑这些问题。与其处理这些额外的复杂性，供应商更容易推销每应用一个集群的方法。
- en: With the fact that there are few multitenant Kubernetes distributions on the
    market, the next question becomes “What are the challenges of making a generic
    Kubernetes cluster multitenant?”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于市场上很少有多租户 Kubernetes 发行版，下一个问题是：“使一个通用的 Kubernetes 集群支持多租户有哪些挑战？”
- en: 'We’re going to break down the answers to this question by impact:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过影响的角度来分析这个问题的答案：
- en: '**Security**: Most containers are just Linux processes. We’ll cover this in
    more detail in *Chapter 12*, *Node Security with Gatekeeper*. What’s important
    to understand for now is that there is very little security that separates processes
    on a host. If you’re running processes from multiple applications, you want to
    make sure that a breakout doesn’t impact other processes.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：大多数容器实际上只是Linux进程。我们将在*第12章*，*使用Gatekeeper进行节点安全*中详细讨论这一点。目前需要理解的是，主机上的进程之间几乎没有安全隔离。如果你在运行来自多个应用程序的进程，你需要确保一个突破不会影响其他进程。'
- en: '**Container breakouts**: While this is important for any cluster, it’s a necessity
    in multitenant clusters. We will cover securing our container runtimes in *Chapter
    13*, *KubeArmor Securing Your Runtime*.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器突破**：尽管这对任何集群都很重要，但在多租户集群中这是必不可少的。我们将在*第13章*，*KubeArmor保护你的运行时*中讨论如何保护我们的容器运行时。'
- en: '**Impact**: Any issues with centralized infrastructure will have adverse impacts
    on multiple applications. This is often referred to as “blast radius.” If there’s
    an upgrade that goes wrong or fails, who’s impacted? If there’s a container breakout,
    who’s impacted?'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**影响**：任何集中式基础设施的问题都会对多个应用程序产生不利影响。这通常被称为“爆炸半径”。如果一次升级出现问题或失败，谁会受到影响？如果发生容器突破，谁会受到影响？'
- en: '**Resource Bottlenecks**: While it’s true that a centralized infrastructure
    gets better utilization of resources, it can also create bottlenecks. How quickly
    can you onboard a new tenant? How much control do application owners have in their
    own tenants? How difficult is it to grant or revoke access? If your multitenant
    solution can’t keep up with application owners, application owners will take on
    the infrastructure themselves. This will lead to wasted resources, configuration
    drift, and difficulty reporting and auditing all of the clusters.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源瓶颈**：虽然集中式基础设施确实可以更好地利用资源，但它也可能会造成瓶颈。你多快能将一个新租户加入进来？应用程序所有者在自己的租户中有多少控制权？授予或撤销访问权限有多困难？如果你的多租户解决方案跟不上应用程序所有者的步伐，应用程序所有者将自行承担基础设施。这将导致资源浪费、配置漂移以及所有集群的报告和审计困难。'
- en: '**Restrictions**: A centralized platform that is overly restrictive will result
    in application owners looking to either maintain their own infrastructure or outsource
    their infrastructure to third-party solutions. This is one of the most common
    issues with any centralized service that can be best illustrated by the continuous
    rise and fall of **Platform as a Service** (**PaaS**) implementations that fail
    to provide the flexibility needed for application workloads.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制**：一个过于限制的集中式平台会导致应用程序所有者选择维护自己的基础设施或将基础设施外包给第三方解决方案。这是任何集中式服务中最常见的问题之一，最能说明这一点的就是**平台即服务**（**PaaS**）实现的兴衰，这些实现未能提供应用工作负载所需的灵活性。'
- en: While these issues can be applied to any centralized service, they do have some
    unique impacts on Kubernetes. For instance, if each application gets its own namespace,
    how can an application properly subdivide that namespace for different logical
    functions? Should you grant multiple namespace per application?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些问题可以应用于任何集中式服务，但它们对Kubernetes确实有一些独特的影响。例如，如果每个应用程序都有自己的命名空间，那么应用程序如何在该命名空间内正确划分不同的逻辑功能？是否应该为每个应用程序授予多个命名空间？
- en: Another major impact on Kubernetes cluster designs is the deployment and management
    of **Custom Resource Definitions** (**CRDs**). CRDs are cluster-level objects,
    and running multiple versions is nearly impossible in the same cluster; as we
    have pointed out in previous chapters, CRDs are growing in popularity as a way
    of storing configuration data. Multitenant clusters may have version conflicts
    that need to be managed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群设计的另一个重大影响是**自定义资源定义**（**CRDs**）的部署和管理。CRDs是集群级别的对象，几乎不可能在同一个集群中运行多个版本；正如我们在前面的章节中指出的，CRDs作为存储配置数据的一种方式越来越受欢迎。多租户集群可能会遇到版本冲突，需要进行管理。
- en: 'Many of these challenges will be addressed in later chapters, but in this chapter,
    we’re going to focus on two aspects:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战将在后续章节中得到解决，但在本章中，我们将重点讨论两个方面：
- en: '**Tenant Boundaries**: What is the scope of a tenant? How much control within
    the boundary does the tenant have?'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**租户边界**：租户的范围是什么？租户在该边界内拥有多少控制权？'
- en: '**Self-Service**: How does a centralized Kubernetes service interact with users?'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助服务**：集中式Kubernetes服务如何与用户交互？'
- en: Both aspects will be addressed by adding two components to our clusters. OpenUnison
    has already been introduced to handle authentication and will be extended for
    its self-service capabilities with namespace as a Service. The other external
    system will be vCluster, from Loft Labs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过向集群中添加两个组件来解决这两个方面的问题。OpenUnison 已经被引入以处理身份验证，并将扩展其自服务功能，提供命名空间即服务（namespace
    as a Service）。另一个外部系统是 Loft Labs 提供的 vCluster。
- en: Since we have already used OpenUnison to demonstrate namespace as a Service,
    we can move on to the additional challenges, like CRD versioning issues in the
    next section using the vCluster project.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经使用 OpenUnison 演示了命名空间即服务，接下来我们可以继续探讨其他挑战，例如使用 vCluster 项目处理 CRD 版本控制问题。
- en: Using vClusters for Tenants
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为租户使用 vCluster
- en: In the *KinD* chapter, we explained that KinD is nested in Docker to provide
    us with a full Kubernetes cluster. We compared this to nesting dolls, where components
    are embedded in other components, which can cause confusion to users who are newer
    to containers and Kubernetes. vCluster is a similar concept – it creates a virtual
    cluster in the main host clusters, and while it does appear to be a standard Kubernetes
    cluster, it is nested within the host clusters. Keep this in mind as you are reading
    the rest of the chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *KinD* 章节中，我们解释了 KinD 是如何嵌套在 Docker 中为我们提供完整的 Kubernetes 集群的。我们将其与套娃进行比较，组件嵌入到其他组件中，这可能会让刚接触容器和
    Kubernetes 的用户感到困惑。vCluster 是一个类似的概念——它在主宿主集群中创建一个虚拟集群，虽然它看起来是一个标准的 Kubernetes
    集群，但它实际上是嵌套在宿主集群中的。在阅读本章其余部分时，请记住这一点。
- en: In the previous section, we walked through the benefits and challenges of multitenancy
    and how those challenges impact Kubernetes. In this section, we’re going to introduce
    the vCluster project from Loft Labs, which allows you to run a Kubernetes control
    plane inside of an unprivileged namespace. This allows each tenant to get their
    own “virtual” Kubernetes infrastructure that they can have complete control over
    without impacting other tenants’ own implementation or other workloads in the
    “main cluster.”
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讲解了多租户的好处与挑战，以及这些挑战如何影响 Kubernetes。本节将介绍 Loft Labs 的 vCluster 项目，它允许你在一个无特权的命名空间内运行
    Kubernetes 控制平面。这使得每个租户可以获得自己的“虚拟”Kubernetes 基础设施，完全控制而不会影响其他租户的实现或“主集群”中的其他工作负载。
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_09_01.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Graphical user interface, application  Description automatically generated](img/B21165_09_01.png)'
- en: 'Figure 9.1: Logical layout of a vCluster'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：vCluster 的逻辑布局
- en: 'In the above diagram, each tenant gets their own namespace, which runs a vCluster.
    The vCluster is a combination of three components:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，每个租户都获得自己的命名空间，其中运行一个 vCluster。vCluster 是由三部分组成的：
- en: '**Database**: Somewhere that can store the vCluster’s internal information.
    This may be `etcd` or a relational database, depending on which cluster type you
    deploy vCluster with.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库**：用于存储 vCluster 内部信息的地方。这可以是 `etcd` 或关系型数据库，具体取决于你部署 vCluster 的集群类型。'
- en: '**API Server**: The vCluster includes its own API server for its pods to interact
    with. This API server is backed by the database managed by the vCluster.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 服务器**：vCluster 包含一个自己的 API 服务器，供其 Pod 与之交互。这个 API 服务器由 vCluster 管理的数据库支持。'
- en: '**Synchronization Engine**: While a vCluster has its own API server, the pods
    are all run in the host cluster. To achieve this, the vCluster synchronizes certain
    objects between the host and vCluster. We’ll cover this in greater detail next.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步引擎**：虽然 vCluster 有自己的 API 服务器，但其所有 Pod 都运行在宿主集群中。为了实现这一点，vCluster 在宿主集群和
    vCluster 之间同步某些对象。我们将在接下来的部分详细介绍这一点。'
- en: The benefit of the vCluster’s approach is that from the `Pod's` perspective,
    it’s working within a private cluster while it’s really running in a main host
    cluster. The tenant can divide its own cluster into whatever namespace suit it
    and deploy CRDs, or operators, as needed.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: vCluster 方法的好处在于，从 `Pod` 的视角来看，它是在一个私有集群内工作，尽管它实际上运行在一个主宿主集群中。租户可以将自己的集群划分为适合的命名空间，并根据需要部署
    CRD 或操作器。
- en: '![A picture containing text  Description automatically generated](img/B21165_09_02.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text  Description automatically generated](img/B21165_09_02.png)'
- en: 'Figure 9.2: Pod’s perspective of a vCluster'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：Pod 视角下的 vCluster
- en: In the above diagram, we see that the pod is deployed into our tenant’s namespace,
    but instead of communicating with the host cluster’s API server, it’s communicating
    with the vCluster’s API server. This is possible because the pod definition that
    gets synchronized from the vCluster into the host cluster has its environment
    variables and DNS overwritten to tell the pod that the host `kubernetes.default.svc`
    points to the vCluster, not the host cluster’s API server.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示意图中，我们看到 Pod 部署到租户的命名空间中，但它并不是与宿主集群的 API 服务器通信，而是与 vCluster 的 API 服务器通信。这是因为从
    vCluster 同步到宿主集群的 Pod 定义，其环境变量和 DNS 被覆盖，指示 Pod 将宿主的 `kubernetes.default.svc` 指向
    vCluster，而不是宿主集群的 API 服务器。
- en: Since the pod runs in the host cluster, and the vCluster runs in the host cluster,
    all of the pods are subject to the `ResourceQuotas` put in place on the namespace.
    This means that any pod deployed to a vCluster is bound by the same rules as a
    pod deployed directly into a namespace including any restrictions created by quotas,
    policies, or other admission controllers. In *Chapter 11*, *Extending Security
    Using Open Policy Agent*, you’ll learn about using admission controllers to enforce
    policies across your cluster. Since the pod is running in the host cluster, you
    only need to apply those policies to your host. This vastly simplifies our security
    implementation because, now, tenants can be given `cluster-admin` access to their
    virtual clusters without compromising the security of the host cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Pod 运行在宿主集群中，而 vCluster 也运行在宿主集群中，因此所有的 Pods 都受到命名空间中设置的 `ResourceQuotas`
    的约束。这意味着任何部署到 vCluster 中的 Pod 都受制于与直接部署到命名空间中的 Pod 相同的规则，包括由配额、策略或其他准入控制器创建的限制。在*第
    11 章*，*使用 Open Policy Agent 扩展安全性*，你将学习如何使用准入控制器在集群中强制执行策略。由于 Pod 运行在宿主集群中，你只需将这些策略应用到宿主集群上。这大大简化了我们的安全实施，因为现在可以将
    `cluster-admin` 访问权限授予虚拟集群中的租户，而不会危及宿主集群的安全性。
- en: Another important note is that because the host cluster is responsible for running
    your pod, it’s also responsible for all of the vClusters `Ingress` traffic. You
    do not have to redeploy your `Ingress` controller on each vCluster – they share
    the host `Ingress` controller, reducing the need for maintaining additional `Ingress`
    deployments or creating multiple wildcard domains for each vCluster.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的说明是，宿主集群负责运行你的 Pod，因此它也负责所有 vCluster 的 `Ingress` 流量。你不必在每个 vCluster 上重新部署
    `Ingress` 控制器——它们共享宿主的 `Ingress` 控制器，减少了维护额外的 `Ingress` 部署或为每个 vCluster 创建多个通配符域名的需要。
- en: Now that we have a basic understanding of what a vCluster is and how it helps
    to address some of the challenges of multitenancy in Kubernetes, the next step
    is to deploy a vCluster and see how the internals work.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们基本理解了什么是 vCluster，以及它如何帮助解决 Kubernetes 中多租户的一些挑战，接下来的步骤是部署一个 vCluster 并查看其内部工作原理。
- en: Deploying vClusters
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 vClusters
- en: In the previous section, we focused on the theory behind how a vCluster works
    and is implemented. In this section, we’re going to deploy a vCluster and a simple
    workload so that we can see what changes have occurred between a pod that runs
    in a vCluster and what is deployed into the host cluster.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们关注了 vCluster 的工作原理及其实现方式。在本节中，我们将部署一个 vCluster 和一个简单的工作负载，以便查看运行在 vCluster
    中的 Pod 与部署到宿主集群中的 Pod 之间发生了什么变化。
- en: 'The first step is to create a new cluster – we will delete the existing KinD
    cluster and deploy a fresh one. We will then execute a script called `deploy_vcluster_cli.sh`
    in the `chapter9` directory:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个新的集群——我们将删除现有的 KinD 集群并部署一个新的集群。然后，我们将在 `chapter9` 目录中执行一个名为 `deploy_vcluster_cli.sh`
    的脚本：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At this point, we have a fresh cluster and the CLI for deploying vClusters.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经有了一个新的集群和部署 vCluster 的 CLI 工具。
- en: 'The next step is to create a vCluster. First, we will create a new `namespace`
    called `tenant1`, and then use the vCluster utility to create a new vCluster called
    `myvcluster` in the new namespace:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个 vCluster。首先，我们将创建一个名为 `tenant1` 的新 `namespace`，然后使用 vCluster 工具在新命名空间中创建一个名为
    `myvcluster` 的新 vCluster：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once the `vcluster` command has completed, we’ll have a running vCluster, built
    using `k3s`, running on the host cluster. Logically, it is its own cluster and
    we can “connect” directly to it using the vClusters kubeconfig or the `vcluster`
    utility.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 `vcluster` 命令完成，我们就会拥有一个正在运行的 vCluster，它是使用 `k3s` 构建的，并在宿主集群中运行。从逻辑上讲，它是一个独立的集群，我们可以通过
    vCluster 的 kubeconfig 或 `vcluster` 工具直接“连接”到它。
- en: '`vCluster` is designed to support multiple Kubernetes cluster implementations.
    The default, and most common, is `k3s`, which is a Kubernetes implementation that
    replaces `etcd` with a relational database and replaces the multiple binaries
    with a single binary. It was originally developed for edge deployments but works
    well for single tenants too. We could use `k0s` from **Mirantis** or even a vanilla
    Kubernetes, but `k3s` does well for most situations.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`vCluster` 旨在支持多个 Kubernetes 集群实现。默认且最常见的是 `k3s`，这是一种 Kubernetes 实现，它用关系数据库替代了
    `etcd`，并将多个二进制文件替换为一个单一的二进制文件。它最初是为边缘部署而开发的，但对于单租户环境也非常适用。我们可以使用 **Mirantis**
    的 `k0s` 或甚至一个原生 Kubernetes，但 `k3s` 对于大多数情况来说表现很好。'
- en: 'Let’s disconnect and see what’s running in our host:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们断开连接，看看在我们的主机上运行了什么：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As shown in the output above, there are two pods running in the `tenant1` namespace:
    CoreDNS and our vCluster. If we look at the services in our namespace, you will
    see a list of services similar to the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面的输出所示，在 `tenant1` 命名空间下有两个 pod 正在运行：CoreDNS 和我们的 vCluster。如果我们查看我们命名空间中的服务，你会看到一个类似下面的服务列表：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are several services set up to point to our vCluster’s API server and
    DNS server that provide access to the vCluster, making it logically appear as
    a “full” standard cluster.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个服务已配置指向我们 vCluster 的 API 服务器和 DNS 服务器，从而提供对 vCluster 的访问，使其在逻辑上看起来像是一个“完整”的标准集群。
- en: 'Now let’s connect to our vCluster and deploy a pod. In the `chapter9/simple`
    directory, we have a pod manifest that we will use for our example. First, we
    will connect to the cluster and deploy the example pod using `kubectl` in the
    `chapter9/simple` directory:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们连接到我们的 vCluster 并部署一个 pod。在 `chapter9/simple` 目录下，我们有一个 pod 清单，将用于我们的示例。首先，我们将连接到集群并使用
    `kubectl` 在 `chapter9/simple` 目录下部署示例 pod：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Notice that the `Pod''s` environment variables use `10.96.237.247` as the IP
    address for the API server; this is the `ClusterIP` from the `mycluster` Service
    that’s running on the host. Also, the nameserver is `10.96.142.24`, which is the
    `ClusterIP` for our vCluster’s `kube-dns Service` in the host. As far as the pod
    is concerned, it thinks it is running inside of the vCluster. It doesn’t know
    anything about the host cluster. Next, disconnect from the vCluster and take a
    look at the pods in our `tenant1` namespace on the host cluster:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Pod` 的环境变量使用 `10.96.237.247` 作为 API 服务器的 IP 地址；这是在主机上运行的 `mycluster` 服务的
    `ClusterIP`。此外，名字服务器是 `10.96.142.24`，这是我们 vCluster 的 `kube-dns 服务` 在主机上的 `ClusterIP`。就
    pod 而言，它认为自己是在 vCluster 内部运行的。它对主机集群一无所知。接下来，断开与 vCluster 的连接，并查看我们主机集群中 `tenant1`
    命名空间下的 pods：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Notice that your vCluster’s pod is running in your host cluster. The `Pod`''s
    name on the host includes both the name of the pod and the namespace from the
    vCluster. Let’s take a look at the pod definition. We’re not going to put all
    of the output here because it would take up multiple pages. What we want to point
    out is that in addition to our original definition, the pod includes a hard-coded
    `env` section:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您的 vCluster 的 pod 正在您的主机集群中运行。主机上 `Pod` 的名称包括 pod 的名称和来自 vCluster 的命名空间。让我们来看一下
    pod 的定义。我们不会在这里放入所有输出，因为它会占用多页。我们想指出的是，除了我们的原始定义外，pod 还包括一个硬编码的 `env` 部分：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It also includes its own `hostAliases`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包括自己的 `hostAliases`：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So, while the `Pod` is running in our host cluster, all of the things that tell
    the pod where it’s running are pointing to our vCluster.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管 `Pod` 正在我们的主机集群中运行，但所有告诉 pod 它在哪里运行的内容都指向了我们的 vCluster。
- en: In this section, we launched our first vCluster and a pod in that vCluster to
    see how it gets mutated to run in our host cluster. In the next section, we’re
    going to look at how we can access our vCluster with an eye on the same enterprise
    security we’re required to use in our host cluster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们启动了我们的第一个 vCluster 和一个在该 vCluster 中运行的 pod，以观察它如何被改变以便在我们的主机集群中运行。在下一部分，我们将研究如何访问我们的
    vCluster，同时保持与主机集群相同的企业安全要求。
- en: Securely Accessing vClusters
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全访问 vClusters
- en: In the previous section, we deployed a simple vCluster and accessed the vCluster
    using the `vcluster connect` command. This command first creates a port-forward
    to the vCluster’s API server `Service` and then adds a context with a master certificate
    to our `kubectl` configuration file, which is similar to our KinD cluster.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们部署了一个简单的 vCluster，并使用 `vcluster connect` 命令访问了 vCluster。该命令首先创建一个端口转发到
    vCluster 的 API 服务器 `Service`，然后将一个带有主证书的上下文添加到我们的 `kubectl` 配置文件中，这类似于我们的 KinD
    集群。
- en: 'We spent much of *Chapter 6*, *Integrating Authentication into Your Cluster*,
    walking through why this is an anti-pattern, and those reasons still apply to
    vClusters. You’re still going to need to integrate enterprise authentication into
    your vCluster. Let’s look at two approaches:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第六章*，*将认证集成到你的集群中*，花费了大量篇幅讲解了为什么这是一个反模式，这些理由同样适用于虚拟集群（vClusters）。你仍然需要将企业认证集成到你的虚拟集群中。让我们看看两种方法：
- en: '**Decentralized**: You can leave authentication as an exercise to the cluster
    owner. This negates many of the advantages of multitenancy and will require that
    each cluster is treated as a new integration into your enterprise’s identity system.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去中心化**：你可以将认证留给集群所有者处理。这会消解多租户的一些优点，并且要求每个集群都当作一个新的集成来接入企业的身份系统。'
- en: '**Centralized**: If you host an **identity provider** (**IdP**) in your host
    cluster, you can tie each vCluster to that IdP instead of directly to the centralized
    identity store. In addition to providing centralized authentication, this approach
    makes it easier to automate the onboarding of new vClusters and limits the amount
    of secret information, such as credentials, that needs to be stored in the vCluster.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中化**：如果你在主机集群中托管**身份提供者**（**IdP**），你可以将每个虚拟集群与该IdP关联，而不是直接连接到集中式身份存储。除了提供集中认证外，这种方法还使得自动化新虚拟集群的加入变得更容易，并且限制了需要存储在虚拟集群中的凭证等敏感信息的数量。'
- en: The choice is clear when working in a multitenant environment; your host cluster
    should also host central authentication.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在多租户环境中，选择是明确的；你的主机集群也应该承载中央认证功能。
- en: The next issue to understand regarding vCluster access is the network path to
    your vCluster. The `vcluster` command creates a local port-forward to your API
    server. This means that every time a user wants to use their API server, they’ll
    need to set up a port-forward to their API server. This isn’t a great **user experience**
    (**UX**) and can be error-prone. It would be better to set up a direct connection
    to our vCluster’s API server, as we would for any standard Kubernetes cluster.
    The challenge with setting up direct network access to our vCluster’s API server
    is that while it’s a `NodePort`, nodes are rarely exposed directly to the network.
    They usually sit behind a load balancer and rely on `Ingress` controllers to provide
    access to cluster resources.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 关于虚拟集群访问，接下来需要理解的是到虚拟集群的网络路径。`vcluster`命令会创建一个本地的端口转发到你的API服务器。这意味着每当用户想要使用他们的API服务器时，都需要设置端口转发到API服务器。这并不是一个理想的**用户体验**（**UX**），并且容易出错。最好是直接连接到我们虚拟集群的API服务器，就像我们为任何标准Kubernetes集群所做的一样。设置直接网络访问到虚拟集群的API服务器的挑战在于，尽管它是`NodePort`，但节点通常不会直接暴露到网络上。它们通常会被负载均衡器包围，并依赖`Ingress`控制器来提供对集群资源的访问。
- en: The answer is to use the application infrastructure our host cluster already
    provides for our vClusters.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是使用我们主机集群已经为虚拟集群提供的应用基础设施。
- en: In *Chapter 6*, *Integrating Authentication into Your Cluster*, we talked about
    using impersonating proxies with cloud-managed clusters. The same scenario can
    be applied to vClusters. While you can configure `k3s` to use OIDC for authentication,
    using an impersonating proxy vastly simplifies the network management because
    we’re not creating new load balancers or infrastructure to support our vClusters.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第六章*，*将认证集成到你的集群中*，我们讨论了如何在云管理的集群中使用代理伪装。相同的场景也可以应用于虚拟集群。虽然你可以配置`k3s`使用OIDC进行认证，但使用代理伪装大大简化了网络管理，因为我们不需要创建新的负载均衡器或基础设施来支持我们的虚拟集群。
- en: '![Diagram  Description automatically generated](img/B21165_09_03.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B21165_09_03.png)'
- en: 'Figure 9.3: vCluster with authentication'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：带认证的虚拟集群
- en: In the above diagram, we can see how the networking and authentication come
    together in our host cluster. The host cluster will have an OpenUnison that authenticates
    users to our Active Directory. Our vCluster will have its own OpenUnison with
    a trust established with our host cluster’s OpenUnison. The vCluster will use
    kube-oidc-proxy to translate the authentication tokens from OpenUnison into impersonation
    headers to our vCluster’s API server. This approach gives us a central authentication
    and networking system, while also making it easier for vCluster owners to incorporate
    their own management applications without having to get the host cluster team
    involved. Local cluster management applications such as **ArgoCD** and **Grafana**
    can all be integrated into the vCluster’s OpenUnison instead of the host cluster’s
    OpenUnison.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，我们可以看到网络和身份验证如何在我们的主集群中结合。主集群将拥有一个OpenUnison，用于验证用户身份并连接到我们的Active Directory。我们的vCluster将有自己的OpenUnison，并与主集群的OpenUnison建立信任关系。vCluster将使用kube-oidc-proxy将OpenUnison的身份验证令牌转换为模拟头，以便传递给vCluster的API服务器。这种方式为我们提供了一个集中式的身份验证和网络系统，同时也使得vCluster所有者能够轻松地整合自己的管理应用程序，而无需主集群团队的介入。像**ArgoCD**和**Grafana**这样的本地集群管理应用程序都可以集成到vCluster的OpenUnison中，而不是主集群的OpenUnison中。
- en: 'To show our setup in action, the first thing we need to do is update our vCluster
    so that it will synchronize `Ingress` objects from our vCluster into our host
    cluster, using the vcluster tool. In the `chapter/host` directory, we have an
    updated values file called `vcluster-values.yaml`; we will use this values file
    to upgrade the vCluster in the `tenant1` namespace:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们的设置如何运行，首先需要更新我们的vCluster，以便它能够通过vcluster工具将`Ingress`对象从vCluster同步到主集群。在`chapter/host`目录中，我们有一个名为`vcluster-values.yaml`的更新值文件；我们将使用此值文件来升级`tenant1`命名空间中的vCluster：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This command will update our vCluster to synchronize the `Ingress` objects
    we create in our vCluster into our host cluster. Next, we’ll need OpenUnison running
    in our host cluster:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将更新我们的vCluster，以便将我们在vCluster中创建的`Ingress`对象同步到主集群中。接下来，我们需要在主集群中运行OpenUnison：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Before deploying anything, we want to make sure that we’re running against
    the host, not our vCluster. The script we just ran is similar to what we ran in
    *Chapter 6*, *Integrating Authentication into Your Cluster*; it will deploy our
    “Active Directory” and OpenUnison to the vCluster. Once OpenUnison has been deployed,
    the last step is to run the satellite deployment process for OpenUnison:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署之前，我们需要确保我们是在主集群上运行，而不是在vCluster上。我们刚才运行的脚本与*第6章*中的内容相似，*将身份验证集成到集群中*；它将把“Active
    Directory”和OpenUnison部署到vCluster中。一旦OpenUnison被部署，最后一步是运行OpenUnison的卫星部署过程：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This script is like the host’s deployment script with some key differences:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本类似于主集群的部署脚本，但有一些关键的不同之处：
- en: The values for our OpenUnison do not contain any authentication information.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们OpenUnison的值不包含任何身份验证信息。
- en: The values for our OpenUnison have a different cluster name from our host cluster.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们OpenUnison的值与主集群有不同的集群名称。
- en: Instead of running `ouctl install-auth-portal`, the script runs `ouctl install-satelite`,
    which sets up OpenUnison to use OIDC between the satellite cluster and the host
    cluster. This command creates the `oidc` section of the `values.yaml` file for
    us.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 脚本并未运行`ouctl install-auth-portal`，而是运行`ouctl install-satelite`，该命令设置OpenUnison在卫星集群和主集群之间使用OIDC。这个命令为我们创建了`values.yaml`文件中的`oidc`部分。
- en: Once the script has completed executing, you can log in to OpenUnison just as
    you did in *Chapter 6*. In your browser, go to `https://k8sou.apps.X-X-X-X.nip.io`,
    where X-X-X-X is the IP address of your cluster, but with dashes instead of dots.
    Since our cluster is at `192.168.2.82`, we use `https://k8sou.apps.192-168-2-82.nip.io/`.
    To log in, use the user `mmosley` with the password `start123`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本执行完毕后，您可以像在*第6章*中一样登录OpenUnison。在浏览器中，访问`https://k8sou.apps.X-X-X-X.nip.io`，其中X-X-X-X是您集群的IP地址，但使用破折号代替点。由于我们的集群位于`192.168.2.82`，我们使用`https://k8sou.apps.192-168-2-82.nip.io/`。登录时，请使用用户`mmosley`和密码`start123`。
- en: Once you’re logged in, you’ll see that there is now a tree with options for
    **Host Cluster** and **tenant1**. You can click on **tenant1**, then click on
    the **tenant1 Tokens** badge.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，您会看到现在有一个树形结构，其中包含**主集群**和**tenant1**的选项。您可以点击**tenant1**，然后点击**tenant1
    Tokens**标签。
- en: '![Graphical user interface, application, website, Teams  Description automatically
    generated](img/B21165_09_04.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，网站，Teams  描述自动生成](img/B21165_09_04.png)'
- en: 'Figure 9.4: OpenUnison portal page'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：OpenUnison门户页面
- en: 'Once the new page loads, you can grab your `kubectl` configuration and paste
    it into your terminal:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 新页面加载后，您可以获取您的`kubectl`配置并将其粘贴到终端中：
- en: '`![Graphical user interface, text, application  Description automatically generated](img/B21165_09_05.png)`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`![图形用户界面，文本，应用程序 描述自动生成](img/B21165_09_05.png)`'
- en: 'Figure 9.5: OpenUnison kubectl configuration generator'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：OpenUnison kubectl配置生成器
- en: Depending on which client you’re running on, you can paste this command into
    a Windows or Linux/macOS terminal and start using your vCluster without having
    to distribute the `vcluster` CLI tool and while using your enterprise’s authentication
    requirements.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您正在使用的客户端，您可以将此命令粘贴到Windows或Linux/macOS终端中，并开始使用您的vCluster，而无需分发`vcluster`
    CLI工具，并且可以使用您企业的身份验证要求。
- en: In this section, we looked at how to integrate enterprise authentication into
    our vClusters and how to provide consistent network access to our vClusters as
    well. In the next section, we’ll explore how to integrate our vClusters with external
    services, such as HashiCorp’s Vault.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了如何将企业身份验证集成到我们的vCluster中，以及如何为我们的vCluster提供一致的网络访问。在下一节中，我们将探讨如何将我们的vCluster与外部服务（如HashiCorp的Vault）集成。
- en: Accessing External Services from a vCluster
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从vCluster访问外部服务
- en: In the previous chapter, we integrated a HashiCorp Vault instance into our cluster.
    Our pods communicated with Vault using the tokens projected into our pods, allowing
    us to authenticate to Vault without a pre-shared key or token and using short-lived
    tokens. Relying on short-lived tokens reduces the risk that a compromised token
    can be used against your cluster.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将HashiCorp Vault实例集成到我们的集群中。我们的Pods使用投影到Pod中的令牌与Vault进行通信，使我们能够在没有共享密钥或令牌的情况下进行Vault身份验证，并使用短期令牌。依赖短期令牌减少了被泄露的令牌被用于攻击集群的风险。
- en: The pod-based identity used with Vault gets more complex with vClusters because
    the keys used to create the `Pod's` tokens are unique to the vCluster. Also, Vault
    needs to know about each vCluster in order to verify the projected tokens used
    in the vCluster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与Vault一起使用的基于Pod的身份在vCluster中变得更加复杂，因为用于创建`Pod`令牌的密钥是vCluster特有的。此外，Vault需要了解每个vCluster，以便验证在vCluster中使用的投影令牌。
- en: If we are running our own Vault in our host cluster, we could automate the onboarding
    so that each new vCluster is registered with Vault as its own cluster. The challenge
    with this approach is that Vault is a complex system that’s often run by its own
    team with its own onboarding process. Adding a new vCluster in a way that works
    for the team that owns Vault may not be as simple as calling some APIs. Therefore,
    before we can implement a strategy for integrating our vClusters into Vault, we
    need to examine how vClusters handle identity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在宿主集群中运行自己的Vault，我们可以自动化入驻过程，以便每个新的vCluster都能作为独立集群注册到Vault中。这种方法的挑战在于Vault是一个复杂的系统，通常由专门的团队运行，并有自己的入驻流程。以适合拥有Vault的团队的方式添加新vCluster，可能并不像调用一些API那么简单。因此，在我们实施将vCluster集成到Vault的策略之前，需要先了解vCluster如何处理身份。
- en: 'A pod running in a vCluster has two distinct identities:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在vCluster中的Pod有两个不同的身份：
- en: '**vCluster Identity**: The token that is projected into our pod from a vCluster
    is scoped to the API server for the vCluster. It was signed by a unique key that
    the host cluster has no knowledge of. It is associated with the `ServiceAccount`
    the pod runs as inside of the vCluster’s API server.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vCluster身份**：从vCluster投影到我们Pod中的令牌作用范围仅限于vCluster的API服务器。该令牌由宿主集群无法识别的唯一密钥签名。它与Pod在vCluster的API服务器内运行时所使用的`ServiceAccount`相关联。'
- en: '**Host Cluster Identity**: While the pod is defined on the vCluster, it’s executed
    in the host cluster. This means that the security context of the pod will run
    and it requires a distinct identity from the vCluster. It will have its own name
    and its own signing keys.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宿主集群身份**：尽管Pod在vCluster中定义，但它是在宿主集群中执行的。这意味着Pod的安全上下文将以宿主集群为基础运行，并且需要与vCluster不同的身份。它将拥有自己的名称和签名密钥。'
- en: 'If we inspect a pod from our vCluster as synchronized into our host cluster,
    we’ll see that there’s an annotation that contains a token in it:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查从vCluster同步到宿主集群的Pod，我们会看到一个注解，其中包含一个令牌：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This token is injected into our pod via the `fieldPath` configuration later
    in the pod. This could be a security issue since anything that logs the `Pod`''s
    creation, such as an audit log, can now leak a token. The vCluster project has
    a configuration to generate `Secret` objects in the host cluster for project tokens
    so that they’re not in the pod manifest. Adding the following to our `values.yaml`
    file will fix this issue:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个令牌是通过 Pod 中后续的 `fieldPath` 配置注入到我们的 Pod 中的。这可能是一个安全问题，因为任何记录 `Pod` 创建的日志（如审计日志）现在都可能泄露令牌。vCluster
    项目有一个配置，用于在主集群中为项目令牌生成 `Secret` 对象，这样它们就不会出现在 Pod 清单中。将以下内容添加到我们的 `values.yaml`
    文件中将解决这个问题：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With that done, let’s update our cluster and redeploy all the `Pods`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些操作后，让我们更新集群并重新部署所有 `Pods`：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In a moment, the pods inside of the vCluster will come back. Inspecting the
    `Pod`, we see that the token is no longer in the `Pod`'s manifest but is now mounted
    to a `Secret` in the host. This is certainly an improvement, as audit systems
    are generally more discreet about logging the contents of `Secrets`. Next, let’s
    inspect our token’s claims.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一会儿，vCluster 中的 Pod 将恢复。检查 `Pod` 后，我们看到令牌不再出现在 `Pod` 的清单中，而是被挂载到了主机中的 `Secret`
    上。这无疑是一个改进，因为审计系统通常会对记录 `Secrets` 的内容更加谨慎。接下来，让我们检查一下令牌的声明。
- en: 'If we inspect this token, we’ll see some issues:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查这个令牌，我们会看到一些问题：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `exp` and `iat` claims are in bold because, when you translate this from
    Unix Epoch time to something a human can understand, this token is good from `Sunday,
    September 24, 2023 2:15:42 AM` until `Wednesday, September 21, 2033 2:15:42 AM`.
    That’s a ten-year token! This ignores the fact that the token was configured in
    the pod to only be good for ten minutes. This is a known issue in vCluster. The
    good news is that the tokens themselves are projected, so when the pod they’re
    projected into dies, the API server will no longer accept these tokens.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`exp` 和 `iat` 声明是加粗的，因为当你将其从 Unix 纪元时间转换为人类可以理解的时间时，该令牌有效的时间是从`2023年9月24日星期天凌晨2:15:42`到`2033年9月21日星期三凌晨2:15:42`。这可是一个十年的令牌！这忽略了令牌在
    Pod 中被配置为只在十分钟内有效的事实。这是 vCluster 中的已知问题。好消息是，令牌本身是投影的，因此当它们投影到的 Pod 死亡时，API 服务器将不再接受这些令牌。'
- en: The issue of vCluster token length comes into play when accessing external services
    because this will be true of any token we generate, not just tokens for the vCluster
    API server. When we integrated our clusters into Vault in the previous chapter,
    we did so using our `Pod's` identity so that we could leverage shorter-lived tokens
    that aren’t static and have well-defined expirations. A ten-year token is effectively
    a token with no expiration. The main mitigation is that we configured Vault to
    verify the status of the token before accepting it, so a token bound to a destroyed
    pod will be rejected by Vault.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: vCluster 令牌长度的问题出现在访问外部服务时，因为这对我们生成的任何令牌都适用，不仅仅是 vCluster API 服务器的令牌。当我们在上一章中将集群集成到
    Vault 时，我们使用了 `Pod` 的身份，这样我们就能利用短期有效且不静态的令牌，并且它们有明确的过期时间。十年的令牌实际上是一个没有过期时间的令牌。主要的缓解措施是，我们配置了
    Vault 来验证令牌的状态，在接受之前检查令牌是否有效，因此绑定到已销毁 Pod 的令牌将被 Vault 拒绝。
- en: 'The alternative to using the vCluster’s injected identity is to leverage the
    host cluster’s injected identity. This identity will be governed by the same rules
    as any other identity generated by the `TokenRequest` API in the host cluster.
    There are two issues with this approach:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 vCluster 注入的身份的替代方法是利用主集群注入的身份。这个身份将受到与主集群中 `TokenRequest` API 生成的其他身份相同的规则约束。采用这种方法有两个问题：
- en: '**vCluster disables host tokens**: In the host synced pod, `automountServiceAccountToken`
    is false. This is to prevent a collision between the vCluster and the host cluster
    because our pod shouldn’t know the host cluster exists! We can get around this
    by creating a mutating webhook that will add a `TokenRequest` API projection in
    the host cluster that can be accessed by our pod.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vCluster 禁用主机令牌**：在主机同步的 Pod 中，`automountServiceAccountToken` 为 false。这是为了防止
    vCluster 和主集群之间发生冲突，因为我们的 Pod 不应该知道主集群的存在！我们可以通过创建一个变异 webhook 来解决这个问题，它将在主集群中添加一个
    `TokenRequest` API 投影，供我们的 Pod 访问。'
- en: '**Host tokens don’t have vCluster namespaces**: When we do generate a host
    token for our synced pod, the namespace will be embedded in the name of the `ServiceAccount`,
    not as a claim in the token. This means that most external services’ policy languages
    will not be able to accept policies based on the host token, but configured via
    a namespace without creating a new policy for each vCluster namespace.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机令牌没有 vCluster 命名空间**：当我们为已同步的 pod 生成主机令牌时，命名空间将嵌入在 `ServiceAccount` 的名称中，而不是作为令牌中的声明。这意味着大多数外部服务的策略语言将无法基于主机令牌接受策略，而是通过命名空间配置，无需为每个
    vCluster 命名空间创建新策略。'
- en: These approaches both have benefits and drawbacks. The biggest benefit to using
    vCluster tokens is that you can easily create a policy that allows you to limit
    access to secrets based on namespaces inside of your vCluster without creating
    new policies for each namespace. The downside is the issues with vCluster tokens
    and the fact that you now need to onboard each individual vCluster into your Vault.
    Using host tokens better mitigates the issues with vCluster tokens, but you’re
    not able to easily create generic policies for each vCluster in Vault.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都有优缺点。使用 vCluster 令牌的最大好处是，你可以轻松创建一个策略，允许你基于 vCluster 内的命名空间来限制对机密的访问，而无需为每个命名空间创建新的策略。缺点是
    vCluster 令牌存在问题，并且现在你需要将每个单独的 vCluster 纳入到 Vault 中。使用主机令牌可以更好地缓解 vCluster 令牌的问题，但你无法轻松为
    Vault 中的每个 vCluster 创建通用策略。
- en: In this section, we spent time understanding how vClusters manage pod identities
    and how those identities can be used to interact with external services, such
    as Vault. In the next section, we will spend time on what’s needed to create a
    highly available vCluster and manage operations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们花时间了解了 vCluster 如何管理 pod 身份，以及这些身份如何用于与外部服务（如 Vault）进行交互。在下一节中，我们将讨论创建高可用
    vCluster 和管理操作所需的内容。
- en: Creating and Operating High-Availability vClusters
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和操作高可用 vClusters
- en: So far in this chapter, we’ve focused on the theory of how vClusters work, how
    to access a vCluster securely, and how vClusters handle pod identity to interact
    with external systems. In this section, we’ll focus on how to deploy and manage
    vClusters for high availability. Much of the documentation and examples for vCluster
    focuses on vCluster as a development or testing tool. For the use cases discussed
    earlier in this chapter, we want to focus on creating vClusters that can run production
    workloads. The first part of building production-ready vClusters is to understand
    how to run a vCluster in a way that allows for failures or downtime of individual
    components without hampering the vCluster’s ability to run.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本章我们已经专注于 vCluster 如何工作、如何安全地访问 vCluster，以及 vCluster 如何处理 pod 身份以与外部系统交互。在本节中，我们将专注于如何部署和管理高可用
    vClusters。许多 vCluster 的文档和示例都将其作为开发或测试工具来讨论。对于本章前面讨论的用例，我们希望专注于创建能够运行生产工作负载的 vClusters。构建生产就绪
    vClusters 的第一步是理解如何以允许个别组件发生故障或停机的方式运行 vCluster，而不会影响 vCluster 的运行能力。
- en: Understanding vCluster High Availability
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 vCluster 高可用性
- en: 'Let’s define the goal of a highly available vCluster and the gap between our
    current deployments and that goal. When you have a highly available vCluster,
    you want to make sure that:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个高可用 vCluster 的目标以及我们当前部署与该目标之间的差距。当你拥有一个高可用 vCluster 时，你需要确保：
- en: You can continue to interact with the API server during an upgrade or migration
    to another physical node in either the host cluster or the vCluster.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在升级或迁移到另一个物理节点时，你可以继续与 API 服务器交互，无论是在主机集群还是 vCluster 中。
- en: If there’s a catastrophic issue, you can restore from a backup.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果发生灾难性问题，你可以从备份中恢复。
- en: When running a vCluster, the first point about being able to interact with an
    API server becomes clear while upgrading the host cluster’s nodes. During that
    upgrade process, you want your API server to be able to continue to run. You want
    to be able to continue syncing objects from your virtual API server into your
    host; you also want pods that interact with the API server to still be able to
    do so during a physical host upgrade. For instance, if you’re using OpenUnison,
    then you want it to be able to create session objects so users can interact with
    their vClusters while host cluster operations are happening.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 vCluster 时，关于能够与 API 服务器交互的第一个要点会在升级主机集群节点时变得清晰。在升级过程中，你希望 API 服务器能够继续运行。你希望能够继续从虚拟
    API 服务器同步对象到主机；你还希望与 API 服务器交互的 pod 在主机升级时仍然能够正常工作。例如，如果你使用的是 OpenUnison，那么你希望它能够创建会话对象，以便用户可以在主机集群操作进行时与他们的
    vCluster 交互。
- en: The second point about disaster recovery is also important. We hope to never
    need it, but what happens if we’ve irrevocably broken our vCluster? Can we restore
    back to a point that we know was functional?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 灾难恢复的第二个要点也非常重要。我们希望永远不需要它，但如果我们的 vCluster 被不可逆地损坏，怎么办？我们能否恢复到一个已知是正常的状态？
- en: The first aspect of understanding how to run a highly available vCluster is
    that it will need multiple instances of pods that run the API server, syncer,
    and CoreDNS. If we look at our `tenant1` namespace, we’ll see that our vCluster
    has one pod that is associated with a `StatefulSet` that hosts the vCluster’s
    API server and syncer. There is also a pod that is synced from inside the vCluster
    for CoreDNS. We’d want there to be at least two (better if three) instances of
    each of these pods so that we can tell our API server to use `PodDisruptionBudget`
    to make sure we have a minimum number of instances running so that one can be
    brought down for whatever event is occurring.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何运行高可用 vCluster 的第一个方面是，它将需要多个实例的 pod 来运行 API 服务器、同步器和 CoreDNS。如果我们查看 `tenant1`
    命名空间，我们会看到 vCluster 中有一个 pod 与一个 `StatefulSet` 关联，该 `StatefulSet` 托管 vCluster
    的 API 服务器和同步器。还有一个 pod 是从 vCluster 内部同步的，用于 CoreDNS。我们希望每个 pod 至少有两个实例（最好是三个），这样我们就可以告诉
    API 服务器使用 `PodDisruptionBudget`，确保我们有最少数量的实例运行，以便在发生某些事件时，可以停掉其中一个实例。
- en: 'The second aspect to understand is how vCluster manages data. Our current deployment
    uses `k3s`, which uses a local SQLite database with data stored on a `PersistentVolume`.
    This works well for development, but for a production cluster, we want each component
    of our vCluster to be working off the same data. For `k3s`-based vClusters, this
    means using one of the supported relational databases or `etcd`. We could deploy
    `etcd`, but a relational database is generally easier to manage. We’re going to
    deploy our database in-cluster, but it wouldn’t be unusual to use an external
    database as well. In our exercises, we’ll use MySQL. We won’t worry about building
    a highly available database for our examples, since each database has its own
    mechanisms for high availability. If this were a production deployment though,
    you’d want to make sure that your database is built using the project’s recommended
    HA deployment and that you have a regular backup and recovery plan in place. With
    that said, let’s start by tearing down our current cluster and creating a new
    one:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个要理解的方面是 vCluster 如何管理数据。我们当前的部署使用了 `k3s`，它使用本地 SQLite 数据库，并将数据存储在 `PersistentVolume`
    上。这对于开发来说效果不错，但对于生产集群，我们希望每个 vCluster 组件都能使用相同的数据。对于基于 `k3s` 的 vCluster，这意味着需要使用支持的关系型数据库或
    `etcd`。我们可以部署 `etcd`，但关系型数据库通常更易于管理。我们将在集群内部署数据库，但使用外部数据库也是完全可行的。在我们的练习中，我们将使用
    MySQL。我们不会为示例构建高可用数据库，因为每个数据库都有其自身的高可用性机制。不过，如果这是生产部署，你需要确保数据库是按照项目推荐的高可用部署方式构建的，并且有定期的备份和恢复计划。话虽如此，让我们从拆除当前集群并创建新集群开始：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Wait for the new multi-node cluster to finish launching. Once it’s running,
    deploy MySQL:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 等待新的多节点集群启动完成。一旦它运行起来，部署 MySQL：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If the `deploy_mysql.sh` script fails with “Can’t connect to local MySQL server
    through socket,” wait a moment and rerun it. It’s safe to rerun. This script:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `deploy_mysql.sh` 脚本因“无法通过套接字连接到本地 MySQL 服务器”而失败，请稍等片刻然后重新运行。重新运行是安全的。该脚本：
- en: Deploys the cert-manager project with self-signed `ClusterIssuers`.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自签名的 `ClusterIssuers` 部署 cert-manager 项目。
- en: Creates TLS keypairs for MySQL.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 MySQL 创建 TLS 密钥对。
- en: Installs MySQL as a `StatefulSet` and configures it to accept TLS authentication.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装MySQL作为`StatefulSet`并配置它以接受TLS认证。
- en: Creates a database for our cluster and a user that’s configured to authenticate
    via TLS.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为我们的集群创建一个数据库，并配置一个用户通过TLS认证进行身份验证。
- en: 'With MySQL deployed and configured for TLS authentication, we’ll next create
    the `tenant` namespace and a certificate that will map to our database user:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署并配置好MySQL的TLS认证后，我们接下来将创建`tenant`命名空间，并创建一个证书来映射到我们的数据库用户：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we can deploy our vCluster:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以部署我们的vCluster：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It will take a few minutes, but you’ll have four pods in the `tenant1` namespace:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这将花费几分钟，但你将在`tenant1`命名空间中拥有四个Pod：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can now leverage `PodDisruptionBudget` to tell Kubernetes to keep one of
    the vCluster pods running during upgrades.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以利用`PodDisruptionBudget`来告诉Kubernetes，在升级过程中保持一个vCluster Pod持续运行。
- en: Speaking of upgrades, the next question is how to upgrade our vCluster. Now
    that we have a highly available vCluster, we can look to upgrade our vCluster
    to a new version.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 说到升级，下一个问题是如何升级我们的vCluster。现在我们有了一个高可用性的vCluster，我们可以考虑将vCluster升级到新版本。
- en: Upgrading vClusters
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 升级vCluster
- en: It’s important that you know how to upgrade your vClusters. You’ll want to make
    sure that your vCluster and host cluster don’t drift too far apart. While the
    pods that are synced into your host cluster will communicate with your vCluster’s
    API server, any impact on the synchronized pods (and other synchronized objects)
    could impact your workloads.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要了解如何升级你的vCluster。你要确保你的vCluster和主集群不会分离得太远。虽然同步到主集群中的Pod将与vCluster的API服务器进行通信，但对同步Pod（和其他同步对象）的任何影响都可能影响你的工作负载。
- en: 'Given the importance of staying up to date, it is great to report that upgrading
    a vCluster is incredibly easy. It’s important to remember that vCluster orchestrates
    the clusters and synchronizes objects, but the clusters themselves are managed
    by their own implementation. In our deployments, we’re using `k3s`, which will
    upgrade its data storage in the database when the new pods are deployed. Since
    the `vcluster create` command is a wrapper for Helm, all we need to do is update
    our values with the new image and redeploy:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于保持最新状态的重要性，值得报告的是，升级vCluster非常简单。重要的是要记住，vCluster负责编排集群并同步对象，但集群本身是由它们自己的实现进行管理的。在我们的部署中，我们使用`k3s`，它会在部署新Pod时升级数据库中的数据存储。由于`vcluster
    create`命令是Helm的一个包装器，我们只需要用新镜像更新我们的值并重新部署：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This command upgrades our vCluster to use a `k3s 1.30` image, which is just
    running a Helm upgrade on our installed chart. You’re leveraging the power of
    Kubernetes to simplify upgrades! Once it’s done running, you can check that the
    pods are now running `k3s 1.30`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将我们的vCluster升级为使用`k3s 1.30`镜像，实际上就是对我们安装的Helm图表执行升级。你正在利用Kubernetes的强大功能来简化升级！运行完成后，你可以检查Pod是否已经在运行`k3s
    1.30`：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We’ve covered creating highly available clusters and how to upgrade our vClusters.
    This is enough to embark on building a multitenant cluster. In the next section,
    we’ll integrate what we have learned to build out a multitenant cluster where
    each tenant gets their own vCluster.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了如何创建高可用集群以及如何升级vCluster。这些内容足以开始构建一个多租户集群。在下一节中，我们将整合所学内容，构建一个每个租户都有自己vCluster的多租户集群。
- en: Building a Multitenant Cluster with Self Service
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自助服务构建多租户集群
- en: In the previous sections, we explored how multitenancy works, how the vCluster
    project helps to address multitenancy challenges, and how to configure a vCluster
    with secure access and high availability. Each of these individual components
    was addressed as a separate component. The next question is how to integrate all
    these components into a single service. In this section, we’ll walk through creating
    a self-service platform for a multitenant cluster.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了多租户如何工作，vCluster项目如何帮助解决多租户挑战，以及如何配置具有安全访问和高可用性的vCluster。每个独立的组件都作为一个单独的部分进行讨论。接下来的问题是如何将这些组件整合成一个单一的服务。在本节中，我们将演示如何为多租户集群创建一个自助平台。
- en: One of the most important aspects of multitenancy is repeatability. Can you
    create each tenant the same way consistently? In addition to making sure that
    your approach is repeatable, what’s the amount of work that a customer needs to
    go through to get a new tenant? Remember that this book has a focus on enterprise,
    and enterprises almost always have compliance requirements. You also need to consider
    how to integrate your compliance requirements into the onboarding process.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户的一个最重要方面是可重复性。你能否以一致的方式创建每个租户？除了确保方法是可重复的外，客户需要花费多少工作来获取一个新租户？记住，本书聚焦于企业，企业几乎总是有合规性要求。你还需要考虑如何将合规性要求整合到入驻过程当中。
- en: The combination of needing repeatability and compliance often leads to the need
    for a self-service portal for onboarding new tenants. Creating a self-service
    portal has become the focus of many projects, often as part of a “Platform Engineering”
    initiative. We’re going to build our self-service platform from OpenUnison’s namespace
    as a Service portal. Using OpenUnison as our starting point, let’s us focus on
    how the components will integrate, rather than diving into the specifics of writing
    the code for the integrations. This multitenant self-service onboarding portal
    will serve as a starting point that we’ll add to as we explore more aspects of
    multitenancy through this book.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对可重复性和合规性的需求通常会导致需要为新租户提供自助服务门户。创建自助服务门户已经成为许多项目的重点，通常作为“平台工程”计划的一部分。我们将从 OpenUnison
    的命名空间作为服务门户来构建我们的自助服务平台。以 OpenUnison 作为起点，可以让我们专注于组件如何集成，而不是深入编写集成代码的具体细节。这个多租户自助入驻门户将作为起点，随着我们在本书中探索更多多租户方面的内容，逐步完善。
- en: We’re going to approach our multitenant cluster first by defining our requirements,
    then analyze how each of those requirements will be fulfilled, and finally, we’ll
    roll out our cluster and portal. Once we’re done with this section, you’ll have
    the start of a multitenant platform you can build from.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过定义需求来接近我们的多租户集群，然后分析每个需求如何得到满足，最后推出我们的集群和门户。完成本节后，你将拥有一个可以构建的多租户平台的起始框架。
- en: Analyzing Requirements
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析需求
- en: 'Our requirements for each individual tenant will be like requirements for a
    physical cluster. We’re going to want to:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个租户的要求将类似于物理集群的要求。我们将希望：
- en: '**Isolate tenants by authorization**: Who should have access to make updates
    to each tenant? What drives access? So far, we’ve been mainly concerned with cluster
    administrators, but now we need to worry about tenant administrators.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过授权隔离租户**：谁应该有权限更新每个租户？什么驱动了访问权限？到目前为止，我们主要关注集群管理员，但现在我们需要关注租户管理员。'
- en: '**Enforce enterprise authentication**: When a developer or admin accesses a
    tenant, we’ll need to ensure that we’re doing so using our enterprise authentication.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强制企业身份验证**：当开发者或管理员访问租户时，我们需要确保使用的是企业身份验证。'
- en: '**Externalized Secrets**: We want to make sure our source of truth for secret
    data is outside of our cluster. This will make it easier for our security team
    to audit usage.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部化秘密**：我们要确保秘密数据的真实来源位于集群之外，这样可以让我们的安全团队更容易进行审计。'
- en: '**High Availability and Disaster Recovery**: There are going to be times that
    we need to impact if a tenant’s API server is running. We’ll need to rely on Kubernetes
    to make sure that, even during those times, there’s a way for tenants to do their
    work.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性与灾难恢复**：在某些情况下，我们需要关注租户的 API 服务器是否在运行。我们需要依赖 Kubernetes 确保即使在这些情况下，租户也能继续进行工作。'
- en: '**Encryption in Transit**: All connections between components need to be encrypted.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传输中的加密**：所有组件之间的连接都需要加密。'
- en: '**No Secrets in Helm Charts**: Keeping secret data in a chart would mean that
    it’s stored as a `Secret` in our namespace, violating the requirement to externalize
    secret data.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Helm Charts 中无秘密**：将秘密数据保存在图表中意味着它作为 `Secret` 存储在我们的命名空间中，从而违反了外部化秘密数据的要求。'
- en: 'We’ve worked with most of these requirements already in this chapter. The key
    question is, “How do we pull everything together and automate it?” Having read
    through this chapter and looked through the scripts, you can probably see where
    this implementation is going. Just like any enterprise project, we need to understand
    how silos are going to impact our implementation. For our platform, we’re going
    to assume that:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中已经处理了大多数这些需求。关键问题是，“我们如何将一切整合并自动化？”通过阅读本章并查看脚本，你应该能大致看到这个实现的方向。就像任何企业项目一样，我们需要了解孤岛效应如何影响我们的实施。对于我们的平台，我们假设：
- en: '**Active Directory cannot be automatically updated**: It’s not unusual that
    you won’t be given the ability to create your own groups via an API to **Active
    Directory** (**AD**). While interacting with AD only requires LDAP capabilities,
    compliance requirements often dictate that a formal process is followed for creating
    groups and adding members to those groups.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Active Directory 不能自动更新**：通常情况下，你不会被赋予通过 API 在 **Active Directory**（**AD**）中创建自己组的权限。尽管与
    AD 的交互只需要 LDAP 功能，但合规性要求通常规定必须遵循正式流程来创建组并将成员添加到这些组中。'
- en: '**Vault can be automated**: Since Vault is API enabled and we have a good relationship
    with the Vault team, they’ll let us automate the onboarding of new tenants directly.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vault 可以自动化**：由于 Vault 已启用 API，并且我们与 Vault 团队有良好的关系，他们将允许我们直接自动化新租户的入驻过程。'
- en: '**Intra-Cluster Communication does not require the Enterprise CA**: Enterprises
    often have their own **certificate authorities** (**CAs**) for generating TLS
    certificates. These CAs are generally not exposed to an external facing API and
    are not able to issue intermediate CAs that can be used by a local cert-manager
    instance. We’ll use a CA specific to our cluster for issuing all certificates
    for use within the cluster.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群内部通信不需要企业 CA**：企业通常拥有自己的 **证书颁发机构**（**CAs**）来生成 TLS 证书。这些 CA 通常不会暴露给外部
    API，也不能颁发可以由本地证书管理器实例使用的中间 CA。我们将使用特定于我们集群的 CA 来颁发所有用于集群内的证书。'
- en: '**Host Cluster-Managed MySQL**: We’re going to host our MySQL instance on the
    cluster, but we won’t dive into operations around MySQL. We’ll assume that it’s
    been deployed as highly available. Database administration is its own discipline,
    and we won’t pretend to be able to cover it in this section.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机集群托管 MySQL**：我们将在集群上托管 MySQL 实例，但不会深入探讨 MySQL 相关的操作。我们假设它已经以高可用方式部署。数据库管理是一个独立的学科，我们不会假装能在这一部分覆盖它。'
- en: With these requirements and assumptions in hand, the next step is to plan out
    how to implement our multitenant platform.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了这些需求和假设后，下一步是规划如何实现我们的多租户平台。
- en: Designing the Multitenant Platform
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计多租户平台
- en: 'In the previous section, we defined our requirements. Now, let’s construct
    a matrix of tools that will tell us what each component will be responsible for:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们定义了需求。现在，让我们构建一个工具矩阵，来告诉我们每个组件的责任：
- en: '| **Requirement** | **Component** | **Notes** |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **需求** | **组件** | **备注** |'
- en: '| Portal Authentication | OpenUnison + Active Directory | OpenUnison will capture
    credentials; Active Directory will verify them. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 门户认证 | OpenUnison + Active Directory | OpenUnison 将捕获凭据，Active Directory
    将验证凭据。 |'
- en: '| Tenant | Kubernetes namespace + vCluster | Each tenant will receive their
    own namespace in the host cluster, with a vCluster deployed to it. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 租户 | Kubernetes 命名空间 + vCluster | 每个租户将在主机集群中获得自己的命名空间，并部署一个 vCluster。 |'
- en: '| Tenant Authentication | OpenUnison | Each tenant will receive its own OpenUnison
    instance. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 租户认证 | OpenUnison | 每个租户将获得自己的 OpenUnison 实例。 |'
- en: '| Authorization | OpenUnison with Active Directory Groups | Each tenant will
    have a unique Active Directory group that will provide administrative capabilities.
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 授权 | OpenUnison 与 Active Directory 组 | 每个租户将拥有一个唯一的 Active Directory 组，该组提供管理权限。
    |'
- en: '| Certificate Generation | `cert-manager` Project | `cert-manager` will generate
    the keys needed to communicate between vCluster and MySQL. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 证书生成 | `cert-manager` 项目 | `cert-manager` 将生成用于 vCluster 与 MySQL 之间通信所需的密钥。
    |'
- en: '| Secrets Management | Centralized Vault | Each tenant will receive its own
    Vault database that will be enabled with Kubernetes authentication. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 秘密管理 | 集中化 Vault | 每个租户将获得自己的 Vault 数据库，并启用 Kubernetes 认证。 |'
- en: '| Orchestration | OpenUnison | We’ll use OpenUnison’s workflow engine for onboarding
    new tenants. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 编排 | OpenUnison | 我们将使用 OpenUnison 的工作流引擎来进行新租户的入驻。 |'
- en: 'Table 9.1: Implementation matrix'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1：实现矩阵
- en: 'Given our requirements and implementation matrix, our multitenant platform
    will look like this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的需求和实施矩阵，我们的多租户平台将如下所示：
- en: '![Diagram  Description automatically generated](img/B21165_09_06.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B21165_09_06.png)'
- en: 'Figure 9.6: Multitenant platform design'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：多租户平台设计
- en: 'Using the above diagram, let’s walk through the steps that will need to occur
    to implement our platform:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述图示，让我们走一遍实现平台所需的步骤：
- en: OpenUnison will create a namespace and RoleBinding to our Active Directory group
    to the `admin` `ClusterRole`.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenUnison 将创建一个命名空间并将 RoleBinding 绑定到我们的 Active Directory 组，赋予其 `admin` `ClusterRole`
    权限。
- en: OpenUnison will generate a `Certificate` object in our tenant’s namespace, which
    will be used by our vCluster to communicate with MySQL.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenUnison 将在租户的命名空间中生成一个 `Certificate` 对象，vCluster 将使用它与 MySQL 通信。
- en: OpenUnison will create a database in MySQL for the vCluster and a user tied
    to the certificate generated in Step 2.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenUnison 将在 MySQL 中为 vCluster 创建一个数据库，并创建与步骤 2 中生成的证书关联的用户。
- en: OpenUnison will deploy a `Job` that will run the `vcluster` command and deploy
    the tenant’s vCluster.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenUnison 将部署一个 `Job`，该任务将运行 `vcluster` 命令并部署租户的 vCluster。
- en: OpenUnison will deploy a `Job` that will deploy the Kubernetes Dashboard, deploy
    OpenUnison, and integrate the vCluster OpenUnison into the host cluster’s OpenUnison.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenUnison 将部署一个 `Job`，该任务将部署 Kubernetes Dashboard，部署 OpenUnison，并将 vCluster
    OpenUnison 集成到宿主集群的 OpenUnison 中。
- en: OpenUnison will create an authentication policy in Vault that will allow tokens
    from our tenant’s vCluster to authenticate to Vault using local pod identities.
    It will also run a `Job` that will install the vault sidecar into our cluster.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenUnison 将在 Vault 中创建一个身份验证策略，允许租户的 vCluster 的令牌使用本地 Pod 身份验证 Vault。它还将运行一个
    `Job`，该任务将在我们的集群中安装 Vault sidecar。
- en: We’re giving our vCluster capabilities in the centralized Vault for retrieving
    secrets. In an enterprise deployment, you’d also want to control who can log in
    to Vault using the CLI and web interface using the same authentication and authorization
    as our clusters to tailor access, but that’s beyond the scope of this chapter
    (and book).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在集中式 Vault 中为 vCluster 提供检索机密的能力。在企业部署中，你还需要控制谁可以使用 CLI 和 Web 界面登录到 Vault，并使用与我们集群相同的身份验证和授权来定制访问权限，但这超出了本章节（以及本书）的范围。
- en: Finally, you could use any automation engine you’d like to perform these tasks,
    such as **Terraform** or **Pulumi**. If you want to use one of these tools instead,
    the same concepts can be used and translated into the implementation-specific
    details. Now that we’ve designed our onboarding process, let’s deploy it.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用任何你喜欢的自动化引擎来执行这些任务，例如 **Terraform** 或 **Pulumi**。如果你想使用其中一种工具，仍然可以应用相同的概念，并将其转换为特定实现的细节。现在我们已经设计好了我们的入驻流程，让我们开始部署它。
- en: Deploying Our Multitenant Platform
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署我们的多租户平台
- en: 'The previous section focused on the requirements and design of our multitenant
    platform. In this section, we’re going to deploy the platform and walk through
    deploying a tenant. The first step is to start with a fresh cluster:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上一部分主要介绍了我们多租户平台的要求和设计。在本部分，我们将部署该平台，并演示如何部署一个租户。第一步是从一个新的集群开始：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once your cluster is running, the next step is to deploy the portal. We scripted
    everything:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群启动，下一步是部署门户。我们已经将所有步骤脚本化：
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This script does quite a bit:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本完成了很多工作：
- en: Deploys `cert-manager` with internal CAs for our cluster
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署配置了我们内部 CA 的 `cert-manager` 以供我们的集群使用
- en: Deploys MySQL configured with our internal CA
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署配置了我们内部 CA 的 MySQL
- en: Deploys OpenUnison, using impersonation, and deploys our customizations for
    vCluster
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 OpenUnison，使用模拟身份，并部署我们为 vCluster 定制的内容
- en: Deploys Vault
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 Vault
- en: Integrates Vault and our control plane cluster
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集成 Vault 和我们的控制平面集群
- en: Enables OpenUnison to create new authentication mechanisms and policies in Vault
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使 OpenUnison 能够在 Vault 中创建新的身份验证机制和策略
- en: Depending on how much horsepower your infrastructure has, this script can take
    ten to fifteen minutes to run. Once deployed, the first step will be to log in
    to the portal at `https://k8sou.apps.IP.nip.io/`, where IP is your IP address
    with the dots changed to dashes. My cluster’s IP is `192.168.2.82`, so the URL
    is `https://k8sou.apps.192-168-2-102.nip.io/`. Use the user `mmosley` with the
    password `start123`. You’ll notice a new badge called **New Kubernetes Namespace**.
    Click on that badge.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的基础设施性能，这个脚本可能需要十到十五分钟才能运行完毕。部署完成后，第一步是登录到门户网站`https://k8sou.apps.IP.nip.io/`，其中IP是您的IP地址，点号替换为破折号。我的集群IP是`192.168.2.82`，因此URL为`https://k8sou.apps.192-168-2-102.nip.io/`。使用用户`mmosley`和密码`start123`登录。您会看到一个新的徽章，名为**新Kubernetes命名空间**。点击该徽章。
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_09_07.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B21165_09_07.png)'
- en: 'Figure 9.7: OpenUnison front page with the New Kubernetes Namespace badge'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：带有新Kubernetes命名空间徽章的OpenUnison首页
- en: On the next screen, you’ll be asked to provide some information for the new
    namespace (and tenant). We created two groups in our “Active Directory” for managing
    access to our tenant. While, out of the box, OpenUnison supports both the admin
    and view `ClusterRole` for mappings, we’re going to focus on the admin `ClusterRole`
    mapping. The admin group for our namespace will also be the `cluster-admin` for
    our tenant vCluster. This means any user that is added to this group in Active
    Directory will gain `cluster-admin` access to our vCluster for this tenant. Fill
    out the form as you see in *Figure 9.8* and click **SAVE**.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个屏幕中，您将被要求为新命名空间（和租户）提供一些信息。我们在“Active Directory”中创建了两个组来管理对租户的访问。虽然OpenUnison默认支持`ClusterRole`的admin和view角色映射，但我们将重点关注admin
    `ClusterRole`映射。我们命名空间的admin组也将成为租户vCluster的`cluster-admin`。这意味着任何被添加到此Active
    Directory组的用户都将获得对该租户vCluster的`cluster-admin`权限。按照*图9.8*中所示填写表单，并点击**保存**。
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_09_08.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件描述自动生成](img/B21165_09_08.png)'
- en: 'Figure 9.8: New Namespace'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：新命名空间
- en: Once saved, close this tab to return to the main portal and hit Refresh. You’ll
    see that there is a new menu option on the left-hand side called **Open Approvals**.
    OpenUnison is designed around self-service, so the assumption is that the tenant
    owners will request that a new tenant be deployed. In this case, mmosley will
    be both the tenant owner and the approver. Click on **Open Approvals** and click
    **Act on Request**
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 保存后，关闭此标签页以返回到主门户并点击刷新。您将看到左侧有一个新的菜单选项，名为**开放审批**。OpenUnison的设计基于自服务，因此假设租户所有者会请求部署新的租户。在这种情况下，mmosley将同时作为租户所有者和审批人。点击**开放审批**，然后点击**处理请求**。
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_09_09.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B21165_09_09.png)'
- en: 'Figure 9.9: Approval screen'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：审批屏幕
- en: Fill in the **Justification** field and click on **APPROVE REQUEST** and **CONFIRM
    APPROVAL**. This will approve the user’s request and launch a workflow that implements
    the steps we designed in *Figure 9.6*. This workflow will take five to ten minutes,
    depending on the horsepower of your cluster. Usually, OpenUnison will send the
    requestor an email once a workflow is complete, but we’re using an SMTP blackhole
    here to pull in all emails that are generated to make the lab implementation easier.
    You’ll have to wait until the `tenant1` namespace is created and the OpenUnison
    instance is running. If you look in the `tenant1` namespace on the host cluster,
    you’ll see that the `vault-agent-injector` pod is running. This lets you know
    the rollout is complete.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 填写**理由**字段，点击**批准请求**并**确认审批**。这将批准用户的请求并启动一个工作流，执行我们在*图9.6*中设计的步骤。这个工作流会持续五到十分钟，具体取决于集群的性能。通常，OpenUnison会在工作流完成后向请求者发送电子邮件，但我们在这里使用了一个SMTP黑洞来收集所有生成的邮件，以便简化实验室实现。您需要等到`tenant1`命名空间被创建并且OpenUnison实例正在运行。如果您查看主机集群中的`tenant1`命名空间，您会看到`vault-agent-injector`
    pod正在运行。这表示部署已完成。
- en: 'To track your vCluster’s deployment, there are three pods to look at:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟踪您的vCluster部署，有三个pod需要查看：
- en: '`onboard-vcluster-openunison-tenant1` – The pod from this Job contains the
    logs for creating and deploying vCluster into your tenant’s namespace.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onboard-vcluster-openunison-tenant1` – 该Job中的pod包含用于创建和部署vCluster到您租户命名空间的日志。'
- en: '`deploy-helm-vcluster-teant1` – The pod from this Job integrates Vault.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deploy-helm-vcluster-teant1` – 该作业中的 pod 集成了 Vault。'
- en: '`openunison-orchestra` – The pod from this deployment runs OpenUnison’s onboarding
    workflows.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openunison-orchestra` – 该部署中的 pod 运行 OpenUnison 的入门工作流。'
- en: If there are any errors in the process, you’ll find them here.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果过程中出现任何错误，您可以在这里找到它们。
- en: Now that our tenant has been deployed, we can log in and deploy a pod. Log out
    of OpenUnison, and log back in using the user name `jjackson` and the password
    `start123`. The `jjackson` user is a member of our admin group in Active Directory,
    so they’ll immediately be able to access and administer the vCluster in the `tenant1`
    namespace.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的租户已经部署完毕，我们可以登录并部署一个 pod。退出 OpenUnison，然后使用用户名 `jjackson` 和密码 `start123`
    重新登录。`jjackson` 用户是我们 Active Directory 中管理员组的成员，因此他们将立即能够访问并管理 `tenant1` 命名空间中的
    vCluster。
- en: '![Graphical user interface, application, website, Teams  Description automatically
    generated](img/B21165_09_10.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，网站，Teams  描述自动生成](img/B21165_09_10.png)'
- en: 'Figure 9.10: Access to tenant1 vCluster'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：访问 tenant1 vCluster
- en: 'The `jjackson` user is able to interact with our vCluster the same way they
    would with the host cluster, using either the dashboard or directly via the CLI.
    We’re going to use `jjackson`''s session to log in to our `tenant` vCluster and
    deploy a pod that uses secret data from our Vault. First, `ssh` into your cluster’s
    host on a new session and create a secret in our Vault for our pod to consume:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`jjackson` 用户能够像使用主集群一样与我们的 vCluster 交互，可以通过仪表盘或直接通过 CLI 进行操作。我们将使用 `jjackson`
    的会话登录到我们的 `tenant` vCluster，并部署一个使用 Vault 中密钥数据的 pod。首先，在新会话中 `ssh` 进入您的集群主机，并为我们的
    pod 创建一个 Vault 密钥：'
- en: '[PRE24]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The last command creates our secret data in the Vault. Note that the path that
    we created specifies that we’re working with the `tenant1` vCluster in the `default`
    `namespace`. The way our cluster is deployed, only pods with `ServiceAccounts`
    in the `default` `namespace` for our `tenant1` vCluster will be able to access
    the `some-password` value.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一条命令在 Vault 中创建了我们的密钥数据。请注意，我们创建的路径指定了我们正在使用 `default` `namespace` 中的 `tenant1`
    vCluster。根据我们的集群部署方式，只有 `tenant1` vCluster 中 `default` `namespace` 下的 pod 才能访问
    `some-password` 值。
- en: 'Next, let’s log in to our vCluster using `jjackson`. First, set your `KUBECONFIG`
    variable to a temporary file and set `jjackson`''s session up using the `tenant1`
    token:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 `jjackson` 登录到我们的 vCluster。首先，将 `KUBECONFIG` 变量设置为临时文件，并使用 `tenant1`
    令牌设置 `jjackson` 的会话：
- en: '[PRE25]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The pod was able to authenticate to our Vault using its own `ServiceAccount`''s
    identity to retrieve the secret! We did have to make two updates to our pod for
    our vCluster to connect to Vault:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 该 pod 能够使用其自身的 `ServiceAccount` 身份认证连接到我们的 Vault 以检索密钥！为了让我们的 vCluster 连接到 Vault，我们确实需要对
    pod 进行两项更新：
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We had to add a new `annotation` to tell the Vault sidecar where to authenticate.
    The `auth/vcluster-tenant1` authentication path was created by our onboarding
    workflow. We also needed to set the requested role to `cluster-read`, which was
    also created by the onboarding workflow. Finally, we needed to tell the sidecar
    where to look up our secret data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加一个新的 `annotation` 来告诉 Vault sidecar 在哪里进行身份认证。`auth/vcluster-tenant1`
    认证路径是由我们的入门工作流创建的。我们还需要将请求的角色设置为 `cluster-read`，该角色也是由入门工作流创建的。最后，我们需要告诉 sidecar
    在哪里查找我们的密钥数据。
- en: With that, we’ve now built the start of a self-service multitenant portal! We’re
    going to expand on this portal as we dive into more topics that are important
    to multitenancy. If you want to dive into the code for how we automated the vCluster
    onboarding, `chapter9/multitenant/vlcluster-multitenant` is the Helm chart that
    holds the custom workflows and `templates/workflows/onboard-vcluster.yaml` is
    the starting point for all the work that gets done. We broke up each major step
    in its own workflow to make it easier to read.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，我们已经构建了自助式多租户门户的开端！随着我们深入探索更多与多租户相关的重要话题，我们将扩展这个门户。如果您想深入了解我们如何自动化 vCluster
    入门过程的代码，`chapter9/multitenant/vlcluster-multitenant` 是包含自定义工作流的 Helm chart，而 `templates/workflows/onboard-vcluster.yaml`
    是所有工作的起始点。我们将每个主要步骤拆分到各自的工作流中，以便更易阅读。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Multitenancy is an important topic in modern Kubernetes deployments. Providing
    a shared infrastructure for multiple tenants cuts down on resource utilization
    and can provide more flexibility while creating the isolation needed to maintain
    both security and compliance. In this chapter, we worked through the benefits
    and challenges of multitenancy in Kubernetes, introduced the vCluster project,
    and learned how to deploy vClusters to support multiple tenants. Finally, we walked
    through implementing a self-service multitenant portal and integrated our Vault
    deployment so tenants could have their own secrets management.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户是现代 Kubernetes 部署中的一个重要话题。为多个租户提供共享基础设施可以减少资源使用，并且在创建所需的隔离以维护安全性和合规性的同时，提供更多的灵活性。在本章中，我们讨论了
    Kubernetes 中多租户的好处与挑战，介绍了 vCluster 项目，并学习了如何部署 vCluster 来支持多个租户。最后，我们演示了如何实现一个自助式多租户门户，并集成了我们的
    Vault 部署，以便租户能够拥有自己的密钥管理。
- en: In the next chapter, we’ll dive into the security of the Kubernetes Dashboard.
    We’ve used it and deployed it in the last few chapters, and now we’re going to
    understand how its security works and how those lessons learned apply to other
    cluster management systems too.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入研究 Kubernetes 仪表板的安全性。我们在前几章中使用并部署了它，现在我们将了解其安全性是如何运作的，以及这些经验如何应用到其他集群管理系统中。
- en: Questions
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Kubernetes Custom Resource Definitions can support multiple versions.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 自定义资源定义可以支持多个版本。
- en: 'True'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: What is the security boundary in Kubernetes?
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 中的安全边界是什么？
- en: pods
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pods
- en: Containers
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器
- en: NetworkPolicies
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络策略
- en: Namespaces
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命名空间
- en: Where do the pods in a vCluster run?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: vCluster 中的 Pods 运行在哪里？
- en: In the vCluster
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 vCluster 中
- en: In the host cluster
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主集群中
- en: There are no pods
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有 Pods
- en: vClusters have their own `Ingress` controllers.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: vClusters 有自己的 `Ingress` 控制器。
- en: 'True'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: vClusters share keys with host clusters.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: vClusters 与主集群共享密钥。
- en: 'True'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: Answers
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: b – False – There’s some version management, but generally you can only have
    one version of a CRD.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b – 错误 – 虽然有一些版本管理，但通常只能拥有一个 CRD 的版本。
- en: d – Namespaces are the security boundary in Kubernetes.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: d – 命名空间是 Kubernetes 中的安全边界。
- en: b – When a pod is created in a vCluster, the syncer creates a matching pod in
    the host cluster for scheduling.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b – 当在 vCluster 中创建一个 Pod 时，同步器会在主集群中创建一个匹配的 Pod 进行调度。
- en: b – False – Generally, the `Ingress` object is synced into the host cluster.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b – 错误 – 通常，`Ingress` 对象会同步到主集群中。
- en: b – False – Each vCluster gets its own unique keys to identify it.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b – 错误 – 每个 vCluster 都有自己独特的密钥来标识它。
- en: Join our book’s Discord space
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们本书的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask Me Anything* session with
    the authors:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的 Discord 工作空间，每月与作者进行一次 *问我任何问题* 的环节：
- en: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
- en: '![](img/QR_Code965214276169525265.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code965214276169525265.png)'
