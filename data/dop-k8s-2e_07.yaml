- en: Monitoring and Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: 'Monitoring and logging are crucial parts of a site''s reliability. So far,
    we''ve learned how to use various controllers to take care of our application.
    We have also looked at how to utilize services together with Ingress to serve
    our web applications, both internally and externally. In this chapter, we''ll
    gain more visibility over our applications by looking at the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和日志记录是站点可靠性的关键部分。到目前为止，我们已经学会了如何使用各种控制器来管理我们的应用程序。我们还了解了如何利用服务和Ingress一同为我们的Web应用程序提供服务，无论是内部还是外部。在本章中，我们将通过以下主题，进一步了解我们的应用程序：
- en: Getting a status snapshot of a container
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取容器的状态快照
- en: Monitoring in Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的监控
- en: Converging metrics from Kubernetes with Prometheus
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Prometheus汇聚Kubernetes中的度量
- en: Various concepts to do with logging in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Kubernetes中日志记录相关的各种概念
- en: Logging with Fluentd and Elasticsearch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Fluentd和Elasticsearch进行日志记录
- en: Gaining insights into traffic between services using Istio
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Istio获取服务之间流量的洞察
- en: Inspecting a container
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查容器
- en: Whenever our application behaves abnormally, we need to figure out what has
    happened with our system. We can do this by checking logs, resource usage, a watchdog,
    or even getting into the running host directly to dig out problems. In Kubernetes,
    we have `kubectl get` and `kubectl describe`, which can query controller states
    about our deployments. This helps us determine whether an application has crashed
    or whether it is working as desired.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们的应用程序表现异常时，我们需要找出系统发生了什么。我们可以通过检查日志、资源使用情况、监视器，甚至直接进入正在运行的主机来深入排查问题。在Kubernetes中，我们有`kubectl
    get`和`kubectl describe`，它们可以查询我们部署的控制器状态。这帮助我们判断应用程序是否崩溃，或者是否按预期工作。
- en: 'If we want to know what is going on using the output of our application, we
    also have `kubectl logs`, which redirects a container''s `stdout` and `stderr`
    to our Terminal. For CPU and memory usage stats, there''s also a `top`-like command
    we can employ, which is `kubectl top`. `kubectl top node` gives an overview of
    the resource usage of nodes, while `kubectl top pod <POD_NAME>` displays per-pod
    usage:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想了解应用程序输出的情况，我们可以使用`kubectl logs`，它将容器的`stdout`和`stderr`重定向到我们的终端。对于CPU和内存使用情况的统计信息，我们还可以使用类似`top`的命令——`kubectl
    top`。`kubectl top node`可以概览节点的资源使用情况，而`kubectl top pod <POD_NAME>`显示每个Pod的使用情况：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To use `kubectl top`, you'll need the metrics-server or Heapster (if you're
    using Kubernetes prior to 1.13) deployed in your cluster. We'll discuss this later
    in the chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`kubectl top`，你需要在集群中部署`metrics-server`或Heapster（如果你使用的是1.13版本之前的Kubernetes）。我们将在本章稍后讨论这个内容。
- en: 'What if we leave something such as logs inside a container and they are not
    sent out anywhere? We know that there''s a `docker exec` execute command inside
    a running container, but it''s unlikely that we will have access to nodes every
    time. Fortunately, `kubectl` allows us to do the same thing with the `kubectl
    exec` command. Its usage is similar to `docker exec`. For example, we can run
    a shell inside the container in a pod as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一些如日志的东西留在容器内而没有发送出去怎么办？我们知道`docker exec`可以在运行中的容器内执行命令，但每次都访问节点的可能性不大。幸运的是，`kubectl`也允许我们用`kubectl
    exec`命令做同样的事情。它的用法与`docker exec`类似。例如，我们可以在Pod中的容器内运行一个shell，如下所示：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is pretty much the same as logging onto a host via SSH. It enables us to
    troubleshoot with tools we are familiar with, as we've done previously without
    containers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这与通过SSH登录主机非常相似。它让我们能够使用熟悉的工具进行故障排除，正如我们之前在没有容器时所做的那样。
- en: If the container is built by `FROM scratch`, the `kubectl exec` trick may not
    work well because the core utilities such as the shell (`sh` or `bash`) might
    not be present inside the container. Before ephemeral containers, there was no
    official support for this problem. If we happen to have a `tar` binary inside
    our running container, we can use `kubectl cp` to copy some binaries into the
    container in order to carry out troubleshooting. If we're lucky and we have privileged
    access to the node the container runs on, we can utilize `docker cp`, which doesn't
    require a `tar` binary inside the container, to move the utilities we need into
    the container.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器是通过`FROM scratch`构建的，那么`kubectl exec`命令可能无法正常工作，因为容器内可能没有核心实用工具，如shell（`sh`或`bash`）。在短命容器出现之前，Kubernetes并未官方支持这个问题。如果我们恰好在运行的容器内有`tar`二进制文件，我们可以使用`kubectl
    cp`将一些二进制文件复制到容器中进行故障排除。如果我们幸运地拥有对容器运行节点的特权访问权限，我们可以利用`docker cp`，该命令不需要容器内有`tar`二进制文件，便能将所需的工具移入容器中。
- en: The Kubernetes dashboard
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 仪表盘
- en: 'In addition to the command-line utility, there is a dashboard that aggregates
    almost all the information we just discussed and displays the data in a decent
    web UI:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了命令行工具外，还有一个仪表盘，它汇总了我们刚刚讨论的几乎所有信息，并以良好的 Web 用户界面展示数据：
- en: '![](img/98e325ab-7c4c-4d55-9e8e-8e12029d6bc2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98e325ab-7c4c-4d55-9e8e-8e12029d6bc2.png)'
- en: 'This is actually a general purpose graphical user interface of a Kubernetes
    cluster as it also allows us to create, edit, and delete resources. Deploying
    it is quite easy; all we need to do is apply a template:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个 Kubernetes 集群的通用图形用户界面，它还允许我们创建、编辑和删除资源。部署它非常简单；我们只需要应用一个模板：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Many managed Kubernetes services, such as **Google Kubernetes Engine** (**GKE**),
    provide an option to pre-deploy a dashboard in the cluster so that we don''t need
    to install it ourselves. To determine whether the dashboard exists in our cluster
    or not, use `kubectl cluster-info`. If it''s installed, we''ll see the message `kubernetes-dashboard
    is running at ...` as shown in the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多托管的 Kubernetes 服务，如**Google Kubernetes Engine**（**GKE**），提供预部署仪表盘到集群的选项，这样我们就不需要自己安装它了。要确定仪表盘是否存在于我们的集群中，可以使用`kubectl
    cluster-info`。如果它已安装，我们将看到消息`kubernetes-dashboard is running at ...`，如下所示：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The service for the dashboard deployed with the preceding default template or
    provisioned by cloud providers is usually `ClusterIP`. We've learned a bunch of
    ways to access a service inside a cluster, but here let's just use the simplest
    built-in proxy, `kubectl proxy`, to establish the connection between our Terminal
    and our Kubernetes API server. Once the proxy is up, we are then able to access
    the dashboard at `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`.
    Port `8001` is the default port of the `kubectl proxy` command.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述默认模板或由云提供商预部署的仪表盘服务通常是`ClusterIP`。我们已经学习了多种访问集群内部服务的方法，但在这里我们只使用最简单的内置代理`kubectl
    proxy`，来建立我们的终端与 Kubernetes API 服务器之间的连接。一旦代理启动，我们就可以通过`http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`访问仪表盘。端口`8001`是`kubectl
    proxy`命令的默认端口。
- en: The dashboard deployed with the previous template wouldn't be one of the services
    listed in the output of `kubectl cluster-info` as it's not managed by the **addon
    manager**. The addon manager ensures that the objects it manages are active, and
    it's enabled in most managed Kubernetes services in order to protect the cluster
    components. Take a look at the following repository for more information: [https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面模板部署的仪表盘不会出现在`kubectl cluster-info`输出的服务列表中，因为它不是由**插件管理器**管理的。插件管理器确保它管理的对象处于活动状态，并且在大多数托管的
    Kubernetes 服务中启用，以保护集群组件。有关更多信息，请查看以下仓库：[https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager)。
- en: 'The methods to authenticate to the dashboard vary between cluster setups. For
    example, the token that allows `kubectl` to access a GKE cluster can also be used
    to log in to the dashboard. It can either be found in `kubeconfig`, or obtained
    via the one-liner shown in the following (supposing the current context is the
    one in use):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 身份验证方法在不同的集群配置之间有所不同。例如，允许`kubectl`访问GKE集群的令牌，也可以用来登录仪表盘。它可以在`kubeconfig`中找到，或者通过下面显示的单行命令获取（假设当前使用的是该上下文）：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we skip the sign in, the service account for the dashboard would be used
    instead. For other access options, check the wiki page of the dashboard's project
    to choose one that suits your cluster setup: [https://github.com/kubernetes/dashboard/wiki/Access-control#authentication](https://github.com/kubernetes/dashboard/wiki/Access-control#authentication).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们跳过登录过程，仪表盘的服务账户将会被使用。有关其他访问选项，请查看仪表盘项目的 wiki 页面，以选择适合您集群配置的方式：[https://github.com/kubernetes/dashboard/wiki/Access-control#authentication](https://github.com/kubernetes/dashboard/wiki/Access-control#authentication)。
- en: As with `kubectl top`, to display the CPU and memory stats, you'll need a metric
    server deployed in your cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与`kubectl top`一样，要显示 CPU 和内存统计信息，您需要在集群中部署一个指标服务器。
- en: Monitoring in Kubernetes
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 中的监控
- en: We now know how to examine our applications in Kubernetes. However, we are not
    yet confident enough to answer more complex questions, such as how healthy our
    application is, what changes have been made to the CPU usage from the new patch,
    when our databases will run out of capacity, and why our site rejects any requests.
    We therefore need a monitoring system to collect metrics from various sources,
    store and analyze the data received, and then respond to exceptions. In a classical
    setup of a monitoring system, we would gather metrics from at least three different
    sources to measure our service's availability, as well as its quality.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道如何在Kubernetes中检查我们的应用程序。然而，我们还没有足够的信心回答更复杂的问题，例如我们的应用程序有多健康、从新补丁中CPU使用率发生了什么变化、我们的数据库何时会耗尽容量，以及为什么我们的网站会拒绝任何请求。因此，我们需要一个监控系统来收集来自各个来源的度量指标，存储并分析接收到的数据，然后响应异常。在传统的监控系统设置中，我们通常会从至少三个不同的来源收集度量数据，以衡量服务的可用性以及其质量。
- en: Monitoring applications
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控应用程序
- en: The data we are concerned with relates to the internal states of our running
    application. Collecting this data gives us more information about what's going
    on inside our service. The data may be to do with the goal the application is
    designed to achieve, or the runtime data intrinsic to the application. Obtaining
    this data often requires us to manually instruct our program to expose the internal
    data to the monitoring pipeline because only we, the service owner, know what
    data is meaningful, and also because it's often hard to get information such as
    the size of records in the memory for a cache service externally.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的数据与我们正在运行的应用程序的内部状态有关。收集这些数据可以让我们更深入了解服务内部的运行情况。这些数据可能与应用程序设计的目标有关，也可能是应用程序本身固有的运行时数据。获取这些数据通常需要我们手动指示程序将内部数据暴露给监控管道，因为只有我们，作为服务的拥有者，知道哪些数据是有意义的，同时因为外部很难获取像缓存服务中记录在内存中的大小等信息。
- en: The ways in which applications interact with the monitoring system differ significantly.
    For example, if we need data about the statistics of a MySQL database, we could
    set an agent that periodically queries the information and performance schema
    for the raw data, such as numbers of SQL queries accumulated at the time, and
    transform them to the format for our monitoring system. In a Golang application,
    as another example, we might expose the runtime information via the `expvar` package
    and its interface and then find another way to ship the information to our monitoring
    backend. To alleviate the potential difficulty of these steps, the **OpenMetrics **([https://openmetrics.io/](https://openmetrics.io/))
    project endeavours to provide a standardized format for exchanging telemetry between
    different applications and monitoring systems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序与监控系统交互的方式有很大的差异。例如，如果我们需要有关MySQL数据库统计信息的数据，我们可以设置一个代理，定期查询信息和性能模式以获取原始数据，如当前时刻的SQL查询数量，并将其转换为我们监控系统所需的格式。以Golang应用程序为例，我们可能会通过`expvar`包及其接口暴露运行时信息，然后找到另一种方式将信息传输到监控后端。为了缓解这些步骤可能带来的困难，**OpenMetrics**（[https://openmetrics.io/](https://openmetrics.io/)）项目致力于为不同应用程序和监控系统之间交换遥测数据提供标准化格式。
- en: In addition to time series metrics, we may also want to use profiling tools
    in conjunction with tracing tools to assert the performance of our program. This
    is especially important nowadays, as an application might be composed of dozens
    of services in a distributed way. Without utilizing tracing tools such as **OpenTracing**
    ([http://opentracing.io](http://opentracing.io)), identifying the reasons behind
    performance declines can be extremely difficult.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了时间序列度量外，我们还可能希望结合使用分析工具和追踪工具来验证程序的性能。如今，这一点尤为重要，因为一个应用程序可能由数十个分布式服务组成。如果不使用诸如**OpenTracing**（[http://opentracing.io](http://opentracing.io)）之类的追踪工具，找出性能下降背后的原因将变得异常困难。
- en: Monitoring infrastructure
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控基础设施
- en: 'The term infrastructure may be too broad here, but if we simply consider where
    our application runs and how it interacts with other components and users, it
    is obvious what we should monitor: the application hosts and the connecting network.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的“基础设施”一词可能太广泛了，但如果我们简单地考虑应用程序运行的地方以及它如何与其他组件和用户交互，那么我们应该监控的内容就显而易见：应用程序主机和连接的网络。
- en: As collecting tasks at the host is a common practice for system monitoring,
    it is usually performed by agents provided by the monitoring framework. The agent
    extracts and sends out comprehensive metrics about a host, such as loads, disks,
    connections, or other process statistics that help us determine the health of
    a host.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在主机上收集任务是系统监控的常见做法，它通常由监控框架提供的代理执行。代理会提取并发送关于主机的综合指标，例如负载、磁盘、连接或其他进程统计信息，这些有助于我们确定主机的健康状况。
- en: For the network, these can be merely the web server software and the network
    interface on the same host, plus perhaps a load balancer, or even within a platform
    such as Istio. Although the way to collect telemetry data about the previously
    mentioned components depends on their actual setup, in general, the metrics we'd
    like to measure would be traffic, latency, and errors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络来说，这些组件可以仅仅是同一主机上的网页服务器软件和网络接口，或者可能还有负载均衡器，甚至是像 Istio 这样的平台。尽管收集关于前述组件的遥测数据的方式取决于它们的实际设置，但一般来说，我们希望衡量的指标是流量、延迟和错误。
- en: Monitoring external dependencies
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控外部依赖
- en: Aside from the aforementioned two components, we also need to check the statuses
    of dependent components, such as the utilization of external storage, or the consumption
    rate of a queue. For instance, let's say we have an application that subscribes
    to a queue as an input and executes tasks from that queue. In this case, we'd
    also need to consider metrics such as the queue length and the consumption rate.
    If the consumption rate is low and the queue length keeps growing, our application
    may have trouble.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的两个组件，我们还需要检查依赖组件的状态，例如外部存储的使用情况，或者队列的消耗速率。例如，假设我们有一个订阅了队列作为输入并从该队列执行任务的应用程序。在这种情况下，我们还需要考虑像队列长度和消耗速率这样的指标。如果消耗速率较低，而队列长度不断增长，我们的应用程序可能会遇到问题。
- en: These principles also apply to containers on Kubernetes, as running a container
    on a host is almost identical to running a process. However, because of the subtle
    distinction between the way in which containers on Kubernetes and on traditional
    hosts utilize resources, we still need to adjust our monitoring strategy accordingly.
    For instance, containers of an application on Kubernetes would be spread across
    multiple hosts and would not always be on the same hosts. It would be difficult
    to produce a consistent recording of one application if we are still adopting
    a host-centric monitoring approach. Therefore, rather than observing resource
    usage at the host only, we should add a container layer to our monitoring stack.
    Moreover, since Kubernetes is the infrastructure for our applications, it is important
    to take this into account as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则同样适用于 Kubernetes 上的容器，因为在主机上运行容器几乎与运行进程相同。然而，由于 Kubernetes 上的容器和传统主机上的容器在资源使用方式上的细微区别，我们仍然需要相应地调整我们的监控策略。例如，Kubernetes
    上应用程序的容器会分布在多个主机上，并不总是处于同一主机。如果我们仍然采用以主机为中心的监控方式，那么生成一份一致的应用程序记录将会非常困难。因此，我们应该在监控堆栈中添加一个容器层，而不是仅仅观察主机上的资源使用情况。此外，既然
    Kubernetes 是我们应用程序的基础设施，我们也应该将这一点考虑在内。
- en: Monitoring containers
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控容器
- en: 'As a container is basically a thin wrapper around our program and dependent
    runtime libraries, the metrics collected at the container level would be similar
    to the metrics we get at the container host, particularly with regard to the use
    of system resources. Although collecting these metrics from both the containers
    and their hosts might seem redundant, it actually allows us to solve problems
    related to monitoring moving containers. The idea is quite simple: what we need
    to do is attach logical information to metrics, such as pod labels or their controller
    names. In this way, metrics coming from containers across distinct hosts can be
    grouped meaningfully. Consider the following diagram. Let''s say we want to know
    how many bytes were transmitted (**tx**) on **App 2**. We could add up the **tx**
    metrics that have the **App 2** label, which would give us a total of **20 MB**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于容器基本上是我们程序和依赖运行时库的一个薄层包装，因此在容器级别收集的指标与在容器主机上收集的指标类似，特别是关于系统资源使用的情况。尽管从容器和其主机上收集这些指标似乎是冗余的，但实际上，它让我们能够解决与监控移动容器相关的问题。这个想法非常简单：我们需要做的是将逻辑信息附加到指标上，例如
    pod 标签或其控制器名称。通过这种方式，来自不同主机的容器的指标可以有意义地进行分组。考虑以下图示。假设我们想知道在**App 2**上传输了多少字节（**tx**）。我们可以将具有**App
    2**标签的**tx**指标加起来，这样我们就能得到总计**20 MB**的数据：
- en: '![](img/1b3e8dd6-03dd-4c9d-8378-bbfadbec9442.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b3e8dd6-03dd-4c9d-8378-bbfadbec9442.png)'
- en: Another difference is that metrics related to CPU throttling are reported at
    the container level only. If performance issues are encountered in a certain application
    but the CPU resource on the host is spare, we can check if it's throttled with
    the associated metrics.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别是，关于CPU限速的指标仅在容器级别报告。如果在某个应用程序遇到性能问题，但主机上的CPU资源尚充足，我们可以检查是否由于限速导致问题，并查看相关的指标。
- en: Monitoring Kubernetes
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控Kubernetes
- en: Kubernetes is responsible for managing, scheduling, and orchestrating our applications.
    Once an application has crashed, Kubernetes is one of the first places we would
    like to look at. In particular, when a crash happens after rolling out a new deployment,
    the state of the associated objects would be reflected instantly on Kubernetes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes负责管理、调度和编排我们的应用程序。一旦应用程序崩溃，Kubernetes通常是我们首先查看的地方。特别是当新部署后发生崩溃时，相关对象的状态会即时在Kubernetes中反映出来。
- en: 'To sum up, the components that should be monitored are illustrated in the following
    diagram:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，应该监控的组件在下图中进行了说明：
- en: '![](img/615e72b5-50dc-42e9-8028-55a750b41ceb.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/615e72b5-50dc-42e9-8028-55a750b41ceb.png)'
- en: Getting monitoring essentials for Kubernetes
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取Kubernetes监控要点
- en: Since monitoring is an important part of operating a service, the existing monitoring
    system in our infrastructure might already provide solutions for collecting metrics
    from common sources like well-known open source software and the operating system.
    As for applications run on Kubernetes, let's have a look at what Kubernetes and
    its ecosystem offer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于监控是服务运营的重要组成部分，我们基础设施中的现有监控系统可能已经提供了从常见来源（如知名的开源软件和操作系统）收集指标的解决方案。至于运行在Kubernetes上的应用程序，让我们看看Kubernetes及其生态系统提供了什么。
- en: 'To collect metrics of containers managed by Kubernetes, we don''t have to install
    any special controller on the Kubernetes master node, nor any metrics collector
    inside our containers. This is basically done by kubelet, which gathers various
    telemetries from a node, and exposes them in the following API endpoints (as of
    Kubernetes 1.13):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要收集由Kubernetes管理的容器的指标，我们不需要在Kubernetes主节点上安装任何特殊的控制器，也不需要在容器内部安装任何指标收集器。这基本上是由kubelet完成的，它从节点收集各种遥测数据，并在以下API端点中暴露出来（截至Kubernetes
    1.13版本）：
- en: '`/metrics/cadvisor`: This API endpoint is used for cAdvisor container metrics
    that are in Prometheus format'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/metrics/cadvisor`：此API端点用于Prometheus格式的cAdvisor容器指标。'
- en: '`/spec/`: This API endpoint exports machine specifications'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/spec/`：此API端点导出机器规格。'
- en: '`/stats/`: This API endpoint also exports cAdvisor container metrics but in
    JSON format'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/stats/`：此API端点也导出cAdvisor容器指标，但以JSON格式呈现。'
- en: '`/stats/summary`: This endpoint contains various data aggregated by kubelet.
    It''s also known as the Summary API'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/stats/summary`：此端点包含kubelet聚合的各种数据，也称为Summary API。'
- en: The metrics under the bare path `/metrics/` relate to kubelet's internal statistics.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`/metrics/`路径下的指标与kubelet的内部统计数据相关。'
- en: The Prometheus format ([https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/))
    is the predecessor of the OpenMetrics format, so it is also known as OpenMetrics
    v0.0.4 after OpenMetrics was published. If our monitoring system supports this
    kind of format, we can configure it to pull metrics from kubelet's Prometheus
    endpoint (`/metrics/cadvisor)`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus格式（[https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)）是OpenMetrics格式的前身，因此在OpenMetrics发布后，它也被称为OpenMetrics
    v0.0.4。如果我们的监控系统支持这种格式，我们可以配置它从kubelet的Prometheus端点（`/metrics/cadvisor`）拉取指标。
- en: To access those endpoints, kubelet has two TCP ports, `10250` and `10255`. Port
    `10250` is the safer one and the one that it is recommended to use in production
    as it's an HTTPS endpoint and protected by Kubernetes' authentication and authorization
    system. `10255` is in plain HTTP, which should be used restrictively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这些端点，kubelet有两个TCP端口，`10250`和`10255`。端口`10250`是更安全的，建议在生产环境中使用，因为它是HTTPS端点，并且受到Kubernetes身份验证和授权系统的保护。`10255`使用普通HTTP，应该限制使用。
- en: cAdvisor ([https://github.com/google/cadvisor](https://github.com/google/cadvisor))
    is a widely used container-level metrics collector. Put simply, cAdvisor aggregates
    the resource usage and performance statistics of every container running on a
    machine. Its code is currently sold inside kubelet, so we don't need to deploy
    it separately. However, since it focuses on certain container runtimes and Linux
    containers only, which may not suit future Kubernetes releases for different container
    runtimes, there won't be an integrated cAdvisor in future releases of Kubernetes.
    In addition to this, not all cAdvisor metrics are currently published by kubelet.
    Therefore, if we need that data, we'll need to deploy cAdvisor by ourselves. Notice
    that the deployment of cAdvisor is one per host instead of one per container,
    which is more reasonable for containerized applications, and we can use DaemonSet
    to deploy it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor ([https://github.com/google/cadvisor](https://github.com/google/cadvisor))
    是一个广泛使用的容器级度量收集器。简单来说，cAdvisor 聚合每个在机器上运行的容器的资源使用和性能统计信息。其代码目前已内嵌在 kubelet 中，因此我们无需单独部署它。然而，由于它仅关注某些容器运行时和
    Linux 容器，这可能不适应未来 Kubernetes 对不同容器运行时的支持，因此未来的 Kubernetes 版本将不再集成 cAdvisor。除此之外，并非所有
    cAdvisor 度量信息目前都由 kubelet 发布。因此，如果我们需要这些数据，就需要自行部署 cAdvisor。请注意，cAdvisor 的部署是按主机而非容器进行的，这对于容器化应用程序更为合理，我们可以使用
    DaemonSet 来部署它。
- en: Another important component in the monitoring pipeline is the metrics server
    ([https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)).
    This aggregates monitoring statistics from the summary API by kubelet on each
    node and acts as an abstraction layer between Kubernetes' other components and
    the real metrics sources. To be more specific, the metrics server implements the
    resource metrics API under the aggregation layer, so other intra-cluster components
    can get the data from a unified API path (`/api/metrics.k8s.io`). In this instance,
    `kubectl top` and kube-dashboard get data from the resource metrics API.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 监控管道中的另一个重要组件是度量服务器 ([https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server))。它通过
    kubelet 的 summary API 聚合每个节点的监控统计信息，并充当 Kubernetes 其他组件与真实度量源之间的抽象层。更具体地说，度量服务器在聚合层下实现了资源度量
    API，因此集群内部的其他组件可以通过统一的 API 路径 (`/api/metrics.k8s.io`) 获取数据。在这种情况下，`kubectl top`
    和 kube-dashboard 从资源度量 API 获取数据。
- en: 'The following diagram illustrates how the metrics server interacts with other
    components in a cluster:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了度量服务器如何与集群中的其他组件进行交互：
- en: '![](img/0a39ab6f-33c1-4de4-8e45-a307480732c1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a39ab6f-33c1-4de4-8e45-a307480732c1.png)'
- en: If you're using an older version of Kubernetes, the role of the metrics server
    will be played by Heapster([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是旧版本的 Kubernetes，度量服务器的角色将由 Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster))
    执行。
- en: 'Most installations of Kubernetes deploy the metrics server by default. If we
    need to do this manually, we can download the manifest of the metrics server and
    apply them:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 Kubernetes 安装默认会部署度量服务器。如果我们需要手动部署，可以下载度量服务器的 manifest 文件并应用它们：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While kubelet metrics are focused on system metrics, we also want to see the
    logical states of objects displayed on our monitoring dashboard. `kube-state-metrics`
    ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    is the piece that completes our monitoring stack. It watches Kubernetes masters
    and transforms the object statuses we see from `kubectl get` or `kubectl describe` into
    metrics in the Prometheus format. We are therefore able to scrape the states into
    metrics storage and then be alerted on events such as unexplainable restart counts.
    Download the templates to install as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 kubelet 的度量关注的是系统度量，但我们还希望在监控仪表板上查看对象的逻辑状态。`kube-state-metrics` ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    是完成我们监控堆栈的组成部分。它监控 Kubernetes master 节点，并将我们通过 `kubectl get` 或 `kubectl describe`
    查看对象的状态转换为 Prometheus 格式的度量。因此，我们能够将这些状态抓取到度量存储中，并在发生诸如无法解释的重启次数等事件时发出警报。下载以下模板进行安装：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Afterward, we can view the state metrics from the `kube-state-metrics` service
    inside our cluster:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以在集群内部通过 `kube-state-metrics` 服务查看状态度量：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Hands-on monitoring
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的监控
- en: So far, we've learned about a wide range of principles that are required to
    create an impervious monitoring system in Kubernetes, which allows us to build
    a robust service. It's time to implement one. Because the vast majority of Kubernetes
    components expose their instrumented metrics on a conventional path in Prometheus
    format, we are free to use any monitoring tool with which we are acquainted, as
    long as the tool understands the format. In this section, we'll set up an example
    with Prometheus. Its popularity in the Kubernetes ecosystem is not only due to its
    power, but also for its backing by the **Cloud Native Computing Foundation** ([https://www.cncf.io/](https://www.cncf.io/)),
    which also sponsors the Kubernetes project.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了创建一个坚固的监控系统所需的一系列原理，这使我们能够构建一个强大的服务。现在是时候实施一个了。由于绝大多数 Kubernetes
    组件都会以 Prometheus 格式在常规路径上暴露它们的度量指标，我们可以自由使用任何熟悉的监控工具，只要该工具能够理解这种格式。在这一节中，我们将设置一个使用
    Prometheus 的示例。Prometheus 在 Kubernetes 生态系统中的流行，不仅因为它的强大功能，还因为它得到了**云原生计算基金会**（[https://www.cncf.io/](https://www.cncf.io/)）的支持，该基金会也赞助了
    Kubernetes 项目。
- en: Getting to know Prometheus
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 Prometheus
- en: 'The Prometheus framework is made up of several components, as illustrated in
    the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 框架由多个组件组成，如下图所示：
- en: '![](img/17d18acd-69ac-49f6-9ba2-616b11862cf5.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17d18acd-69ac-49f6-9ba2-616b11862cf5.png)'
- en: As with all other monitoring frameworks, Prometheus relies on agents scraping
    statistics from the components of our system. Those agents are the exporters shown
    to the left of the diagram. Besides this, Prometheus adopts the pull model of
    metric collection, which is to say that it does not receive metrics passively,
    but actively pulls data from the metrics' endpoints on the exporters. If an application
    exposes a metric's endpoint, Prometheus is able to scrape that data as well. The
    default storage backend is an embedded TSDB, and can be switched to other remote
    storage types such as InfluxDB or Graphite. Prometheus is also responsible for
    triggering alerts according to preconfigured rules in **Alertmanager**, which
    handles alarm tasks. It groups alarms received and dispatches them to tools that
    actually send messages, such as email, **Slack **([https://slack.com/](https://slack.com/)),
    **PagerDuty **([https://www.pagerduty.com/](https://www.pagerduty.com/)), and
    so on. In addition to alerts, we also want to visualize the collected metrics
    to get a quick overview of our system, which is where Grafana comes in handy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有其他监控框架一样，Prometheus 依赖代理从我们系统的组件中抓取统计信息。这些代理就是图中左侧所示的 exporters。除此之外，Prometheus
    采用拉取模型来收集度量指标，也就是说，它并不是被动接收度量指标，而是主动从 exporters 的度量端点拉取数据。如果应用程序暴露了一个度量端点，Prometheus
    也能够抓取该数据。默认的存储后端是嵌入式的 TSDB，并且可以切换到其他远程存储类型，例如 InfluxDB 或 Graphite。Prometheus 还负责根据**Alertmanager**中预配置的规则触发告警，Alertmanager
    处理告警任务。它会将接收到的告警分组并发送到实际发送消息的工具，例如电子邮件、**Slack**（[https://slack.com/](https://slack.com/)）、**PagerDuty**（[https://www.pagerduty.com/](https://www.pagerduty.com/)）等。除了告警，我们还希望将收集的度量指标可视化，以便快速了解我们的系统，这时
    Grafana 就派上用场了。
- en: Aside from collecting data, alerting is one of the most important concepts to
    do with monitoring. However, alerting is more relevant to business concerns, which
    is out of the scope of this chapter. Therefore, in this section, we'll focus on metric collection
    with Prometheus and won't look any closer at Alertmanager.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据收集，告警是监控中最重要的概念之一。然而，告警更与业务相关，这超出了本章的范围。因此，在这一节中，我们将重点讨论使用 Prometheus 进行度量指标的收集，而不会深入探讨
    Alertmanager。
- en: Deploying Prometheus
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Prometheus
- en: The templates we've prepared for this chapter can be found at the following
    link: [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本章准备的模板可以通过以下链接找到：[https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7)。
- en: 'Under `7-1_prometheus` are the manifests of components to be used for this
    section, including a Prometheus deployment, exporters, and related resources.
    These will be deployed at a dedicated namespace, `monitoring`, except those components
    required to work in `kube-system` namespaces. Please review them carefully. For
    now, let''s create our resources in the following order:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `7-1_prometheus` 下是本节中要使用的组件清单，包括 Prometheus 部署、导出器和相关资源。这些将部署在专用命名空间 `monitoring`
    中，除了那些需要在 `kube-system` 命名空间中工作的组件。请仔细审查它们。目前，我们按照以下顺序创建我们的资源：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resource usage, such as storage and memory, at the provided manifest for
    Prometheus is confined to a relatively low level. If you''d like to use them in
    a more realistic way, you can adjust your parameters according to your actual
    requirements. After the Prometheus server is up, we can connect to its web UI
    at port `9090` with `kubectl port-forward`. We can also use NodePort or Ingress
    to connect to the UI if we modify its service (`prometheus/prom-svc.yml`) accordingly.
    The first page we will see when entering the UI is the Prometheus expression browser,
    where we build queries and visualize metrics. Under the default settings, Prometheus
    will collect metrics by itself. All valid scraping targets can be found at the `/targets` path.
    To speak to Prometheus, we have to gain some understanding of its language: **PromQL**.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 提供的清单中，资源使用量（如存储和内存）被限制在一个相对较低的水平。如果你希望以更实际的方式使用它们，可以根据实际需求调整参数。在
    Prometheus 服务器启动后，我们可以通过 `kubectl port-forward` 连接到其 Web UI，端口为 `9090`。如果我们修改相应的服务（`prometheus/prom-svc.yml`），也可以使用
    NodePort 或 Ingress 连接到 UI。进入 UI 后，我们将看到的第一页是 Prometheus 表达式浏览器，我们可以在其中构建查询并可视化指标。在默认设置下，Prometheus
    会自动收集指标。所有有效的抓取目标可以在 `/targets` 路径下找到。要与 Prometheus 交互，我们需要了解其语言：**PromQL**。
- en: To run Prometheus in production, there is also a **Prometheus Operator** ([https://github.com/coreos/prometheus-operator](https://github.com/coreos/prometheus-operator)),
    which aims to simplify the monitoring task in Kubernetes by CoreOS.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要在生产环境中运行 Prometheus，还可以使用 **Prometheus Operator** ([https://github.com/coreos/prometheus-operator](https://github.com/coreos/prometheus-operator))，该工具旨在通过
    CoreOS 简化 Kubernetes 中的监控任务。
- en: Working with PromQL
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PromQL
- en: 'PromQL has three data types: **instant vectors**, **range vectors**, and **scalars**.
    An instant vector is a time series of data samples; a range vector is a set of
    time series containing data within a certain time range; and a scalar is a numeric
    floating value. Metrics stored inside Prometheus are identified with a metric
    name and labels, and we can find the name of any collected metric with the drop-down
    list next to the Execute button in the expression browser. If we query Prometheus
    using a metric name, say `http_requests_total`, we''ll get lots of results, as
    instant vectors often have the same name but with different labels. Likewise,
    we can also query a particular set of labels using the `{}` syntax. For example,
    the query `{code="400",method="get"}` means that we want any metric that has the
    labels `code`, `method` equal to `400`, and `get`. Combining names and labels
    in a query is also valid, such as `http_requests_total{code="400",method="get"}`.
    PromQL grants us the ability to inspect our applications or systems based on lots
    of different parameters, so long as the related metrics are collected.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: PromQL 有三种数据类型：**瞬时向量**、**范围向量**和**标量**。瞬时向量是一组数据样本的时间序列；范围向量是包含某个时间范围内数据的时间序列集合；标量是一个数值型浮动值。存储在
    Prometheus 中的指标通过指标名称和标签来标识，我们可以通过表达式浏览器中执行按钮旁边的下拉列表找到任何已收集指标的名称。如果我们使用一个指标名称查询
    Prometheus，比如 `http_requests_total`，我们会得到很多结果，因为瞬时向量通常有相同的名称，但具有不同的标签。同样，我们也可以使用
    `{}` 语法查询特定的标签集。例如，查询 `{code="400",method="get"}` 表示我们想要任何标签 `code` 和 `method`
    分别为 `400` 和 `get` 的指标。将名称和标签结合在查询中也是有效的，如 `http_requests_total{code="400",method="get"}`。PromQL
    使我们能够根据各种不同的参数检查我们的应用程序或系统，只要相关的指标被收集。
- en: 'In addition to the basic queries just mentioned, PromQL has many other functionalities.
    For example, we can query labels with regex and logical operators, joining and
    aggregating metrics with functions, and even performing operations between different
    metrics. For instance, the following expression gives us the total memory consumed
    by a `kube-dns` pod in the `kube-system` namespace:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了刚才提到的基本查询，PromQL 还有许多其他功能。例如，我们可以使用正则表达式和逻辑运算符查询标签，使用函数连接和聚合指标，甚至执行不同指标之间的操作。例如，以下表达式给出了
    `kube-dns` pod 在 `kube-system` 命名空间中消耗的总内存：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: More detailed documentation can be found at the official Prometheus site ([https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/)).
    This will help you to unleash the power of Prometheus.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的文档可以在官方 Prometheus 网站找到（[https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/)）。这将帮助你发挥
    Prometheus 的强大功能。
- en: Discovering targets in Kubernetes
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中发现目标
- en: 'Since Prometheus only pulls metrics from endpoints it knows, we have to explicitly
    tell it where we''d like to collect data from. Under the `/config` path is a page
    that lists the current configured targets to pull. By default, there would be
    one job that runs against Prometheus itself, and this can be found in the conventional
    scraping path, `/metrics`. If we are connecting to the endpoint, we would see
    a very long text page, as shown in the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Prometheus 仅从它知道的端点拉取指标，我们必须明确告知它我们希望从哪里收集数据。在`/config`路径下，有一页列出了当前配置的目标以供抓取。默认情况下，会有一个作业针对
    Prometheus 本身运行，并可以在常规抓取路径`/metrics`中找到。如果我们连接到该端点，我们将看到一页非常长的文本，如下所示：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is the Prometheus metrics format we've mentioned several times before.
    Next time we see a page like this, we will know that it's a metrics endpoint. The
    job to scrape Prometheus is a static target in the default configuration file.
    However, due to the fact that containers in Kubernetes are created and destroyed
    dynamically, it is really difficult to find out the exact address of a container,
    let alone set it in Prometheus. In some cases, we may utilize the service DNS
    as a static metrics target, but this still cannot solve all cases. For instance,
    if we'd like to know how many requests are coming to each pod behind a service individually,
    setting a job to scrape the service might get a result from random pods instead
    of from all of them. Fortunately, Prometheus helps us overcome this problem with
    its ability to discover services inside Kubernetes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们之前提到过的 Prometheus 指标格式。下次我们看到类似的页面时，我们就知道那是一个指标端点。抓取 Prometheus 的作业是默认配置文件中的静态目标。然而，由于
    Kubernetes 中的容器是动态创建和销毁的，因此很难找出容器的准确地址，更不用说在 Prometheus 中设置它。在某些情况下，我们可能会利用服务的
    DNS 作为静态指标目标，但这仍然无法解决所有问题。例如，如果我们想知道每个 pod 背后的服务收到多少请求，设置一个作业来抓取服务可能会从随机的 pod
    中获取结果，而不是从所有 pod 获取结果。幸运的是，Prometheus 通过其发现 Kubernetes 内部服务的能力帮助我们克服了这个问题。
- en: 'To be more specific, Prometheus is able to query Kubernetes about the information
    of running services. It can then add them to or delete them from the target configuration
    accordingly. Five discovery mechanisms are currently supported:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，Prometheus 能够查询 Kubernetes 关于正在运行的服务的信息。然后，它可以根据情况将它们添加到或从目标配置中删除。目前支持五种发现机制：
- en: The **node** discovery mode creates one target per node. The target port would
    be kubelet's HTTPS port (`10250`) by default.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**node**发现模式为每个节点创建一个目标。默认情况下，目标端口将是 kubelet 的 HTTPS 端口（`10250`）。'
- en: The **service** discovery mode creates a target for every `Service` object.
    All defined target ports in a service would become a scraping target.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**service**发现模式为每个`Service`对象创建一个目标。服务中定义的所有目标端口都会成为抓取目标。'
- en: The **pod** discovery mode works in a similar way to the service discovery role;
    it creates a target per pod and it exposes all the defined container ports for
    each pod. If there is no port defined in a pod's template, it would still create
    a scraping target with its address only.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pod**发现模式与服务发现角色类似；它为每个 pod 创建一个目标，并暴露每个 pod 所定义的所有容器端口。如果在 pod 模板中没有定义端口，它仍然会仅使用地址创建抓取目标。'
- en: The **endpoints** mode discovers the `Endpoint` objects created by a service.
    For example, if a service is backed by three pods with two ports each, we'll have
    six scraping targets. In addition, for a pod, not only ports that expose to a
    service, but also other declared container ports would be discovered.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端点**模式发现由服务创建的`Endpoint`对象。例如，如果一个服务由三个 pod 支持，每个 pod 有两个端口，那么我们将有六个抓取目标。此外，对于一个
    pod，不仅仅是暴露给服务的端口，其他声明的容器端口也会被发现。'
- en: The **ingress** mode creates one target per Ingress path. As an Ingress object
    can route requests to more than one service, and each service might have own metrics
    set, this mode allows us to configure all those targets at once.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ingress**模式为每个 Ingress 路径创建一个目标。由于一个 Ingress 对象可以将请求路由到多个服务，并且每个服务可能有自己的一套指标，这种模式允许我们一次性配置所有这些目标。'
- en: 'The following diagram illustrates four discovery mechanisms. The left-hand
    ones are the resources in Kubernetes, and those on the right are the targets created
    in Prometheus:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了四种发现机制。左侧是 Kubernetes 中的资源，右侧是 Prometheus 中创建的目标：
- en: '![](img/f1c1fb9a-04ba-464f-ba6e-34234bca4453.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1c1fb9a-04ba-464f-ba6e-34234bca4453.png)'
- en: 'Generally speaking, not all exposed ports are served as a metrics endpoint,
    so we certainly don''t want Prometheus to grab everything it discovers in our
    cluster, but instead to only collect marked resources. To achieve this in Prometheus,
    a conventional method is to utilize annotations on resource manifests to distinguish
    which targets are to be grabbed, and then we can filter out those non-annotated
    targets using the `relabel` module in the Prometheus configuration. Consider this
    example configuration:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，并不是所有暴露的端口都作为度量端点提供服务，因此我们当然不希望 Prometheus 抓取它在集群中发现的所有内容，而是只收集标记的资源。为了在
    Prometheus 中实现这一点，一种常见的方法是利用资源清单上的注解来区分哪些目标需要被抓取，然后我们可以使用 Prometheus 配置中的 `relabel`
    模块过滤掉那些没有注解的目标。请看这个示例配置：
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This tells Prometheus to keep only targets with the `__meta_kubernetes_pod_annotation_{name}` label and
    the value `true`. The label is fetched from the annotation field on the pod''s
    specification, as shown in the following snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉 Prometheus 只保留具有 `__meta_kubernetes_pod_annotation_{name}` 标签并且值为 `true`
    的目标。这个标签是从 pod 规格中的注解字段获取的，下面是一个片段：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that Prometheus would translate every character that is not in the range `[a-zA-Z0-9_]` to
    `_`, so we can also write the previous annotation as `mycom-io-scrape: "true"`.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，Prometheus 会将所有不在 `[a-zA-Z0-9_]` 范围内的字符转换为 `_`，因此我们也可以将之前的注解写为 `mycom-io-scrape:
    "true"`。'
- en: 'By combining those annotations and the label filtering rule, we can precisely
    control the targets that need to be collected. Some commonly-used annotations
    in Prometheus are listed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些注解和标签过滤规则，我们可以精确控制需要收集的目标。以下是一些 Prometheus 中常用的注解：
- en: '`prometheus.io/scrape: "true"`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus.io/scrape: "true"`'
- en: '`prometheus.io/path: "/metrics"`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus.io/path: "/metrics"`'
- en: '`prometheus.io/port: "9090"`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus.io/port: "9090"`'
- en: '`prometheus.io/scheme: "https"`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus.io/scheme: "https"`'
- en: '`prometheus.io/probe: "true"`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus.io/probe: "true"`'
- en: 'Those annotations can be seen at `Deployment` objects (for their pods) and
    `Service` objects. The following template snippet shows a common use case:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些注解可以在 `Deployment` 对象（其对应的 pod）和 `Service` 对象中看到。以下模板片段展示了一个常见的用例：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'By applying the following configuration, Prometheus will translate the discovered
    target in endpoints mode into `http://<pod_ip_of_the_service>:9090/monitoring`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用以下配置，Prometheus 将把端点模式下发现的目标转换为 `http://<pod_ip_of_the_service>:9090/monitoring`：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can use the `prometheus.io/probe` annotation in Prometheus to denote whether
    a service should be added to the probing target or not. The probing task would
    be executed by the Blackbox exporter ([https://github.com/prometheus/blackbox_exporter](https://github.com/prometheus/blackbox_exporter)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Prometheus 中使用 `prometheus.io/probe` 注解来表示一个服务是否应该被添加到探测目标中。探测任务将由 Blackbox
    exporter 执行（[https://github.com/prometheus/blackbox_exporter](https://github.com/prometheus/blackbox_exporter)）。
- en: The purpose of probing is to determine the quality of connectivity between a
    probe and the target service. The availability of the target service would also
    be evaluated, as a probe could act as a customer. Because of this, where we put
    the probes is also a thing that should be taken into consideration if we want
    the probing to be meaningful.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 探测的目的是确定探测器与目标服务之间连接的质量。目标服务的可用性也会被评估，因为探测器可以充当客户。因此，我们放置探测器的位置也是一个需要考虑的因素，如果我们希望探测具有意义的话。
- en: 'Occasionally, we might want the metrics from any single pod under a service,
    not from all pods of a service. Since most endpoint objects are not created manually,
    the endpoint discovery mode uses the annotations inherited from a service. This
    means that if we annotate a service, the annotation will be visible in both the
    service discovery and endpoint discovery modes simultaneously, which prevents
    us from distinguishing whether the targets should be scraped per endpoint or per
    service. To solve this problem, we could use `prometheus.io/scrape: "true"` to
    denote endpoints that are to be scraped, and use another annotation like `prometheus.io/scrape_service_only:
    "true"` to tell Prometheus to create exactly one target for this service.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '有时，我们可能只想获取某个服务下单个 Pod 的指标，而不是获取该服务下所有 Pod 的指标。由于大多数端点对象不是手动创建的，端点发现模式使用了从服务继承的注解。这意味着，如果我们为服务添加注解，该注解将在服务发现和端点发现模式中同时可见，这使得我们无法区分目标应该是按端点抓取还是按服务抓取。为了解决这个问题，我们可以使用`prometheus.io/scrape:
    "true"`来标记需要被抓取的端点，并使用另一个注解如`prometheus.io/scrape_service_only: "true"`来告诉 Prometheus
    仅为该服务创建一个目标。'
- en: 'The `prom-config-k8s.yml` template under our example repository contains some
    basic configurations to discover Kubernetes resources for Prometheus. Apply it
    as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们示例仓库中的`prom-config-k8s.yml`模板包含了一些基本配置，用于为 Prometheus 发现 Kubernetes 资源。按如下方式应用：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Because the resource in the template is a ConfigMap, which stores data in the `etcd` consensus
    storage, it takes a few seconds to become consistent. Afterward, we can reload
    Prometheus by sending a `SIGHUP` to the process:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因为模板中的资源是一个 ConfigMap，它将数据存储在`etcd`共识存储中，所以需要几秒钟才能变得一致。之后，我们可以通过向进程发送`SIGHUP`来重新加载
    Prometheus：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The provided template is based on this example from Prometheus' official repository.
    You can find out further uses at the following link, which also includes the target
    discovery for the Blackbox exporter: [https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml).
    We have also passed over the details of how the actions in the configuration actually
    work; to find out more, consult the official documentation: [https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的模板基于 Prometheus 官方仓库中的这个示例。你可以通过以下链接了解更多用途，其中还包括针对 Blackbox exporter 的目标发现：[https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml)。我们也略过了配置中动作是如何实际工作的细节；欲了解更多，请参考官方文档：[https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration)。
- en: Gathering data from Kubernetes
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 Kubernetes 收集数据
- en: 'The steps for implementing the monitoring layers discussed previously in Prometheus
    are now quite clear:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Prometheus 中实现之前讨论的监控层的步骤现在非常清晰：
- en: Install the exporters
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装导出器
- en: Annotate them with appropriate tags
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用适当的标签标注它们
- en: Collect them on auto-discovered endpoints
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在自动发现的端点上收集数据
- en: 'The host layer monitoring in Prometheus is done by the node exporter ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)).
    Its Kubernetes template can be found under the examples for this chapter, and
    it contains one DaemonSet with a scrape annotation. Install it as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 中的主机层监控由节点导出器（[https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)）完成。它的
    Kubernetes 模板可以在本章的示例中找到，并包含一个带有抓取注解的 DaemonSet。按如下方式安装：
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Its corresponding target in Prometheus will be discovered and created by the
    pod discovery role if using the example configuration.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用示例配置，它在 Prometheus 中的对应目标将通过 Pod 发现角色被发现并创建。
- en: The container layer collector should be kubelet. Consequently, discovering it
    with the node mode is the only thing we need to do.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 容器层的收集器应为 kubelet。因此，使用节点模式发现它是我们需要做的唯一事情。
- en: Kubernetes monitoring is done by `kube-state-metrics`, which was also introduced
    previously. It also comes with Prometheus annotations, which means we don't need
    to do anything else to configure it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 监控由`kube-state-metrics`完成，之前也介绍过它。它还带有 Prometheus 注解，这意味着我们不需要做任何其他配置。
- en: At this point, we've already set up a strong monitoring stack based on Prometheus.
    With respect to the application and the external resource monitoring, there are
    extensive exporters in the Prometheus ecosystem to support the monitoring of various
    components inside our system. For instance, if we need statistics on our MySQL
    database, we could just install MySQL Server Exporter ([https://github.com/prometheus/mysqld_exporter](https://github.com/prometheus/mysqld_exporter)),
    which offers comprehensive and useful metrics.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经基于Prometheus设置了强大的监控堆栈。关于应用程序和外部资源监控，Prometheus生态系统中有广泛的导出器来支持我们系统内各种组件的监控。例如，如果我们需要MySQL数据库的统计信息，只需安装MySQL服务器导出器（[https://github.com/prometheus/mysqld_exporter](https://github.com/prometheus/mysqld_exporter)），它提供了全面且有用的指标。
- en: 'In addition to the metrics that we have already described, there are some other
    useful metrics from Kubernetes components that play an important role:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们已经描述的指标之外，Kubernetes组件还有一些其他有用的指标起着重要作用：
- en: '**Kubernetes API server**: The API server exposes its stats at `/metrics`,
    and this target is enabled by default.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes API服务器**：API服务器在`/metrics`上公开其统计信息，默认情况下启用此目标。'
- en: '`kube-controller-manager`: This component exposes metrics on port `10252`,
    but it''s invisible on some managed Kubernetes services such as GKE. If you''re
    on a self-hosted cluster, applying `kubernetes/self/kube-controller-manager-metrics-svc.yml` creates
    endpoints for Prometheus.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`：此组件在`10252`端口上公开指标，但在某些托管的Kubernetes服务（如GKE）上不可见。如果您使用自托管集群，应用`kubernetes/self/kube-controller-manager-metrics-svc.yml`会为Prometheus创建端点。'
- en: '`kube-scheduler`: This uses port `10251`, and it''s also not visible on clusters
    by GKE. `kubernetes/self/kube-scheduler-metrics-svc.yml` is the template for creating
    a target to Prometheus.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`：使用`10251`端口，在GKE集群中也不可见。`kubernetes/self/kube-scheduler-metrics-svc.yml`用于创建指向Prometheus的目标模板。'
- en: '`kube-dns`: DNS in Kubernetes is managed by CoreDNS, which exposes its stats
    at port `9153`. The corresponding template is `kubernetes/self/ core-dns-metrics-svc.yml`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-dns`：Kubernetes中的DNS由CoreDNS管理，其统计信息通过`9153`端口暴露。相应的模板是`kubernetes/self/core-dns-metrics-svc.yml`。'
- en: '`etcd`: The `etcd` cluster also has a Prometheus metrics endpoint on port `2379`.
    If your `etcd` cluster is self-hosted and managed by Kubernetes, you can use `kubernetes/self/etcd-server.yml` as
    a reference.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`：`etcd`集群还在`2379`端口上提供Prometheus指标端点。如果您的`etcd`集群是自托管且由Kubernetes管理的，可以使用`kubernetes/self/etcd-server.yml`作为参考。'
- en: '**Nginx ingress controller**: The nginx controller publishes metrics at port
    `10254`, and will give you rich information about the state of nginx, as well
    as the duration, size, method, and status code of traffic routed by nginx. A full
    guide can be found here: [https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nginx Ingress Controller**：Nginx控制器通过`10254`端口发布指标，提供有关nginx状态以及nginx路由的持续时间、大小、方法和状态码的详细信息。完整的指南可以在这里找到：[https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md)。'
- en: The DNS in Kubernetes is served by `skydns` and it also has a metrics path exposed
    on the container. The typical setup in a `kube-dns` pod using `skydns` has two
    containers, `dnsmasq` and `sky-dns`, and their metrics ports are `10054` and `10055` respectively.
    The corresponding template is `kubernetes/self/ skydns-metrics-svc.yml` if we
    need it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的DNS由`skydns`提供，容器还暴露了一个指标路径。在使用`skydns`的`kube-dns` pod中，通常有两个容器，分别是`dnsmasq`和`sky-dns`，它们的指标端口分别是`10054`和`10055`。如果需要，对应的模板是`kubernetes/self/skydns-metrics-svc.yml`。
- en: Visualizing metrics with Grafana
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Grafana可视化指标
- en: The expression browser has a built-in graph panel that enables us to see the
    metrics, but it's not designed to serve as a visualization dashboard for daily
    routines. Grafana is the best option for Prometheus. We discussed how to set up
    Grafana in [Chapter 4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml), *Managing
    Stateful Workloads*, and we also provided templates in the repository for this
    chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式浏览器具有内置的图形面板，可帮助我们查看指标，但不适合作为日常例行工作的可视化仪表板。Grafana是Prometheus的最佳选择。我们在[第4章](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml)《管理有状态工作负载》中讨论了如何设置Grafana，并在该章节的存储库中提供了模板。
- en: 'To see Prometheus metrics in Grafana, we first have to add a data source. The
    following configurations are required to connect to our Prometheus server:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Grafana 中查看 Prometheus 指标，首先必须添加一个数据源。连接到我们的 Prometheus 服务器需要以下配置：
- en: '**Type**: `Prometheus`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**：`Prometheus`'
- en: '**URL**: `http://prometheus-svc.monitoring:9090`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网址**：`http://prometheus-svc.monitoring:9090`'
- en: 'Once it''s connected, we can import a dashboard. On Grafana''s sharing page
    ([https://grafana.com/dashboards?dataSource=prometheus](https://grafana.com/dashboards?dataSource=prometheus)),
    we can find rich off-the-shelf dashboards. The following screenshot is from dashboard
    `#1621`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦连接上，我们可以导入仪表盘。在 Grafana 的共享页面（[https://grafana.com/dashboards?dataSource=prometheus](https://grafana.com/dashboards?dataSource=prometheus)）上，我们可以找到丰富的现成仪表盘。以下截图来自仪表盘`#1621`：
- en: '![](img/08d48869-5de2-484e-8e3d-c0fcb1ca428a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08d48869-5de2-484e-8e3d-c0fcb1ca428a.png)'
- en: Because the graphs are drawn by data from Prometheus, we are capable of plotting
    any data we want, as long as we master PromQL.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些图表是由 Prometheus 的数据绘制的，我们可以根据需要绘制任何数据，只要我们掌握 PromQL。
- en: The content of a dashboard might vary significantly as every application focuses
    on different things. It is not a good idea, however, to put everything into one
    huge dashboard. The USE method ([http://www.brendangregg.com/usemethod.html](http://www.brendangregg.com/usemethod.html))
    and the four golden signals ([https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html](https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html#xref_monitoring_golden-signals))
    provide a good start for building a monitoring dashboard.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表盘的内容可能会有很大差异，因为每个应用程序关注的重点不同。然而，将所有内容放到一个巨大的仪表盘中并不是一个好主意。USE 方法（[http://www.brendangregg.com/usemethod.html](http://www.brendangregg.com/usemethod.html)）和四个黄金信号（[https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html](https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html#xref_monitoring_golden-signals)）为构建监控仪表盘提供了一个良好的起点。
- en: Logging events
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志事件
- en: Monitoring with a quantitative time series of the system status enables us to
    quickly identify which components in our system have failed, but it still isn't
    capable of diagnosing the root cause of a problem. What we need is a logging system
    that gathers, persists, and searches logs, by means of correlating events with
    the anomalies detected. Surely, in addition to troubleshooting and postmortem
    analysis of system failures, there are also various business use cases that need
    a logging system.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对系统状态的定量时间序列进行监控，我们能够快速识别系统中哪些组件发生了故障，但它仍然无法诊断问题的根本原因。我们需要的是一个日志系统，通过关联事件和检测到的异常来收集、持久化并搜索日志。确实，除了排查故障和系统故障后的事后分析，日志系统还有许多其他商业应用场景。
- en: 'In general, there are two main components in a logging system: the logging
    agent and the logging backend. The former is an abstract layer of a program. It
    gathers, transforms, and dispatches logs to the logging backend. A logging backend
    warehouses all logs received. As with monitoring, the most challenging part of
    building a logging system for Kubernetes is determining how to gather logs from
    containers to a centralized logging backend. Typically, there are three ways to
    send out the logs of a program:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，日志系统主要由两个组件组成：日志代理和日志后端。前者是程序的抽象层，负责收集、转换并将日志发送到日志后端。日志后端则存储接收到的所有日志。与监控类似，为
    Kubernetes 构建日志系统时最具挑战性的部分是确定如何从容器中收集日志并将其发送到集中式日志后端。通常，发送程序日志的方式有三种：
- en: Dumping everything to `stdout`/`stderr`.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有内容转储到`stdout`/`stderr`。
- en: Writing log files to the filesystem.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志文件写入文件系统。
- en: Sending logs to a logging agent or logging to the backend directly. Programs
    in Kubernetes are also able to emit logs in the same manner, so long as we understand
    how log streams flow in Kubernetes.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志发送到日志代理或直接记录到后端。Kubernetes 中的程序也能够以相同的方式发出日志，只要我们理解 Kubernetes 中日志流的流动方式。
- en: Patterns of aggregating logs
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合日志的模式
- en: For programs that log to a logging agent or a backend directly, whether they
    are inside Kubernetes or not doesn't actually matter, because they technically
    don't send out logs through Kubernetes. In other cases, we'd use the following
    two patterns for logging.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直接向日志代理或后端记录日志的程序，无论它们是否在 Kubernetes 内部运行，实际上都无关紧要，因为它们从技术上讲并不是通过 Kubernetes
    发送日志。在其他情况下，我们将使用以下两种日志模式。
- en: Collecting logs with a logging agent per node
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个节点使用日志代理收集日志
- en: We know that messages we retrieved via `kubectl logs` are streams redirected
    from the `stdout`/`stderr` of a container, but it's obviously not a good idea
    to collect logs with `kubectl logs`. In fact, `kubectl logs` gets logs from kubelet,
    and kubelet aggregates logs from the container runtime underneath the host path, `/var/log/containers/`.
    The naming pattern of logs is `{pod_name}_{namespace}_{container_name}_{container_id}.log`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，通过`kubectl logs`获取的消息是从容器的`stdout`/`stderr`重定向来的流，但显然用`kubectl logs`收集日志并不是一个好主意。实际上，`kubectl
    logs`是从kubelet获取日志，而kubelet会从容器运行时的宿主路径`/var/log/containers/`聚合日志。日志的命名规则为`{pod_name}_{namespace}_{container_name}_{container_id}.log`。
- en: 'Therefore, what we need to do to converge the standard streams of running containers
    is to set up logging agents on every node and configure them to tail and forward
    log files under the path, as shown in the following diagram:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要做的是，在每个节点上设置日志代理，并配置它们跟踪并转发路径下的日志文件，如下图所示：
- en: '![](img/09c29935-5094-4b6a-a1d8-2ee6602013c5.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09c29935-5094-4b6a-a1d8-2ee6602013c5.png)'
- en: 'In practice, we''d also configure the logging agent to tail the logs of the
    system and the Kubernetes components under `/var/log` on masters and nodes, such
    as the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们还会配置日志代理来跟踪主节点和节点下`/var/log`中的系统和Kubernetes组件的日志，如下所示：
- en: '`kube-proxy.log`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy.log`'
- en: '`kube-apiserver.log`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver.log`'
- en: '`kube-scheduler.log`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler.log`'
- en: '`kube-controller-manager.log`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager.log`'
- en: '`etcd.log`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd.log`'
- en: If the Kubernetes components are managed by `systemd`, the log would be present
    in `journald`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Kubernetes组件由`systemd`管理，则日志将显示在`journald`中。
- en: Aside from `stdout`/`stderr`, if the logs of an application are stored as files
    in the container and persisted via the `hostPath` volume, a node logging agent
    is capable of passing them to a node. However, for each exported log file, we
    have to customize their corresponding configurations in the logging agent so that
    they can be dispatched correctly. Moreover, we also need to name log files properly
    to prevent any collisions and to take care of log rotation manageable, which makes
    it an unscalable and unmanageable mechanism.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`stdout`/`stderr`，如果一个应用程序的日志作为文件存储在容器内，并通过`hostPath`卷持久化，那么一个节点日志代理能够将它们传递到节点上。然而，对于每个导出的日志文件，我们必须在日志代理中自定义它们的相应配置，以便正确地转发它们。此外，我们还需要合理命名日志文件，以避免任何冲突，并确保日志旋转可管理，这使得这种机制不可扩展且难以管理。
- en: Running a sidecar container to forward written logs
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行一个sidecar容器来转发写入的日志
- en: 'It can be difficult to modify our application to write logs to standard streams
    rather than log files, and we often want to avoid the troubles brought about by
    logging to `hostPath` volumes. In this situation, we could run a sidecar container
    to deal with logging for a pod. In other words, each application pod would have
    two containers sharing the same `emptyDir` volume, so that the sidecar container
    can follow logs from the application container and send them outside their pod,
    as shown in the following diagram:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 修改应用程序以将日志写入标准流而不是日志文件可能会很困难，而且我们通常希望避免将日志写入`hostPath`卷带来的麻烦。在这种情况下，我们可以运行一个sidecar容器来处理pod的日志。换句话说，每个应用程序pod将有两个共享相同`emptyDir`卷的容器，以便sidecar容器可以跟踪应用程序容器的日志并将它们发送到pod外部，如下图所示：
- en: '![](img/8f39cc8c-95df-4b7b-87e4-d7467bf42d8f.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f39cc8c-95df-4b7b-87e4-d7467bf42d8f.png)'
- en: 'Although we don''t need to worry about managing log files anymore, chores such
    as configuring logging agents for each pod and attaching metadata from Kubernetes
    to log entries still takes extra effort. Another option would be to use the sidecar
    container to output logs to standard streams instead of running a dedicated logging
    agent, such as the following pod example. In this case, the application container
    unremittingly writes messages to `/var/log/myapp.log` and the sidecar tails `myapp.log`
    in the shared volume:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不再需要担心管理日志文件，但仍然需要额外的工作，如为每个pod配置日志代理并将Kubernetes的元数据附加到日志条目上。另一种选择是使用sidecar容器将日志输出到标准流，而不是运行专门的日志代理，如以下pod示例所示。在这种情况下，应用程序容器持续不断地写入消息到`/var/log/myapp.log`，而sidecar容器则在共享卷中跟踪`myapp.log`：
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can see the written log with `kubectl logs`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`kubectl logs`查看书写日志：
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Ingesting Kubernetes state events
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取Kubernetes状态事件
- en: The event messages we saw in the output of `kubectl describe` contain valuable
    information and complement the metrics gathered by `kube-state-metrics`. This
    allows us to determine exactly what happened to our pods or nodes. Consequently,
    those event messages should be part of our logging essentials, together with system
    and application logs. In order to achieve this, we'll need something to watch
    Kubernetes API servers and aggregate events into a logging sink. The event objects
    inside Kubernetes are also stored in `etcd`, but tapping into the storage to get
    those event objects might require a lot of work. Projects such as eventrouter
    ([https://github.com/heptiolabs/eventrouter](https://github.com/heptiolabs/eventrouter))
    can help in this scenario. Eventrouter works by translating event objects to structured
    messages and emitting them to its `stdout`. As a result, our logging system can
    treat those events as normal logs while keeping the metadata of events.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`kubectl describe`输出中看到的事件消息包含了宝贵的信息，并补充了`kube-state-metrics`收集的指标。这使我们能够准确地了解我们的pods或节点发生了什么。因此，这些事件消息应该成为我们的日志记录基本内容的一部分，与系统日志和应用日志一起使用。为了实现这一点，我们需要一些工具来监控Kubernetes
    API服务器，并将事件聚合到日志存储中。Kubernetes中的事件对象也存储在`etcd`中，但访问存储以获取这些事件对象可能需要大量的工作。像eventrouter（[https://github.com/heptiolabs/eventrouter](https://github.com/heptiolabs/eventrouter)）这样的项目可以在这种情况下提供帮助。Eventrouter的工作方式是将事件对象转换为结构化消息，并将其发送到`stdout`。因此，我们的日志系统可以将这些事件视为普通日志，同时保留事件的元数据。
- en: There are various other alternatives. One is Event Exporter ([https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter)),
    although this only supports StackDriver, a monitoring solution on Google Cloud
    Platform. Another alternative is the eventer, part of Heapster. This supports
    Elasticsearch, InfluxDB, Riemann, and Google Cloud Logging as its sink. Eventer
    can also output to `stdout` directly if the logging system we're using is not
    supported. However, as Heapster was replaced by the metric server, the development
    of the eventer was also dropped.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些选择。一个是Event Exporter（[https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter)），尽管它仅支持StackDriver，这是一个Google
    Cloud Platform上的监控解决方案。另一个选择是eventer，它是Heapster的一部分。它支持Elasticsearch、InfluxDB、Riemann和Google
    Cloud Logging作为输出目标。如果我们使用的日志系统不被支持，Eventer还可以直接输出到`stdout`。然而，由于Heapster被metric
    server替代，eventer的开发也被放弃了。
- en: Logging with Fluent Bit and Elasticsearch
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Fluent Bit和Elasticsearch进行日志记录
- en: 'So far, we''ve discussed various logging scenarios that we may encounter in
    the real world. It''s now time to roll up our sleeves and fabricate a logging
    system. The architectures of logging systems and monitoring systems are pretty
    much the same in a number of ways: they both have collectors, storage, and consumers
    such as BI tools or a search engine. The components might vary significantly,
    depending on the needs. For instance, we might process some logs on the fly to
    extract real-time information, while we might just archive other logs to durable
    storage for further use, such as for batch reporting or meeting compliance requirements.
    All in all, as long as we have a way to ship logs out of our container, we can
    always integrate other tools into our system. The following diagram depicts some
    possible use cases:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了在现实世界中可能遇到的各种日志记录场景。现在是时候卷起袖子，构建一个日志系统了。日志系统和监控系统的架构在很多方面都非常相似：它们都有收集器、存储和消费者，如BI工具或搜索引擎。根据需求，组件可能会有很大的差异。例如，我们可能会实时处理一些日志以提取实时信息，而对于其他日志，我们可能只是将它们存档到持久存储中以供后续使用，比如批量报告或满足合规要求。总的来说，只要我们有办法将日志从容器中发送出去，就可以将其他工具集成到我们的系统中。下图展示了一些可能的使用场景：
- en: '![](img/a3148c73-67fd-4e8a-8161-cb65891a4bfd.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3148c73-67fd-4e8a-8161-cb65891a4bfd.png)'
- en: In this section, we're going to set up the most fundamental logging system.
    Its components include Fluent Bit, Elasticsearch, and Kibana. The templates for
    this section can be found under `7-3_efk`, and they are to be deployed to the `logging` namespace.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将设置最基本的日志系统。它的组件包括Fluent Bit、Elasticsearch和Kibana。本节的模板可以在`7-3_efk`中找到，它们需要部署到`logging`命名空间。
- en: 'Elasticsearch is a powerful text search and analysis engine, which makes it
    an ideal choice for analyzing the logs from everything running in our cluster.
    The Elasticsearch template for this chapter uses a very simple setup to demonstrate
    the concept. If you''d like to deploy an Elasticsearch cluster for production
    use, using the StatefulSet controller to set up a cluster and tuning Elasticsearch
    with proper configurations, as we discussed in [Chapter 4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml),
    *Managing Stateful Workloads,* is recommended. We can deploy an Elasticsearch
    instance and a logging namespace with the following template ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk)):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是一个强大的文本搜索和分析引擎，使其成为分析集群中所有运行日志的理想选择。本章节的 Elasticsearch 模板使用了一个非常简单的设置来演示该概念。如果你希望部署一个用于生产的
    Elasticsearch 集群，建议使用 StatefulSet 控制器来设置集群，并通过适当的配置调优 Elasticsearch，正如我们在[第 4
    章](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml)《管理有状态工作负载》中所讨论的那样。我们可以通过以下模板部署一个
    Elasticsearch 实例和一个日志命名空间（[https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk)）：
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We know that Elasticsearch is ready if we get a response from `es-logging-svc:9200`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从 `es-logging-svc:9200` 获得响应，那么就说明 Elasticsearch 已经准备好。
- en: Elasticsearch is a great document search engine. However, it might not be as
    good when it comes to persisting a large amount of logs. Fortunately, there are
    various solutions that allow us to use Elasticsearch to index documents stored
    in other storage.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是一个优秀的文档搜索引擎。然而，在持久化大量日志时，它可能表现得不如预期。幸运的是，有多种解决方案允许我们使用 Elasticsearch
    索引存储在其他存储中的文档。
- en: The next step is to set up a node logging agent. As we'd run this on every node,
    we want it to be as light as possible in terms of node resource use; hence why
    we opted for Fluent Bit ([https://fluentbit.io/](https://fluentbit.io/)). Fluent Bit
    features lower memory footprints, which makes it a competent logging agent for
    our requirement, which is to ship all the logs out of a node.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置一个节点日志代理。由于我们会在每个节点上运行此代理，我们希望它在节点资源使用上尽可能轻量；这也是我们选择 Fluent Bit 的原因（[https://fluentbit.io/](https://fluentbit.io/)）。Fluent
    Bit 具有更低的内存占用，使其成为满足我们需求的合适日志代理，即将所有日志从节点中导出。
- en: As the implementation of Fluent Bit aims to minimize resource usage, it has
    reduced its functions to a very limited set. If we want to have a greater degree
    of freedom to combine parsers and filters for different applications in the logging
    layer, we could use Fluent Bit's sibling project, Fluentd ([https://www.fluentd.org/](https://www.fluentd.org/)),
    which is much more extensible and flexible but consumes more resources than Fluent Bit.
    Since Fluent Bit is able to forward logs to Fluentd, a common method is to use
    Fluent Bit as the node logging agent and Fluentd as the aggregator, like in the
    previous figure.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Fluent Bit 的实现目标是最大限度地减少资源使用，因此它将功能减少到了一个非常有限的集合。如果我们希望在日志层中有更大的自由度来组合解析器和过滤器以适应不同的应用程序，我们可以使用
    Fluent Bit 的兄弟项目 Fluentd（[https://www.fluentd.org/](https://www.fluentd.org/)），它更加可扩展和灵活，但比
    Fluent Bit 消耗更多资源。由于 Fluent Bit 能够将日志转发到 Fluentd，因此一种常见的方法是将 Fluent Bit 用作节点日志代理，将
    Fluentd 用作聚合器，就像前面图示的那样。
- en: 'In our example, Fluent Bit is configured as the first logging pattern. This
    means that it collects logs with a logging agent per node and sends them to Elasticsearch
    directly:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，Fluent Bit 被配置为第一个日志模式。这意味着它会通过每个节点的日志代理收集日志，并直接将其发送到 Elasticsearch：
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The ConfigMap for Fluent Bit is already configured to tail container logs under
    `/var/log/containers` and the logs of certain system components under `/var/log`.
    Fluent Bit can also expose its stats metrics in Prometheus format on port `2020`,
    which is configured in the DaemonSet template.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Fluent Bit 的 ConfigMap 已经配置为跟踪 `/var/log/containers` 下的容器日志和 `/var/log` 下某些系统组件的日志。Fluent
    Bit 还可以在端口 `2020` 上以 Prometheus 格式暴露其统计数据，这在 DaemonSet 模板中进行了配置。
- en: Due to stability issues and the need for flexibility, it is still common to
    use Fluentd as a logging agent. The templates can be found under `logging-agent/fluentd`
    in our example, or at the official repository here: [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于稳定性问题和对灵活性的需求，使用 Fluentd 作为日志代理仍然很常见。在我们的示例中，可以在 `logging-agent/fluentd` 下找到模板，或者可以在官方仓库中找到它们：[https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset)。
- en: 'To use Kubernetes events, we can use the `eventrouter`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Kubernetes 事件，我们可以使用 `eventrouter`：
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will start to print events in JSON format at the `stdout` stream, so that
    we can index them in Elasticsearch.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这将开始以 JSON 格式在 `stdout` 流中打印事件，以便我们可以在 Elasticsearch 中对其进行索引。
- en: 'To see logs emitted to Elasticsearch, we can invoke the search API of Elasticsearch,
    but there''s a better option: Kibana, a web interface that allows us to play with
    Elasticsearch. Deploy everything under `kibana` in the examples for this section
    with the following command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看发送到 Elasticsearch 的日志，我们可以调用 Elasticsearch 的搜索 API，但还有一个更好的选择：Kibana，它是一个允许我们与
    Elasticsearch 交互的 web 界面。在本节的示例中，使用以下命令将所有内容部署到 `kibana` 下：
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Grafana also supports reading data from Elasticsearch: [http://docs.grafana.org/features/datasources/elasticsearch/](http://docs.grafana.org/features/datasources/elasticsearch/).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 也支持从 Elasticsearch 读取数据：[http://docs.grafana.org/features/datasources/elasticsearch/](http://docs.grafana.org/features/datasources/elasticsearch/)。
- en: 'Kibana, in our example, is listening to port `5601`. After exposing the service
    from your cluster and connecting to it with any browser, you can start to search
    logs from Kubernetes. In our example Fluent Bit configuration, the logs routed
    by eventrouter would be under the index named `kube-event-*`, while logs from
    other containers could be found at the index named `kube-container-*`. The following
    screenshot shows what a event message looks like in Kibana:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，Kibana 监听端口 `5601`。将服务从集群中暴露并使用任何浏览器连接后，您可以开始搜索来自 Kubernetes 的日志。在我们的
    Fluent Bit 配置示例中，通过 eventrouter 路由的日志将位于名为 `kube-event-*` 的索引下，而来自其他容器的日志则可以在名为
    `kube-container-*` 的索引中找到。以下截图展示了 Kibana 中的事件消息样式：
- en: '![](img/ff011e81-793b-44b5-8af8-5fa037b5bccb.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff011e81-793b-44b5-8af8-5fa037b5bccb.png)'
- en: Extracting metrics from logs
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从日志中提取指标
- en: 'The monitoring and logging system we built around our application on top of
    Kubernetes is shown in the following diagram:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们围绕 Kubernetes 构建的应用程序监控和日志系统，如下图所示：
- en: '![](img/d1e76951-6c2c-40ed-89b9-727e2ac25097.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1e76951-6c2c-40ed-89b9-727e2ac25097.png)'
- en: The logging part and the monitoring part look like two independent tracks, but
    the value of the logs is much more than a collection of short texts. This is structured
    data and usually emitted with timestamps; because of this, if we can parse information
    from logs and project the extracted vector into the time dimension according to
    the timestamps, it will become a time series metric and be available in Prometheus.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 日志部分和监控部分看起来像是两条独立的轨道，但日志的价值远不止是短文本的集合。这是结构化数据，通常带有时间戳；因此，如果我们能够从日志中解析信息，并根据时间戳将提取的向量投影到时间维度，它将成为一个时间序列指标，并可用于
    Prometheus。
- en: 'For example, an access log entry from any of the web servers may look as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，任何一个 web 服务器的访问日志条目可能如下所示：
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This consists of data such as the request IP address, the time, the method,
    the handler, and so on. If we demarcate log segments by their meanings, the counted
    sections can then be regarded as a metric sample, as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据包括请求的 IP 地址、时间、方法、处理程序等。如果我们根据日志段的含义划分它们，那么计数的部分可以视为一个指标样本，如下所示：
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After the transformation, tracing the log over time will be more intuitive.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 转换之后，随着时间的推移，追踪日志将更加直观。
- en: To organize logs into the Prometheus format, tools such as mtail ([https://github.com/google/mtail](https://github.com/google/mtail)),
    Grok Exporter ([https://github.com/fstab/grok_exporter](https://github.com/fstab/grok_exporter)),
    or Fluentd ([https://github.com/fluent/fluent-plugin-prometheus](https://github.com/fluent/fluent-plugin-prometheus))
    are all widely used to extract log entries into metrics.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将日志组织成 Prometheus 格式，诸如 mtail ([https://github.com/google/mtail](https://github.com/google/mtail))、Grok
    Exporter ([https://github.com/fstab/grok_exporter](https://github.com/fstab/grok_exporter))
    或 Fluentd ([https://github.com/fluent/fluent-plugin-prometheus](https://github.com/fluent/fluent-plugin-prometheus))
    等工具，广泛用于将日志条目提取为指标。
- en: Arguably, lots of applications nowadays support outputting structured metrics
    directly, and we can always instrument our own application for this type of information.
    However, not everything in our tech stack provides us with a convenient way to
    get their internal states, especially operating system utilities, such as `ntpd`.
    It's still worth having this kind of tool in our monitoring stack to help us improve
    the observability of our infrastructure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，现在很多应用程序支持直接输出结构化指标，我们总是可以为自己的应用程序加入此类信息的监控。然而，并不是所有的技术栈都能为我们提供方便的方式来获取其内部状态，尤其是操作系统工具，如
    `ntpd`。因此，将此类工具纳入我们的监控栈，帮助提升基础设施的可观察性，仍然是值得的。
- en: Incorporating data from Istio
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成Istio的数据
- en: 'In a service mesh, the gateway between every service is the front proxy. For
    this reason, the front proxy is, unsurprisingly, a rich information source for
    things running inside the mesh. However, if our tech stack already has similar
    components, such as load balancers or reverse proxies for internal services, then
    what''s the difference between collecting traffic data from them and the service
    mesh proxy? Let''s consider the classical setup:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务网格中，每个服务之间的网关是前端代理。因此，前端代理显然是网格内部运行的事物的重要信息来源。然而，如果我们的技术栈中已经有类似的组件，如用于内部服务的负载均衡器或反向代理，那么从它们收集流量数据与服务网格代理有何不同？让我们考虑经典设置：
- en: '![](img/2295bd70-c49d-4c36-ae39-c0826f5fba2c.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2295bd70-c49d-4c36-ae39-c0826f5fba2c.png)'
- en: '**SVC-A** and **SVC-B** make requests to **SVC-C**. The data gathered from
    the load balancer for **SVC-C** represents the quality of **SVC-C**. However,
    as we don''t have any visibility over the path from the clients to **SVC-C**,
    the only way to measure the quality between **SVC-A** or **SVC-B** and **SVC-C**
    is either by relying on a mechanism built on the client side, or by putting probes
    in the network that the clients are in. For a service mesh, take a look at the
    following diagram:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVC-A**和**SVC-B**向**SVC-C**发起请求。从负载均衡器收集到的**SVC-C**的数据代表了**SVC-C**的质量。然而，由于我们无法看到客户端到**SVC-C**的路径，衡量**SVC-A**或**SVC-B**与**SVC-C**之间质量的唯一方式是依赖客户端侧构建的机制，或在客户端所在的网络中放置探针。对于服务网格，看看以下图表：'
- en: '![](img/f5d17716-b859-4103-9096-c0f407f8a340.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5d17716-b859-4103-9096-c0f407f8a340.png)'
- en: Here, we want to know the quality of **SVC-C**. In this setup, **SVC-A** and
    **SVC-B** communicate with **SVC-C** via their sidecar proxies, so if we collect
    metrics about requests that go to **SVC-C** from all client-side proxies, we can
    also get the same data from the server-side load balancer, plus the missing measurement
    between **SVC-C** and its clients. In other words, we can have a consolidated
    way to measure not only how **SVC-C** performs, but also the quality between **SVC-C**
    and its clients. This augmented information also helps us to locate failures when
    triaging a problem.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们想要了解**SVC-C**的质量。在此设置中，**SVC-A**和**SVC-B**通过它们的sidecar代理与**SVC-C**通信，因此，如果我们收集所有客户端侧代理向**SVC-C**发出的请求的指标，我们也可以从服务器端的负载均衡器获得相同的数据，并补充**SVC-C**与其客户端之间缺失的度量。换句话说，我们可以有一种统一的方式来衡量不仅**SVC-C**的性能，还包括**SVC-C**与其客户端之间的质量。这些增强的信息也有助于我们在排查问题时定位故障。
- en: The Istio adapter model
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Istio适配器模型
- en: Mixer is the component that manages telemetry in Istio's architecture. It takes
    the statistics from the side proxy, deployed along with the application container,
    and interacts with other backend components through its adapters. For instance,
    our monitoring backend is Prometheus, so we can utilize the Prometheus adapter
    of mixer to transform the metrics we get from envoy proxies into a Prometheus
    metrics path.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Mixer是Istio架构中管理遥测的组件。它从与应用容器一起部署的边缘代理获取统计信息，并通过适配器与其他后端组件进行交互。例如，我们的监控后端是Prometheus，因此我们可以利用Mixer的Prometheus适配器将从envoy代理获取的指标转换为Prometheus指标路径。
- en: 'The way in which access logs go through the pipeline to our Fluentd/Fluent Bit
    logging backend is the same as in the one we built previously, the one that ships
    logs into Elasticsearch. The interactions between Istio components and the monitoring
    backends are illustrated in the following diagram:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 访问日志如何通过管道流向我们的Fluentd/Fluent Bit日志后端，与我们之前构建的管道相同，该管道将日志传送到Elasticsearch。Istio组件与监控后端之间的交互在以下图表中有所示：
- en: '![](img/fbbf5dda-9872-40f9-bbd3-7069e5c4d2a7.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbbf5dda-9872-40f9-bbd3-7069e5c4d2a7.png)'
- en: Configuring Istio for existing infrastructure
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为现有基础设施配置Istio
- en: The adapter model allows us to fetch the monitoring data from Mixer components
    easily. It requires the configurations that we will explore in the following sections.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器模型使我们能够轻松地从Mixer组件中获取监控数据。它需要在接下来的章节中探讨的配置。
- en: Mixer templates
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mixer 模板
- en: 'A Mixer template defines which data Mixer should organize, and in what form
    the data should be in. To get metrics and access logs, we need the `metric` and
    `logentry` templates. For instance, the following template tells Mixer to output
    the log with the source and destination name, the method, the request URL, and
    so on:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Mixer 模板定义了Mixer应该组织哪些数据，以及数据应以何种形式呈现。为了获取指标和访问日志，我们需要`metric`和`logentry`模板。例如，以下模板指示Mixer输出包含源和目标名称、方法、请求URL等的日志：
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The complete reference for each kind of template can be found here: [https://istio.io/docs/reference/config/policy-and-telemetry/templates/](https://istio.io/docs/reference/config/policy-and-telemetry/templates/).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 每种模板的完整参考可以在这里找到：[https://istio.io/docs/reference/config/policy-and-telemetry/templates/](https://istio.io/docs/reference/config/policy-and-telemetry/templates/)。
- en: Handler adapters
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理器适配器
- en: 'A handler adapter declares the way the Mixer should interact with handlers.
    For the previous `logentry`, we can have a handler definition that looks as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器适配器声明了Mixer与处理器交互的方式。对于前面的`logentry`，我们可以有一个如下所示的处理器定义：
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: From this code snippet, Mixer knows a destination that can receive the `logentry`.
    The capabilities of every type of adapter differ significantly. For example, the
    `fluentd` adapter can only accept the `logentry` template, and `Prometheus` is
    only able to deal with the `metric` template, while the Stackdriver can take `metric`,
    `logentry`, and `tracespan` templates. All supported adapters are listed here: [https://istio.io/docs/reference/config/policy-and-telemetry/adapters/](https://istio.io/docs/reference/config/policy-and-telemetry/adapters/).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个代码片段来看，Mixer知道一个可以接收`logentry`的目标。每种类型的适配器功能差异很大。例如，`fluentd`适配器只能接受`logentry`模板，而`Prometheus`只能处理`metric`模板，而Stackdriver可以处理`metric`、`logentry`和`tracespan`模板。所有支持的适配器列在这里：[https://istio.io/docs/reference/config/policy-and-telemetry/adapters/](https://istio.io/docs/reference/config/policy-and-telemetry/adapters/)。
- en: Rules
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规则
- en: 'Rules are the binding between a template and a handler. If we already have
    an `accesslog`, `logentry` and a `fluentd` handler in the previous examples, then
    a rule such as this one associates the two entities:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 规则是模板与处理器之间的绑定。如果我们在之前的示例中已经有了`accesslog`、`logentry`和`fluentd`处理器，那么类似这样的规则将这两个实体关联起来：
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once the rule is applied, the mixer knows it should send the access logs in
    the format defined previously to the `fluentd` at `fluentd-aggegater-svc.logging:24224`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦规则应用，Mixer就知道它应该以之前定义的格式将访问日志发送到`fluentd`，地址为`fluentd-aggegater-svc.logging:24224`。
- en: The example of deploying a `fluentd` instance that takes inputs from the TCP
    socket can be found under `7_3efk/logging-agent/fluentd-aggregator` ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator)),
    and is configured to forward logs to the Elasticsearch instance we deployed previously.
    The three Istio templates for access logs can be found under `7-4_istio_fluentd_accesslog.yml` ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml)).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个`fluentd`实例来接收来自TCP套接字的输入的示例可以在`7_3efk/logging-agent/fluentd-aggregator`下找到（[https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator)），并配置为将日志转发到我们之前部署的Elasticsearch实例。访问日志的三个Istio模板可以在`7-4_istio_fluentd_accesslog.yml`下找到（[https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml)）。
- en: Let's now think about metrics. If Istio is deployed by the official chart with
    Prometheus enabled (it is enabled by default), then there will be a Prometheus
    instance in your cluster under the `istio-system` namespace. Additionally, Prometheus
    would be preconfigured to gather metrics from the Istio components. However, for
    various reasons, we may want to use our own Prometheus deployment, or make the
    one that comes with Istio dedicated to metrics from Istio components only. On
    the other hand, we know that the Prometheus architecture is flexible, and as long
    as the target components expose their metrics endpoint, we can configure our own
    Prometheus instance to scrape those endpoints.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一下指标。如果Istio是通过官方图表部署，并且启用了Prometheus（默认启用），那么在`istio-system`命名空间下的集群中将会有一个Prometheus实例。此外，Prometheus会被预配置以收集Istio组件的指标。然而，出于各种原因，我们可能希望使用自己的Prometheus部署，或者将Istio附带的Prometheus专门用于收集Istio组件的指标。另一方面，我们知道Prometheus架构是灵活的，只要目标组件暴露它们的指标端点，我们就可以配置自己的Prometheus实例来抓取这些端点。
- en: 'Some useful endpoints from Istio components are listed here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 下面列出了来自Istio组件的一些有用端点：
- en: '`<all-components>:9093/metrics`: Every Istio component exposes their internal
    states on port `9093`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<all-components>:9093/metrics`：每个 Istio 组件都会在端口 `9093` 上暴露其内部状态。'
- en: '`<envoy-sidecar>:15090/stats/prometheus`: Every envoy proxy prints the raw
    stats here. If we want to monitor our application, it is advisable to use the
    mixer template to sort out the metrics first.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<envoy-sidecar>:15090/stats/prometheus`：每个 envoy 代理都会在此处打印原始统计数据。如果我们想要监控我们的应用程序，建议先使用
    mixer 模板整理指标。'
- en: '`<istio-telemetry-pods>:42422/metrics`: The metrics configured by the Prometheus
    adapter and processed by mixer will be available here. Note that the metrics from
    an envoy sidecar are only available in the telemetry pod that the envoy reports to.
    In other words, we should use the endpoint discovery mode of Prometheus to collect
    metrics from all telemetry pods instead of scraping data from the telemetry service.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<istio-telemetry-pods>:42422/metrics`：由 Prometheus 适配器配置并由 mixer 处理的指标将在此处提供。请注意，来自
    envoy sidecar 的指标仅在 envoy 上报的 **telemetry pod** 中可用。换句话说，我们应当使用 Prometheus 的端点发现模式来收集所有
    telemetry pod 的指标，而不是直接从 telemetry 服务中抓取数据。'
- en: 'By default, the following metrics will be configured and available in the Prometheus
    path:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，以下指标将被配置并在 Prometheus 路径中提供：
- en: '`requests_total`'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests_total`'
- en: '`request_duration_seconds`'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`request_duration_seconds`'
- en: '`request_bytes`'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`request_bytes`'
- en: '`response_bytes`'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`response_bytes`'
- en: '`tcp_sent_bytes_total`'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tcp_sent_bytes_total`'
- en: '`tcp_received_bytes_total`'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tcp_received_bytes_total`'
- en: 'Another way to make the metrics collected by the Prometheus instance, deployed
    along with official Istio releases, available to our Prometheus is by using the
    federation setup. This involves setting up one Prometheus instance to scrape metrics
    stored inside another Prometheus instance. This way, we can regard the Prometheus
    for Istio as the collector for all Istio-related metrics. The path for the federation
    feature is at `/federate`. Say we want to get all the metrics with the label `{job="istio-mesh"}`,
    the query parameter would be as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使 Prometheus 实例（与官方 Istio 发布版一起部署的）收集的指标对我们的 Prometheus 可用的方法是使用联合设置。这涉及到设置一个
    Prometheus 实例来抓取存储在另一个 Prometheus 实例中的指标。通过这种方式，我们可以将 Istio 的 Prometheus 看作是所有
    Istio 相关指标的收集器。联合功能的路径为 `/federate`。假设我们想获取所有标签为 `{job="istio-mesh"}` 的指标，则查询参数如下：
- en: '[PRE29]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As a result, by adding a few configuration lines, we can easily integrate Istio
    metrics into the existing monitoring pipeline. For a full reference on federation,
    take a look at the official documentation: [https://prometheus.io/docs/prometheus/latest/federation/](https://prometheus.io/docs/prometheus/latest/federation/).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，通过添加几行配置，我们可以轻松地将 Istio 指标集成到现有的监控管道中。有关联合的完整参考，请查看官方文档：[https://prometheus.io/docs/prometheus/latest/federation/](https://prometheus.io/docs/prometheus/latest/federation/)。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: At the start of this chapter, we described how to get the status of running
    containers quickly by means of built-in functions such as `kubectl`. Then, we
    expanded the discussion to look at the concepts and principles of monitoring,
    including why, what, and how to monitor our application on Kubernetes. Afterward,
    we built a monitoring system with Prometheus as the core, and set up exporters
    to collect metrics from our application, system components, and Kubernetes units.
    The fundamentals of Prometheus, such as its architecture and query domain-specific
    language were also introduced, so we can now use metrics to gain insights into
    our cluster, as well as the applications running inside, to not only retrospectively troubleshoot,
    but also detect potential failures. After that, we described common logging patterns
    and how to deal with them in Kubernetes, and deployed an EFK stack to converge
    logs. Finally, we turned to another important piece of infrastructure between
    Kubernetes and our applications, the service mesh, to get finer precision when
    monitoring telemetry. The system we built in this chapter enhances the reliability
    of our service.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们描述了如何通过内置函数（如 `kubectl`）快速获取正在运行的容器状态。然后，我们扩展了讨论，探讨了监控的概念和原理，包括为什么、监控什么以及如何在
    Kubernetes 上监控我们的应用程序。之后，我们构建了一个以 Prometheus 为核心的监控系统，并设置了导出器来收集来自我们的应用程序、系统组件和
    Kubernetes 单元的指标。还介绍了 Prometheus 的基本原理，如其架构和查询领域特定语言，因此我们现在可以使用指标深入了解我们的集群以及运行其中的应用程序，不仅能回顾性地排查故障，还能检测潜在的故障。之后，我们描述了常见的
    **日志** 模式以及如何在 Kubernetes 中处理这些模式，并部署了 EFK 堆栈来整合日志。最后，我们转向了 Kubernetes 和应用程序之间的另一个重要基础设施组件——服务网格，以便在监控遥测时获得更精细的精度。本章中构建的系统提升了我们服务的可靠性。
- en: In [Chapter 8](a7a72300-181d-41ad-a08a-7e42744d365f.xhtml), *Resource Management
    and Scaling*, we'll leverage those metrics to optimize the resources used by our
    services.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](a7a72300-181d-41ad-a08a-7e42744d365f.xhtml)中，*资源管理与扩展*，我们将利用这些指标来优化我们的服务所使用的资源。
