- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: In today’s data-driven world, the ability to process and analyze vast amounts
    of data has become a critical competitive advantage for businesses across industries.
    Big data technologies have emerged as powerful tools to handle the ever-increasing
    volume, velocity, and variety of data, enabling organizations to extract valuable
    insights and drive informed decision-making. However, managing and scaling these
    technologies can be a daunting task, often requiring significant infrastructure
    and operational overhead.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今数据驱动的世界里，处理和分析海量数据的能力已成为各行各业企业的重要竞争优势。大数据技术作为处理日益增长的数据量、速度和多样性的强大工具，帮助组织提取有价值的见解并推动基于数据的决策。然而，管理和扩展这些技术可能是一项艰巨的任务，通常需要大量的基础设施和运营开销。
- en: Enter Kubernetes, the open source container orchestration platform that has
    revolutionized the way we deploy and manage applications. By providing a standardized
    and automated approach to container management, Kubernetes has simplified the
    deployment and scaling of complex applications, including big data workloads.
    This book aims to bridge the gap between these two powerful technologies, guiding
    you through the process of implementing a robust and scalable big data architecture
    on Kubernetes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 Kubernetes，这一开源容器编排平台已经彻底改变了我们部署和管理应用程序的方式。通过提供标准化和自动化的容器管理方法，Kubernetes
    简化了复杂应用程序（包括大数据工作负载）的部署和扩展。本书旨在弥合这两种强大技术之间的差距，引导你通过在 Kubernetes 上实施强大且可扩展的大数据架构的过程。
- en: Throughout the chapters, you will embark on a comprehensive journey, starting
    with the fundamentals of containers and Kubernetes architecture. You will learn
    how to build and deploy Docker images, understand the core components of Kubernetes,
    and gain hands-on experience in setting up local and cloud-based Kubernetes clusters.
    This solid foundation will prepare you for the subsequent chapters, where you
    will dive into the world of the modern data stack.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在各章内容中，你将开始一段全面的学习之旅，从容器和 Kubernetes 架构的基础知识入手。你将学习如何构建和部署 Docker 镜像，了解 Kubernetes
    的核心组件，并获得在本地和云端设置 Kubernetes 集群的实践经验。这一扎实的基础将为你后续深入现代数据堆栈的学习打下基础。
- en: The book will introduce you to the most widely adopted tools in the big data
    ecosystem, such as Apache Spark for data processing, Apache Airflow for pipeline
    orchestration, and Apache Kafka for real-time data ingestion. You will not only
    learn the theoretical concepts behind these technologies but also gain practical
    experience in implementing them on Kubernetes. Through a series of hands-on exercises
    and projects, you will develop a deep understanding of how to build and deploy
    data pipelines, process large datasets, and orchestrate complex workflows on a
    Kubernetes cluster.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将向你介绍大数据生态系统中最广泛采用的工具，如用于数据处理的 Apache Spark、用于管道编排的 Apache Airflow 以及用于实时数据摄取的
    Apache Kafka。你不仅将学习这些技术背后的理论概念，还将获得在 Kubernetes 上实施这些技术的实践经验。通过一系列动手实践和项目，你将深入理解如何构建和部署数据管道、处理大数据集，并在
    Kubernetes 集群上编排复杂的工作流。
- en: As the book progresses, you will explore advanced topics such as deploying a
    data consumption layer with tools such as Trino and Elasticsearch and integrating
    generative AI workloads using Amazon Bedrock. These topics will equip you with
    the knowledge and skills necessary to build and maintain a robust and scalable
    big data architecture on Kubernetes, ensuring efficient data processing, analysis,
    and analytics application deployment.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着书籍的进展，你将探索更高级的主题，例如使用 Trino 和 Elasticsearch 部署数据消费层，以及通过 Amazon Bedrock 集成生成性
    AI 工作负载。这些主题将为你提供在 Kubernetes 上构建和维护强大、可扩展的大数据架构所需的知识和技能，确保高效的数据处理、分析和分析应用程序的部署。
- en: By the end of this book, you will have gained a comprehensive understanding
    of the synergy between big data and Kubernetes, enabling you to leverage the power
    of these technologies to drive innovation and business growth. Whether you are
    a data engineer, a DevOps professional, or a technology enthusiast, this book
    will provide you with the practical knowledge and hands-on experience needed to
    successfully implement and manage big data workloads on Kubernetes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的结尾，你将全面了解大数据与 Kubernetes 之间的协同作用，使你能够利用这些技术的强大功能推动创新和业务增长。无论你是数据工程师、DevOps
    专业人士，还是技术爱好者，本书将为你提供在 Kubernetes 上成功实施和管理大数据工作负载所需的实用知识和动手经验。
- en: Who this book is for
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适用对象
- en: If you are a data engineer, a cloud architect, a DevOps professional, a data
    or science manager, or a technology enthusiast, this book is for you. You should
    have a basic background in Python and SQL programming, and basic knowledge of
    Apache Spark, Apache Kafka, and Apache Airflow. A basic understanding of Docker
    and Git will also be helpful.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是数据工程师、云架构师、DevOps 专业人士、数据或科学经理，或技术爱好者，那么本书适合你。你应该具备 Python 和 SQL 编程的基础知识，以及对
    Apache Spark、Apache Kafka 和 Apache Airflow 的基本了解。对 Docker 和 Git 的基本理解也将有所帮助。
- en: What this book covers
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容
- en: '[*Chapter 1*](B21927_01.xhtml#_idTextAnchor015), *Getting Started with Containers*,
    embarks on a journey to understand containers and Docker, the foundational technologies
    for modern application deployment. You’ll learn how to install Docker and run
    your first container image, experiencing the power of containerization firsthand.
    Additionally, you’ll dive into the intricacies of Dockerfiles, mastering the art
    of crafting concise and functional container images. Through practical examples,
    including the construction of a simple API and a data processing job with Python,
    you’ll grasp the nuances of containerizing services and jobs. By the end of this
    chapter, you’ll have the opportunity to solidify your newfound knowledge by building
    your own job and API, laying the groundwork for a portfolio of practical container-based
    applications.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 1 章*](B21927_01.xhtml#_idTextAnchor015)，*容器入门*，带你开始了解容器和 Docker，这是现代应用程序部署的基础技术。你将学习如何安装
    Docker 并运行你的第一个容器镜像，亲身体验容器化的强大功能。此外，你还将深入研究 Dockerfile 的细节，掌握制作简洁且功能完善的容器镜像的技巧。通过实践示例，包括使用
    Python 构建简单的 API 和数据处理任务，你将掌握将服务和任务容器化的技巧。到本章结束时，你将有机会通过构建自己的任务和 API 来巩固新获得的知识，为一系列基于容器的实际应用程序构建作品集奠定基础。'
- en: '[*Chapter 2*](B21927_02.xhtml#_idTextAnchor031), *Kubernetes Architecture*,
    introduces you to the core components that make up the Kubernetes architecture.
    You will learn about the control plane components such as the API server, etcd,
    scheduler, and controller manager, as well as the worker node components such
    as kubelet, kube-proxy, and container runtime. The chapter will explain the roles
    and responsibilities of each component, and how they interact with each other
    to ensure the smooth operation of a Kubernetes cluster. Additionally, you will
    gain an understanding of the key concepts in Kubernetes, including pods, deployments,
    services, jobs, stateful sets, persistent volumes, ConfigMaps, and secrets. By
    the end of this chapter, you will have a solid foundation in the architecture
    and core concepts of Kubernetes, preparing you for hands-on experience in the
    subsequent chapters.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 2 章*](B21927_02.xhtml#_idTextAnchor031)，*Kubernetes 架构*，将向你介绍构成 Kubernetes
    架构的核心组件。你将了解控制平面组件，如 API 服务器、etcd、调度器和控制器管理器，以及工作节点组件，如 kubelet、kube-proxy 和容器运行时。本章将解释每个组件的角色和职责，以及它们如何相互作用以确保
    Kubernetes 集群的平稳运行。此外，你将深入了解 Kubernetes 中的关键概念，包括 pod、部署、服务、任务、有状态集合、持久化卷、ConfigMaps
    和 Secrets。本章结束时，你将对 Kubernetes 的架构和核心概念有坚实的基础，为随后的实践操作做好准备。'
- en: '[*Chapter 3*](B21927_03.xhtml#_idTextAnchor053), *Kubernetes – Hands On*, guides
    you through the process of deploying a local Kubernetes cluster using kind, and
    a cloud-based cluster on AWS using Amazon EKS. You will learn the minimal AWS
    account configuration required to successfully deploy an EKS cluster. After setting
    up the clusters, you will have the opportunity to choose between deploying your
    applications on the local or cloud environment. Regardless of your choice, you
    will retake the API and data processing jobs developed in [*Chapter 1*](B21927_01.xhtml#_idTextAnchor015)
    and deploy them to Kubernetes. This hands-on experience will solidify your understanding
    of Kubernetes concepts and prepare you for more advanced topics in the following
    chapters.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 3 章*](B21927_03.xhtml#_idTextAnchor053)，*Kubernetes – 实践操作*，将指导你通过使用 kind
    部署本地 Kubernetes 集群的过程，并使用 Amazon EKS 部署基于云的集群。你将学习成功部署 EKS 集群所需的最小 AWS 账户配置。设置集群后，你将有机会选择将应用程序部署到本地环境还是云环境。无论你做出什么选择，你都将重新进行[*第
    1 章*](B21927_01.xhtml#_idTextAnchor015)中开发的 API 和数据处理工作，并将其部署到 Kubernetes。这个实践经验将巩固你对
    Kubernetes 概念的理解，并为后续章节的更高级话题做好准备。'
- en: '[*Chapter 4*](B21927_04.xhtml#_idTextAnchor070), *The Modern Data Stack*, introduces
    you to the most well-known data architecture designs, with a focus on the “lambda”
    architecture. You will learn about the tools that make up the modern data stack,
    which is a set of technologies used to implement a data lake(house) architecture.
    Among these tools are Apache Spark for data processing, Apache Airflow for data
    pipeline orchestration, and Apache Kafka for real-time event streaming and data
    ingestion. This chapter will provide a conceptual introduction to these tools
    and how they work together to build the core technology assets of a data lake(house)
    architecture.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第4章*](B21927_04.xhtml#_idTextAnchor070)，*现代数据架构*，向你介绍了最著名的数据架构设计，重点讲解了“lambda”架构。你将了解构成现代数据架构的数据湖（房）架构所需的工具。这些工具包括用于数据处理的Apache
    Spark、用于数据管道编排的Apache Airflow，以及用于实时事件流处理和数据摄取的Apache Kafka。本章将为你提供这些工具的概念性介绍，并展示它们如何协同工作，构建数据湖（房）架构的核心技术资产。'
- en: '[*Chapter 5*](B21927_05.xhtml#_idTextAnchor092), *Big Data Processing with
    Apache Spark*, introduces you to Apache Spark, one of the most popular tools for
    big data processing. You will understand the core components of a Spark program,
    how it scales and handles distributed processing, and best practices for working
    with Spark. You will implement simple data processing tasks using both the DataFrames
    API and the Spark SQL API, leveraging Python to interact with Spark. The chapter
    will guide you through installing Spark locally for testing purposes, enabling
    you to gain hands-on experience with this powerful tool before deploying it on
    a larger scale.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第5章*](B21927_05.xhtml#_idTextAnchor092)，*使用Apache Spark进行大数据处理*，向你介绍了Apache
    Spark，这是一种广泛使用的大数据处理工具。你将了解Spark程序的核心组件，如何扩展和处理分布式计算，以及与Spark协作的最佳实践。你将通过DataFrames
    API和Spark SQL API实现简单的数据处理任务，并使用Python与Spark进行交互。本章将指导你在本地安装Spark以进行测试，帮助你在将其部署到更大规模之前，先获得与这个强大工具的实际操作经验。'
- en: '[*Chapter 6*](B21927_06.xhtml#_idTextAnchor112), *Apache Airflow for Building
    Pipelines*, introduces you to Apache Airflow, a widely adopted open source tool
    for data pipeline orchestration. You will learn how to install Airflow using Docker
    and Astro CLI, making the setup process straightforward. The chapter will familiarize
    you with Airflow’s core features and the most commonly used operators for data
    engineering tasks. Additionally, you will gain insights into best practices for
    building resilient and efficient data pipelines that leverage Airflow’s capabilities
    to the fullest. By the end of this chapter, you will have a solid understanding
    of how to orchestrate complex data workflows using Airflow, a crucial skill for
    any data engineer or data architect working with big data on Kubernetes.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第6章*](B21927_06.xhtml#_idTextAnchor112)，*使用Apache Airflow构建数据管道*，向你介绍了Apache
    Airflow，这是一种被广泛采用的开源工具，用于数据管道的编排。你将了解如何使用Docker和Astro CLI安装Airflow，使得安装过程变得简单。本章将让你熟悉Airflow的核心功能和最常用的操作符，以便执行数据工程任务。此外，你将深入了解如何构建具有韧性和高效性的可靠数据管道，充分利用Airflow的能力。通过本章的学习，你将牢固掌握如何使用Airflow编排复杂的数据工作流，这是任何从事大数据工作并在Kubernetes上运行的数工程师或数据架构师所必备的技能。'
- en: '[*Chapter 7*](B21927_07.xhtml#_idTextAnchor122), *Apache Kafka for Real-Time
    Events and Data Ingestion*, introduces you to Apache Kafka, a distributed event
    streaming platform that is widely used for building real-time data pipelines and
    streaming applications. You will understand Kafka’s architecture and how it scales
    while being resilient, enabling it to handle high volumes of real-time data with
    low latency. You will learn about Kafka’s distributed topics design, which underpins
    its robust performance for real-time events. The chapter will guide you through
    running Kafka locally with Docker and implementing basic reading and writing operations
    on topics. Additionally, you will explore different strategies for data replication
    and topic distribution, ensuring you can design and implement efficient and reliable
    Kafka clusters.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第7章*](B21927_07.xhtml#_idTextAnchor122)，*使用Apache Kafka进行实时事件流处理和数据摄取*，向你介绍了Apache
    Kafka，这是一种分布式事件流平台，广泛应用于构建实时数据管道和流式应用程序。你将理解Kafka的架构，以及它如何扩展并保持韧性，使其能够处理高容量的实时数据并保持低延迟。你将学习Kafka的分布式主题设计，这一设计支撑了其强大的实时事件处理能力。本章将引导你通过Docker在本地运行Kafka，并实施基本的读取和写入操作。此外，你还将探讨不同的数据复制和主题分发策略，确保你能够设计并实现高效可靠的Kafka集群。'
- en: '[*Chapter 8*](B21927_08.xhtml#_idTextAnchor134), *Deploying the Big Data Stack
    on Kubernetes*, guides you through the process of deploying the big data tools
    you learned about in the previous chapters on a Kubernetes cluster. You will start
    by building bash scripts to deploy the Spark operator and run `SparkApplications`
    on Kubernetes. Next, you will deploy Apache Airflow to Kubernetes, enabling you
    to orchestrate data pipelines within the cluster. Additionally, you will deploy
    Apache Kafka on Kubernetes using both the ephemeral cluster and JBOD techniques.
    The Kafka Connect cluster will also be deployed, along with connectors to migrate
    data from SQL databases to persistent object storage. By the end of this chapter,
    you will have a fully functional big data stack running on Kubernetes, ready for
    further exploration and development.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第8章*](B21927_08.xhtml#_idTextAnchor134)，*在Kubernetes上部署大数据技术栈*，引导你了解如何将前几章学到的大数据工具部署到Kubernetes集群中。你将从编写bash脚本开始，部署Spark操作符并在Kubernetes上运行`SparkApplications`。接下来，你将部署Apache
    Airflow到Kubernetes，从而在集群内协调数据管道。此外，你还将使用临时集群和JBOD技术将Apache Kafka部署到Kubernetes。Kafka
    Connect集群也会被部署，并通过连接器将数据从SQL数据库迁移到持久化对象存储。到本章结束时，你将拥有一个完全可运行的大数据技术栈，准备好进行进一步的探索和开发。'
- en: '[*Chapter 9*](B21927_09.xhtml#_idTextAnchor141), *Data Consumption Layer*,
    guides you through the process of securely making data available for business
    analysts in a big data architecture deployed on Kubernetes. You will start by
    gaining an overview of working on a modern approach using a “data lake engine”
    instead of a data warehouse. In this chapter, you will become familiar with Trino
    for data consumption directly from a data lake through Kubernetes. You will understand
    how a data lake engine works, deploy it into Kubernetes, and monitor query execution
    and history. Additionally, for real-time data, you will get familiar with Elasticsearch
    and Kibana for data consumption. You will deploy these tools, and learn how to
    index data in them and how to build a simple data visualization with Kibana.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第9章*](B21927_09.xhtml#_idTextAnchor141)，*数据消费层*，引导你了解如何在Kubernetes上部署的大数据架构中，安全地将数据提供给业务分析师。你将从了解使用“数据湖引擎”而非数据仓库的现代方法开始。在本章中，你将熟悉Trino，了解如何直接通过Kubernetes从数据湖中进行数据消费。你将理解数据湖引擎的工作原理，将其部署到Kubernetes中，并监控查询执行和历史记录。此外，对于实时数据，你将熟悉Elasticsearch和Kibana进行数据消费。你将部署这些工具，并学习如何在其中索引数据以及如何使用Kibana构建简单的数据可视化。'
- en: '[*Chapter 10*](B21927_10.xhtml#_idTextAnchor154), *Building a Big Data Pipeline
    in Kubernetes*, guides you through the process of deploying and orchestrating
    two complete data pipelines, one for batch processing and another for real-time
    processing, on a Kubernetes cluster. You will connect all the tools you’ve learned
    about throughout the book, such as Apache Spark, Apache Airflow, Apache Kafka,
    and Trino, to build a single, complex solution. You will deploy these tools on
    Kubernetes, write code for data processing and orchestration, and make the data
    available for querying through a SQL engine. By the end of this chapter, you will
    have hands-on experience in building and managing a comprehensive big data pipeline
    on Kubernetes, integrating various components and technologies into a cohesive
    and scalable architecture.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第10章*](B21927_10.xhtml#_idTextAnchor154)，*在Kubernetes中构建大数据管道*，引导你了解如何在Kubernetes集群中部署和协调两个完整的数据管道，一个用于批处理，另一个用于实时处理。你将连接本书中学到的所有工具，如Apache
    Spark、Apache Airflow、Apache Kafka和Trino，构建一个单一的复杂解决方案。你将在Kubernetes上部署这些工具，编写数据处理和协调的代码，并通过SQL引擎使数据可供查询。到本章结束时，你将拥有在Kubernetes上构建和管理一个综合大数据管道的实际经验，将各种组件和技术整合成一个统一且可扩展的架构。'
- en: '[*Chapter 11*](B21927_11.xhtml#_idTextAnchor167), *Generative AI on Kubernetes*,
    guides you through the process of deploying a generative AI application on Kubernetes
    using Amazon Bedrock as a service suite for foundational models. You will learn
    how to connect your application to a knowledge base serving as a **Retrieval-Augmented
    Generation** (**RAG**) layer, which enhances the AI model’s capabilities by providing
    access to external information sources. Additionally, you will discover how to
    automate task execution by the AI models with agents, enabling seamless integration
    of generative AI into your workflows. By the end of this chapter, you will have
    a solid understanding of how to leverage the power of generative AI on Kubernetes,
    unlocking new possibilities for personalized customer experiences, intelligent
    assistants, and automated business analytics.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第11章*](B21927_11.xhtml#_idTextAnchor167)，*Kubernetes上的生成式AI*，引导你完成在Kubernetes上部署生成式AI应用的过程，使用Amazon
    Bedrock作为基础模型的服务套件。你将学习如何将应用连接到作为**检索增强生成**（**RAG**）层的知识库，这通过提供对外部信息源的访问，增强了AI模型的能力。此外，你将发现如何通过代理自动化任务执行，使生成式AI无缝集成到你的工作流中。在本章结束时，你将对如何在Kubernetes上利用生成式AI的力量有一个坚实的理解，从而为个性化客户体验、智能助手和自动化商业分析开启新可能。'
- en: '[*Chapter 12*](B21927_12.xhtml#_idTextAnchor183), *Where to Go from Here*,
    guides you through the next steps in your journey toward mastering big data and
    Kubernetes. You will explore crucial concepts and technologies that are essential
    for building robust and scalable solutions on Kubernetes. This includes monitoring
    strategies for both Kubernetes and your applications, implementing a service mesh
    for efficient communication, securing your cluster and applications, enabling
    automated scalability, embracing GitOps and CI/CD practices for streamlined deployment
    and management, and Kubernetes cost control. For each topic, you’ll receive an
    overview and recommendations on the technologies to explore further, empowering
    you to deepen your knowledge and skills in these areas.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第12章*](B21927_12.xhtml#_idTextAnchor183)，*从这里开始的旅程*，引导你迈向掌握大数据和Kubernetes的下一步。你将探索一些关键概念和技术，这些技术对在Kubernetes上构建强大且可扩展的解决方案至关重要。包括Kubernetes和应用程序的监控策略、实现高效通信的服务网格、保护你的集群和应用程序、启用自动扩展、采用GitOps和CI/CD实践以简化部署和管理，以及Kubernetes成本控制。每个主题，你都将获得概述和对进一步探索相关技术的建议，帮助你加深这些领域的知识和技能。'
- en: To get the most out of this book
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了最大程度地利用本书
- en: Some basics in Python programming knowledge and experience with Spark, Docker,
    Airflow, Kafka, and Git will help you get the most out of this book.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Python编程基础知识以及对Spark、Docker、Airflow、Kafka和Git的了解将帮助你更好地利用本书的内容。
- en: '| **Software/hardware covered in** **the book** | **Operating** **system requirements**
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **本书涵盖的软件/硬件** | **操作系统要求** |'
- en: '| --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Python>=3.9 | Windows, macOS, or Linux |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Python>=3.9 | Windows、macOS或Linux |'
- en: '| Docker, the latest version available | Linux |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Docker，最新版本 | Linux |'
- en: '| Docker Desktop, the latest version available | Windows or macOS |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Docker Desktop，最新版本 | Windows或macOS |'
- en: '| Kubectl | Windows, macOS, or Linux |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| Kubectl | Windows、macOS或Linux |'
- en: '| Awscli | Windows, macOS, or Linux |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Awscli | Windows、macOS或Linux |'
- en: '| Eksctl | Windows, macOS, or Linux |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Eksctl | Windows、macOS或Linux |'
- en: '| DBeaver Community Edition | Windows, macOS, or Linux |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| DBeaver社区版 | Windows、macOS或Linux |'
- en: All guidance needed for software installation will be provided in each chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每一章都会提供软件安装所需的所有指导。
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book’s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting** **of code.**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你正在使用本书的电子版，我们建议你亲自输入代码，或从本书的GitHub仓库访问代码（链接在下一节提供）。这样可以帮助你避免因复制和粘贴代码而产生的潜在错误**
    **。**'
- en: Download the example code files
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes).
    If there’s an update to the code, it will be updated in the GitHub repository.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub下载本书的示例代码文件，地址为 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)。如果代码有更新，它将在GitHub仓库中更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供来自我们丰富书籍和视频目录的其他代码包，可以在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)找到。快去看看吧！
- en: Conventions used
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了许多文本约定。
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “This command will pull the `hello-world` image from
    the Docker Hub public repository and run the application in it. “'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`文本中的代码`：表示文本中的代码词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟网址、用户输入和 Twitter 账户。示例：“此命令将从
    Docker Hub 公共仓库拉取`hello-world`镜像，并在其中运行应用程序。”'
- en: 'A block of code is set as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出如下所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is how the filename above the code snippet will look:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以上代码片段上方的文件名将显示如下：
- en: '**Cjava.py**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**Cjava.py**'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    “You should ensure that the **Use WSL 2 instead of Hyper-V** option is selected
    on the **Configuration** page.”'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要词汇或屏幕上显示的文字。例如，菜单或对话框中的文字通常以**粗体**显示。示例：“您应该确保在**配置**页面上选中**使用WSL
    2而非Hyper-V**选项。”'
- en: Tips or important notes
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士或重要提示
- en: Appear like this.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 显示为如下格式。
- en: Get in touch
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '**General feedback**: If you have questions about any aspect of this book,
    email us at [customercare@packtpub.com](mailto:customercare@packtpub.com) and
    mention the book title in the subject of your message.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般反馈**：如果您对本书的任何部分有疑问，请通过电子邮件联系我们：[customercare@packtpub.com](mailto:customercare@packtpub.com)，并在邮件主题中注明书名。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误表**：尽管我们已经尽力确保内容的准确性，但难免会有错误。如果您在本书中发现错误，我们将非常感激您能向我们报告。请访问[www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)并填写表格。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packt.com](mailto:copyright@packt.com)
    with a link to the material.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果您在互联网上遇到我们作品的任何非法版本，恳请提供该内容的地址或网站名称。请通过电子邮件联系我们：[copyright@packt.com](mailto:copyright@packt.com)，并附上相关链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有兴趣成为作者**：如果您在某个主题上拥有专长，并且有兴趣编写或参与书籍的撰写，请访问[authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share Your Thoughts
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Once you’ve read *Big Data on Kubernetes*, we’d love to hear your thoughts!
    Please [click here to go straight to the Amazon review page](https://packt.link/r/1-835-46214-6)
    for this book and share your feedback.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完*《Kubernetes上的大数据》*后，我们很乐意听取您的反馈！请[点击这里直接访问本书的亚马逊评论页面](https://packt.link/r/1-835-46214-6)并分享您的意见。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您的评论对我们和技术社区非常重要，帮助我们确保提供优质内容。
- en: Download a free PDF copy of this book
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载本书的免费PDF副本
- en: Thanks for purchasing this book!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢购买本书！
- en: Do you like to read on the go but are unable to carry your print books everywhere?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢随时阅读但无法随身携带纸质书籍？
- en: Is your eBook purchase not compatible with the device of your choice?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您的电子书购买是否与您选择的设备不兼容？
- en: Don’t worry, now with every Packt book you get a DRM-free PDF version of that
    book at no cost.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心，现在每本 Packt 书籍都附赠免费的无DRM版本PDF。
- en: Read anywhere, any place, on any device. Search, copy, and paste code from your
    favorite technical books directly into your application.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随时随地在任何设备上阅读。从您最喜欢的技术书籍中直接搜索、复制并粘贴代码到您的应用程序中。
- en: The perks don’t stop there, you can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 优惠不止于此，您还可以获取独家折扣、通讯和每日精彩免费内容的订阅
- en: 'Follow these simple steps to get the benefits:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以下简单步骤获取这些好处：
- en: Scan the QR code or visit the link below
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描下方的 QR 码或访问以下链接
- en: '![Download a free PDF copy of this book'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![下载本书的免费 PDF 副本'
- en: '](img/B21927_QR_Free_PDF.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B21927_QR_Free_PDF.jpg)'
- en: '[https://packt.link/free-ebook/978-1-83546-214-0](https://packt.link/free-ebook/978-1-83546-214-0)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/free-ebook/978-1-83546-214-0](https://packt.link/free-ebook/978-1-83546-214-0)'
- en: Submit your proof of purchase
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交您的购买证明
- en: That’s it! We’ll send your free PDF and other benefits to your email directly
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就这些了！我们将免费 PDF 和其他福利直接发送到您的电子邮件
- en: Part 1:Docker and Kubernetes
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 部分：Docker 和 Kubernetes
- en: In this part, you will learn about the fundamentals of containerization and
    Kubernetes. You will start by understanding the basics of containers and how to
    build and run Docker images. This will provide you with a solid foundation for
    working with containerized applications. Next, you will dive into the Kubernetes
    architecture, exploring its components, features, and core concepts such as pods,
    deployments, and services. With this knowledge, you will be well equipped to navigate
    the Kubernetes ecosystem. Finally, you will get hands-on experience by deploying
    local and cloud-based Kubernetes clusters and then deploying applications you
    built earlier onto these clusters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分中，您将学习容器化和 Kubernetes 的基础知识。您将首先了解容器的基础知识以及如何构建和运行 Docker 镜像。这将为您在处理容器化应用程序时提供坚实的基础。接下来，您将深入探讨
    Kubernetes 架构，探索其组件、特性以及如 pod、部署和服务等核心概念。有了这些知识，您将能够熟练地操作 Kubernetes 生态系统。最后，您将通过部署本地和云端
    Kubernetes 集群，并将之前构建的应用程序部署到这些集群上，获得实际操作经验。
- en: 'This part contains the following chapters:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '此部分包含以下章节:'
- en: '[*Chapter 1*](B21927_01.xhtml#_idTextAnchor015), Getting Started with Containers'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 1 章*](B21927_01.xhtml#_idTextAnchor015)，开始使用容器'
- en: '[*Chapter 2*](B21927_02.xhtml#_idTextAnchor031), Kubernetes Architecture'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 2 章*](B21927_02.xhtml#_idTextAnchor031)，Kubernetes 架构'
- en: '[*Chapter 3*](B21927_03.xhtml#_idTextAnchor053), Kubernetes – Hands On'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 3 章*](B21927_03.xhtml#_idTextAnchor053)，Kubernetes – Hands On'
