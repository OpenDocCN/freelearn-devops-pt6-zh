- en: Kubernetes on GCP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCP 上的 Kubernetes
- en: '**Google Cloud Platform** (**GCP**) is becoming popular in the public cloud
    industry that''s provided by Google. GCP has concepts that are similar to those
    provided by AWS, such as VPC, Compute Engine, Persistent Disk, Load Balancing,
    and several managed services. In this chapter, you''ll learn about GCP and how
    to set up Kubernetes on GCP through the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Cloud Platform**（**GCP**）是由 Google 提供的公共云服务，越来越受欢迎。GCP 具有与 AWS 类似的概念，如
    VPC、计算引擎、持久磁盘、负载均衡和多个托管服务。在本章中，你将了解 GCP，并通过以下主题学习如何在 GCP 上设置 Kubernetes：'
- en: Understanding GCP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 GCP
- en: Using and understanding GCP components
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用和理解 GCP 组件
- en: Using **Google Kubernetes Engine** (**GKE**), the hosted Kubernetes service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Google Kubernetes Engine**（**GKE**），托管的 Kubernetes 服务
- en: Introduction to GCP
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCP 介绍
- en: GCP was officially launched in 2011\. Unlike AWS, GCP initially provided **PaaS**
    (**Platform as a Service**). Consequently, you can deploy your application directly
    instead of launching a VM. After that, GCP added some services and functionalities.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 于 2011 年正式推出。与 AWS 不同，GCP 最初提供的是**PaaS**（**平台即服务**）。因此，你可以直接部署应用程序，而无需启动虚拟机。之后，GCP
    添加了更多服务和功能。
- en: The most important service for Kubernetes users is GKE, which is a hosted Kubernetes
    service. So, you can get some relief from Kubernetes installation, upgrades, and
    management. This has a pay–as–you–go style approach to using the Kubernetes cluster.
    GKE is also a very active service that keeps providing new versions of Kubernetes
    in a timely manner and keeps coming up with new features and management tools
    for Kubernetes as well.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Kubernetes 用户来说，最重要的服务是 GKE，它是一个托管的 Kubernetes 服务。因此，你可以免去 Kubernetes 安装、升级和管理的麻烦。它采用按需付费的方式使用
    Kubernetes 集群。GKE 也是一个非常活跃的服务，不断提供 Kubernetes 的新版本，并不断推出新的功能和管理工具。
- en: Let's take a look at what kind of foundation and services GCP provides and then
    we'll explore GKE.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 GCP 提供了哪些基础设施和服务，然后再深入探讨 GKE。
- en: GCP components
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCP 组件
- en: GCP provides a web console and **Command-Line Interface** (**CLI**). Both make
    it easy and straightforward to control the GCP infrastructure, but Google accounts
    (such as Gmail) are required for use. Once you have a Google account, go to the
    GCP sign-up page ([https://cloud.google.com/free/](https://cloud.google.com/free/))
    to set up your GCP account.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 提供了 Web 控制台和**命令行界面**（**CLI**）。这两者都使得控制 GCP 基础设施变得简单直观，但使用时需要 Google 帐号（如
    Gmail）。一旦你拥有 Google 帐号，可以前往 GCP 注册页面（[https://cloud.google.com/free/](https://cloud.google.com/free/)）来设置你的
    GCP 帐号。
- en: 'If you want to control the GCP component via CLI, you need to install the Cloud
    SDK ([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)),
    which is similar to the AWS CLI that you can use to list, create, update, and
    delete GCP resources. After installing the Cloud SDK, you need to configure it
    with the following commands to associate it to a GCP account:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想通过 CLI 控制 GCP 组件，你需要安装 Cloud SDK（[https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)），它类似于
    AWS CLI，你可以用它列出、创建、更新和删除 GCP 资源。安装 Cloud SDK 后，你需要使用以下命令进行配置，将其与 GCP 帐号关联：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: VPC
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VPC
- en: VPC in GCP is quite a different policy compared with AWS. First of all, you
    don't need to set the CIDR prefix to VPC. In other words, you can't set CIDR to
    VPC. Instead, you just add one or some subnets to the VPC. Because you have to
    set certain CIDR blocks with a subnet, GCP VPC is therefore identified as a logical
    group of subnets, and subnets within VPC can communicate with each other.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 中的 VPC 与 AWS 的策略有很大不同。首先，你不需要为 VPC 设置 CIDR 前缀。换句话说，你不能为 VPC 设置 CIDR。相反，你只需将一个或多个子网添加到
    VPC 中。由于你必须为子网设置特定的 CIDR 块，因此 GCP 的 VPC 被视为子网的逻辑组合，VPC 内的子网可以相互通信。
- en: 'Note that GCP VPC has two subnet modes, either `auto` or `custom`. If you choose
    `auto`, it will create some subnets on each region with predefined CIDR blocks.
    For example, type the following command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，GCP 的 VPC 有两种子网模式，分别是 `auto` 或 `custom`。如果选择 `auto`，它将在每个区域创建一些具有预定义 CIDR
    块的子网。例如，输入以下命令：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will create 18 subnets as shown in the following screenshot (because,
    as of December 2018, GCP has 18 regions):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建 18 个子网，如下图所示（因为截至 2018 年 12 月，GCP 有 18 个区域）：
- en: '![](img/97317dd4-0e8e-4acf-9ad8-f10b520f604b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97317dd4-0e8e-4acf-9ad8-f10b520f604b.png)'
- en: Auto mode VPC is probably good to start with. However, in auto mode, you can't
    specify the CIDR prefix and 18 subnets from all regions might not fit your use
    case. For example, connect to your on–premise data center via VPN. Another example
    is creating subnets on a specific region only.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自动模式 VPC 可能是一个不错的起点。然而，在自动模式下，你无法指定 CIDR 前缀，而且来自所有区域的 18 个子网可能不适合你的使用场景。例如，连接到本地数据中心通过
    VPN。另一个例子是只在特定区域创建子网。
- en: 'In this case, choose custom mode VPC, then you can create subnets with the
    desired CIDR prefix manually. Type the following command to create a custom mode
    VPC:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，选择自定义模式 VPC，然后你可以手动创建具有所需 CIDR 前缀的子网。输入以下命令创建一个自定义模式 VPC：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Because custom mode VPC won''t create any subnets, as shown in the following
    screenshot, let''s add subnets onto this custom mode VPC:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因为自定义模式 VPC 不会创建任何子网，如下图所示，接下来我们将子网添加到这个自定义模式 VPC：
- en: '![](img/48ae1c35-e025-405c-863c-1d5a2005af64.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48ae1c35-e025-405c-863c-1d5a2005af64.png)'
- en: Subnets
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子网
- en: In GCP, subnets are always found across multiple zones (availability zones)
    within a region. In other words, you can't create subnets on a single zone like
    AWS. You always need to specify entire regions when creating a subnet.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GCP 中，子网总是跨越一个区域内的多个可用区（availability zones）。换句话说，你不能像在 AWS 中那样仅在一个可用区内创建子网。创建子网时，你必须指定整个区域。
- en: In addition, unlike AWS, there are no significant concepts of public and private
    subnets (in AWS, a public subnet has a default route as IGW; on the other hand,
    a private subnet has a default route as the NAT gateway). This is because all
    subnets in GCP have a route to an internet gateway.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与 AWS 不同，GCP 中没有明显的公有和私有子网的概念（在 AWS 中，公有子网的默认路由是 IGW；而私有子网的默认路由是 NAT 网关）。这是因为
    GCP 中的所有子网都有通向互联网网关的路由。
- en: Instead of subnet-level access control, GCP uses host (instance)-level access
    control using **network tags** to ensure network security. This will be described
    in more detail in the following section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 不使用子网级别的访问控制，而是使用主机（实例）级别的访问控制，通过**网络标签**确保网络安全。接下来的部分会详细描述这一点。
- en: It might make network administrators nervous; however, GCP best practice gives
    you a much more simplified and scalable VPC administration because you can add
    subnets at any time to expand entire network blocks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能让网络管理员感到紧张；然而，GCP 的最佳实践提供了一个更加简化且可扩展的 VPC 管理方式，因为你可以随时添加子网来扩展整个网络块。
- en: Technically, you can launch a VM instance and set it up as a NAT gateway or
    HTTP proxy, and then create a custom priority route for the private subnet that
    points to the NAT/proxy instance to achieve an AWS–like private subnet.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你可以启动一个虚拟机实例并将其设置为 NAT 网关或 HTTP 代理，然后为私有子网创建一个自定义优先级路由，指向 NAT/代理实例，从而实现类似
    AWS 的私有子网。
- en: 'Please refer to the following online document for details: [https://cloud.google.com/compute/docs/vpc/special-configurations](https://cloud.google.com/compute/docs/vpc/special-configurations).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参考以下在线文档：[https://cloud.google.com/compute/docs/vpc/special-configurations](https://cloud.google.com/compute/docs/vpc/special-configurations)。
- en: 'One more thing: an interesting and unique concept of GCP VPC is that you can
    add different CIDR prefix network blocks to a single VPC. For example, if you
    have custom mode VPC, then add the following three subnets:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事：GCP VPC 的一个有趣且独特的概念是，你可以将不同的 CIDR 前缀网络块添加到同一个 VPC。例如，如果你有一个自定义模式 VPC，那么可以添加以下三个子网：
- en: '`subnet-a` (`10.0.1.0/24`) from `us-west1`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subnet-a` (`10.0.1.0/24`) 来自 `us-west1`'
- en: '`subnet-b` (`172.16.1.0/24`) from `us-east1`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subnet-b` (`172.16.1.0/24`) 来自 `us-east1`'
- en: '`subnet-c` (`192.168.1.0/24`) from `asia-northeast1`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subnet-c` (`192.168.1.0/24`) 来自 `asia-northeast1`'
- en: 'The following commands will create three subnets from three different regions
    with different CIDR prefixes:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将从三个不同的区域创建三个子网，每个子网具有不同的 CIDR 前缀：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result will be the following web console. If you''re familiar with AWS
    VPC, you won''t believe these combinations of CIDR prefixes are available within
    a single VPC! This means that, whenever you need to expand a network, you can
    assign another CIDR prefix to the VPC:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是如下所示的 web 控制台。如果你熟悉 AWS VPC，你可能不敢相信这些 CIDR 前缀组合可以在一个 VPC 内使用！这意味着，每当你需要扩展网络时，你可以将另一个
    CIDR 前缀分配给这个 VPC：
- en: '![](img/4f340bc1-8c47-4343-b178-3c6e925027de.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f340bc1-8c47-4343-b178-3c6e925027de.png)'
- en: Firewall rules
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防火墙规则
- en: As previously mentioned, the GCP firewall rule is important for achieving network
    security. However, the GCP firewall is more simple and flexible than an AWS **Security
    Group** (**SG**). For example, in AWS, when you launch an EC2 instance, you have
    to assign at least one SG that is tightly coupled with EC2 and SG. On the other
    hand, in GCP, you can't assign any firewall rules directly. Instead, firewall
    rule and VM instance are loosely coupled via a **network tag**. Consequently,
    there's no direct association between the firewall rule and VM instance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，GCP防火墙规则对实现网络安全至关重要。然而，GCP防火墙比AWS **安全组**（**SG**）更简单、更灵活。例如，在AWS中，启动EC2实例时，您必须分配至少一个与EC2和SG紧密耦合的SG。而在GCP中，您不能直接分配任何防火墙规则。相反，防火墙规则和虚拟机实例是通过**网络标签**松散耦合的。因此，防火墙规则与虚拟机实例之间没有直接关联。
- en: 'The following diagram is a comparison between AWS security groups and GCP firewall
    rules. EC2 requires a security group, while the GCP VM instance just sets a tag.
    This is irrespective of whether the corresponding firewall has the same tag or
    not:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了AWS安全组与GCP防火墙规则的比较。EC2需要一个安全组，而GCP虚拟机实例只需设置一个标签，无论相应的防火墙是否具有相同的标签：
- en: '![](img/0a5b3849-c025-48c3-90df-8aebfbdb6ba5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a5b3849-c025-48c3-90df-8aebfbdb6ba5.png)'
- en: 'For example, create a firewall rule for a public host (use the `public` network
    tag) and a private host (use the `private` network tag), as given in the following
    command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为公共主机（使用`public`网络标签）和私有主机（使用`private`网络标签）创建防火墙规则，如以下命令所示：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This creates four firewall rules as shown in the following screenshot. Let''s
    create VM instances to use either the `public` or `private` network tag to see
    how it works:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建如以下截图所示的四个防火墙规则。让我们创建虚拟机实例，使用`public`或`private`网络标签，看看它是如何工作的：
- en: '![](img/2fe8b575-2875-4f41-aebf-8fe951dacc08.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe8b575-2875-4f41-aebf-8fe951dacc08.png)'
- en: VM instances
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟机实例
- en: In GCP, a VM instance is quite similar to AWS EC2\. You can choose from a variety
    of machine (instance) types that have different hardware configurations; you can
    also choose a Linux-or Windows-based OS or your customized OS.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP中，虚拟机实例与AWS EC2相似。您可以从多种机器（实例）类型中选择，这些类型具有不同的硬件配置；您还可以选择基于Linux或Windows的操作系统，或者您自定义的操作系统。
- en: 'As mentioned when talking about firewall rules, you can specify any number
    of network tags. A tag doesn''t necessarily need to be created beforehand. This
    means you can launch VM instances with network tags first, even though a firewall
    rule isn''t created. It''s still valid, but no firewall rule is applied in this
    case. Then you can create a firewall rule with a network tag. Eventually a firewall
    rule will be applied to the VM instances afterward. This is why VM instances and
    firewall rules are loosely coupled, which provides flexibility to the user:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如在讨论防火墙规则时提到的，您可以指定任意数量的网络标签。标签不一定需要事先创建。这意味着您可以先使用网络标签启动虚拟机实例，即使防火墙规则尚未创建。这样仍然有效，但此时不会应用防火墙规则。然后，您可以创建一个具有网络标签的防火墙规则。最终，防火墙规则将应用于虚拟机实例。这就是为什么虚拟机实例和防火墙规则是松散耦合的，提供了更大的灵活性：
- en: '![](img/fcdc88af-8f5c-4ec0-9fe9-3cda5752836e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcdc88af-8f5c-4ec0-9fe9-3cda5752836e.png)'
- en: 'Before launching a VM instance, you need to create an `ssh` public key first
    in the same way as AWS EC2\. The easiest way to do this is to run the following
    command to create and register a new key:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动虚拟机实例之前，您需要首先创建一个`ssh`公钥，方式与AWS EC2相同。最简单的方法是运行以下命令来创建并注册一个新的密钥：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now let's get started by launching a VM instance on GCP.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始在GCP上启动虚拟机实例。
- en: 'Deploy two instances on both `subnet-a` and `subnet-b` as public instances
    (use the `public` network tag) and then launch another instance on the `subnet-a`
    as a private instance (with a `private` network tag):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在`subnet-a`和`subnet-b`上分别部署两个实例作为公共实例（使用`public`网络标签），然后在`subnet-a`上启动另一个实例作为私有实例（使用`private`网络标签）：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/0cd9490b-d1af-4132-afbd-ba3d53d07580.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cd9490b-d1af-4132-afbd-ba3d53d07580.png)'
- en: 'You can log in to those machines to check whether a firewall rule works as
    expected. First of all, you need to add an `ssh` key to `ssh-agent` on your machine:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以登录到这些机器，检查防火墙规则是否按预期工作。首先，您需要将`ssh`密钥添加到您机器上的`ssh-agent`中：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, check whether an ICMP firewall rule can reject traffic from external
    because ICMP only allows public or private tagged hosts, so the ping packet from
    your machine won''t reach the public instance; this is shown in the following
    screenshot:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后检查是否有 ICMP 防火墙规则可以拒绝来自外部的流量，因为 ICMP 仅允许公共或私有标记主机，因此你机器上的 ping 数据包不会到达公共实例；如以下截图所示：
- en: '![](img/02c0efda-8a8c-4e20-b29d-8729532187ef.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02c0efda-8a8c-4e20-b29d-8729532187ef.png)'
- en: 'On the other hand, the public `host` allows `ssh` from your machine, because
    the `public-ssh` rule allows any (`0.0.0.0/0`):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，公共 `host` 允许从你的机器进行 `ssh` 连接，因为 `public-ssh` 规则允许任何来源（`0.0.0.0/0`）：
- en: '![](img/36ba01e0-a9da-4aa0-b7df-b178105b1d00.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36ba01e0-a9da-4aa0-b7df-b178105b1d00.png)'
- en: Of course, this host can ping and `ssh` to private hosts on `subnet-a` (`10.0.1.2`)
    through a private IP address, because of the `internal-icmp` and `private-ssh`
    rules.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这台主机可以通过私有 IP 地址 ping 和 `ssh` 连接到 `subnet-a` 上的私有主机（`10.0.1.2`），因为有 `internal-icmp`
    和 `private-ssh` 规则。
- en: 'Let''s `ssh` to a private host and then install `tomcat8` and the `tomcat8-examples`
    package (this will install the `/examples/` application for Tomcat):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们 `ssh` 连接到一台私有主机，然后安装 `tomcat8` 和 `tomcat8-examples` 包（这将为 Tomcat 安装 `/examples/`
    应用程序）：
- en: '![](img/8cc8f7df-7014-4a20-8ae5-3cb01c96a344.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cc8f7df-7014-4a20-8ae5-3cb01c96a344.png)'
- en: Remember that `subnet-a` is a `10.0.1.0/24` CIDR prefix, but `subnet-b` is a
    `172.16.1.0/24` CIDR prefix. However, within the same VPC, there's connectivity
    with each other. This is the great benefit of using GCP as you can expand a network
    address block whenever this is required.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`subnet-a` 是 `10.0.1.0/24` CIDR 前缀，而 `subnet-b` 是 `172.16.1.0/24` CIDR 前缀。然而，在同一个
    VPC 中，它们是互通的。这是使用 GCP 的一个巨大优势，你可以根据需要扩展网络地址块。
- en: 'Now install `nginx` to public hosts (`public-on-subnet-a` and `public-on-subnet-b`):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在公共主机（`public-on-subnet-a` 和 `public-on-subnet-b`）上安装 `nginx`：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'However, at this moment, you can''t access Tomcat on a private host even if
    it has a public IP address. This is because a private host doesn''t yet have any
    firewall rule that allows `8080/tcp`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前即使私有主机拥有公共 IP 地址，你也无法访问 Tomcat。这是因为私有主机尚未设置允许 `8080/tcp` 的防火墙规则：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Rather than simply creating a firewall rule for Tomcat, we'll set up a LoadBalancer
    to be configured for both nginx and Tomcat in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与其仅仅为 Tomcat 创建防火墙规则，我们将在下一节中设置一个负载均衡器，配置 nginx 和 Tomcat。
- en: Load balancing
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'GCP provides several types of load balancer as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 提供了几种类型的负载均衡器，如下所示：
- en: Layer 4 TCP LoadBalancer
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层 4 TCP 负载均衡器
- en: Layer 4 UDP LoadBalancer
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层 4 UDP 负载均衡器
- en: Layer 7 HTTP(S) LoadBalancer
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层 7 HTTP(S) 负载均衡器
- en: The Layer 4 LoadBalancers (both TCP and UDP) are similar to AWS Classic ELB.
    On the other hand, the Layer 7 HTTP(S) LoadBalancer has content (context)-based
    routing. For example, the URL/image will forward to `instance-a`; everything else
    will forward to `instance-b`. So, it's more like an application-level LoadBalancer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 层 4 负载均衡器（TCP 和 UDP）类似于 AWS Classic ELB。另一方面，层 7 HTTP(S) 负载均衡器具有基于内容（上下文）的路由。例如，URL/图像将转发到
    `instance-a`；其他所有内容将转发到 `instance-b`。因此，它更像是一个应用层负载均衡器。
- en: AWS also provides **Application Load Balancer** (**ALB** or **ELBv2**), which
    is quite similar to the GCP Layer 7 HTTP(S) LoadBalancer. For details, please
    visit [https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 还提供 **应用负载均衡器**（**ALB** 或 **ELBv2**），它与 GCP 的层 7 HTTP(S) 负载均衡器非常相似。有关详细信息，请访问
    [https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/)。
- en: 'In order to set up LoadBalancer, and unlike AWS ELB, there are several steps
    you''ll need to follow in order to configure some items beforehand:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置负载均衡器，与 AWS ELB 不同，你需要遵循几个步骤来预先配置一些项目：
- en: '| **Configuration item** | **Purpose** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **配置项** | **用途** |'
- en: '| Instance group | Determine a group of VM instances or a VM template (OS image).
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 实例组 | 确定一组虚拟机实例或虚拟机模板（操作系统镜像）。 |'
- en: '| Health check | Set health threshold (interval, timeout, and so on) to determine
    instance group health status. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 健康检查 | 设置健康阈值（间隔、超时等）以确定实例组的健康状态。 |'
- en: '| Backend service | Set load threshold (maximum CPU or requests per second)
    and session affinity (sticky session) to the instance group and associate it to
    health check. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 后端服务 | 设置负载阈值（最大 CPU 或每秒请求数）和会话亲和性（粘性会话），并将其与健康检查关联。 |'
- en: '| `url-maps` (LoadBalancer) | This is an actual place-holder to represent an
    L7 LoadBalancer that associates backend services and targets the HTTP(S) proxy.
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `url-maps`（负载均衡器）| 这是一个实际的占位符，代表一个L7负载均衡器，它将后台服务与HTTP(S)代理相关联。 |'
- en: '| Target HTTP(S) proxy | This is a connector that makes relationships between
    frontend forwarding rules to LoadBalancer. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 目标HTTP(S)代理 | 这是一个连接器，它使前端转发规则与负载均衡器之间建立关系。 |'
- en: '| Frontend forwarding rule | Associate between the Target HTTP proxy and IP
    address (ephemeral or static) and port number. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 前端转发规则 | 将目标HTTP代理与IP地址（临时或静态）和端口号关联。 |'
- en: '| External IP (static) | (Optional) allocate a static external IP address for
    LoadBalancer. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 外部IP（静态） | （可选）为负载均衡器分配一个静态外部IP地址。 |'
- en: 'The following diagram is for all of the preceding components'' association
    that constructs the L7 LoadBalancer:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了构建L7负载均衡器时，所有前面提到的组件的关联：
- en: '![](img/68866627-0b45-493e-94c7-85512e92fdb2.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68866627-0b45-493e-94c7-85512e92fdb2.png)'
- en: 'Let''s set up an instance group first. In this example, there are three instance
    groups to create: one for the private hosted Tomcat instance (`8080/tcp`) and
    another two instance groups for public HTTP instances for each zone.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先设置一个实例组。在本示例中，需要创建三个实例组：一个用于私有托管的Tomcat实例（`8080/tcp`），另外两个实例组用于每个区域的公共HTTP实例。
- en: 'To do that, execute the following commands to group three of them:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，执行以下命令，将它们分组在一起：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Health check
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: 'Let''s set the standard settings by executing the following commands:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行以下命令来设置标准配置：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Backend service
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后台服务
- en: 'First of all, we need to create a `backend` service that specifies `health
    check`. Then, we must add each instance group with a threshold: CPU utilization
    up to 80% and the maximum capacity set to 100% for both HTTP and Tomcat:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个指定了`健康检查`的`后台`服务。然后，我们必须为每个实例组添加阈值：CPU使用率最高为80%，HTTP和Tomcat的最大容量都设置为100%：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Creating a LoadBalancer
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建负载均衡器
- en: 'The LoadBalancer needs to bind both `my-http-backend-service` and `my-tomcat-backend-service`.
    In this scenario, only `/examples` and `/examples/*` will be traffic forwarded to
    `my-tomcat-backend-service`. Other than that, every URI forwards traffic to `my-http-backend-service`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器需要绑定`my-http-backend-service`和`my-tomcat-backend-service`。在此场景下，只有`/examples`和`/examples/*`的流量会转发到`my-tomcat-backend-service`。除此之外，所有URI的流量都会转发到`my-http-backend-service`：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you don't specify an `--address` option, ephemeral external IP address will
    be created and assigned.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有指定`--address`选项，则会创建并分配一个临时的外部IP地址。
- en: 'Finally, the LoadBalancer has been created. However, one missing configuration
    remains. Private hosts don''t have any firewall rules to allow Tomcat traffic
    (`8080/tcp`). This is why, when you see the LoadBalancer status, a healthy status
    of `my-tomcat-backend-service` is kept down (`0`):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，负载均衡器已经创建。然而，还有一个缺失的配置。私有主机没有任何防火墙规则来允许Tomcat流量（`8080/tcp`）。因此，当你查看负载均衡器状态时，`my-tomcat-backend-service`的健康状态会保持为不正常（`0`）：
- en: '![](img/5755c23c-61b4-4733-bbc7-7eae6bc32179.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5755c23c-61b4-4733-bbc7-7eae6bc32179.png)'
- en: 'In this case, you need to add one more firewall rule that allows connection
    from LoadBalancer to a private subnet (use the `private` network tag for this).
    According to the GCP documentation ([https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing](https://cloud.google.com/load-balancing/docs/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing)),
    the health check heart-beat will come from the address range `35.191.0.0/16` to `130.211.0.0/22`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要添加一个新的防火墙规则，允许从负载均衡器到私有子网的连接（为此使用`private`网络标签）。根据GCP文档（[https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing](https://cloud.google.com/load-balancing/docs/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing)），健康检查的心跳将来自地址范围`35.191.0.0/16`到`130.211.0.0/22`：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After a few minutes, the `my-tomcat-backend-service` healthy status will be
    up to (`1`); now you can access the LoadBalancer from a web browser. When accessing `/`,
    it should route to `my-http-backend-service`, which has the `nginx` application
    on public hosts:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，`my-tomcat-backend-service`的健康状态将达到（`1`）；现在你可以通过网页浏览器访问负载均衡器。当访问`/`时，应该会路由到`my-http-backend-service`，该服务在公共主机上运行`nginx`应用：
- en: '![](img/6a75d782-df8d-431e-b30f-3c83aa1c8ff0.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a75d782-df8d-431e-b30f-3c83aa1c8ff0.png)'
- en: 'On the other hand, if you access the `/examples/` URL with the same LoadBalancer
    IP address, it will route to `my-tomcat-backend-service`, which is a Tomcat application
    on a private host, as shown in the following screenshot:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您使用相同的负载均衡器IP地址访问`/examples/` URL，它将路由到`my-tomcat-backend-service`，这是一个托管在私有主机上的Tomcat应用，如下图所示：
- en: '![](img/14f4d40d-f84a-42d2-b478-cd3db4b8ac28.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14f4d40d-f84a-42d2-b478-cd3db4b8ac28.png)'
- en: Overall, there are some steps that need to be performed in order to set up a
    LoadBalancer, but it's useful to integrate different HTTP applications onto a
    single LoadBalancer to deliver your service efficiently with minimal resources.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，设置负载均衡器需要执行一些步骤，但将不同的HTTP应用程序集成到一个负载均衡器中，对于高效地交付服务并以最少的资源使用是非常有用的。
- en: Persistent Disk
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久磁盘
- en: GCE also has a storage service called **Persistent Disk** (**PD**) that's quite
    similar to AWS EBS. You can allocate the desired size and types (either standard
    or SSD) on each zone and attach/detach VM instances anytime.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: GCE还提供了一种存储服务，叫做**持久磁盘**（**PD**），它与AWS EBS非常相似。您可以在每个可用区分配所需的大小和类型（标准或SSD），并随时附加/分离VM实例。
- en: 'Let''s create one PD and then attach it to the VM instance. Note that, when
    attaching a PD to the VM instance, both must be in the same zones. This limitation
    is the same as AWS EBS. So, before creating PD, check the VM instance location
    once again:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个PD，然后将其附加到VM实例。请注意，当将PD附加到VM实例时，二者必须位于相同的可用区。这一限制与AWS EBS相同。因此，在创建PD之前，请再次检查VM实例的位置：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s choose `us-west1-a` and then attach it to `public-on-subnet-a`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择`us-west1-a`，然后将其附加到`public-on-subnet-a`：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You may see that the PD has been attached at `/dev/sdb`. Similar to AWS EBS,
    you have to format this disk. Because this is a Linux OS operation, the steps
    are exactly the same as described in [Chapter 10](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml),
    *Kubernetes on AWS*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会看到PD已附加在`/dev/sdb`。类似于AWS EBS，您需要格式化此磁盘。由于这是Linux操作系统的操作，步骤与[第10章](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml)，*Kubernetes
    on AWS*中描述的完全相同。
- en: Google Kubernetes Engine (GKE)
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Kubernetes Engine（GKE）
- en: Overall, some GCP have introduced in previous sections. Now you can start to
    set up Kubernetes on GCP VM instances using those components. You can even use
    open-source Kubernetes provisioning tools such as [kops](https://github.com/kubernetes/kops)
    and [kubespray](https://github.com/kubernetes-sigs/kubespray) too.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，前面介绍了一些GCP的组件。现在，您可以开始使用这些组件在GCP的VM实例上设置Kubernetes。您甚至可以使用开源的Kubernetes配置工具，如[kops](https://github.com/kubernetes/kops)和[kubespray](https://github.com/kubernetes-sigs/kubespray)。
- en: Google Cloud provides GKE On-Prem ([https://cloud.google.com/gke-on-prem/](https://cloud.google.com/gke-on-prem/)),
    which allows the user to set up GKE on their own data center resources. As of
    January 2019, this is an alpha version and not open to everyone yet.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud提供了GKE On-Prem（[https://cloud.google.com/gke-on-prem/](https://cloud.google.com/gke-on-prem/)），允许用户在自己的数据中心资源上设置GKE。从2019年1月起，这还是一个alpha版本，尚未对所有人开放。
- en: However, GCP has a managed Kubernetes service called GKE. Under the hood, this
    uses some GCP components such as VPC, VM instances, PD, firewall rules, and LoadBalancers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GCP有一个托管的Kubernetes服务，叫做GKE。在后台，它使用了GCP的一些组件，如VPC、VM实例、PD、防火墙规则和负载均衡器。
- en: 'Of course, as usual you can use the `kubectl` command to control your Kubernetes
    cluster on GKE, which includes the Cloud SDK. If you haven''t installed the `kubectl`
    command on your machine yet, type the following command to install it via the
    Cloud SDK:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，像往常一样，您可以使用`kubectl`命令来控制GKE上的Kubernetes集群，它包含在Cloud SDK中。如果您还没有在计算机上安装`kubectl`命令，可以通过Cloud
    SDK输入以下命令进行安装：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Setting up your first Kubernetes cluster on GKE
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GKE上设置您的第一个Kubernetes集群
- en: 'You can set up a Kubernetes cluster on GKE using the `gcloud` command. This
    needs to specify several parameters to determine some configurations. One important
    parameter is the network. Here, you have to specify which VPC and subnet you''ll
    deploy. Although GKE supports multiple zones to deploy, you need to specify at
    least one zone for the Kubernetes master node. This time, it uses the following
    parameters to launch a GKE cluster:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`gcloud`命令在GKE上设置Kubernetes集群。这需要指定多个参数以确定一些配置。一个重要的参数是网络。在这里，您必须指定要部署的VPC和子网。尽管GKE支持多个可用区进行部署，但您需要为Kubernetes主节点指定至少一个可用区。这次，使用以下参数启动GKE集群：
- en: '| **Parameter** | **Description** | **Value** |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **描述** | **值** |'
- en: '| `--machine-type` | VM instance type for Kubernetes Node | `f1-micro` |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `--machine-type` | Kubernetes节点的VM实例类型 | `f1-micro` |'
- en: '| `--num-nodes` | Initial number of Kubernetes nodes | `3` |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `--num-nodes` | Kubernetes节点的初始数量 | `3` |'
- en: '| `--network` | Specify GCP VPC | `my-custom-network` |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `--network` | 指定GCP VPC | `my-custom-network` |'
- en: '| `--subnetwork` | Specify GCP Subnet if VPC is a custom mode | `subnet-c`
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `--subnetwork` | 如果VPC为自定义模式，则指定GCP子网 | `subnet-c` |'
- en: '| `--zone` | Specify a single zone | `asia-northeast1-a` |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| `--zone` | 指定一个单一的可用区 | `asia-northeast1-a` |'
- en: '| `--tags` | Network tags that will be assigned to Kubernetes nodes | `private`
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| `--tags` | 将分配给Kubernetes节点的网络标签 | `private` |'
- en: 'In this scenario, you need to type the following commands to launch a Kubernetes
    cluster on GCP. It may take a few minutes to complete because, behind the scenes,
    it''ll launch several VM instances and set up the Kubernetes master and nodes.
    Note that the Kubernetes master and etcd will be fully managed by GCP. This means
    that the master node and etcd don''t consume your VM instances:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要输入以下命令来在GCP上启动Kubernetes集群。由于在后台会启动多个VM实例并设置Kubernetes主节点和节点，因此可能需要几分钟才能完成。请注意，Kubernetes主节点和etcd将由GCP完全管理。这意味着主节点和etcd不会消耗你的VM实例：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we specify the `--tags private` option so that a Kubernetes node VM
    instance has a network tag of `private`. Therefore, it behaves the same as other
    regular VM instances that have `private` tags. Consequently, you can't SSH from
    the public internet and you can't HTTP from the internet either. However, you
    can ping and SSH from another VM instance that has a `public` network tag.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们指定了`--tags private`选项，以便Kubernetes节点VM实例拥有`private`网络标签。因此，它的行为与其他具有`private`标签的普通VM实例相同。结果，你无法从公共互联网进行SSH连接，也无法从互联网进行HTTP访问。不过，你可以从另一个具有`public`网络标签的VM实例进行ping和SSH连接。
- en: Node pool
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点池
- en: When launching the Kubernetes cluster, you can specify the number of nodes using
    the `--num-nodes` option. GKE manages a Kubernetes node as a node pool. This means
    you can manage one or more node pools that are attached to your Kubernetes cluster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动Kubernetes集群时，你可以使用`--num-nodes`选项指定节点数量。GKE将Kubernetes节点管理为一个节点池。这意味着你可以管理附加到Kubernetes集群的一个或多个节点池。
- en: 'What if you need to add or delete nodes? GKE let''s you resize the node pool
    by performing the following command to change Kubernetes node from 3 to 5:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要添加或删除节点怎么办？GKE允许你通过执行以下命令将Kubernetes节点从3个增加到5个：
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Increasing the number of nodes will help if you need to scale out your node
    capacity. However, in this scenario, it still uses the smallest instance type
    (`f1-micro`, which has only 0.6 GB memory). It might not help if a single container
    needs more than 0.6 GB of memory. In this case you need to scale up, which means
    you need to add a larger VM instance type.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 增加节点数量有助于你需要扩展节点容量时。但是，在这种情况下，它仍然使用最小的实例类型（`f1-micro`，仅有0.6 GB内存）。如果单个容器需要超过0.6
    GB的内存，这可能并没有帮助。在这种情况下，你需要进行扩展，即需要添加更大的VM实例类型。
- en: In this case, you have to add another set of node pools onto your cluster. This
    is because, within the same node pool, all VM instances are configured the same.
    Consequently, you can't change the instance type in the same node pool.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你必须向集群添加另一个节点池。这是因为，在同一个节点池内，所有的VM实例配置都是相同的。因此，你无法在同一个节点池中更改实例类型。
- en: Add a new node pool that has two new sets of `g1-small` (1.7 GB memory) VM instance
    types to the cluster. Then you can expand Kubernetes nodes with a different hardware
    configuration.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 向集群添加一个新的节点池，包含两套新的`g1-small`（1.7 GB内存）VM实例类型。然后，你可以使用不同硬件配置扩展Kubernetes节点。
- en: By default, there are some quotas that can create a number limit for VM instances
    within one region (for example, up to eight CPU cores on `us-west1`). If you wish
    to increase this quota, you must change your account to a paid one and then request
    a quota change to GCP. For more details, please read the online documentation
    from [https://cloud.google.com/compute/quotas](https://cloud.google.com/compute/quotas)
    and [https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade](https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，在一个区域内有一些配额会限制VM实例的数量（例如，在`us-west1`区域最多允许8个CPU核心）。如果你希望增加这个配额，你必须将账户升级为付费账户，然后请求GCP更改配额。更多详情，请阅读在线文档：[https://cloud.google.com/compute/quotas](https://cloud.google.com/compute/quotas)
    和 [https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade](https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade)。
- en: 'Run the following command to add an additional node pool that has two instances
    of a `g1-small` instance:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以添加一个额外的节点池，该池包含两个`g1-small`实例：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now you have a total of seven CPU cores and 6.4 GB memory in your cluster, which
    has more capacity. However, due to larger hardware types, the Kubernetes scheduler
    will probably assign a pod to `large-mem-pool` first because it has enough memory
    capacity.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的集群总共有七个 CPU 核心和 6.4 GB 的内存，容量更大。然而，由于硬件类型较大，Kubernetes 调度器可能会优先将 Pod 分配到
    `large-mem-pool`，因为它有足够的内存容量。
- en: However, you may want to preserve the `large-mem-pool` node in case a big application
    needs a large memory size (for example, a Java application). Therefore, you may
    want to differentiate `default-pool` and `large-mem-pool`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可能希望保留 `large-mem-pool` 节点，以防某个大型应用需要较大的内存（例如，一个 Java 应用）。因此，你可能想区分 `default-pool`
    和 `large-mem-pool`。
- en: 'In this case, the Kubernetes label, `beta.kubernetes.io/instance-type`, helps
    to distinguish an instance type of node. Therefore, use `nodeSelector` to specify
    a desired node for the pod. For example, the following `nodeSelector` parameter
    will force the use of the `f1-micro` node for the `nginx` application:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Kubernetes 标签 `beta.kubernetes.io/instance-type` 有助于区分节点的实例类型。因此，可以使用
    `nodeSelector` 来指定 Pod 的目标节点。例如，以下的 `nodeSelector` 参数将强制使用 `f1-micro` 节点来运行 `nginx`
    应用：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If you want to specify a particular label instead of `beta.kubernetes.io/instance-type`,
    use the `--node-labels` option to create a node pool. That assigns your desired
    label to the node pool. For more details, please read the following online document: [https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create](https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想指定一个特定的标签，而不是 `beta.kubernetes.io/instance-type`，可以使用 `--node-labels` 选项来创建节点池。这样可以将你想要的标签分配给节点池。更多细节，请阅读以下在线文档：[https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create](https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create)。
- en: 'Of course, you can feel free to remove a node pool if you no longer need it.
    To do that, run the following command to delete `default-pool` (`f1-micro` x 5
    instances). This operation will involve pod migration (terminate the pod on `default-pool`
    and relaunch it on `large-mem-pool`) automatically if there are some pods running
    at `default-pool`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你不再需要某个节点池，可以随时将其删除。为此，可以运行以下命令删除 `default-pool`（`f1-micro` x 5 实例）。如果
    `default-pool` 上有一些 Pod 在运行，执行此操作时将自动进行 Pod 迁移（终止 `default-pool` 上的 Pod 并将其重新启动在
    `large-mem-pool` 上）：
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You may have noticed that all of the preceding operations happened in a single
    zone (`asia-northeast1-a`). Therefore, if the `asia-northeast1-a` zone gets an
    outage, your cluster will be down. In order to avoid zone failure, you may consider
    setting up a multi-zone cluster.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，所有前述操作都发生在同一个区域（`asia-northeast1-a`）。因此，如果 `asia-northeast1-a` 区域发生故障，你的集群将会宕机。为了避免区域故障，你可以考虑设置多区域集群。
- en: Multi-zone clusters
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多区域集群
- en: GKE supports multi-zone clusters, which allows you to launch Kubernetes nodes
    on multiple zones, but within the same region. In previous examples, Kubernetes
    nodes have been provisioned at `asia-northeast1-a` only, so let's re-provision
    a cluster that has `asia-northeast1-a`, `asia-northeast1-b`, and `asia-northeast1-c`
    in a total of three zones.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 支持多区域集群，这允许你在同一地区的多个区域中启动 Kubernetes 节点。在前面的示例中，Kubernetes 节点只在 `asia-northeast1-a`
    区域中提供，因此让我们重新配置一个集群，包含 `asia-northeast1-a`、`asia-northeast1-b` 和 `asia-northeast1-c`
    三个区域。
- en: It's very simple; you just use the `--node-locations` parameter to specify three
    zones when creating a new cluster.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常简单；你只需在创建新集群时使用 `--node-locations` 参数指定三个区域。
- en: 'Let''s delete the previous cluster and create a new one with the `--node-locations` option:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除之前的集群，并使用 `--node-locations` 选项创建一个新的集群：
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This example will create two nodes per zone (`asia-northeast1-a`, `b`, and
    `c`). Consequently, a total of six nodes will be added:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例将在每个区域创建两个节点（`asia-northeast1-a`、`b` 和 `c`）。因此，总共将添加六个节点：
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You may also distinguish node zones with the `failure-domain.beta.kubernetes.io/zone`
    Kubernetes label so that you can specify the desired zones to deploy a pod.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过 Kubernetes 标签 `failure-domain.beta.kubernetes.io/zone` 来区分节点区域，以便指定想要部署
    Pod 的区域。
- en: Cluster upgrade
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群升级
- en: Once you start to manage Kubernetes, you may encounter difficulties when upgrading
    Kubernetes clusters. Due to the fact that the Kubernetes project is very aggressive,
    there's a new release around every three months, such as version 1.11.0 (released
    on June 27^(th) 2018), 1.12.0 (released on September 27^(th) 2018), and 1.13.0
    (released on December 3^(rd) 2018).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你开始管理Kubernetes，可能会遇到在升级Kubernetes集群时的困难。由于Kubernetes项目更新非常积极，每大约三个月就会发布一个新版本，例如1.11.0（2018年6月27日发布）、1.12.0（2018年9月27日发布）和1.13.0（2018年12月3日发布）。
- en: 'GKE also keeps adding new version support in a timely manner. This allows us
    to upgrade both master and nodes via the `gcloud` command. You can run the following
    command to see which Kubernetes version is supported by GKE:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: GKE还会及时添加对新版本的支持。这使得我们可以通过`gcloud`命令升级master和节点。你可以运行以下命令查看GKE支持的Kubernetes版本：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'So, you can see that the latest supported Kubernetes version is `1.11.4-gke.8`
    on both master and node at the time of writing. Since the previous example installed
    is version `1.10.9-gke.5`, let''s update this to `1.11.4-gke.8`. First of all,
    you need to upgrade `master` first:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以看到在写作时，最新支持的Kubernetes版本是`1.11.4-gke.8`，在master和node上都是如此。由于之前安装的版本是`1.10.9-gke.5`，我们将其更新为`1.11.4-gke.8`。首先，你需要先升级`master`：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This takes around 10 minutes depending on the environment. After that, you
    can verify `MASTER_VERSION` via the following command:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约需要10分钟，具体时间取决于环境。之后，你可以通过以下命令验证`MASTER_VERSION`：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now you can start to upgrade all nodes to version `1.11.4-gke.8`. Because GKE
    tries to perform a rolling upgrade, it''ll perform the following steps on each
    node, one by one:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以开始将所有节点升级到`1.11.4-gke.8`版本。由于GKE尝试执行滚动升级，它将在每个节点上逐一执行以下步骤：
- en: De-register a target node from the cluster
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从集群中取消注册目标节点
- en: Delete old VM instances
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除旧的虚拟机实例
- en: Provision a new VM instance
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个新的虚拟机实例
- en: Set up the node with the `1.11.4-gke.8` version
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置节点为`1.11.4-gke.8`版本
- en: Register to `master`
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册到`master`
- en: 'Therefore, a node upgrade takes much longer than a master upgrade:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，节点的升级比master升级需要更长时间：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'During the rolling upgrade, you can see the node status as follows; this example
    shows the halfway point of a rolling update. Here, two nodes have upgraded to
    `1.11.4-gke.8`, one node is about to upgrade, and the remaining three nodes are
    pending:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在滚动升级过程中，你可以看到节点状态如下；这个例子展示了滚动更新的中途状态。这里，两个节点已经升级到`1.11.4-gke.8`，一个节点即将升级，剩余的三个节点处于待处理状态：
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Kubernetes cloud provider
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes云提供商
- en: GKE also integrates out-of-the-box Kubernetes cloud providers, which deeply
    integrate with the GCP infrastructure, for example, the overlay network by VPC
    route, `StorageClass` by Persistent Disk, and Service by L4 LoadBalancer. One
    of the best integrations is provided by the ingress controller by L7 LoadBalancer.
    Let's take a look at how this works.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: GKE还原生集成了Kubernetes云提供商，深度集成了GCP基础设施，例如，通过VPC路由的覆盖网络（overlay network）、通过持久磁盘（Persistent
    Disk）的`StorageClass`、以及通过L4负载均衡器的服务（Service）。其中最好的集成之一是由L7负载均衡器提供的ingress控制器。让我们来看看它是如何工作的。
- en: StorageClass
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储类（StorageClass）
- en: 'Similarly to EKS on AWS, GKE also sets up `StorageClass` by default; this uses
    Persistent Disk:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于AWS上的EKS，GKE也默认设置`StorageClass`；它使用的是持久磁盘（Persistent Disk）：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Consequently, when creating a persistent volume claim, this will automatically allocate
    the GCP Persistent Disk as the Kubernetes Persistent Volume. For more information
    on persistent volume claim and Dynamic Provisioning, please refer to [Chapter
    4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml), *Managing Stateful Workloads*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当创建持久卷声明（persistent volume claim）时，它会自动分配GCP持久磁盘作为Kubernetes持久卷。有关持久卷声明和动态供应的更多信息，请参考[第4章](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml)，*管理有状态工作负载*：
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: L4 LoadBalancer
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L4 负载均衡器
- en: Similarly to AWS cloud provider, GKE also supports using the L4 LoadBalancer
    for Kubernetes Services. Just specify `Service.spec.type` as the LoadBalancer,
    and then GKE will set up and configure the L4 LoadBalancer automatically.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于AWS云提供商，GKE也支持使用L4负载均衡器为Kubernetes服务（Kubernetes Services）提供负载均衡。只需将`Service.spec.type`指定为LoadBalancer，GKE将自动设置和配置L4负载均衡器。
- en: 'Note that the corresponding firewall rule between the L4 LoadBalancer and the
    Kubernetes node can be created by the cloud provider automatically. This is simple
    but powerful enough if you want to quickly expose your application to the internet:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，L4负载均衡器与Kubernetes节点之间的相应防火墙规则可以由云提供商自动创建。这非常简单，但足够强大，如果你想快速将应用暴露到互联网上，足够用了：
- en: '[PRE32]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: L7 LoadBalancer (ingress)
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L7 负载均衡器（ingress）
- en: GKE also supports Kubernetes ingress, which can set up the GCP L7 LoadBalancer
    to dispatch HTTP requests to the target service based on the URL. You just need
    to set up one or more NodePort services and then create ingress rules to point
    to the services. Behind the scenes, Kubernetes automatically creates and configures
    the following firewall rules; `health check`, `backend` service, `forwarding rule`,
    and `url-maps`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 也支持 Kubernetes ingress，它可以设置 GCP L7 负载均衡器，根据 URL 将 HTTP 请求调度到目标服务。你只需要设置一个或多个
    NodePort 服务，然后创建 ingress 规则指向这些服务。在后台，Kubernetes 会自动创建和配置以下防火墙规则：`health check`、`backend`
    服务、`forwarding rule` 和 `url-maps`。
- en: 'Let''s create same examples that use nginx and Tomcat to deploy to the Kubernetes
    cluster first. These use Kubernetes Services that bind to NodePort instead of
    LoadBalancer:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建相同的示例，使用 nginx 和 Tomcat 部署到 Kubernetes 集群。这些示例使用的是绑定到 NodePort 的 Kubernetes
    服务，而不是 LoadBalancer：
- en: '![](img/55796cc8-048e-4061-8a10-17f60728d29b.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55796cc8-048e-4061-8a10-17f60728d29b.png)'
- en: At present, you can't access Kubernetes Service because there are as yet no
    firewall rules that allow access to it from the internet. Consequently, let's
    create Kubernetes ingress to point to these services.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你无法访问 Kubernetes 服务，因为还没有允许从互联网访问的防火墙规则。因此，让我们创建 Kubernetes ingress 来指向这些服务。
- en: 'You can use `kubectl port-forward <pod name> <your machine available port><:
    service port number>` to access the pod via the Kubernetes API server. For the
    preceding case, use `kubectl port-forward tomcat-670632475-l6h8q 10080:8080`.
    After that, open your web browser to `http://localhost:10080/` and then you can
    directly access the Tomcat pod.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以使用 `kubectl port-forward <pod name> <your machine available port><: service
    port number>` 通过 Kubernetes API 服务器访问 pod。对于上述案例，可以使用 `kubectl port-forward tomcat-670632475-l6h8q
    10080:8080`。之后，打开浏览器并访问 `http://localhost:10080/`，即可直接访问 Tomcat pod。'
- en: 'Kubernetes ingress definition is quite similar to GCP backend service definition
    as it needs to specify a combination of URL path, Kubernetes service name, and
    service port number. In this scenario, the `/` and `/*` URLs point to the `nginx`
    service, while the `/examples` and `/examples/*` URLs also point to the Tomcat
    service, as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes ingress 的定义与 GCP 后端服务的定义非常相似，因为它需要指定 URL 路径、Kubernetes 服务名称和服务端口号的组合。在这种情况下，`/`
    和 `/*` URLs 指向 `nginx` 服务，而 `/examples` 和 `/examples/*` URLs 则指向 Tomcat 服务，如下所示：
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It takes around 10 to 15 minutes to fully configure GCP components such as
    `health check`, `forwarding rule`, `backend` services, and `url-maps`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 完整配置 GCP 组件（如`health check`、`forwarding rule`、`backend` 服务和 `url-maps`）大约需要
    10 到 15 分钟：
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can also check the status on the web console, as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 web 控制台上查看状态，如下所示：
- en: '![](img/5486e313-b1e0-4b4c-aa00-deca29c6cddc.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5486e313-b1e0-4b4c-aa00-deca29c6cddc.png)'
- en: Once you've completed setting up of the L7 LoadBalancer, you can access the
    public IP LoadBalancer address (`http://107.178.253.174/`) to see the nginx page.
    As well as accessing `http://107.178.253.174/examples/`, you can see the `tomcat
    example` page.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 完成 L7 负载均衡器设置后，你可以通过访问公共 IP 负载均衡器地址 (`http://107.178.253.174/`) 查看 nginx 页面。还可以访问
    `http://107.178.253.174/examples/`，你会看到 `tomcat example` 页面。
- en: GKE returns 404 Not found until GKE is fully complete in order to configure
    the LoadBalancer.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GKE 完全配置完 L7 负载均衡器之前，GKE 会返回 404 Not Found。
- en: In the preceding steps, we created and assigned an ephemeral IP address for
    the L7 LoadBalancer. However, the best practice when using L7 LoadBalancer is
    to assign a static IP address instead, because you can also associate DNS (FQDN)
    to the static IP address.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的步骤中，我们为 L7 负载均衡器创建并分配了一个临时 IP 地址。然而，使用 L7 负载均衡器的最佳实践是分配一个静态 IP 地址，因为你还可以将
    DNS（FQDN）与静态 IP 地址关联。
- en: 'To do that, update the ingress setting to add an annotation named `kubernetes.io/ingress.global-static-ip-name`
    to associate a GCP static IP address name, as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，更新 ingress 设置，添加名为 `kubernetes.io/ingress.global-static-ip-name` 的注释，以便将
    GCP 静态 IP 地址名称关联起来，如下所示：
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: So, now you can access the ingress via a static IP address such as `http://35.186.227.252/`
    (`nginx`) and `http://35.186.227.252/examples/` (Tomcat) instead of an ephemeral
    IP address. This benefits to the user and preserves the static IP address. For
    example, when you recreate an ingress, the IP address won't be changed.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过静态 IP 地址访问 ingress，比如 `http://35.186.227.252/` (`nginx`) 和 `http://35.186.227.252/examples/`（Tomcat），而不是通过临时
    IP 地址访问。这对用户有好处，并且能保留静态 IP 地址。例如，当你重新创建 ingress 时，IP 地址不会改变。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed Google Cloud Platform. Its basic concept is similar
    to AWS, but some policies and concepts are different. This is particularly the
    case for Google Container Engine, as this is a very powerful service for using
    Kubernetes as production grade. Kubernetes cluster and node management is quite
    easy to install and upgrade. The cloud provider is also fully integrated with
    GCP (especially ingress, as this can configure the L7 LoadBalancer with one command).
    Consequently, it's highly recommended you try GKE if you plan to use Kubernetes
    on the public cloud.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 Google Cloud Platform。其基本概念与 AWS 相似，但一些政策和概念有所不同。尤其是在 Google Container
    Engine 中，因为这是一个非常强大的服务，适用于将 Kubernetes 用作生产级别的工具。Kubernetes 集群和节点的管理非常容易安装和升级。该云服务提供商也与
    GCP 完全集成（特别是入口控制，因为可以通过一个命令配置 L7 负载均衡器）。因此，如果你计划在公有云上使用 Kubernetes，强烈建议尝试 GKE。
- en: '[Chapter 12](89891610-4ca4-4216-9d76-2613d186421c.xhtml), *Kubernetes on Azure*,
    will explore one more public cloud service named Microsoft Azure, which also provides
    a managed Kubernetes service.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 12 章](89891610-4ca4-4216-9d76-2613d186421c.xhtml)，*Kubernetes on Azure*，将探索另一个公有云服务——Microsoft
    Azure，它也提供托管的 Kubernetes 服务。'
