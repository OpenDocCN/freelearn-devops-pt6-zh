- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Kubernetes – Introduction and Integration with GenAI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes – 简介及与 GenAI 的集成
- en: 'Deploying and managing GenAI workloads at scale presents significant challenges,
    including building the models, packaging them for distribution, and ensuring effective
    deployment and scaling. In this chapter, we will discuss the concepts of containers
    and **Kubernetes** (**K8s**) and why they are emerging as powerful solutions to
    address these complexities. They are becoming the de facto standard for companies
    such as OpenAI ([https://openai.com/index/scaling-kubernetes-to-7500-nodes/](https://openai.com/index/scaling-kubernetes-to-7500-nodes/))
    and Anthropic ([https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412](https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412))
    to deploy GenAI workloads. We will cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模部署和管理 GenAI 工作负载时，会面临诸多挑战，包括构建模型、打包模型进行分发，以及确保有效的部署和扩展。在本章中，我们将讨论容器和**Kubernetes**（**K8s**）的概念，以及它们为什么成为应对这些复杂性强大解决方案的趋势。它们正成为
    OpenAI（[https://openai.com/index/scaling-kubernetes-to-7500-nodes/](https://openai.com/index/scaling-kubernetes-to-7500-nodes/)）和
    Anthropic（[https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412](https://youtu.be/c9NJ6GSeNDM?si=xjei4T9VfZvejD5o&t=2412)）等公司的事实标准，用于部署
    GenAI 工作负载。我们将涵盖以下主要内容：
- en: Understanding containers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解容器
- en: Why containers for GenAI models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择容器来运行 GenAI 模型
- en: What is Kubernetes (K8s)?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Kubernetes（K8s）？
- en: Understanding containers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解容器
- en: Containers have revolutionized the way we manage applications by standardizing
    the packaging format. With the portability of containers, applications are packaged
    as a standard unit of software that packages all of your code and dependencies
    to deploy consistently and reliably across various environments, such as on-premises,
    public, and private clouds. Containers are also considered an evolution of **Virtual
    Machine** (**VM**) technology, where multiple containers are run on the same operating
    system, sharing the underlying kernel to increase the overall server utilization.
    This is a massive advantage of containers as there is no overhead of multiple
    **Operating Systems** (**OSes**) and other OS-level components. So, containers
    can be started and stopped a lot faster while providing isolation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 容器通过标准化打包格式，彻底改变了我们管理应用程序的方式。借助容器的可移植性，应用程序被打包成一个标准的软件单元，包含所有代码和依赖项，以便在各种环境中（如本地、公共云和私有云）一致且可靠地部署。容器还被认为是**虚拟机**（**VM**）技术的演进，其中多个容器在同一个操作系统上运行，共享底层内核以提高整体服务器利用率。这是容器的巨大优势，因为没有多个**操作系统**（**OSes**）和其他操作系统级组件的开销。因此，容器可以更快地启动和停止，同时提供隔离性。
- en: The following figure illustrates the evolution of computing environments, highlighting
    a shift toward higher levels of abstraction and an increased focus on business
    logic.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了计算环境的演变，突出了向更高层次抽象的转变，以及对业务逻辑的关注日益增强。
- en: '![Figure 2.1 – Evolution of container technology](img/B31108_02_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 容器技术的演变](img/B31108_02_01.jpg)'
- en: Figure 2.1 – Evolution of container technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 容器技术的演变
- en: Physical servers offer the least abstraction, with extensive manual configuration,
    and resource inefficiencies. VMs provide a middle ground, by abstracting underlying
    hardware resources using hypervisor technology. This allows you to run multiple
    VMs on the same physical server, increasing the resource utilization and security.
    However, they are bulky and slow to boot up. Containers provide the highest level
    of abstraction by encapsulating applications and dependencies in portable units,
    allowing us to focus more on developing and optimizing business logic rather than
    managing infrastructure. *Figure 2**.2* illustrates the high-level differences
    between VMs and containers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 物理服务器提供最少的抽象，需大量手动配置，并且存在资源低效问题。虚拟机通过使用虚拟化技术来抽象底层硬件资源，提供一种折中的方案。这使得你可以在同一台物理服务器上运行多个虚拟机，从而提高资源利用率和安全性。然而，它们体积庞大，启动速度较慢。容器通过将应用程序和依赖项封装在便于移植的单元中，提供了最高级别的抽象，让我们可以更多地专注于开发和优化业务逻辑，而不是管理基础设施。*图
    2.2* 展示了虚拟机和容器之间的高层次差异。
- en: '![Figure 2.2 – Virtual machines versus containers](img/B31108_02_02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 虚拟机与容器的对比](img/B31108_02_02.jpg)'
- en: Figure 2.2 – Virtual machines versus containers
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 虚拟机与容器的对比
- en: Container technology is made possible because of namespaces and cgroups (control
    groups) in the Linux kernel. They form the foundational building blocks to provide
    isolation and resource limits. A **Linux Namespace** ([https://man7.org/linux/man-pages/man7/namespaces.7.html](https://man7.org/linux/man-pages/man7/namespaces.7.html))
    is an abstraction over resources in the operating system. It partitions OS-level
    resources such that different sets of processes see a different set of resources
    (network, file system, etc.,) even though they are running on the same OS kernel.
    **Cgroups** ([https://man7.org/linux/man-pages/man7/cgroups.7.html](https://man7.org/linux/man-pages/man7/cgroups.7.html))
    govern the isolation and usage of system resources, such as CPU, memory, and network,
    for a group of processes and optionally enforce limits and constraints. These
    capabilities let containers abstract the operating system components for modern
    applications.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 容器技术的实现得益于 Linux 内核中的命名空间（namespaces）和控制组（cgroups）。它们构成了提供隔离和资源限制的基础构件。**Linux
    命名空间** ([https://man7.org/linux/man-pages/man7/namespaces.7.html](https://man7.org/linux/man-pages/man7/namespaces.7.html))
    是操作系统资源的一种抽象。它将操作系统级别的资源进行划分，使得不同的进程集合即使在同一操作系统内核上运行，也能看到不同的资源集（如网络、文件系统等）。**控制组**
    ([https://man7.org/linux/man-pages/man7/cgroups.7.html](https://man7.org/linux/man-pages/man7/cgroups.7.html))
    用于管理一组进程的系统资源的隔离和使用，如 CPU、内存、网络等，并可以选择性地强制执行限制和约束。这些能力使容器能够为现代应用程序抽象操作系统组件。
- en: Container terminology
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器术语
- en: 'The following are some terms associated with containers that will be crucial
    in following along with this book:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些与容器相关的术语，它们在阅读本书时至关重要：
- en: '**Container runtime**: This is a host-level process that is responsible for
    creating, stopping, and starting containers. It interacts with low-level container
    runtimes such as runc to set up namespaces and cgroups for containers. Popular
    examples include containerd, CRI-O, and so on.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：这是一个主机级别的进程，负责创建、停止和启动容器。它与低级容器运行时（如 runc）交互，为容器设置命名空间和控制组。常见的容器运行时包括
    containerd、CRI-O 等。'
- en: '**Container image**: This is a lightweight, standalone, executable package
    that includes everything needed to run a piece of software, including the code,
    runtime, libraries, environment variables, and configuration files. It is created
    using a Dockerfile, a plain text definition file that includes a set of instructions
    to install dependencies, applications, and so on. Typical characteristics of a
    container image include the following:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器镜像**：这是一个轻量级、独立的可执行包，包含了运行软件所需的一切，包括代码、运行时、库、环境变量和配置文件。它是通过 Dockerfile
    创建的，Dockerfile 是一个包含一组指令的纯文本定义文件，用来安装依赖、应用程序等。容器镜像的典型特征包括以下几点：'
- en: '**Self-contained**: It encapsulates everything needed to run software applications.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自包含**：它封装了运行软件应用所需的一切。'
- en: '**Immutable**: It is read-only in nature; any changes would require a new image.'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可变**：它是只读的；任何更改都需要创建新的镜像。'
- en: '**Layered**: Images are built in layers, each layer representing a file system.
    This is what makes images highly efficient as common layers can be shared across
    multiple images.'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层**：镜像是分层构建的，每一层都代表一个文件系统。这使得镜像具有高度的效率，因为相同的层可以在多个镜像之间共享。'
- en: '**Portable**: As the image packages the application and all its dependencies,
    they can be run on any system that supports a container runtime, making them highly
    portable in nature.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植**：由于镜像打包了应用程序及其所有依赖项，因此可以在任何支持容器运行时的系统上运行，使其具有高度的可移植性。'
- en: '**Container registry**: This is a tool used to manage and distribute container
    images. Popular registries include Docker Hub ([https://hub.docker.com/](https://hub.docker.com/)),
    Amazon Elastic Container Registry ([https://aws.amazon.com/ecr/](https://aws.amazon.com/ecr/)),
    Google Artifact Registry ([https://cloud.google.com/artifact-registry](https://cloud.google.com/artifact-registry)),
    and so on. Tools such as Artifactory and Harbor can be used to self-host a registry.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器注册表**：这是一个用于管理和分发容器镜像的工具。常见的注册表包括 Docker Hub ([https://hub.docker.com/](https://hub.docker.com/))、亚马逊弹性容器注册表
    ([https://aws.amazon.com/ecr/](https://aws.amazon.com/ecr/))、谷歌 Artifact Registry
    ([https://cloud.google.com/artifact-registry](https://cloud.google.com/artifact-registry))
    等。工具如 Artifactory 和 Harbor 可以用来自托管注册表。'
- en: '**Container**: This is a running instance or process created from the container
    image by the container runtime.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**：这是一个通过容器运行时从容器镜像创建的正在运行的实例或进程。'
- en: Let’s now understand the high-level container workflow using Docker, a software
    platform designed to help developers build, share, and run container applications.
    Docker follows traditional client-server architecture. When you install Docker
    on a host, it runs a server component called the Docker daemon and a client component
    – the Docker CLI. The **Daemon** is responsible for creating and managing images,
    running containers using those images, and setting up networking, storage, and
    so on. As depicted in *Figure 2**.3*, the Docker CLI is used to interact with
    the Docker daemon process to build and run containers. Once the images are built,
    the Docker daemon can push and pull those images to/from a container registry.
    Storing the images in a container registry allows them to be portable and they
    can be run anywhere a container runtime is available.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过 Docker 来了解高级容器工作流，Docker 是一个旨在帮助开发人员构建、分享和运行容器应用程序的软件平台。Docker 遵循传统的客户端-服务器架构。当你在主机上安装
    Docker 时，它会运行一个名为 Docker 守护进程的服务器组件和一个客户端组件——Docker CLI。**守护进程**负责创建和管理镜像，使用这些镜像运行容器，并设置网络、存储等。如*图
    2.3*所示，Docker CLI 用于与 Docker 守护进程交互，以构建和运行容器。一旦镜像构建完成，Docker 守护进程可以将这些镜像推送到容器注册表，或从容器注册表拉取镜像。将镜像存储在容器注册表中可以使它们具有可移植性，并且可以在任何有容器运行时的地方运行。
- en: '![Figure 2.3 – Docker architecture overview](img/B31108_02_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – Docker 架构概述](img/B31108_02_03.jpg)'
- en: Figure 2.3 – Docker architecture overview
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – Docker 架构概述
- en: Creating a container image
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建容器镜像
- en: 'Let’s create our first hello world container image and run it locally. Use
    the Docker documentation page at [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)
    to install Docker Engine on your machine. The following are the links to install
    Docker Desktop for different operating systems:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建第一个 hello world 容器镜像并在本地运行它。请使用 [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)
    上的 Docker 文档页面，在你的机器上安装 Docker 引擎。以下是不同操作系统安装 Docker Desktop 的链接：
- en: 'Docker Desktop for Linux: [https://docs.docker.com/desktop/install/linux-install/](https://docs.docker.com/desktop/install/linux-install/)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Desktop for Linux: [https://docs.docker.com/desktop/install/linux-install/](https://docs.docker.com/desktop/install/linux-install/)'
- en: 'Docker Desktop for Mac (macOS): [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Desktop for Mac (macOS): [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)'
- en: 'Docker Desktop for Windows: [https://docs.docker.com/desktop/install/windows-install/](https://docs.docker.com/desktop/install/windows-install/)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Desktop for Windows: [https://docs.docker.com/desktop/install/windows-install/](https://docs.docker.com/desktop/install/windows-install/)'
- en: 'In the following Dockerfile, we are creating a simple hello world application
    using the `nginx` server. We will use `nginx` as the parent image and customize
    the `index.html` with our *Hello World!* message. Let’s start by following these
    steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下 Dockerfile 中，我们使用 `nginx` 服务器创建一个简单的 hello world 应用程序。我们将使用 `nginx` 作为父镜像，并将
    `index.html` 自定义为我们的 *Hello World!* 消息。让我们按照以下步骤开始：
- en: 'Create a Dockerfile with the following content:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有以下内容的 Dockerfile：
- en: '[PRE0]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Build a container image with a `v1` tag:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `v1` 标签构建容器镜像：
- en: '[PRE1]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can list the local container images using the following command:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用以下命令列出本地的容器镜像：
- en: '[PRE2]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run a container using the `hello-world` image and bind nginx port `80` to the
    `8080` host port:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `hello-world` 镜像运行容器，并将 nginx 的端口 `80` 映射到主机的 `8080` 端口：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have the image running, we can test the container by calling localhost
    `8080`:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经启动了镜像，我们可以通过访问本地的 `8080` 来测试容器：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As an optional step, you can also push the container image to an `xyz` container
    registry so that you can run it anywhere. Replace `xyz` with your container repository
    name. For example, follow the instructions at [https://www.docker.com/blog/how-to-use-your-own-registry-2/](https://www.docker.com/blog/how-to-use-your-own-registry-2/)
    to create the registry in Docker Hub:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为可选步骤，你也可以将容器镜像推送到 `xyz` 容器注册表，以便你可以在任何地方运行它。将 `xyz` 替换为你的容器仓库名称。例如，按照 [https://www.docker.com/blog/how-to-use-your-own-registry-2/](https://www.docker.com/blog/how-to-use-your-own-registry-2/)
    上的说明创建 Docker Hub 中的注册表：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this section, we learned about the evolution of computing environments, the
    benefits of using containers over traditional physical servers, and VMs. We gained
    an understanding of the overall Docker architecture and various key container
    terminology and built and ran our first hello-world container application. Let’s
    explore why containers are a great fit for GenAI models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了计算环境的演变，使用容器相比传统物理服务器和虚拟机的优势。我们了解了整体的 Docker 架构以及各种关键的容器术语，并构建和运行了我们的第一个
    hello-world 容器应用。接下来，让我们探讨为什么容器是 GenAI 模型的理想选择。
- en: Why containers for GenAI models?
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么为 GenAI 模型选择容器？
- en: A typical challenge with developing ML or GenAI applications involves using
    complex and continuously evolving open source ML frameworks such as PyTorch and
    TensorFlow, ML tool kits such as Hugging Face Transformers, ever-changing GPU
    hardware ecosystems from NVIDIA, and custom accelerators from Amazon, Google,
    and so on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 开发 ML 或 GenAI 应用的一个典型挑战是使用复杂且不断发展的开源 ML 框架，如 PyTorch 和 TensorFlow，ML 工具包如 Hugging
    Face Transformers，以及来自 NVIDIA 的不断变化的 GPU 硬件生态系统和来自亚马逊、谷歌等的定制加速器。
- en: The following figure illustrates various components involved in creating and
    running an ML or GenAI container.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了创建和运行 ML 或 GenAI 容器所涉及的各种组件。
- en: '![Figure 2.4 – A typical GenAI container image](img/B31108_02_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 典型的 GenAI 容器镜像](img/B31108_02_04.jpg)'
- en: Figure 2.4 – A typical GenAI container image
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 典型的 GenAI 容器镜像
- en: 'At the top layers, the container encapsulates various software libraries, deep
    learning frameworks, and user-supplied code. The next set of layers contains hardware-layer-specific
    libraries to interact with GPUs or custom accelerators on the host. The container
    runtime can be used to launch the containers from the container image. Let’s dive
    into the significant benefits of using containers for GenAI workloads:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器的最上层，封装了各种软件库、深度学习框架和用户提供的代码。下一组层包含硬件层特定的库，用于与主机上的 GPU 或定制加速器交互。可以使用容器运行时从容器镜像启动容器。让我们深入探讨使用容器处理
    GenAI 工作负载的显著优势：
- en: '**Dependency management**: This could become crucial due to evolving frameworks
    and interdependencies on specific versions. With containers, we can encapsulate
    the GenAI application code and its dependencies in a container image and use it
    on a developer machine or in the test/production environment consistently.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖管理**：由于框架和版本间的相互依赖，依赖管理可能变得至关重要。通过容器，我们可以将 GenAI 应用程序代码及其依赖项封装在容器镜像中，并在开发者机器或测试/生产环境中始终如一地使用。'
- en: '**Resource access**: GenAI/ML apps are computationally intensive, need access
    to single or multiple GPUs or custom accelerators, and adjust resource allocations
    dynamically based on the workload demand. Containers allow for fine-grained control
    over resource allocation, enabling efficient utilization of the available resources
    and avoiding noisy neighbor situations. Containers can also be scaled horizontally
    or vertically to handle increased demand in the applications.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源访问**：GenAI/ML 应用计算密集型，需访问单个或多个 GPU 或定制加速器，并根据工作负载需求动态调整资源分配。容器允许对资源分配进行细粒度控制，能够高效利用可用资源，并避免邻居噪声问题。容器还可以水平或垂直扩展，以应对应用程序需求的增加。'
- en: '**Model versioning and updates**: Managing different versions of the model
    and keeping respective dependencies up to date without disrupting applications
    could be challenging. With containers, different images can be created and versioned,
    making it easy to track changes, manage different model versions, and seamlessly
    execute rollbacks if needed. We will also explore how to use container orchestration
    engines to automate these updates later, in [*Chapter 11*](B31108_11.xhtml#_idTextAnchor145).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型版本控制和更新**：管理模型的不同版本并保持相关依赖项更新而不干扰应用程序可能是一个挑战。使用容器，可以创建和版本化不同的镜像，便于跟踪更改、管理不同的模型版本，并在需要时无缝地执行回滚。我们还将探讨如何使用容器编排引擎来自动化这些更新，稍后将在
    [*第 11 章*](B31108_11.xhtml#_idTextAnchor145)中进行详细介绍。'
- en: '**Security**: Protecting data during the training and inference stages is crucial
    when developing GenAI apps. With containers, we can enforce strict access controls
    and policies for data access, minimize the attack surface by including only the
    necessary components in the container image, and they also provide a layer of
    isolation between the containers and the underlying host.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：在开发GenAI应用时，保护训练和推理阶段的数据至关重要。通过使用容器，我们可以为数据访问实施严格的访问控制和策略，减少攻击面，只在容器镜像中包含必要的组件，还可以在容器与底层主机之间提供一层隔离。'
- en: By providing isolated, consistent, and reproducible environments, containers
    can simplify dependency management, optimize resource efficiency, streamline model
    deployments, and improve overall system security. Container technology comprehensively
    addresses all challenges presented by GenAI application development, thus making
    it a de facto choice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供隔离、一致和可重现的环境，容器可以简化依赖管理、优化资源效率、简化模型部署并提高整体系统安全性。容器技术全面解决了GenAI应用开发中的所有挑战，因此成为事实上的首选。
- en: Building a GenAI container image
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建GenAI容器镜像
- en: Let’s get a first-hand experience of building our first GenAI container image
    and deploying it locally. To get started, we will download the model files from
    **Hugging Face** ([https://huggingface.co/](https://huggingface.co/)), an AI/ML
    platform that helps users build, deploy, and test machine learning models. Hugging
    Face operates the **Model Hub**, where developers and researchers can share thousands
    of pre-trained models. It also supports various frameworks, including TensorFlow,
    PyTorch, and ONNX.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们亲身体验构建第一个GenAI容器镜像并将其部署到本地。首先，我们将从**Hugging Face**([https://huggingface.co/](https://huggingface.co/))下载模型文件，这是一个帮助用户构建、部署和测试机器学习模型的AI/ML平台。Hugging
    Face运营着**模型中心**，开发者和研究人员可以在这里共享成千上万的预训练模型。它还支持多种框架，包括TensorFlow、PyTorch和ONNX。
- en: 'In this walk-through, we will take one of the popular open source models, **Llama
    2** ([https://llama.meta.com/llama2/](https://llama.meta.com/llama2/)), by **Meta**.
    However, you have to read and comply with the terms and conditions of the model.
    Navigate to the Hugging Face Llama model page ([https://huggingface.co/meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b))
    to request access to the model. The Llama 2 model comes in multiple sizes: 7B,
    13B, and 70B where B stands for billion parameters. The bigger the size of the
    model, the greater the resources needed to host it. Given not all personal laptops
    are equipped with specialized hardware such as GPUs, we will be using the CPU
    version of the Llama 2 model. This was made possible because of **llama.cpp**
    ([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)),
    an open source project aimed at providing an efficient and portable implementation
    of Llama models. It enables us to deploy these models on various platforms, including
    personal computers, without requiring GPUs. llama.cpp applies a custom quantization
    approach to compress the models in a GGUF format, to reduce the size and resource
    requirements.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用**Meta**的一个流行开源模型**Llama 2**([https://llama.meta.com/llama2/](https://llama.meta.com/llama2/))。不过，您需要阅读并遵守模型的条款和条件。请访问Hugging
    Face的Llama模型页面([https://huggingface.co/meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b))以请求访问该模型。Llama
    2模型有多个版本：7B、13B和70B，其中B代表十亿参数。模型的大小越大，所需的资源就越多。考虑到并非所有个人笔记本电脑都配备了专门的硬件（如GPU），我们将使用Llama
    2模型的CPU版本。这得益于**llama.cpp**([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp))，这是一个开源项目，旨在提供Llama模型的高效且可移植的实现。它使我们能够在各种平台上（包括个人电脑）部署这些模型，而无需GPU。llama.cpp采用定制量化方法，将模型压缩为GGUF格式，从而减少模型的大小和资源需求。
- en: 'An inference endpoint is created using the `5000` and accepts a JSON request
    with an input prompt, system message, and so on. Input parameters are then passed
    to the Llama model and model output is returned in the JSON format. Let’s begin
    the process:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`5000`创建推理端点，并接受包含输入提示、系统消息等的JSON请求。输入参数将传递给Llama模型，并以JSON格式返回模型输出。让我们开始这个过程：
- en: 'Install the pre-requisites:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装先决条件：
- en: '`huggingface-cli` from [https://huggingface.co/docs/huggingface_hub/en/installation](https://huggingface.co/docs/huggingface_hub/en/installation)'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从[https://huggingface.co/docs/huggingface_hub/en/installation](https://huggingface.co/docs/huggingface_hub/en/installation)安装`huggingface-cli`
- en: '`jq` from [https://jqlang.github.io/jq/download/](https://jqlang.github.io/jq/download/)'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自[https://jqlang.github.io/jq/download/](https://jqlang.github.io/jq/download/)的`jq`
- en: '**Docker Engine** from [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自[https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)的**Docker
    Engine**
- en: Authenticate to the Hugging Face platform by following the instructions at [https://huggingface.co/docs/huggingface_hub/en/guides/cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照[https://huggingface.co/docs/huggingface_hub/en/guides/cli](https://huggingface.co/docs/huggingface_hub/en/guides/cli)上的说明进行Hugging
    Face平台的身份验证。
- en: 'Download the Llama 2 model using `huggingface-cli`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`huggingface-cli`下载Llama 2模型：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following message is displayed when the model download is complete:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型下载完成后，将显示以下消息：
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create an `app.py` with our Flask code. This code block sets up a Python Flask
    app with a single route, `/predict`, that accepts HTTP POST requests. It uses
    the `llama_cpp` library to load the Llama 2 model, generates a response based
    on the input prompt and system message from the request, and returns the model’s
    response as a JSON object. You can download the `app.py` code from GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py):'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含Flask代码的`app.py`。此代码块设置了一个Python Flask应用程序，具有一个单一路由`/predict`，它接受HTTP
    POST请求。它使用`llama_cpp`库加载Llama 2模型，基于输入提示和请求中的系统消息生成响应，并将模型的响应作为JSON对象返回。你可以从GitHub下载`app.py`代码：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/app.py)：
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create a Dockerfile that packages the Python source code, Llama 2 model, and
    other dependencies. You can download the Dockerfile code from GitHub at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile):'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Dockerfile，将Python源代码、Llama 2模型和其他依赖项打包在一起。你可以从GitHub下载Dockerfile代码：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch2/Dockerfile)：
- en: '[PRE9]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Build the container image:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建容器镜像：
- en: '[PRE10]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'curl:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'curl:'
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will run the inference against our local `my-llama` container and return
    the following LLM response:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将对我们的本地`my-llama`容器进行推理，并返回以下LLM响应：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The input request contains two key attributes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输入请求包含两个关键属性：
- en: '**Prompt**: Input to the LLM'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：输入给LLM的内容'
- en: '**System message**: To set the context and guide the behavior of the LLM during
    the request'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统消息**：在请求过程中设置上下文并引导LLM的行为'
- en: Optionally, we can pass additional attributes such as `max_tokens` to limit
    the length of the generated output.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以传递额外的属性，如`max_tokens`，以限制生成输出的长度。
- en: If you notice the size of the image we built, it will be around 7+ GB. That
    is mainly attributed to the model file and other dependencies. We will explore
    techniques for reducing the size of the image and optimizing the container startup
    time later, in [*Chapter 9*](B31108_09.xhtml#_idTextAnchor113).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到我们构建的镜像大小，它大约是7+ GB。这主要归因于模型文件和其他依赖项。我们将在后续的[*第9章*](B31108_09.xhtml#_idTextAnchor113)中探索减少镜像大小和优化容器启动时间的技术。
- en: It was easy to create and run a single container instance on our local computer.
    Just imagine operating hundreds or thousands of these containers running across
    hundreds or thousands of VMs, and making sure these containers are highly available,
    scalable, load balanced, managing resource allocations, and implementing automated
    deployments and rollbacks. This is where **Container Orchestrators** come to the
    rescue.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们本地计算机上创建并运行一个单一容器实例是很容易的。试想一下，在数百或数千个虚拟机上运行这些容器，并确保这些容器高度可用、可扩展、负载均衡，管理资源分配，并实现自动化部署和回滚。这就是**容器编排器**派上用场的地方。
- en: 'Container orchestrators play a pivotal role in modern software development
    and deployment, addressing all the preceding concerns and managing containerized
    applications at scale. Some of their key benefits are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排器在现代软件开发和部署中发挥着至关重要的作用，解决了所有前述问题，并大规模管理容器化应用程序。它们的一些关键优势如下：
- en: '**High availability and fault tolerance**: They continuously monitor the health
    of the containers at regular intervals and automatically restart/replace failed
    containers, thus ensuring the desired number of containers are always up and running.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性和容错性**：它们会定期监控容器的健康状态，自动重启/替换失败的容器，从而确保所需数量的容器始终处于运行状态。'
- en: '**Scaling**: They enable automatic scaling of the applications to match the
    user load/demand. Container instances are automatically created and removed in
    response to the application load. As applications are scaled, underlying compute
    resources are also scaled to accommodate the new containers.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展性**：它们可以根据用户负载/需求自动扩展应用程序。容器实例会根据应用程序负载自动创建和删除。当应用程序进行扩展时，底层计算资源也会进行扩展，以容纳新的容器。'
- en: '**Automated deployments**: They automate the deployment of containerized applications
    across multiple hosts and rollbacks in case of any failures. We can also implement
    advanced traffic routing patterns such as canary releases and blue/green deployments
    to safely roll out new changes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化部署**：它们自动化容器化应用程序在多个主机上的部署，并在发生故障时进行回滚。我们还可以实现高级流量路由模式，如金丝雀发布和蓝绿部署，以安全地推出新的更改。'
- en: '**Load balancing**: They provide built-in load balancing to distribute incoming
    traffic across multiple container instances, improving performance and reliability.
    They can also integrate with external load-balancing solutions to load-balance
    the traffic.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：它们提供内置的负载均衡，将传入的流量分配到多个容器实例，提升性能和可靠性。它们还可以与外部负载均衡解决方案集成，以进行流量的负载均衡。'
- en: '**Service discovery**: As containers are ephemeral in nature, orchestrators
    provide service discovery features to dynamically discover the container endpoints
    to facilitate inter-service communication. They also manage the container networking,
    including IP address management, DNS resolution, and network segmentation.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务发现**：由于容器本质上是短暂的，编排工具提供服务发现功能，以动态发现容器端点，促进服务间通信。它们还管理容器网络，包括 IP 地址管理、DNS
    解析和网络分段。'
- en: '**Observability**: They can integrate with monitoring and logging tools, providing
    visibility into the health and performance of containerized applications.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**：它们可以与监控和日志工具集成，提供对容器化应用程序健康状况和性能的可视化。'
- en: '**Resource management and advanced scheduling**: They can also manage the resource
    allocation for the containers, including CPU, memory, GPUs, and so on. They enforce
    the resource limits and reservations, preventing noisy neighbor situations. Advanced
    scheduling policies can be used to schedule the applications with special hardware
    needs such as GPUs to the specific hosts.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源管理和高级调度**：它们还可以管理容器的资源分配，包括 CPU、内存、GPU 等。它们强制执行资源限制和预留，防止“吵闹邻居”问题。可以使用高级调度策略，将具有特殊硬件需求（如
    GPU）的应用程序调度到特定主机上。'
- en: 'Now that we understand the need for a container orchestrator, the next question
    is which orchestrator should we pick? There are a number of different open source
    and proprietary orchestrators on the market. Some notable ones are the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了容器编排器的必要性，下一个问题是我们应该选择哪个编排器？市场上有许多不同的开源和专有编排器。一些值得注意的编排器如下：
- en: '**Amazon Elastic Container** **Service** ([https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/))'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elastic Container** **Service** ([https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/))'
- en: '**Azure Container** **Apps** ([https://azure.microsoft.com/en-us/products/container-apps](https://azure.microsoft.com/en-us/products/container-apps))'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Container** **Apps** ([https://azure.microsoft.com/en-us/products/container-apps](https://azure.microsoft.com/en-us/products/container-apps))'
- en: '**Docker** **Swarm** ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/))'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker** **Swarm** ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/))'
- en: '**Apache** **Mesos** ([https://mesos.apache.org/](https://mesos.apache.org/))'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache** **Mesos** ([https://mesos.apache.org/](https://mesos.apache.org/))'
- en: Kubernetes ([https://kubernetes.io/](https://kubernetes.io/)), and managed Kubernetes
    offerings such as **Amazon Elastic Kubernetes** **Service** ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes ([https://kubernetes.io/](https://kubernetes.io/))，以及像**Amazon Elastic
    Kubernetes** **Service** ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))这样的托管
    Kubernetes 服务
- en: '**Google Kubernetes Engine (****GKE)** ([https://cloud.google.com/kubernetes-engine](https://cloud.google.com/kubernetes-engine))'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Kubernetes Engine (GKE)** ([https://cloud.google.com/kubernetes-engine](https://cloud.google.com/kubernetes-engine))'
- en: '**Azure Kubernetes Service (****AKS)** ([https://azure.microsoft.com/en-us/products/kubernetes-service](https://azure.microsoft.com/en-us/products/kubernetes-service))'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Kubernetes Service (****AKS)** ([https://azure.microsoft.com/en-us/products/kubernetes-service](https://azure.microsoft.com/en-us/products/kubernetes-service))'
- en: '**Red Hat** **OpenShift** ([https://www.redhat.com/en/technologies/cloud-computing/openshift](https://www.redhat.com/en/technologies/cloud-computing/openshift))'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Red Hat** **OpenShift** ([https://www.redhat.com/en/technologies/cloud-computing/openshift](https://www.redhat.com/en/technologies/cloud-computing/openshift))'
- en: In this section, we learned about the typical challenges of building GenAI models
    and how we can use container technology to package GenAI code/models, AI/ML frameworks,
    hardware-specific libraries, and other dependencies into an image for consistency,
    reusability, and portability. We also built our first GenAI container image using
    the open source Llama 2 model and ran inference by deploying it locally. Finally,
    we discussed the challenges of managing containerized applications at scale and
    how container orchestrators such as K8s can solve those. In the next section,
    let’s dive into K8s and its architecture, and why it is a great fit to run GenAI
    models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了构建GenAI模型的典型挑战，以及如何利用容器技术将GenAI代码/模型、AI/ML框架、硬件特定库和其他依赖项打包成镜像，以确保一致性、可重用性和可移植性。我们还使用开源Llama
    2模型构建了我们的第一个GenAI容器镜像，并通过在本地部署运行推理。最后，我们讨论了大规模管理容器化应用程序的挑战，以及K8s等容器编排工具如何解决这些问题。在下一节中，让我们深入了解K8s及其架构，看看它为什么非常适合运行GenAI模型。
- en: What is Kubernetes (K8s)?
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Kubernetes（K8s）？
- en: Kubernetes, commonly referred to as K8s, is an open source container orchestration
    platform that automates the deployment, scaling, and management of containerized
    applications. It is the most widely used container orchestration platform ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))
    and has become the de facto choice for many enterprises to run a wide variety
    of workloads.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes，通常称为K8s，是一个开源的容器编排平台，能够自动化容器化应用程序的部署、扩展和管理。它是最广泛使用的容器编排平台 ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))，并且已成为许多企业运行各种工作负载的事实标准。
- en: Originally developed by Google engineers Joe Beda, Brendan Burns, and Craig
    McLuckie in 2014 and now maintained by the **Cloud Native Computing Foundation**
    (**CNCF**), its name came from the Ancient Greek for a pilot or helmsman (the
    person at the helm who steers the ship). It has become the second largest open
    source project in the world, after Linux, and is the primary orchestrator tool
    for 71% of Fortune 100 companies ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/)).
    According to Gartner’s *The CTO’s Guide to Containers and Kubernetes* ([https://www.gartner.com/en/documents/5128231](https://www.gartner.com/en/documents/5128231)),
    by 2027, more than 90% of global organizations will be running containerized applications
    in production.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes最初由Google工程师Joe Beda、Brendan Burns和Craig McLuckie于2014年开发，现在由**云原生计算基金会**（**CNCF**）维护，其名称来源于古希腊语，意为“领航员”或“舵手”（负责掌舵的人）。它已经成为全球第二大开源项目，仅次于Linux，并且是71%财富100强公司使用的主要编排工具
    ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))。根据Gartner的*《CTO的容器与Kubernetes指南》*
    ([https://www.gartner.com/en/documents/5128231](https://www.gartner.com/en/documents/5128231))，到2027年，全球90%以上的组织将在生产环境中运行容器化应用程序。
- en: 'Some notable factors in K8s becoming so popular are as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes（K8s）变得如此受欢迎的几个显著因素如下：
- en: '**Rich community and ecosystem**: With 77K+ contributors from 44 different
    countries and contributed to by 8K+ companies, K8s has the most vibrant and active
    community ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/)).
    The CNCF survey indicates that Kubernetes-related projects (such as Helm, Prometheus,
    and Istio) are also widely adopted, further strengthening its ecosystem.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丰富的社区和生态系统**：K8s拥有来自44个不同国家的77K+贡献者，并由8K+公司贡献，是最具活力和活跃的社区 ([https://www.cncf.io/reports/kubernetes-project-journey-report/](https://www.cncf.io/reports/kubernetes-project-journey-report/))。CNCF的调查表明，与Kubernetes相关的项目（如Helm、Prometheus和Istio）也得到了广泛采用，进一步加强了其生态系统。'
- en: '**Comprehensive features**: K8s offers a rich set of features including automated
    rollouts and rollbacks, self-healing, horizontal scaling, service discovery, and
    load balancing. These capabilities make it a versatile and powerful tool for managing
    containerized applications.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全面的功能**：K8s提供了一系列丰富的功能，包括自动化部署与回滚、自愈、横向扩展、服务发现和负载均衡等。这些功能使其成为管理容器化应用程序的多功能和强大工具。'
- en: '**Portability**: K8s abstracts away the underlying infrastructure, enabling
    applications to be deployed consistently whether on-premises, in public, private,
    or hybrid clouds, or edge locations.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性**：K8s抽象化了底层基础设施，使得应用程序可以在本地、公共云、私有云、混合云或边缘位置一致地进行部署。'
- en: '**Managed K8s services**: The availability of the managed K8s offerings from
    major cloud providers such as Amazon EKS, GKE, and AKS has significantly lowered
    the barrier to entry. These offerings take away the operational complexities of
    running and operating K8s clusters, allowing enterprises to focus on their core
    business objectives.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托管K8s服务**：主要云服务提供商如Amazon EKS、GKE和AKS提供的托管K8s服务大大降低了入门门槛。这些服务去除了运行和操作K8s集群的操作复杂性，使企业能够专注于核心业务目标。'
- en: '**Strong governance**: K8s has been governed by CNCF since 2016, which fosters
    collaborative development and community participation, enabling contributions
    from a diverse group of developers, organizations, and end users. It follows an
    open source model with well-defined governance structures, including **special
    interest groups** (**SIGs**) that focus on specific areas of development and operations.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强大的治理**：自2016年以来，K8s由CNCF进行治理，这促进了协作开发和社区参与，使来自不同开发者、组织和最终用户的贡献得以实现。它遵循开源模式，具有明确的治理结构，包括关注特定开发和操作领域的**特别兴趣小组**（**SIGs**）。'
- en: '**Declarative configuration**: K8s uses a declarative approach for configuration
    management, allowing users to define the desired state of their applications and
    infrastructure using YAML or JSON files. As depicted in *Figure 2**.5*, K8s controllers
    continuously monitor the current state of a resource and automatically reconcile
    to match the desired state, simplifying operations and ensuring consistency.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声明式配置**：K8s采用声明式的方法进行配置管理，允许用户使用YAML或JSON文件定义应用程序和基础设施的期望状态。如*图2.5*所示，K8s控制器持续监控资源的当前状态，并自动进行调整，以匹配期望状态，从而简化操作并确保一致性。'
- en: '![Figure 2.5 – How K8s controllers work](img/B31108_02_05.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – K8s控制器的工作原理](img/B31108_02_05.jpg)'
- en: Figure 2.5 – How K8s controllers work
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – K8s控制器的工作原理
- en: '**Extensibility**: This is probably the most significant reason why K8 became
    so popular. K8s is designed with a modular architecture, supporting custom plugins
    and extensions through well-defined APIs. This enabled developers and companies
    to extend or customize the K8s functionality without modifying the upstream code,
    fostering innovation and adaptability.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：这可能是K8如此受欢迎的最重要原因。K8s设计采用模块化架构，支持通过明确定义的API进行自定义插件和扩展。这使得开发者和公司能够扩展或定制K8s功能，而无需修改上游代码，促进了创新和适应性。'
- en: Next, let’s look at the architecture of Kubernetes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看一下Kubernetes的架构。
- en: Kubernetes architecture
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: 'K8s architecture is based on running clusters that allow your applications/containers
    to run across multiple hosts. Each cluster consists of two types of nodes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: K8s架构基于运行集群，使您的应用程序/容器能够跨多个主机运行。每个集群由两种类型的节点组成：
- en: '**Control plane**: The K8s control plane is the brain behind cluster operations.
    It consists of critical components such as kube-apiserver, etcd, the scheduler
    manager, the controller manager, and the cloud controller manager.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面**：K8s控制平面是集群操作的“大脑”。它由关键组件组成，如kube-apiserver、etcd、调度管理器、控制器管理器和云控制器管理器。'
- en: '**Data plane/worker nodes**: The K8s data plane running on the worker nodes
    is composed of several key components, such as kube-proxy, the kubelet, and the
    **Container Network Interface** (**CNI**) plugin.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据平面/工作节点**：K8s数据平面运行在工作节点上，由多个关键组件组成，如kube-proxy、kubelet和**容器网络接口**（**CNI**）插件。'
- en: '*Figure 2**.6* depicts the high-level K8s cluster architecture. You will notice
    kube-apiserver is the frontend of the K8s control plane and interacts with the
    other control plane components, such as controller managers, etcd, and so on,
    to fulfil the requests. K8s worker nodes host several key components, responsible
    for taking instructions from kube-apiserver, executing them on the worker node,
    and reporting back on the status.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*Figure 2**.6* 描述了高级别的K8s集群架构。您会注意到kube-apiserver是K8s控制平面的前端，并与其他控制平面组件（如控制器管理器、etcd等）交互，以满足请求。K8s工作节点托管多个关键组件，负责从kube-apiserver接收指令，在工作节点上执行它们，并报告状态。'
- en: '![Figure 2.6 – Kubernetes cluster architecture](img/B31108_02_06.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 2.6 – Kubernetes集群架构](img/B31108_02_06.jpg)'
- en: Figure 2.6 – Kubernetes cluster architecture
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – Kubernetes集群架构
- en: Control plane components
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制平面组件
- en: 'The primary components of a control plane are as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '控制平面的主要组件如下所示:'
- en: '**kube-apiserver**: This serves as the entry point or frontend into the K8s
    cluster and the central management component that exposes the K8s API. K8s users,
    administrators, and other components use this API to communicate with the cluster.
    It also communicates with the etcd component to save the state of K8s objects.
    It also handles authentication and authorization, validation, and request processing,
    and communicates with other control plane and data plane components to manage
    the clusters’ states.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-apiserver**：这充当K8s集群的入口点或前端，并且是暴露K8s API的中央管理组件。K8s用户、管理员和其他组件使用此API与集群通信。它还与etcd组件通信以保存K8s对象的状态。它还处理认证和授权、验证和请求处理，并与其他控制平面和数据平面组件通信，以管理集群状态。'
- en: '**etcd**: This is the distributed key-value store and is used as the K8s backing
    store for all cluster state. It stores the configuration data of all K8s objects
    and any updates made to them and ensures the cluster state is always reliable
    and accessible. It’s critical to take regular backups of the etcd database so
    that clusters can be restored in case of any disruptions.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd**：这是分布式键值存储，用作所有集群状态的K8s后端存储。它存储所有K8s对象的配置数据和对它们进行的任何更新，并确保集群状态始终可靠和可访问。定期备份etcd数据库至关重要，以便在任何中断情况下恢复集群。'
- en: '**kube-controller-manager**: This is responsible for managing various controllers
    in the cluster. This includes the default upstream controllers and any custom-built
    ones. Some examples include the following:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-controller-manager**：负责管理集群中的各种控制器。这包括默认的上游控制器和任何定制构建的控制器。一些示例包括以下内容：'
- en: '**Deployment controller**: This watches for K8s deployment objects and manages
    the updates to K8s Pods.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Deployment controller**：监视K8s部署对象，并管理对K8s Pod的更新。'
- en: '**kube-scheduler**: This is responsible for scheduling the K8s Pods on the
    worker nodes. It monitors the API server for newly created Pods and assigns a
    worker node based on the resource availability, scheduling requirements defined
    in the Pod configuration such as nodeSelectors, Pod/node affinity, topology spreads,
    and so on. When unable to schedule a Pod due to resource exhaustion and so on,
    it will mark the Pods as *Pending* so that other operational add-ons such as the
    cluster autoscaler can kick in and add/remove compute capacity (worker nodes)
    to the cluster.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-scheduler**：负责在工作节点上调度K8s Pod。它监视API服务器以获取新创建的Pod，并根据资源可用性、Pod配置中定义的调度要求（如nodeSelectors、Pod/node亲和性、拓扑传播等）分配工作节点。当由于资源耗尽等原因无法调度Pod时，它将标记Pod为*Pending*，以便其他操作附加组件（如集群自动缩放器）可以介入并向集群添加/删除计算容量（工作节点）。'
- en: '**cloud-controller-manager**: This manages the cloud-specific controllers to
    handle the cloud provider API calls for resource management. It’s the gateway
    to the Cloud Provider API from the K8s core and is responsible for creating and
    managing cloud-provider-specific resources (such as nodes, LoadBalancers, and
    so on) based on changes to K8s objects (such as nodes, Services, and so on). Some
    examples include the following:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cloud-controller-manager**：管理云特定控制器，处理云提供商API调用以进行资源管理。它是从K8s核心到云提供商API的网关，负责根据K8s对象的更改（如节点、服务等）创建和管理云提供商特定的资源（如节点、负载均衡器等）。一些示例包括以下内容：'
- en: '**Node controller**: This is responsible for monitoring the health of the worker
    nodes and handling the addition or removal of nodes in the cluster'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Node controller**：负责监控工作节点的健康状况，并处理集群中节点的添加或删除。'
- en: '**Service controller**: This watches for service and node object changes, and
    creates, updates, and deletes cloud provider load balancers accordingly'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务控制器**：它监视服务和节点对象的变化，并相应地创建、更新和删除云提供商负载均衡器。'
- en: Data plane components
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据平面组件
- en: 'The primary components of a data plane are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面的主要组件如下：
- en: '**kubelet**: This is an agent that runs on every worker node in the cluster.
    It’s responsible for taking instructions from kube-apiserver, executing them on
    the respective worker node, and reporting the updates on the node components back
    to the cluster control plane. It interacts with other node components such as
    the container runtime to launch container processes, the CNI plugin to set up
    the container networking, and CSI plugins to manage the persistent volumes and
    so on.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kubelet**：这是一个代理程序，运行在集群中的每个工作节点上。它负责从 kube-apiserver 接收指令，在相应的工作节点上执行这些指令，并将节点组件的更新报告回集群控制平面。它与其他节点组件互动，如容器运行时以启动容器进程，CNI
    插件以设置容器网络，以及 CSI 插件以管理持久化存储卷等。'
- en: '**kube-proxy**: This is a network proxy that runs on each worker node in the
    cluster, implementing the K8s service concept. It maintains the network routing
    rules on the worker node, to allow the network communications to and from your
    Pods from within/outside the cluster. It uses the operating system packet filtering
    layer, such as IP tables, IPVS, and so on, to route the traffic to other endpoints
    in the cluster.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-proxy**：这是一个网络代理，运行在集群中的每个工作节点上，实现了 K8s 服务概念。它维护工作节点上的网络路由规则，允许集群内外的网络通信到达或离开你的
    Pods。它使用操作系统的包过滤层，如 IP tables、IPVS 等，将流量路由到集群中的其他端点。'
- en: '**Container runtime**: containerd is the de facto container runtime responsible
    for launching the containers on the worker node. It is responsible for managing
    the lifecycle of containers in the K8s environment. K8s also supports other container
    runtimes such as CRI-O.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：containerd 是事实上的容器运行时，负责在工作节点上启动容器。它负责管理 K8s 环境中容器的生命周期。K8s 还支持其他容器运行时，如
    CRI-O。'
- en: Apart from these components, it’s essential to deploy additional add-on software
    on the K8s cluster for production operations. These add-ons add capabilities such
    as monitoring, security, and networking.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些组件外，在 K8s 集群中部署额外的附加软件对于生产操作是至关重要的。这些附加组件提供监控、安全和网络等功能。
- en: Add-on software components
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附加软件组件
- en: 'Here are a few examples of add-on software:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些附加软件的示例：
- en: '**CNI plugin**: This is a software add-on that implements container network
    specifications. They adhere to the K8s networking tenets and are responsible for
    allocating IP addresses to K8s Pods ([https://kubernetes.io/docs/concepts/workloads/pods/](https://kubernetes.io/docs/concepts/workloads/pods/))
    and enabling them to communicate with each other within the cluster. Popular add-ons
    in this space are Cilium ([https://github.com/cilium/cilium](https://github.com/cilium/cilium)),
    Calico ([https://github.com/projectcalico/calico](https://github.com/projectcalico/calico)),
    and Amazon VPC CNI ([https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNI 插件**：这是一个实现容器网络规范的软件附加组件。它们遵循 K8s 网络原则，并负责为 K8s Pods 分配 IP 地址（[https://kubernetes.io/docs/concepts/workloads/pods/](https://kubernetes.io/docs/concepts/workloads/pods/)），并使它们能够在集群内相互通信。这个领域的流行附加组件有
    Cilium（[https://github.com/cilium/cilium](https://github.com/cilium/cilium)）、Calico（[https://github.com/projectcalico/calico](https://github.com/projectcalico/calico)）和
    Amazon VPC CNI（[https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)）。'
- en: '**CSI plugin**: This is a software add-on that implements container storage
    interface specifications. They are responsible for providing persistent storage
    volumes to K8s Pods and managing the lifecycle of those volumes. A couple of notable
    add-ons are Amazon EBS CSI driver ([https://github.com/kubernetes-sigs/aws-ebs-csi-driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver))
    and Portworx CSI Driver ([https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi](https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi)).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CSI 插件**：这是一个实现容器存储接口（CSI）规范的软件附加组件。它们负责为 K8s Pods 提供持久存储卷，并管理这些卷的生命周期。一些著名的附加组件包括
    Amazon EBS CSI 驱动程序（[https://github.com/kubernetes-sigs/aws-ebs-csi-driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)）和
    Portworx CSI 驱动程序（[https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi](https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi)）。'
- en: '**CoreDNS**: This is an essential software add-on that provides DNS resolution
    within the cluster. Containers launched in K8s worker nodes automatically include
    this DNS server in their DNS searches.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CoreDNS**：这是一个必不可少的软件附加组件，用于提供集群内的 DNS 解析。在 K8s 工作节点中启动的容器会自动将此 DNS 服务器包含在其
    DNS 查询中。'
- en: '**Monitoring plugins**: This is a software add-on that provides observability
    into the cluster infrastructure and workloads. They extract essential observability
    details such as logs, metrics, and traces and write to monitoring platforms such
    as Prometheus, Amazon CloudWatch, Splunk, Datadog, and New Relic.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控插件**：这是一个软件附加组件，提供对集群基础设施和工作负载的可观察性。它们提取关键的可观察性细节，如日志、指标和跟踪，并将其写入监控平台，如
    Prometheus、Amazon CloudWatch、Splunk、Datadog 和 New Relic。'
- en: '**Device plugins**: Modern AI/ML apps use specialized hardware devices such
    as GPUs from NVIDIA, Intel, and AMD and custom accelerators from Amazon, Google,
    and Meta. K8s provides a device plugin framework that you can use to advertise
    system hardware resources to the kubelet and control plane so that you can make
    scheduling decisions based on their availability.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备插件**：现代 AI/ML 应用程序使用诸如 NVIDIA、Intel 和 AMD 的 GPU，以及 Amazon、Google 和 Meta
    的定制加速器等专用硬件设备。K8s 提供了一个设备插件框架，您可以使用它向 kubelet 和控制平面广播系统硬件资源，以便根据这些资源的可用性做出调度决策。'
- en: This is not an exhaustive list of all K8s components and add-ons. We will dive
    into AI/ML-related add-ons in a later part of the book.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是 K8s 所有组件和附加组件的详尽列表。我们将在本书的后续部分深入探讨与 AI/ML 相关的附加组件。
- en: In this section, we dove into K8s architecture, learned about various control
    plane and data plane components, and explored the advantages of the K8s platform
    and why it became the de facto standard in the community. Let’s understand why
    K8s is a great fit for running GenAI models next.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了 K8s 架构，了解了各种控制平面和数据平面组件，探索了 K8s 平台的优势以及为什么它成为社区中的事实标准。接下来，我们将了解为什么
    K8s 非常适合运行 GenAI 模型。
- en: Why K8s is a great fit for GenAI models
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 K8s 非常适合运行 GenAI 模型
- en: Now that we understand the K8s architecture, its components, and the advantages
    of the platform, let’s discuss how we apply those to solve common challenges with
    operating GenAI models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经理解了 K8s 架构、其组件以及平台的优势，那么接下来让我们讨论如何应用这些优势来解决运行 GenAI 模型时常见的挑战。
- en: Challenges of running GenAI models
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 GenAI 模型的挑战
- en: 'Some common challenges of running GenAI models are as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 GenAI 模型时的一些常见挑战如下：
- en: '**Computational requirements**: GenAI models are increasingly becoming large
    and complex, thus requiring substantial computational resources, including GPUs,
    TPUs, and custom accelerators for training and inference. Managing these resources
    efficiently is crucial to ensure performance and cost-efficiency.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算要求**：GenAI 模型越来越大且复杂，因此需要大量的计算资源，包括 GPU、TPU 和用于训练和推理的定制加速器。有效管理这些资源对于确保性能和成本效率至关重要。'
- en: '**Scalability**: As the demand for AI/ML services increases, scaling GenAI
    models to handle the demand is essential. This requires seamless scaling of computational
    resources without sacrificing the performance and cost of the models.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：随着 AI/ML 服务需求的增加，扩展 GenAI 模型以应对需求至关重要。这要求能够无缝扩展计算资源，同时不牺牲模型的性能和成本。'
- en: '**Observability**: As GenAI models proliferate, it’s critical to understand
    their performance by monitoring both business-level KPIs and the health of the
    overall system using logs and metrics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**：随着 GenAI 模型的普及，了解它们的性能变得至关重要，这需要通过日志和指标监控业务级别的关键绩效指标（KPI）以及整个系统的健康状况。'
- en: '**Data management**: GenAI models rely on vast amounts of data for both training
    and inference. Data preparation, security, and management are critical in increasing
    the model’s accuracy and performance.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据管理**：GenAI 模型依赖于大量的数据进行训练和推理。数据的准备、安全性和管理对提高模型的准确性和性能至关重要。'
- en: '**Deployment complexity**: As we learned earlier in this chapter, all GenAI
    models require custom frameworks, plugin libraries, and other dependencies to
    deploy them. This complexity can lead to deployment issues, delays, and increased
    errors.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署复杂性**：正如我们在本章前面所学，所有 GenAI 模型都需要自定义框架、插件库和其他依赖项来进行部署。这种复杂性可能导致部署问题、延迟和错误增多。'
- en: K8s advantages
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K8s 优势
- en: 'K8s offers several advantages for addressing the challenges of running GenAI
    models, such as the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 提供了若干优势，能够帮助应对运行 GenAI 模型的挑战，具体如下：
- en: '**Efficient resource management**: K8s has a robust resource management system
    built into kube-scheduler. It automates the distribution of K8s Pods into the
    worker nodes while meeting different scheduling requirements/constraints. Schedulers
    can be configured to operate in lowest-cost, random, or bin-pack modes for flexibility.
    With the K8s extensibility, you can develop custom schedulers and use them for
    scheduling workloads. A common application of this is to schedule training or
    inference workloads on devices with custom devices such as AWS Trainium and Inferentia.
    By using K8s, we can implement dynamic resource allocation based on model requirements,
    thus optimizing costs and improving performance.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效的资源管理**：K8s 具有强大的资源管理系统，内置于 kube-scheduler 中。它自动将 K8s Pod 分配到工作节点，同时满足不同的调度要求/约束条件。调度器可以配置为以最低成本、随机或按
    bin-pack 模式进行操作，以提供灵活性。凭借 K8s 的扩展性，您可以开发自定义调度器并用于工作负载的调度。一个常见的应用场景是将训练或推理工作负载调度到具有自定义设备的设备上，例如
    AWS Trainium 和 Inferentia。通过使用 K8s，我们可以根据模型要求实现动态资源分配，从而优化成本并提高性能。'
- en: '**Seamless scalability**: Training or fine-tuning a GenAI model requires a
    significant number of computational resources. Inference endpoints of these models
    also need to scale horizontally based on the workload demand. This will be achieved
    seamlessly using K8s autoscaling mechanisms such as **Horizontal Pod Autoscaling**
    (**HPA**) ([https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)),
    **Vertical Pod Autoscaling** (**VPA**) ([https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)),
    and **Cluster Autoscaling** ([https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/)).
    HPA automatically scales a workload resource (such as a Deployment or StatefulSet)
    to match the workload demand. It does this by creating and deploying new K8s Pods
    in response to the demand. VPA automatically adjusts the resource limits (such
    as CPU, memory, and so on) of Pods to match their actual usage. It helps in optimizing
    resource allocation, ensuring workloads are run efficiently. Cluster autoscaling
    is responsible for ensuring the right number of resources are attached to the
    cluster at all times. We will take a deeper look at these mechanisms in [*Chapter
    6*](B31108_06.xhtml#_idTextAnchor075).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无缝的可扩展性**：训练或微调 GenAI 模型需要大量的计算资源。这些模型的推理端点也需要根据工作负载需求进行水平扩展。这将通过使用 K8s 自动扩展机制无缝实现，如
    **水平 Pod 自动扩展** (**HPA**) ([https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/))，**垂直
    Pod 自动扩展** (**VPA**) ([https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler))，和
    **集群自动扩展** ([https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/))。HPA
    会自动扩展工作负载资源（如 Deployment 或 StatefulSet），以匹配工作负载需求。它通过创建和部署新的 K8s Pods 来响应需求。VPA
    会自动调整 Pod 的资源限制（如 CPU、内存等），以匹配其实际使用情况。它有助于优化资源分配，确保工作负载的高效运行。集群自动扩展负责确保集群始终附加了正确数量的资源。我们将在[*第六章*](B31108_06.xhtml#_idTextAnchor075)中深入探讨这些机制。'
- en: '**Extensibility**: Extensibility plays a crucial role in running GenAI workloads
    on K8s. It allows you to extend the functionality of K8s in a scalable manner
    without modifying the upstream code. We can use custom-built add-ons such as **Kubeflow**
    ([https://www.kubeflow.org/](https://www.kubeflow.org/)), an AI/ML platform that
    provides custom resources for managing ML pipelines, model training, and deployment.
    Hardware companies can also leverage this by developing device plugins to manage
    GPU resources in K8s so that GenAI training and inference workloads are scheduled
    accordingly. GenAI workloads often require specific frameworks such as **PyTorch**,
    **TensorFlow**, and **Jupyter Notebook**. We can use custom-built operators to
    integrate these frameworks and tools for seamless development and deployment.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：可扩展性在 K8s 上运行 GenAI 工作负载中起着至关重要的作用。它允许你在不修改上游代码的情况下，以可扩展的方式扩展 K8s
    的功能。我们可以使用自定义构建的附加组件，如 **Kubeflow**（[https://www.kubeflow.org/](https://www.kubeflow.org/)），一个提供管理
    ML 管道、模型训练和部署的自定义资源的 AI/ML 平台。硬件公司也可以通过开发设备插件来利用这一点，以管理 K8s 中的 GPU 资源，从而确保 GenAI
    的训练和推理工作负载能够按需调度。GenAI 工作负载通常需要特定的框架，如 **PyTorch**、**TensorFlow** 和 **Jupyter
    Notebook**。我们可以使用自定义构建的操作符，将这些框架和工具集成在一起，以实现无缝开发和部署。'
- en: '**Security**: K8s has many in-built security mechanisms to secure GenAI workloads.
    One can use **Role-Based Access Control** (**RBAC**) ([https://kubernetes.io/docs/reference/access-authn-authz/rbac/](https://kubernetes.io/docs/reference/access-authn-authz/rbac/))
    to limit access to resources, ensuring that only authorized users or applications
    can access sensitive data. K8s Secrets or external secret management solutions
    can be used to safeguard sensitive information. K8s network policies can be used
    to implement network segmentation so that only authorized Pods can access data
    stores. On top of this, we can enable security controls such as encryption, audit
    logging, security scanning, and **Pod Security Standards** (**PSS**) ([https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/))
    to ensure a robust security posture. We will explore all these features in detail
    in [*Chapter 9*](B31108_09.xhtml#_idTextAnchor113).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：K8s 有许多内置的安全机制来保护 GenAI 工作负载的安全。可以使用 **基于角色的访问控制**（**RBAC**）（[https://kubernetes.io/docs/reference/access-authn-authz/rbac/](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)）来限制对资源的访问，确保只有授权的用户或应用程序可以访问敏感数据。K8s
    Secrets 或外部秘密管理解决方案可以用于保护敏感信息。K8s 网络策略可以用于实施网络分段，以便只有授权的 Pod 能够访问数据存储。除此之外，我们还可以启用安全控制，如加密、审计日志、安全扫描以及
    **Pod 安全标准**（**PSS**）（[https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/)），以确保强大的安全态势。我们将在[*第9章*](B31108_09.xhtml#_idTextAnchor113)中详细探讨这些功能。'
- en: '**High Availability (HA) and fault tolerance**: These play a critical role
    in running GenAI workloads efficiently and reliably. Foundational model training
    often takes weeks or months. If a node or Pod fails, K8s’s in-built self-healing
    mechanism can automatically schedule the job to another node, thus minimizing
    interruptions. AI frameworks can be used along with this to implement a checkpointing
    strategy, to save the training state periodically. For model inferencing, K8s
    can automatically scale inference Pods based on the demand and recover the failed
    ones by launching replacement Pods. K8s can also perform rolling blue/green updates
    to deploy new model versions and seamlessly roll back in case of failures. We
    will take a deeper look at this topic in [*Chapter 13*](B31108_13.xhtml#_idTextAnchor176).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性（HA）和容错性**：这些在高效、可靠地运行 GenAI 工作负载中起着至关重要的作用。基础模型训练通常需要数周或数月。如果节点或 Pod
    发生故障，K8s 内置的自愈机制可以自动将任务调度到另一个节点，从而最小化中断。AI 框架可以与此一起使用，实施检查点策略，定期保存训练状态。对于模型推理，K8s
    可以根据需求自动扩展推理 Pod，并通过启动替代 Pod 来恢复失败的 Pod。K8s 还可以执行滚动蓝绿更新，以部署新版本的模型，并在出现故障时无缝回滚。我们将在[*第13章*](B31108_13.xhtml#_idTextAnchor176)中深入探讨这个话题。'
- en: '**Rich ecosystem and add-ons**: The Cloud Native Artificial Intelligence Whitepaper
    ([https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf](https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf))
    underscores the growing adoption of Kubernetes-native tools and frameworks to
    streamline the development, training, and deployment of AI models. Notable examples
    include Kubeflow and MLflow for operating end-to-end ML platforms on K8s; KServe,
    Seldon, and RayServe for model serving and scaling; and OpenLLMetry, TruLens,
    and Deepchecks for model observability. This list will continue to grow as the
    industry matures around GenAI use cases.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丰富的生态系统和附加组件**：《云原生人工智能白皮书》([https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf](https://www.cncf.io/wp-content/uploads/2024/03/cloud_native_ai24_031424a-2.pdf))
    强调了采用基于Kubernetes的工具和框架来简化AI模型开发、训练和部署的增长趋势。值得注意的例子包括Kubeflow和MLflow用于在K8s上操作端到端的ML平台；KServe、Seldon和RayServe用于模型服务和扩展；以及OpenLLMetry、TruLens和Deepchecks用于模型可观察性。随着行业围绕GenAI用例的成熟，这个列表将继续增长。'
- en: In this section, we learned about the typical challenges of operating GenAI
    models and looked at the advantages of using K8s to address them. K8s extensibility,
    efficient resource management, security, HA, and fault tolerance capabilities
    make it a great fit to run GenAI models at scale.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了操作GenAI模型的典型挑战，并查看了使用K8s解决这些挑战的优势。K8s的可扩展性、高效的资源管理、安全性、高可用性和容错能力使其非常适合大规模运行GenAI模型。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started with the evolution of compute technologies and how
    containers emerged as a standard to package and ship applications and abstract
    away infrastructure complexities for developers. We discussed the benefits of
    using containers for GenAI models and built and ran our first hello-world, GenAI
    container images. Then we looked at the challenges of running and managing containers
    at scale and how container orchestrator engines such as K8s can help simplify
    that.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从计算技术的演变和容器作为标准封装和交付应用程序以及为开发人员抽象基础设施复杂性的方式开始。我们讨论了在GenAI模型中使用容器的好处，并构建并运行了我们的第一个hello-world，GenAI容器映像。然后，我们看了运行和管理大规模容器的挑战，以及像K8s这样的容器编排引擎如何帮助简化这一过程。
- en: We dove into the high-level K8s architecture and various components that made
    up the control plane and data plane. We also learned how the extensibility, portability,
    declarative nature, and rich community behind K8s made it popular and the de facto
    container orchestrator in the market.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入探讨了K8s的高级架构和构成控制平面和数据平面的各个组件。我们还了解到K8s的可扩展性、可移植性、声明性质和强大的社区使其在市场上受欢迎，并成为事实上的容器编排器。
- en: Finally, we discussed the typical challenges of operating GenAI workloads at
    scale and how K8s is a great fit to address those challenges with its efficient
    resource management, seamless scaling, extensibility, and security capabilities.
    In the next chapter, we will explore how to build a K8s cluster in a cloud environment,
    leverage popular open source tooling to manage GenAI workloads, and deploy our
    *my-llama* container in it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了在大规模运行GenAI工作负载的典型挑战，以及K8s如何通过其高效的资源管理、无缝扩展、可扩展性和安全能力来很好地解决这些挑战。在下一章中，我们将探讨如何在云环境中构建一个K8s集群，利用流行的开源工具来管理GenAI工作负载，并在其中部署我们的*my-llama*容器。
- en: Appendix
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'Following are some great resources for in-depth training on Kubernetes:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是深入学习Kubernetes的一些优秀资源：
- en: 'Kubernetes training website: [https://kubernetes.io/training/](https://kubernetes.io/training/)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes培训网站：[https://kubernetes.io/training/](https://kubernetes.io/training/)
- en: 'Kubernetes course on the Linux Foundation: [https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&_sft_technology=kubernetes](https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&_sft_technology=kubernetes)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux基金会的Kubernetes课程：[https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&_sft_technology=kubernetes](https://training.linuxfoundation.org/full-catalog/?_sft_product_type=training&_sft_technology=kubernetes)
- en: 'Kubernetes Learning Path at KodeKloud: [https://kodekloud.com/learning-path/kubernetes/](https://kodekloud.com/learning-path/kubernetes/)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KodeKloud的Kubernetes学习路径：[https://kodekloud.com/learning-path/kubernetes/](https://kodekloud.com/learning-path/kubernetes/)
