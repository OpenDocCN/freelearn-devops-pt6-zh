- en: '*Chapter 2*: Examining the State of Infrastructure Automation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：审视基础设施自动化的现状'
- en: This chapter will look at the history of infrastructure automation, its evolution,
    and its current state. We will explore how the evolving situation in the cloud-native
    ecosystem and agile engineering practices exposes the limitations of **Infrastructure
    as Code** (**IaC**). We will also examine how control plane-based infrastructure
    automation is a cutting-edge technique that solves the limitations of IaC and
    can change the DevOps operating model to move software engineering further in
    a positive direction.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将回顾基础设施自动化的历史、发展以及现状。我们将探讨云原生生态系统和敏捷工程实践的发展，如何揭示 **基础设施即代码** (**IaC**) 的局限性。我们还将研究基于控制平面的基础设施自动化作为一种前沿技术，如何解决
    IaC 的局限性，并能够改变 DevOps 操作模型，推动软件工程向积极方向发展。
- en: 'The chapter will dive deep into the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨以下主题：
- en: The history of infrastructure automation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施自动化的历史
- en: The limitations of IaC
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IaC 的局限性
- en: The need for end-to-end automation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端到端自动化的需求
- en: Multi-cloud automation requirements
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多云自动化需求
- en: Crossplane as a cloud control plane
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crossplane 作为云控制平面
- en: Other similar projects
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他类似项目
- en: The history of infrastructure automation
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础设施自动化的历史
- en: 'The hardware purchase cycle was the critical factor influencing an organization’s
    infrastructure landscape changes during the 1990s. Back then, there was not much
    emphasis on infrastructure automation. The time spent from receiving an order
    to a physical infrastructure becoming available was much more than the effort
    spent in infrastructure setup. Individual infrastructure engineers and small teams
    automated repetitive scripting tasks without much industry-wide adaptation. Tools
    such as CFEngine, launched in 1993 for automating infrastructure configuration,
    did not have enough adoption during that decade. There was no industry-wide trend
    to invest in automation because of its minimal benefits and return on investment.
    In the 2000s, the idea of infrastructure automation slowly got traction because
    of the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年代，硬件采购周期是影响组织基础设施变更的重要因素。那时，基础设施自动化并未受到太多重视。从接到订单到物理基础设施可用所花费的时间远远超过基础设施设置所花费的精力。个别基础设施工程师和小团队会自动化重复的脚本任务，但行业内并未广泛适应。诸如
    CFEngine 这样的工具（于1993年推出，用于自动化基础设施配置）在那个十年里并未得到广泛采用。由于自动化的效益较小、投资回报率低，行业内并没有投资自动化的趋势。到了2000年代，基础设施自动化的理念逐渐获得关注，原因如下：
- en: Virtualization techniques
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟化技术
- en: The cloud
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云计算
- en: Virtualization brought in the ability to have software representation of resources
    such as memory, CPU, storage, and network using a hypervisor installed over physical
    hardware. It brought us into the era of virtual machines, where machines are abstracted
    away from the underlying physical hardware. We could have multiple virtual machines
    over single hardware. It gave us many advantages, such as lower costs, minimal
    downtime, and effective utilization of resources. But the critical advantage was
    agility in infrastructure engineering, breaking the traditional hardware purchasing
    cycles. While virtualization was there before the 2000s for a long time, it saw
    wide adoption much later because of cloud computing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化技术带来了资源的软件表示能力，例如内存、CPU、存储和网络，这些资源通过安装在物理硬件上的虚拟机监控器（Hypervisor）来实现。它将我们带入了虚拟机时代，在这个时代，机器被从底层物理硬件中抽象出来。我们可以在单一硬件上运行多个虚拟机。它带来了许多优势，如降低成本、最小化停机时间以及有效利用资源。但最关键的优势是基础设施工程的敏捷性，打破了传统的硬件采购周期。虽然虚拟化技术在2000年前就已存在，但由于云计算的普及，它在后期才得到了广泛应用。
- en: Different cloud platforms were launched during the late 2000s, adding more agility.
    We got into the **Infrastructure as a Service** (**IaaS**) era. As we increased
    our velocity of spinning new virtual machines, new problems evolved. The number
    of servers to manage was rapidly growing. Also, virtual machines are transient,
    and we needed to move, modify, and rebuild them quickly. Keeping configurations
    up to date with the preceding scenarios is challenging. We ended up with snowflake
    servers because of an error-prone, intensive human effort to manage the virtual
    machines manually. These limitations made us move toward the widespread adoption
    of infrastructure automation. New tools such as Puppet, Ansible, Chef, and Terraform
    quickly evolved, introducing IaC to manage configuration and provisioning of infrastructure
    the same way as code. Our ability to be agile in infrastructure life cycle management
    and store the relevant code in Git is the foundation for modern infrastructure
    engineering. IaC and IaaS is a deadly combination that provides unique characteristics
    for infrastructure engineering. We made consistent, repeatable, interchangeable,
    and elastic infrastructure provisioning and configuration management.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的云平台在2000年代末推出，增加了更多的灵活性。我们进入了**基础设施即服务**（**IaaS**）时代。随着我们增加创建新虚拟机的速度，新的问题也逐渐出现。需要管理的服务器数量迅速增长。此外，虚拟机是短暂的，我们需要快速迁移、修改和重建它们。保持配置与之前场景的一致性是一个挑战。由于人为管理虚拟机的错误多且繁琐，我们最终得到了“雪花服务器”。这些限制促使我们转向广泛采用基础设施自动化。像
    Puppet、Ansible、Chef 和 Terraform 这样的新工具迅速发展，引入了 IaC，用代码的方式管理配置和基础设施的供应。我们在基础设施生命周期管理中保持敏捷，并将相关代码存储在
    Git 中，这为现代基础设施工程奠定了基础。IaC 和 IaaS 是一个强强联手的组合，为基础设施工程提供了独特的特性。我们实现了可持续、可重复、可互换和弹性的基础设施供应与配置管理。
- en: 'The following diagram summarizes the evolution from scripting to IaC:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表总结了从脚本到 IaC 的进化过程：
- en: '![Figure 2.1 – Infrastructure automation evolution'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – 基础设施自动化的进化'
- en: '](img/Figure_2.1_B17830.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.1_B17830.jpg)'
- en: Figure 2.1 – Infrastructure automation evolution
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 基础设施自动化的进化
- en: The need for the next evolution
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一次进化的需求
- en: 'The cloud became the holy grail of infrastructure as we progressed further.
    Tools such as Terraform, Pulumi, AWS CloudFormation, Google Cloud Deployment Manager,
    and Azure Resource Manager became the center of IaC. While these tools did well
    to fulfill their promises, we can see that the next evolution of infrastructure
    automation is beginning to show up already. Before looking at the next phase of
    infrastructure automation, it’s essential to understand why we need to evolve
    our tools and practices around infrastructure automation. A few recent trends
    in the software industry are triggering the next phase of evolution. These trends
    are the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们不断前进，云计算成为了基础设施的“圣杯”。像 Terraform、Pulumi、AWS CloudFormation、Google Cloud
    Deployment Manager 和 Azure Resource Manager 这样的工具成为了基础设施即代码（IaC）的核心。虽然这些工具很好地履行了它们的承诺，但我们可以看到基础设施自动化的下一次进化已经开始显现。在审视基础设施自动化的下一个阶段之前，了解为什么我们需要围绕基础设施自动化进化我们的工具和实践至关重要。软件行业的一些最新趋势正在推动下一阶段的进化。这些趋势如下：
- en: The limitations of IaC
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IaC 的局限性
- en: The Kubernetes operating model for automation
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 的自动化操作模型
- en: Multi-cloud automation requirements
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多云自动化需求
- en: Let’s look at each of these trends to justify the need for progression toward
    the next phase of infrastructure automation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这些趋势，以证明我们需要向基础设施自动化的下一个阶段进化。
- en: The limitations of IaC
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IaC 的局限性
- en: 'Most of the widely used infrastructure automation tools are template-based,
    such as Terraform, Ansible, and Azure Resource Manager. They do not scale well
    from multiple points of view. It does not mean that IaC tools are not best for
    automation with all due respect. IaC tools have transformed software engineering
    positively for more than a decade. We will attempt to explain how evolving situations
    expose the weakness of template-based IaC tools and how control plane-based tools
    can be an alternative and the next evolutionary step. Let’s pick up Terraform,
    one of the most popular template-based tools, and look at the limitations. The
    following are the different limitation issues with Terraform:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数广泛使用的基础设施自动化工具都是基于模板的，如Terraform、Ansible和Azure资源管理器。从多个角度来看，它们的扩展性较差。这并不意味着IaC工具在自动化中不合适，恕我直言，IaC工具已经积极地推动了软件工程的发展超过十年。我们将尝试解释如何演变的情况暴露了基于模板的IaC工具的弱点，以及基于控制平面的工具如何作为替代方案并成为下一个演化步骤。让我们以Terraform为例，分析它的局限性。以下是Terraform的不同局限性问题：
- en: Missing self-service
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏自助服务
- en: A lack of access control
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏访问控制
- en: Parametrization pitfalls
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数化陷阱
- en: Terminology
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 术语
- en: The amount of knowledge to be possessed and processed to perform a task is called
    **cognitive load**. You will come across the term **high team cognitive load**
    in the upcoming sections, which means that a team must stretch its capacity to
    hold more knowledge than it usually does to perform day-to-day functions.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行任务所需拥有和处理的知识量称为**认知负荷**。你将在接下来的部分中遇到**高团队认知负荷**这一术语，这意味着一个团队必须扩展其处理知识的能力，以便执行日常职能，超出其通常所能承载的知识量。
- en: Missing self-service
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺乏自助服务
- en: 'With Terraform, we have too many templates abstracting thousands of cloud APIs.
    Remembering the usage of each parameter in thousands of templates is not an easy
    job. Also, infrastructure usage policies come from different teams in an organization,
    such as security, compliance, product, and architecture. Implementing Terraform
    automation involves a significant team cognitive load and centralized policy requirements.
    Hence, many organizations prefer to implement infrastructure automation with centralized
    platform teams to avoid increased cognitive load on the product team and enable
    centralized policy management. But template-based automation tools do not support
    APIs, the best way to provide platform self-service. So, we must build Terraform
    modules/libraries to create artificial team boundaries and achieve self-service.
    Modules/libraries are a weak alternative to APIs. They have a couple of problems
    in enabling platform self-service:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Terraform时，我们有太多模板抽象了成千上万的云API。记住每个参数在成千上万模板中的使用方式并不是一件容易的事。此外，基础设施使用策略来自组织中的不同团队，例如安全、合规、产品和架构。实现Terraform自动化涉及显著的团队认知负荷和集中式政策要求。因此，许多组织倾向于通过集中式平台团队来实现基础设施自动化，以避免增加产品团队的认知负荷，并实现集中式政策管理。但基于模板的自动化工具不支持API，这是提供平台自助服务的最佳方式。因此，我们必须构建Terraform模块/库来创造人工团队边界，并实现自助服务。模块/库是API的一个较弱替代方案。它们在实现平台自助服务时存在一些问题：
- en: There is a leak in cognitive load abstraction by the platform team, as using
    Terraform modules/libraries by the product team means learning Terraform fundamentals
    at least.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平台团队在认知负荷抽象方面存在泄漏，因为产品团队使用Terraform模块/库意味着至少要学习Terraform的基本知识。
- en: The team dependencies as modules and libraries require a collaborative model
    of interaction between the product and platform teams rather than a self-service
    model. It is against the modern platform topologies, hindering the agility of
    both platform and product teams.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队间作为模块和库的依赖关系需要产品团队和平台团队之间的协作互动模式，而非自助式模式。这与现代平台拓扑结构相悖，阻碍了平台和产品团队的敏捷性。
- en: Alternatively, some organizations outsource the infrastructure provisioning
    completely to the platform team. The complete centralization hinders the product
    team’s agility, with external coupling for infrastructure provisioning. Few organizations
    even attempt to decentralize the infrastructure management into the product teams.
    A complete decentralization will increase team cognitive load and the difficulty
    of aligning centralized policies across teams. The new evolution needs to find
    the middle ground with correctly abstracted self-service APIs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一些组织将基础设施的配置完全外包给平台团队。完全的集中化会妨碍产品团队的敏捷性，同时导致基础设施配置的外部耦合。很少有组织尝试将基础设施管理下放到产品团队中。完全的去中心化会增加团队的认知负担，并且增加跨团队对齐集中化政策的难度。新的发展需要在正确抽象的自助服务API中找到折衷点。
- en: Lack of access control
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺乏访问控制
- en: As we saw in the previous section, building and using Terraform modules requires
    collaboration between multiple teams. We have access control issues by sharing
    Terraform modules with product teams for infrastructure provisioning and management.
    We cannot have precise **Role Based Access Control** (**RBAC**) on individual
    resources required by the product team, and we will leak the underlying cloud
    credentials with all the necessary permissions required by the modules. For example,
    a Terraform module to provision Cosmos DB requires Azure credentials for database
    and **Virtual Private Cloud** (**VPC**) provisioning. But the access needed for
    the product team is only to create the database, and they don’t need to modify
    the VPC directly. In addition to this, we also have version management issues
    with modules/libraries. It requires a coordinated effort between all product teams,
    creating friction on a module/library’s version upgrade. A highly interoperable
    API-based infrastructure automation abstraction can solve collaboration and access
    control issues.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中看到的，构建和使用Terraform模块需要多个团队之间的协作。通过与产品团队共享Terraform模块进行基础设施配置和管理，我们会遇到访问控制问题。我们无法对产品团队所需的单个资源实现精确的**基于角色的访问控制**（**RBAC**），并且我们将泄露所有必要权限的底层云凭证，这些权限是模块所需要的。例如，一个用于配置Cosmos
    DB的Terraform模块需要Azure凭证来配置数据库和**虚拟私有云**（**VPC**）。但产品团队所需的访问权限仅限于创建数据库，他们不需要直接修改VPC。除此之外，我们还面临模块/库的版本管理问题。它需要所有产品团队之间的协调，这在模块/库版本升级时会产生摩擦。一个高度可互操作的基于API的基础设施自动化抽象可以解决协作和访问控制问题。
- en: Parameterization pitfalls
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数化的陷阱
- en: 'Parameterization pitfalls are one of the general issues with any template-based
    solution, be it an infrastructure automation tool or otherwise. We create parameter
    placeholders for variables with changing values in any template-based solution.
    These solutions are easy to implement, understand, and maintain. They work well
    if we are operating at a small scale. When we try to scale template-based solutions,
    we end up with either one of the following issues:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化的陷阱是任何基于模板的解决方案中的常见问题，无论是基础设施自动化工具还是其他工具。我们为值变化的变量创建参数占位符，在任何基于模板的解决方案中都是如此。这些解决方案易于实现、理解和维护。如果我们在小规模操作时，它们表现得很好。然而，当我们尝试扩展基于模板的解决方案时，我们最终会遇到以下问题之一：
- en: As time passes, we will have requirements to parameterize new variables, and
    slowly, we will expose all the variables at some point in time. It will erode
    the abstraction we created using templates. Looking at any Helm chart will show
    this clearly, where almost everything is a parameter.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们将需要对新变量进行参数化，并且逐步地，我们将在某个时刻暴露所有变量。这将侵蚀我们通过模板创建的抽象。查看任何Helm图表都能清楚地看到这一点，几乎所有内容都是一个参数。
- en: We may decide to fork the main template to implement customization for a specific
    use case. Forks are challenging to keep up to date, and as the number of forks
    increases, it will be challenging to maintain the policies across the templates.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能决定分叉主模板，以实现特定用例的定制。分叉很难保持更新，随着分叉数量的增加，保持模板之间的政策一致性会变得非常困难。
- en: Parameterization is generally not a perfect abstraction when we operate at scale.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化通常不是在大规模操作时完美的抽象。
- en: Important Note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Parameterization pitfalls is a critical topic to understand in detail for DevOps
    engineers. In a later chapter, we will look at the configuration clock, a concept
    of eroding template abstractions as time passes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化的陷阱是DevOps工程师需要详细了解的关键话题。在后续章节中，我们将探讨配置时钟的概念，即随着时间推移模板抽象的侵蚀。
- en: A Kubernetes operating model for automation
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于Kubernetes的自动化操作模型
- en: 'As we saw in the previous chapter, the control theory implementation of Kubernetes
    entirely changed the IT operations around application automation. Infrastructure
    automation as well deserves the same benefits. But traditional infrastructure
    automation tools lack these attributes, as they don’t have an intact control theory
    implementation. Some of the missing features are the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中看到的，Kubernetes的控制理论实现完全改变了应用程序自动化的IT运维方式。基础设施自动化也同样值得享受这些好处。但传统的基础设施自动化工具缺乏这些属性，因为它们没有完整的控制理论实现。以下是一些缺失的特性：
- en: '**Synchronous provisioning** is a crucial scalability issue with Terraform
    or similar automation tools. The resources are provisioned in a sequence, as described
    in the dependencies with conventional automation tools. If infrastructure *A*
    depends on infrastructure *B*, we must respect it while defining the order of
    execution, and if one of the executions fails, the whole automation fails. The
    monolithic representation of infrastructure is the key concern here. With Terraform,
    the monolithic state file is the model representing infrastructure resources.
    Kubernetes-based automation can change this equation. There will be a continuous
    reconciliation to move the current state toward the expected state. Hence, we
    can efficiently manage the dependencies with no order of execution. Infrastructure
    *A* provisioning may fail initially, but continuous reconciliation will eventually
    fix the state once infrastructure *B* is available.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步配置**是Terraform或类似自动化工具中的一个重要可扩展性问题。资源按照常规自动化工具中的依赖关系依次配置。如果基础设施*A*依赖于基础设施*B*，我们在定义执行顺序时必须尊重这一点，并且如果其中一个执行失败，整个自动化将失败。基础设施的单一表示是此问题的关键所在。在Terraform中，单一的状态文件是表示基础设施资源的模型。基于Kubernetes的自动化可以改变这个等式。将会有一个持续的调节过程，将当前状态朝着预期状态推进。因此，我们可以高效地管理依赖关系，而无需执行顺序。基础设施*A*的配置可能会失败，但一旦基础设施*B*可用，持续的调节最终会修复状态。'
- en: '**Modeling team boundaries** is another missing piece with traditional tools.
    The monolithic Terraform state file is not flexible to model different team boundaries.
    In the Kubernetes-based automation model, we have resources represented as individual
    APIs that can be grouped and composed as required by any team structure. We don’t
    need to collect all pieces of automation into a single monolithic model.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建模团队边界**是传统工具中另一个缺失的环节。单一的Terraform状态文件无法灵活地建模不同的团队边界。在基于Kubernetes的自动化模型中，资源以独立的API形式呈现，可以根据任何团队结构的需求进行分组和组合。我们无需将所有自动化内容集成到一个单一的庞大模型中。'
- en: '**Drift management** is the process of maintaining the infrastructure in the
    intended state by protecting it against any unintended or unauthorized changes.
    Changing the **Identity and Access Management** (**IAM**) policy directly in the
    cloud console without changing the relevant automation code is an example of drift.
    Drift management is all about bringing it back to the authorized state. Drift
    management is impossible with no control plane continuously monitoring the infrastructure’s
    condition and performing reconciliation against the last-executed code. Achieving
    drift management with an additional external tool will add complexity and not
    solve all the issues.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**漂移管理**是通过保护基础设施免受任何非预期或未经授权的更改，保持基础设施在预期状态的过程。在云控制台中直接更改**身份与访问管理**（**IAM**）策略，而不更改相关的自动化代码，就是一种漂移的例子。漂移管理的核心是将其恢复到授权的状态。如果没有控制平面持续监控基础设施状态并执行与最后执行代码的调节，漂移管理是不可能实现的。通过额外的外部工具实现漂移管理会增加复杂性，并且不能解决所有问题。'
- en: '**Automating day 2 concerns** in a standard way is another missing piece with
    conventional tools. A Kubernetes-based automation model can provide configuration
    models to support day 2 concerns such as scaling, monitoring, and logging. Also,
    we can use standard extension points (operators) to automate any custom day 2
    problems.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以标准化方式自动化第二天问题**是传统工具中的另一个缺失环节。基于Kubernetes的自动化模型可以提供配置模型，以支持第二天问题，如扩展、监控和日志记录。此外，我们还可以使用标准扩展点（操作器）来自动化任何自定义的第二天问题。'
- en: These are a few essential perspectives on what Kubernetes-based infrastructure
    automation can bring to the table.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是基于Kubernetes的基础设施自动化所能带来的几个关键视角。
- en: Multi-cloud automation requirements
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多云自动化需求
- en: Almost all organizations of a significant size run their workloads in more than
    one cloud provider. There can be many reasons why an organization is determined
    to build its infrastructure supported by multiple cloud providers. We will not
    get into details of these factors, but we must understand the impact of multi-cloud
    on infrastructure management. Typically, a cloud provider offers managed services,
    be it basic IaaS such as Amazon EC2 or more abstracted platforms such as AWS Lambda.
    From the perspective of cloud infrastructure consumers, infrastructure automation
    is all about the provisioning and life cycle management of these managed services
    in an automated fashion after applying all the in-house policies. Organizations
    use infrastructure automation tools to build an abstraction over the cloud infrastructure
    APIs to encode all the in-house policies.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有大规模的组织都会在多个云提供商上运行其工作负载。组织决定建立由多个云提供商支持的基础设施可能有多种原因。我们不会详细讨论这些因素，但我们必须理解多云对基础设施管理的影响。通常，云提供商提供托管服务，无论是像
    Amazon EC2 这样的基础 IaaS，还是像 AWS Lambda 这样的更抽象的平台。从云基础设施消费者的角度来看，基础设施自动化就是在应用所有内部策略之后，以自动化方式进行这些托管服务的配置和生命周期管理。组织使用基础设施自动化工具在云基础设施
    API 上构建抽象，以编码所有内部策略。
- en: To support multi-cloud capability requires a lot of work, as it brings in new
    requirements. Think about the multi-cloud environment. Embedding policies into
    the automation scripts of every cloud provider is a hell of a lot of work. Even
    if we do that after making a significant effort, keeping these policies in sync
    across the automation scripts involves friction and is error-prone. A centralized
    experience in authentication, authorization, billing, monitoring, and logging
    across cloud providers will be an added advantage for an organization to provide
    a unified experience. Achieving these cross-cutting concerns with traditional
    automation tools requires a lot of custom engineering, making our platform team
    big. What we need is a centralized control plane, abstracting cross-cutting concerns
    and policies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 支持多云能力需要大量工作，因为它带来了新的需求。想一想多云环境吧。将策略嵌入到每个云提供商的自动化脚本中是一项艰巨的工作。即使我们在付出了重大努力之后完成了这项工作，保持这些策略在自动化脚本中的同步也涉及摩擦，且容易出错。跨云提供商的身份验证、授权、计费、监控和日志记录的集中式体验，将为组织提供统一的体验，这将是一个额外的优势。通过传统的自动化工具实现这些跨领域的关注点需要大量的定制化工程，使得我们的平台团队规模庞大。我们需要的是一个集中式控制平面，用于抽象跨领域的关注点和策略。
- en: 'The following figure represents how an API-driven centralized control plane
    can provide a unified experience for product teams:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了一个基于 API 的集中式控制平面如何为产品团队提供统一的体验：
- en: '![Figure 2.2 – A multi-cloud control plane'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2 – 多云控制平面'
- en: '](img/Figure_2.2_B17830.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.2_B17830.jpg)'
- en: Figure 2.2 – A multi-cloud control plane
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 多云控制平面
- en: 'Tools such as Terraform or Pulumi help with these problems to some extent,
    but they are not end-to-end automation, have scalability issues, and require custom
    engineering to build on. Also, these tools are not unbiased open source projects.
    The companies who initially created these open source projects and provided enterprise
    offerings dominate the control of the former. Now that we are all convinced that
    the next evolution of infrastructure automation is required, it’s time to define
    the attributes needed by such tools. The subsequent development of infrastructure
    automation should be a control plane-based, fully community-driven solution, powered
    by APIs. The following diagram summarizes the evolution from **Infrastructure
    as Code-** to **Central Control Plane**-based automation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 或 Pulumi 等工具在一定程度上帮助解决了这些问题，但它们并非端到端的自动化，存在可扩展性问题，并且需要定制化工程来构建。此外，这些工具也不是完全公正的开源项目。最初创建这些开源项目并提供企业级解决方案的公司主导了前者的控制。现在，我们都确信基础设施自动化的下一个演进是必需的，是时候定义此类工具所需的属性了。后续的基础设施自动化发展应该是基于控制平面的、完全由社区驱动的解决方案，依赖于
    API。下图总结了从**基础设施即代码**到**集中控制平面**基础自动化的演进：
- en: '![Figure 2.3 – The next evolution'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3 – 下一阶段的演进'
- en: '](img/Figure_2.3_B17830.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.3_B17830.jpg)'
- en: Figure 2.3 – The next evolution
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 下一阶段的演进
- en: Crossplane as a cloud control plane
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Crossplane 作为云控制平面
- en: Crossplane, a modern control plane-based infrastructure automation platform
    built on Kubernetes, matches all the attributes required for the next evolution
    of infrastructure engineering. With Crossplane, we can assemble infrastructure
    from multiple cloud providers to expose them as a high-level API. These APIs can
    provide a universal experience across teams, irrespective of the underlying cloud
    vendor. While composing the APIs for the product team, the platform team can use
    different resource granularity to suit the organization’s structure. Such carefully
    crafted APIs for infrastructure automation will facilitate self-service, multi-persona
    collaboration with precise RBAC, less cognitive load, continuous drift management,
    and dependency management with asynchronous reconciliation. Above all, the platform
    team can compose these APIs in a no-code way with configurations. Finally, we
    can have a lean platform team, as highly recommended by modern team topologies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Crossplane 是一个基于 Kubernetes 的现代控制平面基础设施自动化平台，具备了基础设施工程下一阶段所需的所有特性。通过 Crossplane，我们可以从多个云提供商那里组装基础设施，并将其作为高级
    API 暴露。这些 API 可以为不同团队提供通用的体验，而不受底层云供应商的影响。在为产品团队组合 API 时，平台团队可以根据组织的结构使用不同的资源粒度。这样精心设计的基础设施自动化
    API 将促进自服务、多角色协作、精确的 RBAC、更低的认知负担、持续的漂移管理以及异步调和的依赖管理。最重要的是，平台团队可以通过配置以无代码的方式组合这些
    API。最后，我们可以拥有一个精简的平台团队，这也是现代团队拓扑学所强烈推荐的做法。
- en: 'Crossplane is nothing but a set of custom controllers that extends Kubernetes
    for managing infrastructure from different vendors. Being built on Kubernetes
    as a new API extension, Crossplane inherits all the goodness of the Kubernetes
    operating model and can leverage the rich ecosystem of cloud-native tools. Additionally,
    this can unify the way we automate applications and infrastructure. Crossplane
    can cover end-to-end automation of both day 1 and day 2 concerns. Infrastructure
    provisioning, encoding policies, governance, and security constraints are the
    day 1 concern we can automate. We can cover drift management, upgrades, monitoring,
    and scaling on day 2\. Above all, it follows the Kubernetes model of open source
    governance through the **Cloud Native Computing Foundation** (**CNCF**). The following
    figure represents how Crossplane works with Kubernetes:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Crossplane 仅仅是一组自定义控制器，它扩展了 Kubernetes 用于管理来自不同供应商的基础设施。作为基于 Kubernetes 构建的新
    API 扩展，Crossplane 继承了 Kubernetes 操作模型的所有优点，并能利用丰富的云原生工具生态系统。此外，这也可以统一我们自动化应用程序和基础设施的方式。Crossplane
    可以覆盖端到端的自动化，包括第 1 天和第 2 天的关注点。基础设施的提供、编码策略、治理和安全约束是我们可以自动化的第 1 天关注点。第 2 天我们可以自动化漂移管理、升级、监控和扩展等任务。最重要的是，它遵循
    Kubernetes 的开源治理模型，通过**云原生计算基金会**（**CNCF**）进行管理。下图展示了 Crossplane 如何与 Kubernetes
    一起工作：
- en: '![Figure 2.4 – The Crossplane control plane'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4 – Crossplane 控制平面'
- en: '](img/Figure_2.4_B17830.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.4_B17830.jpg)'
- en: Figure 2.4 – The Crossplane control plane
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – Crossplane 控制平面
- en: To adopt platforms as a universal control plane requires a much closer look
    at open source governance and ecosystem acceptance. The following sections will
    look deep into these aspects.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要将平台作为通用控制平面进行采用，需要更深入地审视开源治理和生态系统的接受度。接下来的章节将深入探讨这些方面。
- en: A universal control plane
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个通用控制平面
- en: 'Launched in 2018 as an open source project, Crossplane took steps to become
    a universally accepted control plane. The project’s donation to CNCF in 2020 was
    the next significant step. It helped Crossplane become a foundation-driven, open
    source initiative with broader participation rather than just becoming another
    open source project. Initially, it was a sandbox project but did not stop there.
    In 2021, it was accepted as an incubating project. Above all, Crossplane is simply
    another extension to Kubernetes, already an accepted platform for application
    DevOps. It also means that the entire ecosystem of tools available for Kubernetes
    is also compatible with Crossplane. Teams can work with the existing set of tools
    without much cognitive load:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Crossplane 作为一个开源项目于 2018 年启动，迈出了成为通用控制平面的步伐。2020 年捐赠给 CNCF 是其下一步重大进展。这帮助 Crossplane
    成为一个由基金会推动的开源项目，吸引了更广泛的参与，而不仅仅是成为另一个开源项目。最初，它是一个沙箱项目，但并没有止步于此。2021 年，它被接受为孵化项目。最重要的是，Crossplane
    只是 Kubernetes 的另一个扩展，而 Kubernetes 已经是一个被广泛接受的应用 DevOps 平台。这也意味着 Kubernetes 上可用的整个工具生态系统也与
    Crossplane 兼容。团队可以在不增加太多认知负担的情况下，使用现有的工具集：
- en: '![Figure 2.5 – The journey'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5 – 旅程'
- en: '](img/Figure_2.5_B17830.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.5_B17830.jpg)'
- en: Figure 2.5 – The journey
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 旅程
- en: 'Crossplane has a few more unique attributes compelling it to be accepted as
    a universal control plane. The attributes are the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Crossplane 还有一些独特的特性，促使它被接受为一个通用的控制平面。这些特性包括：
- en: Open standards for infrastructure vendors
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施供应商的开放标准
- en: Wider participation
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更广泛的参与
- en: Cloud provider partnerships
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云服务提供商合作伙伴关系
- en: Open standards for infrastructure vendors
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础设施供应商的开放标准
- en: Crossplane uses the **Crossplane Resource Model** (**XRM**), an extension of
    the **Kubernetes Resource Model** (**KRM**), as the open standard for infrastructure
    providers. It solves issues such as naming identity, package management, and inter-resource
    references when infrastructure offerings from different vendors are consolidated
    into a single control plane. The Crossplane community has developed these standards
    to enforce how infrastructure providers can integrate into the centralized Crossplane
    control plane. The ability to compose different infrastructures in a uniform and
    no-code way has its foundation on this standardization.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Crossplane 使用 **Crossplane 资源模型**（**XRM**），它是 **Kubernetes 资源模型**（**KRM**）的扩展，作为基础设施提供商的开放标准。它解决了当来自不同供应商的基础设施服务整合到单一控制平面时，诸如命名身份、包管理和跨资源引用等问题。Crossplane
    社区已经制定了这些标准，以规范基础设施提供商如何集成到集中式 Crossplane 控制平面中。在统一且无需编码的方式下组合不同基础设施的能力，正是建立在这种标准化基础上的。
- en: Wider participation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的参与
- en: 'Upbound was the company that initially created Crossplane. They provide enterprise
    offerings for organizations that require support and additional services. But
    to become a universal control plane, Upbound cannot be the only enterprise Crossplane
    provider. Any vendor should be able to make an enterprise offering. With Crossplane
    gaining CNCF incubation status, a lot of work is happening in this area. CNCF
    and the Crossplane community have introduced something called the **Crossplane
    Conformance Program**. It’s an initiative run by CNCF ([https://github.com/cncf/crossplane-conformance](https://github.com/cncf/crossplane-conformance)).
    The idea is to create foundation governance for any vendors to pick up Crossplane
    open source, build additional features, and offer a **CNCF-certified** version.
    It is very similar to **Kubernetes-certified distribution**, a program run by
    CNCF where all vendors pick up the base Kubernetes version and offer it as a certified
    version. The Crossplane Conformance Program works on two levels:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Upbound 是最初创建 Crossplane 的公司。他们为需要支持和额外服务的组织提供企业版服务。但要成为一个通用的控制平面，Upbound 不能是唯一的企业
    Crossplane 提供商。任何供应商都应该能够提供企业版服务。随着 Crossplane 获得 CNCF 孵化状态，许多工作正在这一领域进行。CNCF
    和 Crossplane 社区推出了一项名为 **Crossplane 合规性计划** 的举措。这是由 CNCF 运营的一个项目（[https://github.com/cncf/crossplane-conformance](https://github.com/cncf/crossplane-conformance)）。其目标是为任何供应商提供基础治理，使他们能够采用
    Crossplane 开源项目，构建附加功能，并提供 **CNCF 认证** 版本。这与 **Kubernetes 认证分发版** 类似，Kubernetes
    认证分发版是 CNCF 运行的一个项目，所有供应商基于基础 Kubernetes 版本进行扩展并提供认证版本。Crossplane 合规性计划在两个层面上运行：
- en: '**Providers**: On one level, infrastructure providers will be interested in
    building respective Crossplane controllers to enable customers to use their offerings
    through Crossplane. It requires following the standards set by XRM. CNCF will
    ensure this happens by certifying the providers built by infrastructure vendors.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供者**：在一个层面上，基础设施提供者将有兴趣构建相应的 Crossplane 控制器，以便客户通过 Crossplane 使用他们的产品。这要求遵循
    XRM 所设定的标准。CNCF 将通过认证基础设施供应商构建的提供者来确保这一点。'
- en: '**Distribution**: On another level, many vendors will be interested in providing
    the Crossplane enterprise offering. The Crossplane Conformance Program enables
    this support.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分发**：在另一个层面上，许多供应商将有兴趣提供 Crossplane 企业版服务。Crossplane 合规性计划使这一支持成为可能。'
- en: Read more about the Crossplane Conformance Program at [https://github.com/cncf/crossplane-conformance/blob/main/instructions.md](https://github.com/cncf/crossplane-conformance/blob/main/instructions.md).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://github.com/cncf/crossplane-conformance/blob/main/instructions.md](https://github.com/cncf/crossplane-conformance/blob/main/instructions.md)
    阅读更多关于 Crossplane 合规性计划的信息。
- en: The cloud provider partnerships
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云服务提供商合作伙伴关系
- en: Crossplane has an excellent partnership ecosystem with all major cloud providers.
    There have been production-ready Crossplane providers for all the major cloud
    providers for quite some time now. Initially, IBM joined the Crossplane community
    and released its first version of the provider in 2020\. Similarly, AWS and Azure
    made Crossplane providers part of their code generation pipeline to ensure that
    the newest provider is available up front for all their cloud resources. Alibaba
    is experimenting with Crossplane on many of its internal initiatives and also
    has a production-ready provider. Similarly, there has been a **Google Cloud Platform**
    (**GCP**) provider managed by the community. These partnerships and community
    efforts make Crossplane a compelling, widely accepted universal control plane
    offering.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Crossplane与所有主要云服务提供商建立了出色的合作伙伴关系。现在，各大云服务提供商的生产就绪Crossplane提供者已经存在相当一段时间了。最初，IBM加入了Crossplane社区，并在2020年发布了其第一个提供者版本。同样，AWS和Azure将Crossplane提供者纳入了它们的代码生成流水线，确保最前沿的最新提供者可以用于所有云资源。阿里巴巴在其多个内部项目中实验使用Crossplane，也有一个生产就绪的提供者。同样，社区管理的**Google
    Cloud Platform**（**GCP**）提供者也在其中。这些合作伙伴关系和社区努力使得Crossplane成为一个有吸引力且广泛接受的通用控制平面方案。
- en: Other similar projects
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他类似项目
- en: 'A few other Kubernetes-based infrastructure automation projects share common
    interests and support similar use cases such as Crossplane. These projects extend
    Kubernetes with APIs and custom controllers identical to Crossplane architecture.
    This section will look at those tools to have a comprehensive comparison with
    Crossplane. The following is a list of a few such projects:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些基于Kubernetes的基础设施自动化项目与Crossplane有着共同的兴趣，并支持类似的用例。这些项目通过API和自定义控制器扩展了Kubernetes，架构与Crossplane相同。本节将介绍这些工具，以便与Crossplane进行全面的比较。以下是一些此类项目的列表：
- en: '**Kubernetes’ Service Catalog** by the open service broker enables life cycle
    management of cloud resources from Kubernetes. Like Crossplane, it works as a
    Kubernetes controller extension. But it does not have a solid framework to compose
    infrastructure recipes with policy guardrails. Also, we can’t model the API for
    different team boundaries. The open service broker Kubernetes Service Catalog
    is not designed for platform teams to build reusable recipes with encoded policies.
    Typically, this means that we have to struggle with policy enforcement and a high
    cognitive load on the teams to understand cloud offerings in detail.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes服务目录**是由开放服务代理（open service broker）提供的，旨在从Kubernetes中进行云资源的生命周期管理。像Crossplane一样，它作为Kubernetes控制器扩展工作。但它没有一个完整的框架来组合带有策略约束的基础设施配方。此外，我们无法为不同团队的边界建模API。开放服务代理的Kubernetes服务目录并不适合平台团队构建带有编码策略的可重用配方。通常，这意味着我们需要在政策执行方面面临困难，且团队需要花费较多的认知精力来详细理解云服务的内容。'
- en: '**AWS Controllers for Kubernetes (ACK)** is a Kubernetes-based extension developed
    by AWS to manage its resources from the Kubernetes cluster using controllers.
    Again, it does not have a framework to compose infrastructure recipes and model
    APIs. Also, this does not work cross-cloud and is meant to be used only with AWS.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Kubernetes控制器（ACK）**是AWS开发的基于Kubernetes的扩展，旨在通过控制器从Kubernetes集群中管理其资源。同样，它没有一个框架来组合基础设施配方和建模API。此外，它不支持跨云使用，仅适用于AWS。'
- en: The **GCP Config Connector** is a replacement developed by Google for the GCP
    service catalog. It works like ACK and inherits identical limitations. An additional
    point to note is that the GCP Config Connector is not an open source initiative.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GCP配置连接器**是由Google开发的，用来替代GCP服务目录。它的工作方式类似于ACK，并继承了相同的局限性。需要注意的一点是，GCP配置连接器并不是一个开源项目。'
- en: None of these tools cover an end-to-end automation use case or provide an ability
    to compose resources as recipes. We have already seen the limitations of Terraform,
    AWS CloudFormation, Azure Resource Manager, and similar IaC tools in detail. These
    were the motivations that the Crossplane creators had when developing such universal
    abstraction.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具都无法覆盖端到端的自动化用例，也没有提供将资源组合成配方的能力。我们已经详细了解了Terraform、AWS CloudFormation、Azure
    Resource Manager以及类似的IaC工具的局限性。这些正是Crossplane开发者在开发这种通用抽象时的动机。
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter discussed the details of limitations with IaC. We also looked at
    why it is inevitable to move toward a control plane automation in the evolving
    world of software engineering. It brings us to the end of the first part of this
    book. In summary, part one covered how Kubernetes won the war on application deployment
    automation and how the same pattern is evolving a new trend in infrastructure
    automation. The upcoming sections of the book will take us on a hands-on journey
    to learn Crossplane, Kubernetes configuration management, and ecosystem tools.
    We also will cover the different nuances and building blocks of developing state-of-the-art
    cloud infrastructure automation platforms with Crossplane.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了IaC的局限性细节。我们还探讨了在软件工程不断发展的世界中，为什么向控制平面自动化过渡是不可避免的。这也标志着本书第一部分的结束。总的来说，第一部分介绍了Kubernetes如何赢得应用部署自动化的战斗，以及如何通过相同的模式推动基础设施自动化的新趋势。接下来的章节将带领我们通过动手实践学习Crossplane、Kubernetes配置管理以及生态系统工具。我们还将讨论使用Crossplane开发先进云基础设施自动化平台的不同细微差别和构建块。
- en: In the next chapter, we will learn about automating infrastructure with Crossplane.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习如何使用Crossplane自动化基础设施。
