- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Exposing Your Pods with Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Services 暴露你的 Pods
- en: After reading the previous chapters, you now know how to deploy applications
    on Kubernetes by building Pods, which can contain one container or multiple containers
    in the case of more complex applications. You also know that it is possible to
    decouple applications from their configuration by using Pods and **ConfigMaps**
    together and that Kubernetes is also capable of storing your sensitive configurations,
    thanks to **Secret** objects.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读了前几章之后，你现在已经知道如何通过构建 Pods 来部署应用程序，这些 Pods 可以包含一个容器，或者在更复杂的应用程序中包含多个容器。你还知道，通过将
    Pods 和 **ConfigMaps** 一起使用，可以将应用程序与其配置解耦，并且 Kubernetes 也能够存储你的敏感配置，感谢 **Secret**
    对象。
- en: 'The good news is that with these three resources, you can start deploying applications
    on Kubernetes properly and get your first app running. However, you are still
    missing something important: you need to be able to expose Pods to end users or
    even to other Pods within the Kubernetes cluster. This is where Kubernetes **Services**
    comes in, and that’s the concept we’re going to discover now!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，凭借这三种资源，你可以开始在 Kubernetes 上正确地部署应用程序并让你的第一个应用程序运行。不过，你仍然缺少一些重要的东西：你需要能够将
    Pods 暴露给终端用户，甚至暴露给 Kubernetes 集群中的其他 Pods。这就是 Kubernetes **Services** 的作用，我们现在就来探索这个概念！
- en: In this chapter, we’ll learn about a new Kubernetes resource type called the
    Service. Since Kubernetes Services is a big topic with many things to cover, this
    chapter will be quite big with a lot of information. But after you master these
    Services, you’re going to be able to expose your Pods and get your end users to
    your apps!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习一种新的 Kubernetes 资源类型，称为 Service。由于 Kubernetes Services 是一个涉及众多内容的大主题，本章将包含大量信息，内容也会比较多。但一旦你掌握了这些
    Services，你就能暴露你的 Pods，并将终端用户连接到你的应用程序！
- en: Services are also a key concept to master **high availability** (**HA**) and
    redundancy in your Kubernetes setup. Put simply, it is crucial to master them
    to be effective with Kubernetes!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 服务也是掌握 **高可用性** (**HA**) 和冗余的关键概念。在 Kubernetes 配置中，简单来说，掌握它们对于有效使用 Kubernetes
    至关重要！
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要内容：
- en: Why would you want to expose your Pods?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么你需要暴露你的 Pods？
- en: The `NodePort` Service
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NodePort` Service'
- en: The `ClusterIP` Service
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClusterIP` Service'
- en: The `LoadBalancer` Service
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LoadBalancer` Service'
- en: The `ExternalName` Service type
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExternalName` Service 类型'
- en: Implementing Service readiness using probes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用探针实现服务就绪状态
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To follow along with the examples in this chapter, make sure you have the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本章中的示例，请确保你具备以下条件：
- en: A working Kubernetes cluster (whether this is local or cloud-based is of no
    importance)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个正常工作的 Kubernetes 集群（无论是本地的还是基于云的，都无关紧要）
- en: A working `kubectl` **command-line interface** (**CLI**) configured to communicate
    with the Kubernetes cluster
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个正常工作的 `kubectl` **命令行界面** (**CLI**)，已配置与 Kubernetes 集群进行通信
- en: You can download the latest code samples for this chapter from the official
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从官方 GitHub 仓库下载本章的最新代码示例，地址为 [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08)。
- en: Why would you want to expose your Pods?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么你需要暴露你的 Pods？
- en: In the previous chapters, we discussed the microservice architecture, which
    exposes your functionality through **Representational State Transfer** (**REST**)
    **application programming interfaces** (**APIs**). These APIs rely completely
    on **HyperText Transfer Protocol** (**HTTP**), which means that your microservices
    must be accessible via the web, and thus via an **Internet Protocol** (**IP**)
    address on the network.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了微服务架构，它通过 **表现层状态转移** (**REST**) **应用程序编程接口** (**APIs**) 暴露你的功能。这些
    APIs 完全依赖于 **超文本传输协议** (**HTTP**)，这意味着你的微服务必须可以通过网络访问，因此必须通过 **互联网协议** (**IP**)
    地址进行访问。
- en: While REST APIs are commonly used for communication in microservice architectures,
    it’s important to evaluate other communication protocols such as **gRPC**, particularly
    in scenarios where performance and efficiency between services are critical. gRPC,
    built on **HTTP/2** and using binary serialization (Protocol Buffers), can offer
    significant advantages in distributed systems, such as faster communication, lower
    latency, and support for streaming. Before defaulting to REST, consider whether
    gRPC might be a better fit for your system’s needs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管REST API常用于微服务架构中的通信，但在服务之间的性能和效率至关重要的场景下，评估其他通信协议，如**gRPC**，是很重要的。gRPC基于**HTTP/2**并使用二进制序列化（协议缓冲区），可以在分布式系统中提供显著的优势，例如更快的通信、更低的延迟和支持流媒体。在默认使用REST之前，考虑一下gRPC是否更适合你的系统需求。
- en: In the following section, we will learn about cluster networking in Kubernetes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习Kubernetes中的集群网络。
- en: Cluster networking in Kubernetes
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes中的集群网络
- en: 'Networking is a fundamental aspect of Kubernetes, enabling communication between
    containers, Pods, Services, and external clients. Understanding how Kubernetes
    manages networking helps ensure seamless operation in distributed environments.
    There are four key networking challenges that Kubernetes addresses:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是Kubernetes的一个基本方面，使容器、Pod、服务和外部客户端之间能够进行通信。了解Kubernetes如何管理网络有助于确保分布式环境中的顺利运行。Kubernetes解决了四个关键的网络挑战：
- en: '**Container-to-container communication**: This is solved using Pods, allowing
    containers within the same Pod to communicate through localhost (i.e., internal
    communication).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器到容器的通信**：这通过Pod来解决，允许同一个Pod中的容器通过localhost（即内部通信）进行通信。'
- en: '**Pod-to-pod communication:** Kubernetes enables communication between Pods
    across nodes through its networking model.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod之间的通信**：Kubernetes通过其网络模型实现了跨节点的Pod间通信。'
- en: '**Pod-to-service communication**: Services abstract a set of Pods and provide
    stable endpoints for communication.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod到服务的通信**：服务抽象了一组Pod，并提供稳定的端点进行通信。'
- en: '**External-to-service communication**: This allows traffic from outside the
    cluster to access Services, such as web applications or APIs.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部到服务的通信**：这允许集群外部的流量访问服务，如Web应用程序或API。'
- en: In a Kubernetes cluster, multiple applications run on the same set of machines.
    This creates challenges, such as preventing conflicts when different applications
    use the same network ports.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中，多个应用程序运行在同一组机器上。这会带来一些挑战，例如，当不同的应用程序使用相同的网络端口时，如何防止冲突。
- en: IP address management in Kubernetes
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes中的IP地址管理
- en: 'Kubernetes clusters use non-overlapping IP address ranges for Pods, Services,
    and Nodes. The network model is implemented through the following configurations:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群使用不重叠的IP地址范围来为Pod、服务和节点分配地址。网络模型通过以下配置实现：
- en: '**Pods**: A network plugin, often a **Container Network Interface** (**CNI**)
    plugin, assigns IP addresses to Pods.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pods**：一个网络插件，通常是**容器网络接口**（**CNI**）插件，为Pod分配IP地址。'
- en: '**Services**: The kube-apiserver handles assigning IP addresses to Services.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**：kube-apiserver负责为服务分配IP地址。'
- en: '**Nodes**: IP addresses for Nodes are managed by either the kubelet or the
    cloud-controller-manager.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：节点的IP地址由kubelet或cloud-controller-manager管理。'
- en: You may refer to the following website, [https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/),
    to learn more about networking in Kubernetes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下网站，了解更多Kubernetes中的网络知识：[https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/)。
- en: Now, before exploring the Pod networking and Services, let us understand some
    of the basic technologies in the Kubernetes networking space.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在探讨Pod网络和服务之前，让我们了解一下Kubernetes网络空间中的一些基础技术。
- en: Learning about network plugins
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习网络插件
- en: '**CNI** is a specification and set of libraries developed by the **Cloud Native
    Computing Foundation** (**CNCF**). Its primary purpose is to standardize the configuration
    of network interfaces on Linux containers, enabling seamless communication between
    containers and the external environment.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**CNI**是由**云原生计算基金会**（**CNCF**）开发的一个规范和一组库。其主要目的是标准化Linux容器上网络接口的配置，实现容器与外部环境之间的无缝通信。'
- en: These plugins are essential for implementing the Kubernetes network model, ensuring
    connectivity and communication within the cluster. It’s crucial to choose a CNI
    plugin that aligns with the needs and compatibility requirements of your Kubernetes
    cluster. With various plugins available in the Kubernetes ecosystem, both open-source
    and closed-source, selecting the appropriate plugin is vital for smooth cluster
    operations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些插件对于实现 Kubernetes 网络模型至关重要，确保集群内的连接性和通信。选择一个与您的 Kubernetes 集群需求和兼容性要求相匹配的
    CNI 插件非常重要。在 Kubernetes 生态系统中有各种插件，包括开源和闭源插件，选择合适的插件对于确保集群顺利运行至关重要。
- en: 'Here’s a concise list of CNI plugins with a brief description of each:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 CNI 插件的简洁列表，每个插件的简要描述：
- en: '**Open-source CNI plugins:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**开源 CNI 插件：**'
- en: '**Calico**: Provides networking and network security with a focus on Layer
    3 connectivity and fine-grained policy controls.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Calico**：提供网络和网络安全，重点是第 3 层连接性和细粒度的策略控制。'
- en: '**Flannel**: A simple CNI that offers basic networking for Kubernetes clusters,
    ideal for overlay networks.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flannel**：一个简单的 CNI，为 Kubernetes 集群提供基本的网络功能，适合覆盖网络。'
- en: '**Weave Net**: Focuses on easy setup and encryption, suitable for both cloud
    and on-premises environments.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Weave Net**：专注于简单的设置和加密，适用于云环境和本地环境。'
- en: '**Cilium**: Utilizes **eBPF** for advanced security features and network observability,
    perfect for microservice architectures.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cilium**：利用 **eBPF** 提供先进的安全功能和网络可观察性，完美适用于微服务架构。'
- en: '**Closed-source CNI plugins:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**闭源 CNI 插件：**'
- en: '**AWS VPC CNI**: Integrates Kubernetes Pods directly with AWS VPC for seamless
    IP management and connectivity.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS VPC CNI**：将 Kubernetes Pods 与 AWS VPC 直接集成，实现无缝的 IP 管理和连接。'
- en: '**Azure CNI**: Allows Pods to use IP addresses from the Azure VNet, ensuring
    integration with Azure’s networking infrastructure.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure CNI**：允许 Pods 使用 Azure VNet 中的 IP 地址，确保与 Azure 网络基础设施的集成。'
- en: '**VMware NSX-T**: Provides advanced networking capabilities like micro-segmentation
    and security for Kubernetes in VMware environments.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VMware NSX-T**：为 VMware 环境中的 Kubernetes 提供先进的网络功能，如微分段和安全性。'
- en: In some of the exercises in this book, we will use Calico. Now, let’s find out
    what a service mesh is.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的一些练习中，我们将使用 Calico。现在，让我们了解什么是服务网格。
- en: What is a service mesh?
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是服务网格？
- en: A **service mesh** is an essential infrastructure layer integrated into microservices
    architecture, facilitating inter-service communication. It encompasses functionalities
    like service discovery, load balancing, encryption, authentication, and monitoring,
    all within the network infrastructure. Typically, service meshes are realized
    through lightweight proxies deployed alongside individual services, enabling precise
    control over traffic routing and enforcing policies such as rate limiting and
    circuit breaking. By abstracting complex networking tasks from developers, service
    meshes streamline the development, deployment, and management of microservice
    applications, while also enhancing reliability, security, and observability. Prominent
    service mesh implementations include **Istio**, **Linkerd**, and **Consul Connect**.
    However, it’s worth noting that this topic is beyond the scope of this book.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务网格** 是微服务架构中不可或缺的基础设施层，促进服务间的通信。它包括服务发现、负载均衡、加密、认证和监控等功能，所有这些都在网络基础设施内实现。通常，服务网格是通过部署在各个服务旁边的轻量级代理来实现的，从而精确控制流量路由，并强制执行如速率限制和断路等策略。通过将复杂的网络任务抽象化，服务网格简化了微服务应用程序的开发、部署和管理，同时提高了可靠性、安全性和可观察性。著名的服务网格实现包括**Istio**、**Linkerd**和**Consul
    Connect**。然而，值得注意的是，这个话题超出了本书的范围。'
- en: Next, we’ll explain what they’re used for and how they can help you expose your
    Pod-launched microservices.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解释它们的用途以及它们如何帮助您暴露 Pod 启动的微服务。
- en: Understanding Pod IP assignment
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Pod 的 IP 分配
- en: 'To understand what Services are, we need to talk about Pods for a moment once
    again. On Kubernetes, everything is Pod management: Pods host your applications,
    and they have a special property. Kubernetes assigns them a private IP address
    as soon as they are created on your cluster. Keep that in mind because it is super
    important: each Pod created in your cluster has its unique IP address assigned
    by Kubernetes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解什么是服务，我们需要再次讨论一下 Pods。在 Kubernetes 中，一切都是 Pod 管理：Pods 托管您的应用程序，并且它们具有一个特殊的属性。Kubernetes
    在 Pods 被创建在您的集群中时会为其分配一个私有 IP 地址。记住这一点，因为它非常重要：集群中创建的每个 Pod 都会被 Kubernetes 分配一个唯一的
    IP 地址。
- en: To illustrate this, we’ll start by creating a nginx Pod. We are using an nginx
    container image to create a Pod here, but in fact, it would be the same outcome
    for any container image being used to create a Pod.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们将从创建一个 nginx Pod 开始。我们在这里使用 nginx 容器镜像来创建一个 Pod，但实际上，使用任何容器镜像创建 Pod
    都会得到相同的结果。
- en: 'Let’s do this using the declarative way with the following YAML definition:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用声明式的方法，结合以下 YAML 定义来完成这项操作：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see from the previous YAML, this Pod called `new-nginx-pod` has nothing
    special and will just launch a container named `new-nginx-container` based on
    the `nginx:1.17` container image.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的 YAML 文件中可以看到，这个名为 `new-nginx-pod` 的 Pod 没有什么特别之处，它只会基于 `nginx:1.17` 容器镜像启动一个名为
    `new-nginx-container` 的容器。
- en: 'Once we have this YAML file, we can apply it using the following command to
    get the Pod running on our cluster:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个 YAML 文件，我们可以使用以下命令应用它，让 Pod 在我们的集群上运行：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As soon as this command is called, the Pod gets created on the cluster, and
    as soon as the Pod is created on the cluster, Kubernetes will assign it an IP
    address that will be unique.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦调用这个命令，Pod 就会在集群中创建，并且在 Pod 被创建的瞬间，Kubernetes 会为其分配一个唯一的 IP 地址。
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In our case, the IP address is `10.244.0.109`. This IP address will be unique
    on my Kubernetes cluster and is assigned to this unique Pod.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，IP 地址是 `10.244.0.109`。这个 IP 地址在我的 Kubernetes 集群中是唯一的，并且被分配给这个特定的 Pod。
- en: Of course, if you’re following along on your cluster, you will have a different
    IP address. This IP address is a private IP version 4 (v4) address and only exists
    in the Kubernetes cluster. If you try to type this IP address into your web browser,
    you won’t get anything because this address does not exist on the outside network
    or public internet; it only exists within your Kubernetes cluster.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你在自己的集群中进行操作，你会看到不同的 IP 地址。这个 IP 地址是私有的 IPv4 地址，并且只存在于 Kubernetes 集群中。如果你尝试在浏览器中输入这个
    IP 地址，你将不会得到任何响应，因为这个地址并不存在于外部网络或公共互联网中，它只存在于你的 Kubernetes 集群内。
- en: Regardless of the cloud platform you use—whether it’s **Amazon Web Services**
    (**AWS**), **Google Cloud Platform** (**GCP**), or Azure—the Kubernetes cluster
    leverages a network segment provided by the cloud provider. This segment, usually
    referred to as a **virtual private cloud** (**VPC**), defines a private and isolated
    network, similar to the private IP range from your on-premise LAN. In all cases,
    the CNI plugin used by Kubernetes ensures that each Pod is assigned a unique IP
    address, providing granular isolation at the Pod level. This rule applies across
    all cloud and on-premises environments.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用的是哪种云平台——无论是**Amazon Web Services**（**AWS**）、**Google Cloud Platform**（**GCP**）还是
    Azure——Kubernetes 集群都利用云服务商提供的网络段。这个网络段通常被称为**虚拟私有云**（**VPC**），定义了一个私有且隔离的网络，类似于你本地局域网中的私有
    IP 范围。在所有情况下，Kubernetes 使用的 CNI 插件确保每个 Pod 都被分配一个唯一的 IP 地址，从而在 Pod 层面提供细粒度的隔离。这一规则适用于所有云环境和本地环境。
- en: We will now discover that this IP address assignment is dynamic, as well as
    finding out about the issues it can cause at scale.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将发现，这个 IP 地址分配是动态的，并且还会了解到它在规模化过程中可能会带来的问题。
- en: Understanding the dynamics of Pod IP assignment in Kubernetes
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 中 Pod IP 分配的动态性
- en: 'The IP addresses assigned to the Pods are not static, and if you delete and
    recreate a Pod, you’re going to see that the Pod will get a new IP address that’s
    totally different from the one used before, even if it’s recreated with the exact
    same YAML configuration. To demonstrate that, let’s delete the Pod and recreate
    it using the same YAML file, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给 Pods 的 IP 地址不是静态的，如果你删除并重新创建一个 Pod，你会发现该 Pod 会获得一个新的 IP 地址，这个地址与之前使用的地址完全不同，即使是用相同的
    YAML 配置重新创建的。为了演示这一点，我们来删除 Pod 并使用相同的 YAML 文件重新创建它，如下所示：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now run the `kubectl get pods -o wide` command once more to figure out
    that the new IP address is not the same as before, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以再次运行 `kubectl get pods -o wide` 命令，来确认新的 IP 地址与之前的不同，如下所示：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, the IP address is `10.244.0.110`. This IP address is different from the
    one we had before, `10.244.0.109`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，IP 地址是 `10.244.0.110`。这个 IP 地址与之前的 `10.244.0.109` 不同。
- en: As you can see, when a Pod is destroyed and then recreated, even if you recreate
    it with the same name and the same configuration, it’s going to have a different
    IP address.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，当一个 Pod 被销毁然后重新创建时，即使你用相同的名称和配置重新创建它，它也会有一个不同的 IP 地址。
- en: The reason is that technically, it is not the same Pod but two different Pods;
    that is why Kubernetes assigns two completely different IP addresses.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是从技术上讲，这不是同一个 Pod 而是两个不同的 Pod；这就是为什么 Kubernetes 分配两个完全不同 IP 地址的原因。
- en: Now, imagine you have an application accessing that nginx Pod that’s using its
    IP address to communicate with it. If the nginx Pod gets deleted and recreated
    for some reason, then your application will be broken because the IP address will
    not be valid anymore.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，您有一个应用程序访问那个使用其 IP 地址与之通信的 nginx Pod。如果由于某种原因删除并重新创建 nginx Pod，则您的应用程序将中断，因为该
    IP 地址将不再有效。
- en: In the next section, we’ll discuss why hardcoding Pod IP addresses in your application
    code is not recommended and explore the challenges it presents in a production
    environment. We’ll also look at more reliable methods for ensuring stable communication
    between microservices in Kubernetes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论为什么不建议在应用程序代码中硬编码 Pod IP 地址，并探讨在生产环境中所面临的挑战。我们还将探讨在 Kubernetes 中确保稳定微服务之间通信的更可靠方法。
- en: Not hardcoding the Pod’s IP address in application development
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在应用程序开发中不要硬编码 Pod 的 IP 地址
- en: In production environments, relying on Pod IP addresses for application communication
    poses a significant challenge. Microservices, designed to interact through HTTP
    and relying on TCP/IP, require a reliable method to identify and connect to each
    other.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，依赖 Pod IP 地址进行应用程序通信带来了重大挑战。设计用于通过 HTTP 相互交互且依赖 TCP/IP 的微服务，需要一种可靠的方法来识别并连接彼此。
- en: Therefore, establishing a robust mechanism for retrieving Pod information, not
    just IP addresses, is crucial. This ensures consistent communication even when
    Pods are recreated or rescheduled across worker nodes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，建立一种强大的机制来检索 Pod 信息，而不仅仅是 IP 地址，是至关重要的。这样可以确保即使 Pod 在工作节点间被重新创建或重新调度，通信也能保持一致。
- en: Crucially, avoid hardcoding Pod IP addresses directly in applications because
    the Pod IP addresses are dynamic. The ephemeral nature of Pods, which means they
    can be deleted, recreated, or moved, renders the practice of hardcoding Pod IP
    addresses in an application’s YAML unreliable. If a Pod with a hardcoded IP is
    recreated, applications dependent on it will lose connectivity due to the IP resolving
    to nothing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，应避免直接在应用程序中硬编码 Pod IP 地址，因为 Pod IP 地址是动态的。Pod 的短暂性质意味着它们可以被删除、重新创建或移动，这使得在应用程序的
    YAML 中硬编码 Pod IP 地址的做法变得不可靠。如果具有硬编码 IP 的 Pod 被重新创建，依赖它的应用程序将由于 IP 解析为空而失去连接。
- en: 'There are very concrete cases that we can give where this problem can arise,
    as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提供一些具体案例，说明这个问题可能会出现，如下所述：
- en: A Pod running a microservice *A* has a dependency and calls a microservice *B*
    that is running as another Pod on the same Kubernetes cluster.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行微服务 *A* 的 Pod 有一个依赖项，并调用同一 Kubernetes 集群中另一个 Pod 上运行的微服务 *B*。
- en: An application running as a Pod needs to retrieve some data from a **MySQL**
    server also running as a Pod on the same Kubernetes cluster.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为 Pod 运行的应用程序需要从同一 Kubernetes 集群上也作为 Pod 运行的 **MySQL** 服务器检索一些数据。
- en: An application uses a **Redis cluster** as a caching engine deployed in multiple
    Pods on the same cluster.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个应用程序在同一集群中的多个 Pod 上部署了 **Redis 集群** 作为缓存引擎。
- en: Your end users access an application by calling an IP address, and that IP address
    changes because of a Pod failure.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的最终用户通过调用 IP 地址访问应用程序，由于 Pod 失败，该 IP 地址会发生变化。
- en: Any time you have an interconnection between services or, indeed, any network
    communication, this problem will arise.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 任何时候当服务之间存在互联或任何网络通信时，都会出现这个问题。
- en: The solution to this problem is the usage of the Kubernetes **Service** resource.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的方法是使用 Kubernetes **Service** 资源。
- en: The Service object will act as an intermediate object that will remain on your
    cluster. The Service is not meant to be destroyed, but even if it is destroyed,
    it can be recreated without any impact, as it is the Service name that it used,
    not the IP address. In fact, they can remain on your cluster in the long term
    without causing any issues. Service objects provide a layer of abstraction to
    expose your application running in Pod(s) at the network level without any code
    or configuration changes through its life cycle.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Service 对象将作为一个中间对象留在您的集群上。Service 不应被销毁，但即使被销毁，也可以重新创建而不会有任何影响，因为使用的是 Service
    名称，而不是 IP 地址。实际上，它们可以长期保留在您的集群中而不会引起任何问题。Service 对象提供了一个抽象层，通过整个生命周期在网络层级上暴露在
    Pod(s) 中运行的应用程序，而无需进行任何代码或配置更改。
- en: Understanding how Services route traffic to Pods
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Services 如何将流量路由到 Pods
- en: Kubernetes **Services** exist as resources within your cluster and act as an
    abstraction layer for network traffic management. Services utilize CNI plugins
    to facilitate communication between clients and the Pods behind the Services.
    Services achieve this by creating service endpoints, which represent groups of
    Pods, enabling load balancing and ensuring that traffic reaches healthy instances.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes **Services** 作为集群中的资源存在，并充当网络流量管理的抽象层。Services 利用 CNI 插件促进客户端与后端
    Pods 之间的通信。Services 通过创建服务端点实现这一点，服务端点代表一组 Pods，支持负载均衡并确保流量到达健康的实例。
- en: Kubernetes Services offer a static and reliable way to access Pods within a
    cluster. They provide a DNS name that remains constant even if the underlying
    Pods change due to deployments, scaling, or restarts. Services leverage **service
    discovery** mechanisms and internal **load balancing** to efficiently route traffic
    to healthy Pods behind the scenes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Services 提供了一种静态且可靠的方式来访问集群中的 Pods。即使底层 Pods 因部署、扩展或重启而发生变化，它们提供的
    DNS 名称也会保持不变。Services 利用 **服务发现** 机制和内部 **负载均衡** 有效地将流量路由到健康的 Pods。
- en: '![](img/B22019_08_01.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_01.png)'
- en: 'Figure 8.1: service-webapp is exposing webapp Pods 1, 2, and 3, whereas service-backendapp
    is exposing backendapp Pods 1 and 2'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：service-webapp 正在暴露 webapp Pods 1、2 和 3，而 service-backendapp 正在暴露 backendapp
    Pods 1 和 2。
- en: In fact, Services are deployed to Kubernetes as a resource type, and just as
    with most Kubernetes objects, you can deploy them to your cluster using interactive
    commands or declarative YAML files.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Services 作为资源类型部署到 Kubernetes，就像大多数 Kubernetes 对象一样，你可以使用交互式命令或声明式 YAML
    文件将它们部署到集群中。
- en: 'Like any other resources in Kubernetes, when you create a Service, you’ll have
    to give it a name. This name will be used by Kubernetes to build a DNS name that
    all Pods on your cluster will be able to call. This DNS entry will resolve to
    your Service, which is supposed to remain on your cluster. The only part that
    is quite tricky at your end will be to give a list of Pods to expose to your Services:
    we will discover how to do that in this chapter. Don’t worry—it’s just a configuration
    based on **labels** and **selectors**.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 和 Kubernetes 中的其他资源一样，当你创建一个 Service 时，必须为它指定一个名称。Kubernetes 会使用这个名称来构建一个 DNS
    名称，集群中的所有 Pods 都能访问这个名称。这个 DNS 条目会解析到你的 Service，它应该始终驻留在集群中。唯一有点棘手的部分是你需要为 Service
    提供一个 Pods 列表：我们将在本章中学习如何做到这一点。别担心，这只是一个基于 **标签** 和 **选择器** 的配置。
- en: Once everything is set up, you can just reach the Pods by calling the Service.
    This Service will receive the requests and forward them to the Pods. And that’s
    pretty much it!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有配置完成，你只需通过调用 Service 来访问 Pods。这个 Service 会接收请求并将其转发到 Pods。就这么简单！
- en: Understanding round-robin load balancing in Kubernetes
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 中的轮询负载均衡
- en: 'Kubernetes Services, once configured properly, can expose one or several Pods.
    When multiple Pods are exposed by the same Pod, the requests are evenly load-balanced
    to the Pods behind the Service using the round-robin algorithm, as illustrated
    in the following screenshot:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Services 一旦配置正确，就可以暴露一个或多个 Pods。当同一个 Service 暴露多个 Pods 时，使用轮询算法将请求均匀地负载均衡到背后的
    Pods，如下图所示：
- en: '![](img/B22019_08_02.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_02.png)'
- en: 'Figure 8.2: Service A proxies three requests to the three Pods behind it. At
    scale, each Service will receive 33% of the requests received by the Service'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：Service A 将三个请求代理到它背后的三个 Pods。在大规模情况下，每个 Service 将接收到由 Service 收到的 33%
    请求。
- en: Scaling applications becomes easy. Adding more Pods behind the Service will
    be enough using Pod replicas. You will learn about Deployments and replicas in
    *Chapter 11*, *Using Kubernetes Deployments for Stateless Workloads*. As the Kubernetes
    Service has round-robin logic implemented, it can proxy requests evenly to the
    Pods behind it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展应用变得容易。只需通过 Pod 副本向 Service 添加更多的 Pods 即可。你将在*第 11 章*，*使用 Kubernetes 部署无状态工作负载*中了解有关
    Deployments 和副本的内容。由于 Kubernetes Service 实现了轮询逻辑，它可以将请求均匀地代理到其背后的 Pods。
- en: Kubernetes Services offer more than just round-robin load balancing. While round-robin
    is commonly used in setups leveraging the IPVS mode of kube-proxy, it’s important
    to note that iptables, the default mode in many distributions, often distributes
    traffic using random or hash-based methods instead.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Services 提供的不仅仅是轮询负载均衡。虽然轮询通常在使用 kube-proxy 的 IPVS 模式的设置中使用，但需要注意的是，iptables（许多发行版中的默认模式）通常使用随机或基于哈希的方法来分配流量。
- en: 'Kubernetes also supports additional load-balancing algorithms to meet various
    needs: the fewest connections for balanced workloads, source IP for consistent
    routing, and even custom logic for more intricate scenarios. Users should also
    be aware that IPVS offers more advanced traffic management features like session
    affinity and traffic shaping, which might not be available in iptables mode.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 还支持其他负载均衡算法以满足各种需求：最少连接数用于平衡负载，源 IP 用于一致路由，甚至为更复杂的场景提供自定义逻辑。用户还应注意，IPVS
    提供了更高级的流量管理功能，如会话亲和性和流量整形，而这些功能可能在 iptables 模式下不可用。
- en: Understanding the mode your cluster is using (either iptables or IPVS) can help
    in fine-tuning service behavior based on your scaling and traffic distribution
    requirements. Refer to the documentation to learn more ([https://kubernetes.io/docs/reference/networking/virtual-ips/](https://kubernetes.io/docs/reference/networking/virtual-ips/)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 了解你的集群正在使用的模式（无论是 iptables 还是 IPVS）可以帮助你根据你的扩展和流量分配需求微调服务的行为。有关更多信息，请参考文档 ([https://kubernetes.io/docs/reference/networking/virtual-ips/](https://kubernetes.io/docs/reference/networking/virtual-ips/))。
- en: If the preceding Pod had four replicas, then each of them would receive roughly
    25% of all the requests the service received. If 50 Pods were behind the Service,
    each of them would receive roughly 2% of all the requests received by the Service.
    All you need to understand is that Services behave like load balancers by following
    a specific load-balancing algorithm.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的 Pod 有四个副本，那么每个副本大约会接收到该服务收到的所有请求的 25%。如果该服务背后有 50 个 Pod，那么每个 Pod 大约会接收到该服务收到的所有请求的
    2%。你只需要理解的是，服务通过遵循特定的负载均衡算法，表现得像负载均衡器一样。
- en: Let’s now discover how you can call a Service in Kubernetes from another Pod.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索如何从另一个 Pod 中调用 Kubernetes 中的服务。
- en: Understanding how to call a Service in Kubernetes
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何在 Kubernetes 中调用服务
- en: 'When you create a Service in Kubernetes, it will be attached to two very important
    things, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 Kubernetes 中创建一个服务时，它会附加到两个非常重要的事物，如下所示：
- en: An IP address that will be unique and specific to it (just as Pods get their
    own IP)
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 IP 地址，它是唯一且特定的（就像 Pod 获得它们自己的 IP 一样）
- en: An automatically generated internal DNS name that won’t change and is static
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自动生成的内部 DNS 名称，它不会改变并且是静态的
- en: You’ll be able to use any of the two in order to reach the Service, which will
    then forward your request to the Pod it is configured in the backend. Most of
    the time, though, you’ll call the Service by its generated DNS name, which is
    easy to determine and predictable. Let’s discover how Kubernetes assigns DNS names
    to services.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用其中任何一个来访问服务，然后它将把你的请求转发到其后端配置的 Pod。大多数情况下，你会通过其生成的 DNS 名称调用该服务，它是容易确定且可预测的。让我们探索一下
    Kubernetes 如何为服务分配 DNS 名称。
- en: Understanding how DNS names are generated for Services
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何为服务生成 DNS 名称
- en: The DNS name generated for a Service is derived from its name. For example,
    if you create a Service named `my-app-service`, its DNS name will be `my-app-service.default.svc.cluster.local`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为服务生成的 DNS 名称是根据其名称派生的。例如，如果你创建一个名为 `my-app-service` 的服务，它的 DNS 名称将是 `my-app-service.default.svc.cluster.local`。
- en: 'This one is quite complicated, so let’s break it into smaller parts, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比较复杂，所以我们将它分解成更小的部分，如下所示：
- en: '![](img/B22019_08_03.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_03.png)'
- en: 'Figure 8.3: Service FQDN'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：服务 FQDN
- en: The two moving parts are the first two, which are basically the Service name
    and the namespace where it lives. The DNS name will always end with the `.svc.cluster.local`
    string.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个活动部分是前两个部分，基本上是服务名称和它所在的命名空间。DNS 名称将始终以 `.svc.cluster.local` 字符串结尾。
- en: So, at any moment, from anywhere on your cluster, if you try to use `curl` or
    `wget` to call the `my-app-service.default.svc.cluster.local` address, you know
    that you’ll reach your Service.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在任何时刻，只要你在集群中的任何地方尝试使用 `curl` 或 `wget` 调用 `my-app-service.default.svc.cluster.local`
    地址，你就知道你会到达你的服务。
- en: That name will resolve to the Service as soon as it’s executed from a Pod within
    your cluster. But by default, Services won’t proxy to anything if they are not
    configured to retrieve a list of the Pods to proxy. We will now discover how to
    do that!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该名称将在从集群中的 Pod 执行时解析为该服务。但默认情况下，如果服务没有配置来获取要代理的 Pod 列表，它们不会代理任何内容。我们现在将探索如何做到这一点！
- en: How Services discover and route traffic to Pods in Kubernetes
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务如何在 Kubernetes 中发现并将流量路由到 Pod
- en: 'When working with Services in Kubernetes, you will often come across the idea
    of *exposing* your Pods. Indeed, this is the terminology Kubernetes uses to tell
    that a Service is proxying network traffic to Pods. That terminology is everywhere:
    your colleagues might ask you one day, “*Which Service is exposing that Pod?*”
    The following screenshot shows Pods being exposed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中使用服务时，你经常会遇到“暴露”你的 Pods 这一概念。实际上，这就是 Kubernetes 用来表示服务正在将网络流量代理到
    Pods 上的术语。这个术语无处不在：有一天你的同事可能会问你，“*哪个服务暴露了那个 Pod？*”以下截图显示了被暴露的 Pods：
- en: '![](img/B22019_08_04.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_04.png)'
- en: 'Figure 8.4: Webapp Pods 1, 2, and 3 are exposed by service-webapp, whereas
    backendapp Pods 1 and 2 are exposed by service-backendapp'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：Webapp Pods 1、2 和 3 通过 service-webapp 暴露，而 backendapp Pods 1 和 2 通过 service-backendapp
    暴露。
- en: You can successfully create a Pod and a Service to expose it using `kubectl`
    in literally one command, using the `--expose` parameter. For the sake of this
    example, let us create a nginx Pod with Service as follows.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`kubectl`在一条命令中成功创建一个 Pod 和一个服务来暴露它，使用`--expose`参数。为了演示这个例子，我们创建一个 nginx
    Pod 和服务，如下所示。
- en: 'We will also need to provide a port to the command to tell on which port the
    Service will be accessible:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要为命令提供一个端口，以指定该服务将在哪个端口上可访问：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s now list the Pods and the Service using `kubectl` to demonstrate that
    the following command created both objects:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`kubectl`列出 Pods 和服务，以演示以下命令创建了这两个对象：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see based on the output of the command, both objects were created.
    We said earlier that Services can find the Pods they have to expose based on the
    Pods’ labels. The `nginx` Pod we just created surely has some labels. To show
    them, let’s run the `kubectl get pods nginx --show-labels` command.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从命令的输出结果中看到的那样，两个对象都已经创建。我们之前提到过，服务可以根据 Pod 的标签来查找需要暴露的 Pod。我们刚刚创建的`nginx`
    Pod 当然有一些标签。为了展示这些标签，我们可以运行`kubectl get pods nginx --show-labels`命令。
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see, an `nginx` Pod was created with a label called `run` and a
    value of `nginx`. Let’s now describe the `nginx` Service. It should have a selector
    that matches this label. The code is illustrated here:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，一个名为`run`、值为`nginx`的标签被添加到了创建的`nginx` Pod 上。现在让我们描述一下`nginx`服务。它应该有一个与这个标签匹配的选择器。代码如下：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can clearly see that the Service has a line called `Selector` that matches
    the label assigned to the `nginx` Pod. This way, the link between the two objects
    is made. We’re now 100% sure that the Service can reach the `nginx` Pod and that
    everything should work normally.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地看到，服务有一行叫做 `Selector`，它匹配了分配给`nginx` Pod 的标签。通过这种方式，两个对象之间建立了连接。我们现在可以
    100% 确定，服务可以访问`nginx` Pod，并且一切应该正常工作。
- en: Please note that if you are using the minikube in the lab, you will not be able
    to access the Service outside of your cluster as ClusterIP Services are only accessible
    from inside the cluster. You need to use debugging methods such as `kubectl port-forward`
    or `kubectl proxy` for such scenarios. You will learn how to test ClusterIP-type
    Services in the next section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你在实验室中使用 minikube，你将无法从集群外部访问该服务，因为 ClusterIP 服务只能从集群内部访问。你需要使用调试方法，如`kubectl
    port-forward`或`kubectl proxy`，来处理这种情况。你将在下一节中学习如何测试 ClusterIP 类型的服务。
- en: 'Also, to test the Service access, let us create a temporary port-forwarding,
    as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了测试服务的访问，让我们创建一个临时的端口转发，如下所示：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, open another console and access URL as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，打开另一个控制台并访问以下 URL：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding snippets, port `8080` is the localhost port we used for port-forwarding
    and 80 is the nginx port where the web Service is exposed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，端口`8080`是我们用于端口转发的本地主机端口，而 80 是 nginx 端口，用于暴露 Web 服务。
- en: Please note that the `kubectl port-forward` command will run until you break
    it using *Ctrl+C*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`kubectl port-forward`命令将持续运行，直到你通过 *Ctrl+C* 终止它。
- en: Though it works, we strongly advise you to never do that in production. Services
    are very customizable objects, and the `--expose` parameter hides a lot of their
    features. Instead, you should really use declarative syntax and tweak the YAML
    to fit your exact needs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样可以工作，但我们强烈建议你在生产环境中永远不要这么做。服务是高度可定制的对象，而`--expose`参数隐藏了它们的许多功能。相反，你应该使用声明式语法，并调整
    YAML 文件以满足你的具体需求。
- en: Let’s demonstrate that by using the `dnsutils` container image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`dnsutils`容器镜像来演示这一点。
- en: Using a utility Pod for debugging your Services
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用调试工具 Pod 来调试你的服务。
- en: As your Services are created within your cluster, it is often hard to access
    them as we mentioned earlier, especially if our Pod is meant to remain accessible
    only within your cluster, or if your cluster has no internet connectivity, and
    so on.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您的服务是在集群内创建的，因此通常很难像我们之前提到的那样访问它们，特别是如果我们的Pod旨在仅在您的集群内保持可访问性，或者如果您的集群没有互联网连接等。
- en: In this case, it is good to deploy a debugging Pod in your cluster with just
    some binaries installed into it to run basic networking commands such as `wget`,
    `nslookup`, and so on. Let us use our custom utility container image `quay.io/iamgini/k8sutils:debian12`
    for this purpose.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最好在您的集群中部署一个调试 Pod，只需安装一些二进制文件即可运行基本的网络命令，例如`wget`、`nslookup`等。让我们使用我们的自定义实用容器镜像`quay.io/iamgini/k8sutils:debian12`来实现这个目的。
- en: You can add more tools or utilities inside the utility container image if required;
    refer to the `Chapter-070/Containerfile` for the source.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，您可以在实用容器镜像中添加更多工具或实用程序；参考`Chapter-070/Containerfile`获取源代码。
- en: 'Here, we’re going to curl the `nginx` Pod home page by calling the Service
    that is exposing the Pod. That Service’s name is just `nginx`. Hence, we can forget
    the DNS name Kubernetes assigned to it: `nginx.default.svc.cluster.local`.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将通过调用暴露Pod的服务来对`nginx` Pod的首页运行curl。该服务的名称只是`nginx`。因此，我们可以忽略Kubernetes为其分配的DNS名称：`nginx.default.svc.cluster.local`。
- en: If you try to reach this **uniform resource locator** (**URL**) from a Pod within
    your cluster, you should successfully reach the `nginx` home page.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尝试从集群内部的Pod访问这个**统一资源定位符**（**URL**），您应该可以成功访问`nginx`首页。
- en: The following Pod definition will help us to create a debugging pod with the
    `k8sutils` image.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的Pod定义将帮助我们使用`k8sutils`镜像创建一个调试Pod。
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s run the following command to launch the `k8sutils` Pod on our cluster:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行以下命令，在我们的集群上启动`k8sutils` Pod：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now run the `kubectl get pods` command in order to verify that the Pod was
    launched successfully, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行`kubectl get pods`命令，以验证Pod是否成功启动，如下所示：
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'That’s perfect! Let’s now run the `nslookup` command from the `k8sutils` Pod
    against the Service DNS name, as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在让我们从`k8sutils` Pod运行`nslookup`命令来查询服务的DNS名称，如下所示：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the previous snippet,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，
- en: '`Server: 10.96.0.10` - is the `kube-dns` Service IP (`kubectl get svc kube-dns
    -n kube-system -o wide`). If you are using a different DNS Service, check the
    Service details accordingly.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Server: 10.96.0.10` - 是`kube-dns`服务的IP地址（`kubectl get svc kube-dns -n kube-system
    -o wide`）。如果您使用不同的DNS服务，请相应地检查服务详细信息。'
- en: '`nginx.default.svc.cluster.local` is resolving to the IP address of `nginx`
    Service (`kubectl get svc nginx -o wide`), which is `10.106.124`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nginx.default.svc.cluster.local`解析为`nginx`服务的IP地址（`kubectl get svc nginx -o
    wide`），即`10.106.124`。'
- en: 'Everything looks good. Let’s now run a `curl` command to check whether we can
    retrieve the `nginx` home page, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都很好。现在让我们运行一个`curl`命令来检查我们是否可以获取`nginx`首页，如下所示：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Everything is perfect here! We successfully called the `nginx` Service by using
    the `k8sutils` debug Pod, as illustrated in the following screenshot:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一切都很完美！我们成功通过使用`k8sutils`调试Pod调用了`nginx`服务，如下截图所示：
- en: '![](img/B22019_08_05.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_05.png)'
- en: 'Figure 8.5: The k8sutils Pod is used to run curl against the nginx Service
    to communicate with the nginx Pod behind the Service'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：k8sutils Pod用于对nginx服务运行curl，与服务背后的nginx Pod通信
- en: Keep in mind that you need to deploy a `k8sutils` Pod inside the cluster to
    be able to debug the Service. Indeed, the `nginx.default.svc.cluster.local` DNS
    name is not a public one and can only be accessible from within the cluster.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您需要在集群内部署一个`k8sutils` Pod以便调试服务。确实，`nginx.default.svc.cluster.local` DNS名称不是公共名称，只能从集群内部访问。
- en: Let’s explain why you should not use `expose` imperatively to expose your Pods
    in the next section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释为什么不应该使用`expose`命令来暴露您的Pod，在下一节中。
- en: Understanding the drawbacks of direct kubectl expose in Kubernetes
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解在Kubernetes中直接使用kubectl expose的缺点
- en: It is not recommended to use `kubectl expose` to create Services because you
    won’t get much control over how the Service gets created. By default, `kubectl
    expose` will create a `ClusterIP` Service, but you might want to create a `NodePort`
    Service.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议使用`kubectl expose`来创建服务，因为您无法控制服务的创建方式。默认情况下，`kubectl expose`将创建一个`ClusterIP`服务，但您可能希望创建一个`NodePort`服务。
- en: Defining the Service type is also possible using the imperative syntax, but
    in the end, the command you’ll have to issue is going to be very long and complex
    to understand. That’s why we encourage you to not use the `expose` option and
    stick with declarative syntax for complex objects such as Services.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用命令式语法定义 Service 类型，但最终你需要执行的命令将会非常长且难以理解。因此，我们鼓励你不要使用 `expose` 选项，而是对像
    Service 这样复杂的对象使用声明式语法。
- en: Let’s now discuss how DNS names are generated in Kubernetes when using Services.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论在 Kubernetes 中使用 Services 时，DNS 名称是如何生成的。
- en: Understanding how DNS names are generated for Services
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 DNS 名称是如何为 Services 生成的
- en: You now know that Kubernetes Service-to-Pod communication relies entirely on
    labels on the Pod side and selectors on the Service side.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经知道，Kubernetes 中的 Service 到 Pod 的通信完全依赖于 Pod 端的标签和 Service 端的选择器。
- en: If you don’t use both correctly, communication cannot be established between
    the two.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有正确使用这两者之间的配合，通信将无法建立。
- en: 'The workflow goes like this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 流程如下：
- en: You create some Pods, and you set the labels arbitrarily.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你创建了一些 Pods，并且随意设置了标签。
- en: You create a Service and configure its selector to match the Pods’ labels.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你创建一个 Service，并配置其选择器以匹配 Pods 的标签。
- en: The Service starts and looks for Pods that match its selector.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Service 启动后会查找与其选择器匹配的 Pods。
- en: You call the Service through its DNS or its IP (DNS is way easier).
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过 DNS 或 IP 调用 Service（DNS 更为便捷）。
- en: The Service forwards the traffic to one of the Pods that matches its labels.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Service 会将流量转发到与其标签匹配的某个 Pod。
- en: If you look at the previous example achieved using the imperative style with
    the `kubectl expose` parameter, you’ll notice that the Pod and the Services were
    respectively configured with proper labels (on the Pod side) and selector (on
    the Service side), which is why the Pod is successfully exposed. Please note that
    in your real-life cases, you will need to use the appropriate labels for your
    Pods instead of default labels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看之前通过命令式风格和 `kubectl expose` 参数实现的例子，你会发现 Pod 和 Service 分别使用了合适的标签（在 Pod
    端）和选择器（在 Service 端），这就是为什么 Pod 能够成功暴露的原因。请注意，在实际应用中，你需要为 Pods 使用合适的标签，而不是默认标签。
- en: Besides that, you must understand now that there are not one but several types
    of Services in Kubernetes—let us learn more about that.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，你现在必须理解，Kubernetes 中不仅有一种类型的 Service，而是有几种类型的 Service——让我们更深入地了解一下。
- en: Understanding the different types of Services
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解不同类型的 Services
- en: There are several types of Services in Kubernetes. Although there is only one
    kind called Service in Kubernetes, that kind can be configured differently to
    achieve different results.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有几种类型的 Service。虽然 Kubernetes 中只有一种叫做 Service 的类型，但这种类型可以通过不同的配置来实现不同的效果。
- en: 'Fortunately for us, no matter which type of Service you choose, the goal remains
    the same: to expose your Pods using a single static interface.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，不论你选择哪种类型的 Service，目标始终不变：使用单一的静态接口来暴露你的 Pods。
- en: Each type of Service has its own function and its own use, so basically, there’s
    one Service for each use case. A Service cannot be of multiple types at once,
    but you can still expose the same Pods using two Services’ objects with different
    types as long as the Services’ objects are named differently so that Kubernetes
    can assign different DNS names.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的 Service 都有其特定的功能和用途，因此基本上，每种用途都有对应的 Service。一个 Service 不能同时属于多种类型，但你仍然可以通过使用具有不同类型的两个
    Service 对象来暴露相同的 Pods，只要这些 Service 对象的名称不同，Kubernetes 就能为其分配不同的 DNS 名称。
- en: 'In this chapter, we will discover the three main types of Services, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍三种主要类型的 Services，如下所示：
- en: '`NodePort`: This type binds a port from an ephemeral port range of the host
    machine (the worker node) to a port on the Pod, making it available publicly.
    By calling the port of the host machine, you’ll reach the associated Kubernetes
    Pod. That’s the way to reach your Pods for traffic coming from outside your cluster.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NodePort`：这种类型将主机机器（工作节点）上的临时端口范围内的一个端口绑定到 Pod 上的一个端口，从而使其可以公开访问。通过调用主机机器的端口，你将能够访问关联的
    Kubernetes Pod。这是从集群外部访问你的 Pods 的方法。'
- en: '`ClusterIP`: The `ClusterIP` Service is the one that should be used for private
    communication between Pods within the Kubernetes cluster. This is the one we experimented
    with in this chapter and is the one created by `kubectl expose` by default. This
    is certainly the most commonly used of them all because it allows inter-communication
    between Pods: as its name suggests, it has a static IP that is set cluster-wide.
    By reaching its IP address, you’ll be redirected to the Pod behind it. If more
    than one Pod is behind it, the `ClusterIP` Service will provide a load-balancing
    mechanism following the round-robin or other algorithms.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClusterIP`：`ClusterIP` 服务是用于 Kubernetes 集群内 Pod 之间私密通信的服务。这就是我们在本章中实验过的服务，也是
    `kubectl expose` 默认创建的服务。它无疑是最常用的，因为它允许 Pod 之间的相互通信：正如它的名字所示，它有一个集群范围内设置的静态 IP
    地址。通过访问该 IP 地址，你将被重定向到其背后的 Pod。如果有多个 Pod 在背后，`ClusterIP` 服务将提供负载均衡机制，采用轮询或其他算法。'
- en: '`LoadBalancer`: The `LoadBalancer` Service in Kubernetes streamlines the process
    of exposing your Pods to external traffic. It achieves this by automatically provisioning
    a cloud-specific load balancer, such as AWS ELB, when operating on supported cloud
    platforms. This removes the necessity for the manual setup of external load balancers
    within cloud environments. However, it’s crucial to recognize that this Service
    is not compatible with bare-metal or non-cloud-managed clusters unless configured
    manually. While alternative solutions like Terraform exist for managing cloud
    infrastructure, the `LoadBalancer` Service provides a convenient option for seamlessly
    integrating cloud-native load balancers into your Kubernetes deployments, particularly
    in cloud-centric scenarios. Keep in mind that its suitability hinges on your specific
    requirements and infrastructure configuration.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LoadBalancer`：Kubernetes 中的 `LoadBalancer` 服务简化了将 Pod 暴露给外部流量的过程。它通过在支持的云平台上自动配置一个云特定的负载均衡器（如
    AWS ELB）来实现这一点。这消除了在云环境中手动设置外部负载均衡器的需求。然而，需要认识到，除非手动配置，否则该服务不支持裸金属或非云管理的集群。虽然
    Terraform 等替代方案可以用于管理云基础设施，但 `LoadBalancer` 服务为无缝集成云原生负载均衡器到 Kubernetes 部署中提供了一个便捷的选项，特别是在以云为中心的场景中。请记住，它的适用性取决于你的具体需求和基础设施配置。'
- en: Now, let’s immediately dive into the first type of Service—the `NodePort` one.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们立即深入了解第一种类型的服务——`NodePort` 服务。
- en: As mentioned earlier, this one is going to be very useful to access our Pods
    from outside the cluster in our development environments, by attaching Pods to
    the Kubernetes node’s port.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这将非常有用，能够让我们在开发环境中通过将 Pod 绑定到 Kubernetes 节点的端口，从外部访问我们的 Pod。
- en: The NodePort Service
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NodePort 服务
- en: '`NodePort` is a Kubernetes Service type designed to make Pods reachable from
    a port available on the host machine, the worker node. In this section, we’re
    going to discover this type of port and be fully focused on `NodePort` Services!'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort` 是一种 Kubernetes 服务类型，旨在使 Pod 通过主机机器（工作节点）上可用的端口进行访问。在本节中，我们将深入了解这种类型的端口，并专注于
    `NodePort` 服务！'
- en: Why do you need NodePort Services?
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么你需要 NodePort 服务？
- en: The first thing to understand is that `NodePort` Services allow us to access
    a Pod running on a Kubernetes node, on a port of the node itself. After you expose
    Pods using the `NodePort` type Service, you’ll be able to reach the Pods by getting
    the IP address of the node and the port of the `NodePort` Service, such as `<node_ip_address>:<node
    port>`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要理解的是，`NodePort` 服务允许我们通过 Kubernetes 节点上的端口访问正在运行的 Pod。在你通过 `NodePort` 类型的服务暴露
    Pod 后，你将能够通过获取节点的 IP 地址和 `NodePort` 服务的端口（如 `<node_ip_address>:<node port>`）来访问这些
    Pod。
- en: The port can be declared in your YAML declaration or can be randomly assigned
    by Kubernetes. Let’s illustrate all of this by declaring some Kubernetes objects.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 端口可以在你的 YAML 声明中声明，或者由 Kubernetes 随机分配。让我们通过声明一些 Kubernetes 对象来说明这一点。
- en: Most of the time, the `NodePort` Service is used as an entry point to your Kubernetes
    cluster. In the following example, we will create two Pods based on the `containous/whoami`
    container image available on Docker Hub, which is a very nice container image
    that will simply print the container hostname.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，`NodePort` 服务作为 Kubernetes 集群的入口点。在接下来的示例中，我们将基于 Docker Hub 上的 `containous/whoami`
    容器镜像创建两个 Pod，这个容器镜像非常简洁，会简单地打印出容器的主机名。
- en: We will create two Pods so that we get two containers with different hostnames,
    and we will expose them using a `NodePort` Service.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建两个Pods，以便得到两个具有不同主机名的容器，并通过`NodePort`服务暴露它们。
- en: Creating two containous/whoami Pods
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建两个containous/whoami Pods
- en: Let’s start by creating two Pods, without forgetting about adding one or more
    labels, because we will need labels to tell the Service which Pods it’s going
    to expose.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建两个Pods，别忘了添加一个或多个标签，因为我们将需要标签来告诉服务哪些Pods将被暴露。
- en: 'We are also going to open the port on the Pod side. That won’t make it exposed
    on its own, but it will open a port the Service will be able to reach. The code
    is illustrated here:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要在Pod端打开端口。虽然这并不会使它自动暴露，但它会打开一个Service能够访问的端口。代码如下：
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can run a `kubectl get pods` command in order to verify that our two
    Pods are running correctly. We can also add the `--show-labels` parameter in order
    to display the labels as part of the command output, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行`kubectl get pods`命令来验证我们的两个Pods是否正确运行。我们还可以添加`--show-labels`参数，以便在命令输出中显示标签，如下所示：
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Everything seems to be okay! Now that we have two Pods created with a label
    set for each of them, we will be able to expose them using a Service. We’re now
    going to discover the YAML manifest file that will create the `NodePort` Service
    to expose these two Pods.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都正常！现在我们已经创建了两个带有标签的Pods，我们将能够通过一个Service将它们暴露出去。接下来，我们将了解用于创建`NodePort`服务并暴露这两个Pods的YAML清单文件。
- en: Understanding NodePort YAML definition
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解NodePort YAML定义
- en: Since Services are quite complex resources, it is better to create Services
    using a YAML file rather than direct command input.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于服务是相当复杂的资源，最好通过YAML文件而不是直接命令输入来创建服务。
- en: 'Here is the YAML file that will expose the `whoami1` and `whoamo2` Pods using
    a `NodePort` Service:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将通过`NodePort`服务暴露`whoami1`和`whoami2` Pods的YAML文件：
- en: '[PRE20]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This YAML can be difficult to understand because it refers to three different
    ports as well as a `selector` block.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个YAML可能比较难理解，因为它涉及到三个不同的端口以及一个`selector`块。
- en: 'Before explaining the YAML file, let’s apply it and check if the Service was
    correctly created afterwards, as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释YAML文件之前，让我们先应用它并检查Service是否被正确创建，操作如下：
- en: '[PRE21]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The previous `kubectl get services` command indicated that the Service was properly
    created!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的`kubectl get services`命令表明Service已正确创建！
- en: The selector block is crucial for NodePort Services, acting as a label filter
    to determine which pods the Service exposes. Essentially, it tells the Service
    what pods to route traffic to. Without a selector, the Service remains inactive.
    In this example, the selector targets pods with the label key “app” and the value
    “`whoami`". This effectively exposes both the “`whoami1`" and “`whoami2`" pods
    through the Service.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: selector块对于NodePort服务至关重要，它充当标签过滤器来确定服务暴露哪些pods。它基本上告诉服务要将流量路由到哪些pods。如果没有selector，服务将保持不活动。在此示例中，selector定位带有标签键“app”和标签值“`whoami`”的pods。这将通过Service有效地暴露“`whoami1`”和“`whoami2`”两个pods。
- en: Next, we have `type` as a child key under `spec,` where we specify the type
    of our Service. When we create `ClusterIP` or `LoadBalancer` Services, we will
    have to update this line. Here, we’re creating a `NodePort` Service, so that’s
    fine for us.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在`spec`下有一个`type`子键，用来指定我们的Service类型。当我们创建`ClusterIP`或`LoadBalancer`服务时，需要更新此行。在这里，我们创建的是`NodePort`服务，因此对我们来说这行是正确的。
- en: 'The last thing that is quite hard to understand is that `ports` block. Here,
    we define a map of multiple port combinations. We indicated three ports, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个比较难理解的部分是`ports`块。在这里，我们定义了多个端口组合的映射。我们指明了三个端口，如下所示：
- en: '`nodePort`: The port on the host machine/worker node you want this `NodePort`
    service to be accessible from. Here, we’re specifying port `30001`, which makes
    this NodePort Service accessible from port `30001` on the IP address of the worker
    node. You’ll be reaching this `NodePort` Service and the Pods it exposes by calling
    the following address: `<WORKER_NODE_IP_ADDRESS>:30001`. This `NodePort` setting
    cannot be set arbitrarily. Indeed, on a default Kubernetes installation, it can
    be a port from the `30000` - `32767` range.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nodePort`: 您希望此`NodePort`服务可以从中访问的主机机器/工作节点上的端口。在这里，我们指定了端口`30001`，这使得此`NodePort`服务可以通过工作节点的IP地址上的端口`30001`进行访问。您将通过调用以下地址来访问此`NodePort`服务及其所暴露的Pods：`<WORKER_NODE_IP_ADDRESS>:30001`。此`NodePort`设置不能随意设定。事实上，在默认的Kubernetes安装中，它只能是`30000`到`32767`范围内的端口。'
- en: '`port`: This setting indicates the port of the `NodePort` Service itself. It
    can be hard to understand, but `NodePort` Services do have a port of their own
    too, and this is where you specify it. You can put whatever you want here if it
    is a valid port.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`port`：此设置表示 `NodePort` 服务本身的端口。理解起来可能有点难，但 `NodePort` 服务确实有自己的端口，这就是你在此处指定它的地方。只要它是有效端口，你可以随便填入任何你想要的值。'
- en: '`targetPort`: As you might expect, `targetPort` is the port of the targeted
    Pods. It is where the application runs: the port where the NodePort will forward
    traffic to the Pod found by the selector mentioned previously.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targetPort`：如你所料，`targetPort` 是目标 Pods 的端口。它是应用程序运行的地方：NodePort 会将流量转发到通过前述选择器找到的
    Pod 上的端口。'
- en: 'Here is a quick diagram to sum all of this up:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一张简明的图表来总结这一切：
- en: '![](img/B22019_08_06.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_06.png)'
- en: 'Figure 8.6: NodePort Service. There are three ports involved in the NodePort
    setup—nodePort is on the worker n006Fde, the port is on the Service itself, and
    targetPort is on the top'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：NodePort 服务。NodePort 设置中涉及三个端口——nodePort 位于工作节点 n006Fde 上，端口位于 Service
    本身，targetPort 位于顶部。
- en: In this case, TCP port `31001` is used as the external port on each Node. If
    you do not specify `nodePort`, it will be allocated `dynamically` using the range.
    For internal communication, this Service still behaves like a simple `ClusterIP`
    Service, and you can use its `ClusterIP` address.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，TCP 端口 `31001` 被用作每个节点的外部端口。如果你没有指定 `nodePort`，它将使用动态分配的范围分配端口。对于内部通信，该服务仍然像一个简单的
    `ClusterIP` 服务一样工作，你可以使用它的 `ClusterIP` 地址。
- en: For convenience and to reduce complexity, the `NodePort` Service port and target
    port (the Pods’ port) are often defined to the same value.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见并减少复杂性，`NodePort` 服务端口和目标端口（Pod 的端口）通常定义为相同的值。
- en: Making sure NodePort works as expected
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保 NodePort 按预期工作
- en: To try out your `NodePort` setup, the first thing to do is to retrieve the public
    IP of your machine running it. In our example, we are running a single-machine
    Kubernetes setup with `minikube` locally. On AWS, GCP, or Azure, your node might
    have a public IP address or a private one if you access your node with a **virtual
    private network** (**VPN**).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试你的 `NodePort` 设置，首先要做的是获取运行该服务的机器的公共 IP 地址。在我们的例子中，我们使用的是本地的单机 Kubernetes
    设置，并通过 `minikube` 运行。若在 AWS、GCP 或 Azure 上，你的节点可能拥有公共 IP 地址，或者如果通过 **虚拟专用网络**（**VPN**）访问节点，则可能是私有
    IP 地址。
- en: 'On `minikube`, the easiest way to retrieve the IP address is to issue the following
    command:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `minikube` 上，获取 IP 地址的最简单方法是执行以下命令：
- en: '[PRE22]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we have all the information, we can open a web browser and enter the
    URL to access the `NodePort` Service and the Pods running. You should see the
    round-robin algorithm in place and reaching `whoami1` and then `whoami2`, and
    so on. The `NodePort` Service is doing its job!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有信息，可以打开浏览器并输入 URL 来访问 `NodePort` 服务和正在运行的 Pods。你应该能看到轮询算法的执行，访问 `whoami1`
    然后是 `whoami2`，依此类推。`NodePort` 服务正在按预期工作！
- en: Is this setup production-ready?
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这个设置是生产就绪的吗？
- en: This question might not have a definitive answer as it depends on your configuration.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能没有明确的答案，因为它取决于你的配置。
- en: '`NodePort` provides a way to expose Pods to the outside world by exposing them
    on a Node port. With the current setup, you have no HA: if your two Pods were
    to fail, you have no way to relaunch them automatically, so your Service wouldn’t
    be able to forward traffic to anything, resulting in a poor experience for your
    end user.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort` 提供了一种通过在节点端口上暴露 Pods 来将其公开到外部世界的方式。在当前的设置中，你没有高可用性（HA）：如果你的两个 Pods
    出现故障，你将无法自动重新启动它们，因此你的服务将无法将流量转发到任何地方，导致最终用户体验较差。'
- en: Please note that when we create the Pods using Deployments and replicasets,
    Kubernetes will create the new Pods in other available nodes. We will learn about
    Deployments and replicasets in *Chapter 11*, *Using Kubernetes Deployments for
    Stateless Workloads*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们使用 Deployments 和 replicasets 创建 Pods 时，Kubernetes 会在其他可用节点中创建新的 Pods。我们将在*第
    11 章*《使用 Kubernetes 部署无状态工作负载》中学习 Deployments 和 replicasets。
- en: Another problem is the fact that the choice of port is limited. Indeed, by default,
    you are just forced to use a port in the `30000-32767` range, and as it’s forced,
    it will be inconvenient for a lot of people. Indeed, if you want to expose an
    HTTP application, you’ll want to use port `80` or `443` of your frontal machine
    rather than a port in the `30000`-`32767` range, because all web browsers are
    configured with ports `80` and `443` as standard HTTP and **HTTP Secure** (**HTTPS**)
    ports.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是端口选择的限制。事实上，默认情况下，你只能使用 `30000-32767` 范围内的端口，而这种限制对很多人来说会很不方便。实际上，如果你想暴露一个
    HTTP 应用程序，你可能会希望使用前端机器的 `80` 或 `443` 端口，而不是 `30000` 到 `32767` 范围内的端口，因为所有的 web
    浏览器都将 `80` 和 `443` 端口配置为标准 HTTP 和 **HTTPS**（**HTTP Secure**）端口。
- en: 'The solution to this consists of using a tiered architecture. Indeed, a lot
    of Kubernetes architects tend to not expose a `NodePort` Service as the first
    layer in architecture but to put the Kubernetes cluster behind a reverse proxy,
    such as the AWS Application Load Balancer, and so on. Two other concepts of Kubernetes
    are the `Ingress` and `IngressController` objects: these two objects allow you
    to configure a reverse proxy such as `nginx` or HAProxy directly from Kubernetes
    objects and help you to make your application publicly accessible as the first
    layer of entry to Kubernetes. But this is way beyond the scope of Kubernetes Services.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用分层架构。事实上，许多 Kubernetes 架构师倾向于不将 `NodePort` 服务暴露为架构中的第一层，而是将 Kubernetes
    集群放置在反向代理后面，例如 AWS 应用负载均衡器（AWS Application Load Balancer）等。Kubernetes 的两个其他概念是
    `Ingress` 和 `IngressController` 对象：这两个对象允许你直接从 Kubernetes 对象配置反向代理，例如 `nginx`
    或 HAProxy，并帮助你将应用程序作为 Kubernetes 入口的第一层使其公开可访问。但这已经超出了 Kubernetes 服务的范围。
- en: Let us explore some more information about `NodePort` in the next sections,
    including how to list the Services and how to add Pods to the `NodePort` Services.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中探索更多有关 `NodePort` 的信息，包括如何列出服务以及如何将 Pods 添加到 `NodePort` 服务。
- en: Listing NodePort Services
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列出 NodePort 服务
- en: Listing `NodePort` Services is achieved through the usage of the `kubectl` command-line
    tool. You must simply issue a `kubectl get services` command to fetch the Services
    created within your cluster.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列出 `NodePort` 服务可以通过使用 `kubectl` 命令行工具来实现。你只需要执行 `kubectl get services` 命令来获取集群中创建的服务。
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: That being said, let’s now discover how we can update `NodePort` Services to
    have them do what we want.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，现在让我们来了解如何更新 `NodePort` 服务，以便使其执行我们想要的操作。
- en: Adding more Pods to NodePort Services
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向 NodePort 服务添加更多的 Pods
- en: If you want to add a Pod to the pool served by your Services, it’s very easy.
    In fact, you just need to add a new Pod that matches the label selector defined
    on the Service—Kubernetes will take care of the rest. The Pod will be part of
    the pool served by the Service. If you delete a Pod, it will be deleted from the
    pool of Services as soon as it enters the `Terminating` state.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将 Pod 添加到服务所提供的池中，这是非常容易的。事实上，你只需要添加一个与服务定义的标签选择器匹配的新 Pod——Kubernetes 会处理剩下的工作。这个
    Pod 将成为服务所提供池的一部分。如果你删除一个 Pod，它将在进入 `Terminating` 状态后被从服务池中删除。
- en: Kubernetes handles Service traffic based on Pod availability—for example, if
    you have three replicas of a web server and one goes down, creating an additional
    replica that matches the label selector on the Service will be enough. You’ll
    discover later, in *Chapter 11*, *Using Kubernetes Deployments for Stateless Workloads*,
    that this behavior can be entirely automated.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 基于 Pod 可用性来处理服务流量——例如，如果你有三个副本的 web 服务器，其中一个出现故障，创建一个匹配服务标签选择器的新副本就足够了。你将在后面的
    *第 11 章*，*使用 Kubernetes 部署无状态工作负载* 中发现，这种行为可以完全自动化。
- en: Describing NodePort Services
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述 NodePort 服务
- en: 'Describing `NodePort` Services is super easy and is achieved with the help
    of the `kubectl describe` command, just as with any other Kubernetes object. Let
    us explore the details of the `nodeport-whoami` Service in the following command’s
    output:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 描述 `NodePort` 服务非常简单，可以通过 `kubectl describe` 命令来实现，就像任何其他 Kubernetes 对象一样。让我们在以下命令的输出中探索
    `nodeport-whoami` 服务的详细信息：
- en: '[PRE24]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the preceding output, we can see several details, including the following
    items:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，我们可以看到几个详细信息，包括以下内容：
- en: '`IP:10.98.160.98`: The `ClusterIP` assigned to the Service. It’s the internal
    IP address that other Services in the cluster can use to access this Service.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IP:10.98.160.98`：分配给服务的 `ClusterIP`。这是集群中其他服务可以用来访问此服务的内部 IP 地址。'
- en: '`Port: <unset> 80/TCP`: The Service listens on port `80` with the TCP protocol.
    The `<unset>` means that no name was given for the port.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Port: <unset> 80/TCP`：该服务监听端口 `80` 并使用 TCP 协议。`<unset>` 表示该端口没有指定名称。'
- en: '`NodePort: <unset> 30001/TCP`: The `NodePort` is the port on each node in the
    cluster through which external traffic can access the Service. Here, it’s set
    to port `30001`, allowing external access to the Service via any node’s IP at
    port `30001`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NodePort: <unset> 30001/TCP`：`NodePort` 是集群中每个节点上的端口，通过这个端口外部流量可以访问该服务。在这里，它设置为端口
    `30001`，允许通过任何节点的 IP 地址和端口 `30001` 来访问该服务。'
- en: '`Endpoints: 10.244.0.16:80, 10.244.0.17:80`: The actual IP addresses and ports
    of the Pods behind the Service. In this case, two Pods are backing the Service,
    reachable at `10.244.0.16:80` and `10.244.0.17:80`.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Endpoints: 10.244.0.16:80, 10.244.0.17:80`：服务背后实际的 Pods 的 IP 地址和端口。在这种情况下，两个
    Pods 支持该服务，可以通过 `10.244.0.16:80` 和 `10.244.0.17:80` 访问。'
- en: In the next section, we will learn how to delete a Service using the `kubectl
    delete svc` command.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习如何使用 `kubectl delete svc` 命令删除一个服务。
- en: Deleting Services
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除服务
- en: Deleting a Service, whether it is a `NodePort` Service or not, should not be
    done often. Indeed, whereas Pods are supposed to be easy to delete and recreate,
    Services are supposed to be for the long term. They provide a consistent way to
    expose your Pod, and deleting them will impact how your applications can be reached.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 删除服务，无论它是否为 `NodePort` 服务，都不应频繁进行。事实上，虽然 Pods 应该是易于删除和重建的，但服务应该是长期存在的。它们为你的
    Pod 提供了一种一致的暴露方式，删除它们将影响到你的应用程序如何被访问。
- en: 'Therefore, you should be careful when deleting Services: it won’t delete the
    Pods behind it, but they won’t be accessible anymore from outside of your cluster!'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你在删除服务时需要小心：它不会删除服务背后的 Pods，但这些 Pods 将无法再从集群外部访问！
- en: 'Here is the command to delete the Service created to expose the `whoami1` and
    `whoami2` Pods:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是删除用来暴露 `whoami1` 和 `whoami2` Pods 的服务的命令：
- en: '[PRE25]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You can run a `kubectl get svc` command now to check that the Service was properly
    destroyed, and then access it once more through the web browser by refreshing
    it. You’ll notice that the application is not reachable anymore, but the Pods
    will remain on the cluster. Pods and Services have completely independent life
    cycles. If you want to delete Pods, then you’ll need to delete them separately.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以运行 `kubectl get svc` 命令来检查服务是否已正确删除，然后通过刷新网页浏览器再次访问它。你会发现应用程序不再可访问，但 Pods
    将继续存在于集群中。Pods 和服务有完全独立的生命周期。如果你想删除 Pods，那么你需要单独删除它们。
- en: You probably remember the `kubectl port-forward` command we used when we created
    an nginx Pod and tested it to display the home page. You might think `NodePort`
    and `kubectl port-forward` are the same thing, but they are not. Let’s explain
    quickly the difference between the two in the upcoming section.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们在创建 nginx Pod 并测试显示主页时使用的 `kubectl port-forward` 命令。你可能认为 `NodePort`
    和 `kubectl port-forward` 是相同的，但它们并不是。接下来的部分，我们将简要解释这两者之间的区别。
- en: NodePort or kubectl port-forward?
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodePort 还是 kubectl port-forward？
- en: It might be tempting to compare `NodePort` Services with the `kubectl port-forward`
    command because, so far, we have used these two methods to access a running Pod
    in our cluster using a web browser.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `NodePort` 服务与 `kubectl port-forward` 命令进行比较可能会很有诱惑力，因为到目前为止，我们已经使用这两种方法通过网页浏览器访问集群中正在运行的
    Pod。
- en: The `kubectl port-forward` command is a testing tool, whereas `NodePort` Services
    are for real use cases and are a production-ready feature.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl port-forward` 命令是一个测试工具，而 `NodePort` 服务则用于实际应用场景，并且是一个生产就绪的功能。'
- en: Keep in mind that `kubectl port-forward` must be kept open in your terminal
    session for it to work. As soon as the command is killed, the port forwarding
    is stopped too, and your application will become inaccessible from outside the
    cluster once more. It is only a testing tool meant to be used by the `kubectl`
    user and is just one of the useful tools bundled into the `kubectl` CLI.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`kubectl port-forward` 必须保持在终端会话中开启才能工作。一旦该命令被终止，端口转发也会停止，且你的应用程序将再次无法从集群外部访问。它只是一个测试工具，供
    `kubectl` 用户使用，并且是 `kubectl` CLI 中捆绑的有用工具之一。
- en: '`NodePort`, on the other hand, is really meant for production use and is a
    long-term production-ready solution. It doesn’t require `kubectl` to work and
    makes your application accessible to anyone calling the Service, provided the
    Service is properly configured and the Pods are correctly labeled.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`NodePort` 确实是为了生产环境使用的，并且是一个长期的生产就绪解决方案。它不需要 `kubectl` 就能工作，并且使您的应用程序可以被任何调用该服务的人访问，只要该服务配置正确，Pods
    标签正确。
- en: Simply put, if you just need to test your app, go for `kubectl port-forward`.
    If you need to expose your Pod to the outside world for real, go for `NodePort`.
    Don’t create `NodePort` for testing, and don’t try to use `kubectl port-forward`
    for production! Stick with one tool for each use case!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，如果您只需要测试应用程序，使用 `kubectl port-forward`。如果您需要将 Pod 真正暴露到外部世界，选择 `NodePort`。不要为了测试而创建
    `NodePort`，也不要在生产环境中使用 `kubectl port-forward`！每种用例都选择合适的工具！
- en: Now, we will discover another type of Kubernetes Service called `ClusterIP`.
    This one is probably the most widely used of them all, even more than the `NodePort`
    type!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探索另一种 Kubernetes 服务类型，叫做 `ClusterIP`。这可能是最广泛使用的一种，甚至比 `NodePort` 类型还要常用！
- en: The ClusterIP Service
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ClusterIP 服务
- en: We’re now going to discover another type of Service called `ClusterIP`. Now,
    `ClusterIP` is, in fact, the simplest type of Service Kubernetes provides. With
    a `ClusterIP` Service, you can expose your Pod so that other Pods in Kubernetes
    can communicate with it via its IP address or DNS name.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将了解另一种服务类型，称为 `ClusterIP`。实际上，`ClusterIP` 是 Kubernetes 提供的最简单的服务类型。有了 `ClusterIP`
    服务，您可以暴露您的 Pod，使得 Kubernetes 中的其他 Pods 可以通过其 IP 地址或 DNS 名称与其通信。
- en: Why do you need ClusterIP Services?
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么需要 `ClusterIP` 服务？
- en: 'The `ClusterIP` Service type greatly resembles the `NodePort` Service type,
    but they have one big difference: `NodePort` Services are meant to expose Pods
    to the outside world, whereas `ClusterIP` Services are meant to expose Pods to
    other Pods inside the Kubernetes cluster.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP` 服务类型与 `NodePort` 服务类型非常相似，但它们有一个很大的区别：`NodePort` 服务旨在将 Pod 暴露给外部世界，而
    `ClusterIP` 服务旨在将 Pod 暴露给 Kubernetes 集群内部的其他 Pod。'
- en: 'Indeed, `ClusterIP` Services are the Services that allow different Pods in
    the same cluster to communicate with each other through a static interface: the
    `ClusterIP` `Service` object itself.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，`ClusterIP` 服务允许同一集群中的不同 Pods 通过静态接口互相通信：即 `ClusterIP` 服务对象本身。
- en: '`ClusterIP` answers exactly the same need for a static DNS name or IP address
    we had with the `NodePort` Service: if a Pod fails, is recreated, deleted, relaunched,
    and so on, then Kubernetes will assign it another IP address. `ClusterIP` Services
    are here to remediate this issue, by providing an internal DNS name only accessible
    from within your cluster that will resolve to the Pods defined by the label selector.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP` 完全解决了我们在 `NodePort` 服务中遇到的对静态 DNS 名称或 IP 地址的需求：如果 Pod 失败、被重新创建、删除、重新启动等等，那么
    Kubernetes 会为其分配另一个 IP 地址。`ClusterIP` 服务通过提供一个仅能从集群内部访问的内部 DNS 名称，解决了这个问题，该名称会解析到由标签选择器定义的
    Pods。'
- en: As the name `ClusterIP` suggests, this Service grants a static IP within the
    cluster! Let’s now discover how to expose our Pods using `ClusterIP`! Keep in
    mind that `ClusterIP` Services are not accessible from outside the cluster—they
    are only meant for inter-Pod communication.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 `ClusterIP` 这个名字所暗示的，这种服务会在集群内分配一个静态 IP 地址！现在让我们来看看如何使用 `ClusterIP` 暴露我们的
    Pods！请记住，`ClusterIP` 服务不能从集群外部访问——它们仅用于集群内部 Pod 之间的通信。
- en: How do I know if I need NodePort or ClusterIP Services to expose my Pods?
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我怎么知道我需要 `NodePort` 还是 `ClusterIP` 服务来暴露我的 Pods？
- en: Choosing between the two types of Services is extremely simple, basically because
    they are not meant for the same thing.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这两种服务类型非常简单，基本上是因为它们不是为同样的用途设计的。
- en: If you need your app to be accessible from outside the cluster, then you’ll
    need a `NodePort` Service (or other Services we will be exploring later in this
    chapter), but if your need is for the app to be accessible from inside the cluster,
    then you’ll need a `ClusterIP` Service. `ClusterIP` Services are also good for
    stateless applications that can be scaled, destroyed, recreated, and so on. The
    reason is that the `ClusterIP` Service will maintain a static entry point to a
    whole pool of Pods without being constrained by a port on the worker node, unlike
    the `NodePort` Service.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要让你的应用程序可以从集群外部访问，那么你需要一个`NodePort`服务（或者本章后面将探讨的其他服务），但如果你的需求是让应用程序从集群内部访问，那么你需要一个`ClusterIP`服务。`ClusterIP`服务也适用于可以扩展、销毁、重新创建等的无状态应用程序。原因在于，`ClusterIP`服务将维持对整个Pod池的静态入口点，而不受工作节点端口的限制，这与`NodePort`服务不同。
- en: 'The `ClusterIP` Service exposes Pods using internally visible virtual IP addresses
    managed by kube-proxy on each Node. This means that the Service will be reachable
    from within the cluster only. We have visualized the ClusterIP Service principles
    in the following diagram:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP`服务通过kube-proxy在每个节点上管理的内部可见虚拟IP地址来暴露Pods。这意味着该服务仅能在集群内部访问。我们已经在下面的图表中展示了`ClusterIP`服务的原理：'
- en: '![](img/B22019_08_07.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_07.png)'
- en: 'Figure 8.7: ClusterIP Service'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：`ClusterIP`服务
- en: In the preceding image, the `ClusterIP` Service is configured in such a way
    that it will map requests coming from its IP and TCP port `8080` to the container’s
    TCP port `80`. The actual `ClusterIP` address is assigned dynamically unless you
    specify one explicitly in the specifications. The internal DNS Service in a Kubernetes
    cluster is responsible for resolving the `nginx-deployment-example` name to the
    actual `ClusterIP` address as a part of Service discovery.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图片中，`ClusterIP`服务配置成将来自其IP和TCP端口`8080`的请求映射到容器的TCP端口`80`。实际的`ClusterIP`地址是动态分配的，除非你在规格中明确指定了一个。Kubernetes集群中的内部DNS服务负责将`nginx-deployment-example`名称解析为实际的`ClusterIP`地址，作为服务发现的一部分。
- en: kube-proxy handles the management of virtual IP addresses on the nodes and adjusts
    the forwarding rules accordingly. Services in Kubernetes are essentially logical
    constructs within the cluster. There isn’t a separate physical process running
    inside the cluster for each Service to handle proxying. Instead, kube-proxy performs
    the necessary proxying and routing based on these logical Services.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy负责节点上虚拟IP地址的管理，并相应地调整转发规则。Kubernetes中的服务本质上是集群内的逻辑构造。集群内并没有为每个服务单独运行物理进程来处理代理。而是，kube-proxy基于这些逻辑服务执行必要的代理和路由操作。
- en: Contrary to `NodePort` Services, `ClusterIP` Services will not take one port
    of the worker node, and thus it is impossible to reach it from outside the Kubernetes
    cluster.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 与`NodePort`服务不同，`ClusterIP`服务不会占用工作节点的一个端口，因此无法从Kubernetes集群外部访问。
- en: Keep in mind that nothing prevents you from using both types of Services for
    the same pool of Pods. Indeed, if you have an app that should be publicly accessible
    but also privately exposed to other Pods, then you can simply create two Services,
    one `NodePort` Service and one `ClusterIP` Service.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，没有任何东西可以阻止你为同一组Pods使用两种类型的服务。实际上，如果你有一个应用程序需要对外部可访问，但又需要私密地暴露给其他Pods，那么你可以简单地创建两个服务，一个是`NodePort`服务，另一个是`ClusterIP`服务。
- en: In this specific use case, you’ll simply have to name the two Services differently
    so that they won’t conflict when creating them against `kube-apiserver`. Nothing
    else prevents you from doing so!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的用例中，你只需要为这两个服务命名不同，以便在创建时避免与`kube-apiserver`发生冲突。没有其他任何东西能阻止你这样做！
- en: Listing ClusterIP Services
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列出`ClusterIP`服务
- en: 'Listing `ClusterIP` Services is easy. It’s basically the same command as the
    one used for `NodePort` Services. Here is the command to run:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 列出`ClusterIP`服务非常简单。它基本上与用于`NodePort`服务的命令相同。这里是要运行的命令：
- en: '[PRE26]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As always, this command lists the Services with their type added to the output.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，这个命令会列出服务，并将其类型添加到输出中。
- en: Creating ClusterIP Services using the imperative way
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用命令式方法创建`ClusterIP`服务
- en: 'Creating `ClusterIP` Services can be achieved with a lot of different methods.
    Since it is an extremely used feature, there are lots of ways to create these,
    as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`ClusterIP`服务可以通过多种不同的方法来实现。由于它是一个广泛使用的功能，因此有很多方法可以创建这些服务，如下所示：
- en: Using the `--expose` parameter or `kubectl expose` method (the imperative way)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`--expose`参数或`kubectl expose`方法（命令式方式）
- en: Using a YAML manifest file (the declarative way)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YAML清单文件（声明式方式）
- en: 'The imperative way consists of using the --`expose` method. This will create
    a `ClusterIP` Service directly from a `kubectl run` command. In the following
    example, we will create an `nginx-clusterip` Pod as well as a `ClusterIP` Service
    to expose them both at the same time. Using the `--expose` parameter will also
    require defining a `ClusterIP` port. `ClusterIP` will listen to make the Pod reachable.
    The code is illustrated here:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 命令式方式是使用`--expose`方法。这将通过`kubectl run`命令直接创建`ClusterIP`服务。在以下示例中，我们将创建一个`nginx-clusterip`
    Pod，并创建一个`ClusterIP`服务来同时暴露它们。使用`--expose`参数还需要定义`ClusterIP`端口。`ClusterIP`将监听端口，以使Pod可访问。代码如下所示：
- en: '[PRE27]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, we get both a Pod and a Service to expose it. Let’s describe
    the Service.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们得到了一个Pod和一个暴露它的服务。让我们来描述这个服务。
- en: Describing ClusterIP Services
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描述ClusterIP服务
- en: Describing `ClusterIP` Services is the same process as describing any type of
    object in Kubernetes and is achieved using the `kubectl describe` command. You
    just need to know the name of the Service in order to describe to achieve that.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 描述`ClusterIP`服务的过程与描述Kubernetes中任何类型的对象相同，使用`kubectl describe`命令来完成。你只需要知道服务的名称，即可进行描述。
- en: 'Here, I’m going to the `ClusterIP` Service created previously:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我将访问之前创建的`ClusterIP`服务：
- en: '[PRE28]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output of this command shows us the `Selector` block, which shows that
    the `ClusterIP` Service was created by the `--expose` parameter with the proper
    label configured. This label matches the `nginx-clusterip` Pod we created at the
    same time. To be sure about that, let’s display the labels of the said Pod, as
    follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令的输出显示了`Selector`块，表明`ClusterIP`服务是通过`--expose`参数创建的，并且已正确配置标签。这个标签与我们同时创建的`nginx-clusterip`
    Pod相匹配。为了确认这一点，让我们显示该Pod的标签，如下所示：
- en: '[PRE29]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see, the selector on the Service matches the labels defined on the
    Pod. Communication is thus established between the two. We will now call the `ClusterIP`
    Service directly from another Pod on the cluster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，服务上的选择器与Pod上定义的标签相匹配。因此，二者之间建立了通信。接下来，我们将从集群中的另一个Pod直接调用`ClusterIP`服务。
- en: 'Since the `ClusterIP` Service is named `nginx-clusterip`, we know that it is
    reachable at this address: `nginx-clusterip.default.svc.cluster.local`.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`ClusterIP`服务名为`nginx-clusterip`，我们知道它可以通过这个地址访问：`nginx-clusterip.default.svc.cluster.local`。
- en: 'Let’s reuse the `k8sutils` container, as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用`k8sutils`容器，如下所示：
- en: '[PRE30]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `ClusterIP` Service correctly forwarded the request to the nginx Pod, and
    we do have the nginx default home page. The Service is working!
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP`服务正确地将请求转发到nginx Pod，我们确实看到了nginx的默认首页。服务正常工作！'
- en: We did not use `containous/whoami` as a web Service this time, but keep in mind
    that the `ClusterIP` Service is also doing load balancing internally following
    the round-robin algorithm. If you have 10 Pods behind a `ClusterIP` Service and
    your Service received 1,000 requests, then each Pod is going to receive 100 requests.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们没有使用`containous/whoami`作为Web服务，但请记住，`ClusterIP`服务内部也在执行负载均衡，采用轮询算法。如果你在`ClusterIP`服务后有10个Pod，并且该服务接收到1,000个请求，那么每个Pod将收到100个请求。
- en: Let’s now discover how to create a `ClusterIP` Service using YAML.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解如何使用YAML创建`ClusterIP`服务。
- en: Creating ClusterIP Services using the declarative way
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用声明式方式创建`ClusterIP`服务
- en: '`ClusterIP` Services can also be created the declarative way by applying YAML
    configuration files against `kube-apiserver`.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP`服务也可以通过应用YAML配置文件对`kube-apiserver`来声明式创建。'
- en: 'Here’s a YAML manifest file we can use to create the exact same `ClusterIP`
    Service we created before the imperative way:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个YAML清单文件，我们可以用它来创建与之前命令式创建的完全相同的`ClusterIP`服务：
- en: '[PRE31]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Take some time to read the comments in the YAML, especially the `port` and `targetPort`
    ones.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 花点时间阅读YAML中的注释，特别是关于`port`和`targetPort`的部分。
- en: Indeed, `ClusterIP` Services have their own port independent of the one exposed
    on the Pod side. You reach the `ClusterIP` Service by calling its DNS name and
    its port, and the traffic is going to be forwarded to the destination port on
    the Pods matching the labels and selectors.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，`ClusterIP`服务具有独立于Pod端暴露端口的端口。你通过调用其DNS名称及端口来访问`ClusterIP`服务，流量将被转发到匹配标签和选择器的Pods的目标端口。
- en: Keep in mind that no worker node port is involved here. The ports we are mentioning
    when it comes to `ClusterIP` scenarios have absolutely nothing to do with the
    host machine!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这里没有涉及工作节点端口。我们在讨论 `ClusterIP` 场景时提到的端口与主机机器完全无关！
- en: 'Before we continue with the next section, you can clean up the environment
    by deleting the `nginx-clusterip` Service as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一节之前，你可以通过以下方式删除 `nginx-clusterip` 服务来清理环境：
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Keep in mind that deleting the cluster won’t delete the Pods exposed by it.
    It is a different process; you’ll need to delete Pods separately. We will now
    discover one additional resource related to `ClusterIP` Services, which are headless
    Services.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，删除集群不会删除它暴露的 Pods。这是一个不同的过程；你需要单独删除 Pods。接下来我们将介绍一个与 `ClusterIP` 服务相关的额外资源——无头服务。
- en: Understanding headless Services
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解无头服务
- en: Headless Services are derived from the `ClusterIP` Service. They are not technically
    a dedicated type of Service (such as `NodePort` or `ClusterIP`), but they are
    an option from `ClusterIP`.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务源自 `ClusterIP` 服务。它们技术上不是一种专用类型的服务（如 `NodePort` 或 `ClusterIP`），而是 `ClusterIP`
    的一种选项。
- en: 'Headless Services can be configured by setting the `.spec.clusterIP` option
    to `None` in a YAML configuration file for the `ClusterIP` Service. Here is an
    example derived from our previous YAML file:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务可以通过在 `ClusterIP` 服务的 YAML 配置文件中将 `.spec.clusterIP` 选项设置为 `None` 来配置。以下是从我们之前的
    YAML 文件中派生的示例：
- en: '[PRE33]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: A headless Service roughly consists of a `ClusterIP` Service without load balancing
    and without a pre-allocated `ClusterIP` address. Thus, the load-balancing logic
    and the interfacing with the Pod are not defined by Kubernetes.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 一个无头服务大致由一个没有负载均衡和没有预分配 `ClusterIP` 地址的 `ClusterIP` 服务组成。因此，负载均衡逻辑和与 Pod 的接口不由
    Kubernetes 定义。
- en: Since a headless Service has no IP address, you are going to reach the Pod behind
    it directly, without the proxying and the load-balancing logic. What the headless
    Service does is return you the DNS names of the Pods behind it so that you can
    reach them directly. There is still a little load-balancing logic here, but it
    is implemented at the DNS level, not as Kubernetes logic.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无头服务没有 IP 地址，你将直接访问其背后的 Pod，而没有代理和负载均衡逻辑。无头服务的作用是返回背后 Pods 的 DNS 名称，以便你可以直接访问它们。这里仍然存在一些负载均衡逻辑，但它是在
    DNS 层面实现的，而不是作为 Kubernetes 的逻辑。
- en: When you use a normal `ClusterIP` Service, you’ll always reach one static IP
    address allocated to the Service and this is going to be your proxy to communicate
    with the Pod behind it. With a headless Service, the `ClusterIP` Service will
    just return the DNS names of the Pods behind it and the client will have the responsibility
    to establish a connection with the DNS name of its choosing.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用普通的 `ClusterIP` 服务时，你将始终访问分配给该服务的一个静态 IP 地址，这将作为与其背后 Pod 通信的代理。使用无头服务时，`ClusterIP`
    服务只会返回背后 Pods 的 DNS 名称，客户端将负责与其选择的 DNS 名称建立连接。
- en: Headless Services in Kubernetes are primarily used in scenarios where direct
    communication with individual Pods is required, rather than with a single endpoint
    or load-balanced set of Pods.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的无头服务主要用于需要与单个 Pod 直接通信的场景，而不是与单一端点或负载均衡的 Pods 集合进行通信。
- en: They are helpful when you want to build connectivity with clustered stateful
    Services such as **Lightweight Directory Access Protocol** (**LDAP**). In that
    case, you may want to use an LDAP client that will have access to the different
    DNS names of the Pods hosting the LDAP server, and this can’t be done with a normal
    `ClusterIP` Service since it will bring both a static IP and Kubernetes’ implementation
    of load balancing. Let’s now briefly introduce another type of Service called
    `LoadBalancer`.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想与集群状态服务（如**轻量目录访问协议**（**LDAP**））建立连接时，无头服务非常有用。在这种情况下，你可能想使用一个 LDAP 客户端，它将能够访问托管
    LDAP 服务器的 Pods 的不同 DNS 名称，而这不能通过普通的 `ClusterIP` 服务实现，因为普通的 `ClusterIP` 服务会带来一个静态
    IP 地址以及 Kubernetes 的负载均衡实现。现在，让我们简要介绍另一种类型的服务，叫做 `LoadBalancer`。
- en: The LoadBalancer Service
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡服务
- en: '`LoadBalancer` Services are very interesting to explain because this Service
    relies on the cloud platform where the Kubernetes cluster is provisioned. For
    it to work, it is thus required to use Kubernetes on a cloud platform that supports
    the `LoadBalancer` Service type.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancer` 服务非常有趣，因为它依赖于 Kubernetes 集群所在的云平台。为了使其工作，必须使用支持 `LoadBalancer`
    服务类型的云平台上的 Kubernetes。'
- en: For cloud providers that offer external load balancers, specifying the `type`
    field as `LoadBalancer` configures a load balancer for your Service. The creation
    of the load balancer occurs asynchronously, and details about the provisioned
    balancer are made available in the `.status.loadBalancer` field of the Service.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提供外部负载均衡器的云服务提供商，指定`type`字段为`LoadBalancer`会为你的服务配置负载均衡器。负载均衡器的创建是异步进行的，关于所提供负载均衡器的详细信息将在服务的`.status.loadBalancer`字段中提供。
- en: Certain cloud providers offer the option to define the `loadBalancerIP`. In
    such instances, the load balancer is generated with the specified `loadBalancerIP`.
    If the `loadBalancerIP` is not provided, the load balancer is configured with
    an ephemeral IP address. However, if you specify a `loadBalancerIP` on a cloud
    provider that does not support this feature, the provided `loadBalancerIP` is
    disregarded.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 某些云服务提供商提供定义`loadBalancerIP`的选项。在这种情况下，负载均衡器会使用指定的`loadBalancerIP`进行生成。如果没有提供`loadBalancerIP`，负载均衡器将配置为一个临时IP地址。然而，如果你在不支持此功能的云服务提供商上指定了`loadBalancerIP`，则提供的`loadBalancerIP`将被忽略。
- en: For a Service with its type set to LoadBalancer, the `.spec.loadBalancerClass`
    field allows you to utilize a load balancer implementation other than the default
    provided by the cloud provider. When `.spec.loadBalancerClass` is not specified,
    the `LoadBalancer` type of Service will automatically use the default load balancer
    implementation provided by the cloud provider, assuming the cluster is configured
    with a cloud provider using the `--cloud-provider` component flag. However, if
    you specify `.spec.loadBalancerClass`, it indicates that a load balancer implementation
    matching the specified class is actively monitoring for Services. In such cases,
    any default load balancer implementation, such as the one provided by the cloud
    provider, will disregard Services with this field set. It’s important to note
    that `spec.loadBalancerClass` can only be set on a Service of type `LoadBalancer`
    and, once set, it cannot be altered. Additionally, the value of `spec.loadBalancerClass`
    must adhere to a label-style identifier format, optionally with a prefix like
    `internal-vip` or `example.com/internal-vip`, with unprefixed names being reserved
    for end users.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类型设置为LoadBalancer的服务，`.spec.loadBalancerClass`字段允许你使用云服务提供商默认负载均衡器以外的负载均衡器实现。当`.spec.loadBalancerClass`未指定时，`LoadBalancer`类型的服务将自动使用云服务提供商提供的默认负载均衡器实现，前提是集群已使用`--cloud-provider`组件标志配置了云服务提供商。然而，如果你指定了`.spec.loadBalancerClass`，则表示一个与指定类匹配的负载均衡器实现正在积极监控服务。在这种情况下，任何默认的负载均衡器实现，例如云服务提供商提供的负载均衡器，将忽略设置了该字段的服务。需要注意的是，`spec.loadBalancerClass`只能在`LoadBalancer`类型的服务上设置，并且一旦设置，就不能更改。此外，`spec.loadBalancerClass`的值必须符合标签样式的标识符格式，前缀可选，如`internal-vip`或`example.com/internal-vip`，而不带前缀的名称保留给终端用户使用。
- en: 'The Kubernetes `Loadbalancer` type Service principles have been visualized
    in the following diagram:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了Kubernetes `Loadbalancer`类型服务的原理：
- en: '![](img/B22019_08_08.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_08.png)'
- en: 'Figure 8.8: LoadBalancer Service'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：LoadBalancer 服务
- en: In the next section, we will learn about the supported cloud providers for `LoadBalancer`
    Service types.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习支持`LoadBalancer`服务类型的云服务提供商。
- en: Supported cloud providers for the LoadBalancer Service type
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持`LoadBalancer`服务类型的云服务提供商
- en: 'Not all cloud providers support the `LoadBalancer` Service type, but we can
    name a few that do support it. These are as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有云服务提供商都支持`LoadBalancer`服务类型，但我们可以列出几个支持的服务提供商。它们如下：
- en: AWS
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS
- en: GCP
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP
- en: Azure
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure
- en: OpenStack
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack
- en: The list is not exhaustive, but it’s good to know that all three major public
    cloud providers are supported.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表并不详尽，但值得注意的是，所有三大主要公共云服务提供商都受支持。
- en: 'If your cloud provider is supported, keep in mind that the load-balancing logic
    will be the one implemented by the cloud provider: you have less control over
    how the traffic will be routed to your Pods from Kubernetes, and you will have
    to know how the load-balancer component of your cloud provider works. Consider
    it as a third-party component implemented as a Kubernetes resource.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的云服务提供商受支持，请记住，负载均衡的逻辑将由云服务提供商实现：你对流量如何从Kubernetes路由到你的Pods的控制较少，你需要了解你的云服务提供商的负载均衡器组件如何工作。将其视为作为Kubernetes资源实现的第三方组件。
- en: Should the LoadBalancer Service type be used?
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 是否应该使用LoadBalancer服务类型？
- en: This question is difficult to answer but a lot of people tend to not use a `LoadBalancer`
    Service type for a few reasons.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题很难回答，但许多人因以下几个原因倾向于不使用`LoadBalancer`服务类型。
- en: The main reason is that `LoadBalancer` Services are nearly impossible to configure
    from Kubernetes. Indeed, if you must use a cloud provider, it is better to configure
    it from the tooling provided by the provider rather than from Kubernetes. The
    `LoadBalancer` Service type is a generic way to provision a `LoadBalancer` Service
    but does not expose all the advanced features that the cloud provider may provide.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 主要原因是，`LoadBalancer`服务几乎无法从Kubernetes进行配置。事实上，如果必须使用云服务提供商，最好通过提供商提供的工具进行配置，而不是通过Kubernetes来配置。`LoadBalancer`服务类型是一个通用的方式来提供`LoadBalancer`服务，但无法暴露云服务提供商可能提供的所有高级功能。
- en: Also, load balancers provided by cloud providers often come with additional
    costs, which can vary depending on the provider and the amount of traffic being
    handled.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，云服务提供商提供的负载均衡器通常会产生额外费用，具体费用取决于提供商和处理的流量量。
- en: In the next section, we will learn about another Service type called `ExternalName`
    Services.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习另一种服务类型，称为`ExternalName`服务。
- en: The ExternalName Service type
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ExternalName服务类型
- en: '`ExternalName` Services are a powerful way to connect your Kubernetes cluster
    to external resources like databases, APIs, or Services hosted outside the cluster.
    They work by mapping a Service in your cluster to a DNS name instead of Pods within
    the cluster. This allows your applications inside the cluster to seamlessly access
    the external resource without needing to know its IP address or internal details.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExternalName`服务是一种强大的方式，可以将你的Kubernetes集群连接到外部资源，如数据库、API或托管在集群外部的服务。它们通过将集群中的服务映射到DNS名称而不是集群内的Pod来工作。这使得集群内的应用程序能够无缝访问外部资源，而无需知道其IP地址或内部细节。'
- en: '[PRE34]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here’s how it works: instead of linking the external names or IP address to
    internal Pods, you simply define a DNS name like `app-db.database.example.com`
    in the Service configuration. Now, when your applications within the cluster try
    to access `mysql-db`, the magic happens—the cluster’s DNS Service points them
    to your external database! They interact with it seamlessly, just like any other
    Service, but the redirection occurs at the DNS level, keeping things clean and
    transparent.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的：你无需将外部名称或IP地址链接到内部Pod，而是简单地在服务配置中定义一个DNS名称，如`app-db.database.example.com`。现在，当集群中的应用程序尝试访问`mysql-db`时，魔法发生了——集群的DNS服务将它们指向你的外部数据库！它们与数据库的交互是无缝的，就像任何其他服务一样，但重定向发生在DNS级别，保持了简洁和透明。
- en: '![](img/B22019_08_09.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_09.png)'
- en: 'Figure 8.9: ExternalName Service type'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9：ExternalName服务类型
- en: The `ExternalName` Service can be a Service hosted in another Kubernetes namespace,
    a Service outside of the Kubernetes cluster, a Service hosted in another Kubernetes
    cluster, and so on.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExternalName`服务可以是托管在另一个Kubernetes命名空间中的服务，Kubernetes集群外部的服务，托管在另一个Kubernetes集群中的服务，等等。'
- en: 'This approach offers several benefits:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有几个好处：
- en: '**Simplified configuration**: Applications only need to know the Service name,
    not the external resource details, making things much easier.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化配置**：应用程序只需要知道服务名称，而不需要了解外部资源的详细信息，这使得配置变得更加简单。'
- en: '**Flexible resource management**: If you later move the database into your
    cluster, you can simply update the Service and manage it internally without affecting
    your applications.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的资源管理**：如果你稍后将数据库迁移到集群内，你只需更新服务并在集群内管理它，而不会影响到应用程序。'
- en: '**Enhanced security**: Sensitive information like IP addresses stays hidden
    within the cluster, improving overall security.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的安全性**：像IP地址这样的敏感信息会隐藏在集群内，从而提高整体安全性。'
- en: Remember, `ExternalName` Services are all about connecting to external resources.
    For internal resource access within your cluster, stick to regular or headless
    Services.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`ExternalName`服务的重点是连接到外部资源。如果是集群内的资源访问，请使用常规服务或无头服务。
- en: Now that we have learned the different Service types in Kubernetes, let us explore
    how to use probes to ensure Service availability in the following sections.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了Kubernetes中的不同服务类型，接下来让我们探索如何使用探针来确保服务可用性。
- en: Implementing Service readiness using probes
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用探针实现服务就绪性
- en: When you create a Service to expose an application running inside Pods, Kubernetes
    doesn’t automatically verify the health of that application. The Pods may be up
    and running, but the application itself could still have issues, and the Service
    will continue to route traffic to it. This could result in users or other applications
    receiving errors or no response at all. To prevent this, Kubernetes provides health
    check mechanisms called probes. In the following section, we’ll explore the different
    types of probes—liveness, readiness, and startup probes—and how they help ensure
    your application is functioning properly.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个Service来暴露运行在Pods中的应用程序时，Kubernetes不会自动验证该应用程序的健康状况。Pods可能已经启动并运行，但应用程序本身可能仍然存在问题，而Service将继续向其路由流量。这可能导致用户或其他应用程序接收到错误或完全没有响应。为了防止这种情况发生，Kubernetes提供了名为探针的健康检查机制。在接下来的部分，我们将探讨不同类型的探针——存活探针、就绪探针和启动探针——以及它们如何帮助确保你的应用程序正常运行。
- en: What is ReadinessProbe and why do you need it?
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是ReadinessProbe，为什么需要它？
- en: '`ReadinessProbe`, along with `LivenessProbe`, is an important aspect to master
    if you want to provide the best possible experience to your end user. We will
    first discover how to implement `ReadinessProbe` and how it can help you ensure
    your containers are fully ready to serve traffic.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReadinessProbe`与`LivenessProbe`一起，是掌握Kubernetes中提供最佳用户体验的重要方面。我们将首先了解如何实现`ReadinessProbe`以及它如何帮助确保容器已完全准备好接受流量。'
- en: Readiness probes are technically not part of Services, but it is important to
    discover this feature alongside Kubernetes Services.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 就技术而言，Readiness probes并不是Services的一部分，但在了解Kubernetes Services的同时，了解这个特性非常重要。
- en: 'Just as with everything in Kubernetes, `ReadinessProbe` was implemented to
    bring a solution to a problem. This problem is this: how to ensure a Pod is fully
    ready before it can receive traffic, possibly from a Service.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Kubernetes中的所有功能一样，`ReadinessProbe`是为了解决一个问题而实现的。这个问题是：如何确保Pod在可以接收流量之前完全准备好，可能是来自Service的流量。
- en: 'Services obey a simple rule: they serve traffic to every Pod that matches their
    label selector. As soon as a Pod gets provisioned, if this Pod’s labels match
    the selector of Service in your cluster, then this Service will immediately start
    forwarding traffic to it. This can lead to a simple problem: if the app is not
    fully launched, because it has a slow launch process or requires some configuration
    from a remote API, and so on, then it might receive traffic from Services before
    being ready for it. The result would be a poor **user experience** (**UX**).'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Services遵循一个简单规则：它们会向每个与其标签选择器匹配的Pod提供流量。一旦Pod被分配，如果该Pod的标签与集群中Service的选择器匹配，那么该Service将立即开始向它转发流量。这可能会导致一个简单的问题：如果应用程序尚未完全启动，因为它有一个缓慢的启动过程，或者需要从远程API获取一些配置，等等，那么它可能会在准备好之前就收到来自Services的流量。结果将是一个糟糕的**用户体验**（**UX**）。
- en: To make sure this scenario never happens, we can use the feature called `ReadinessProbe`,
    which is an additional configuration to add to a Pod’s configuration.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保此场景永远不会发生，我们可以使用名为`ReadinessProbe`的功能，这是一种需要添加到Pod配置中的附加配置。
- en: When a Pod is configured with a readiness probe, it can send a signal to the
    control plane that it is not ready to receive traffic, and when a Pod is not ready,
    Services won’t forward any traffic to it. Let’s see how we can implement a readiness
    probe.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod配置了就绪探针时，它可以向控制平面发送信号，表示它还没有准备好接收流量，而当Pod尚未准备好时，Services将不会向其转发流量。让我们来看看如何实现就绪探针。
- en: Implementing ReadinessProbe
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现ReadinessProbe
- en: '`ReadinessProbe` implementation is achieved by adding some configuration data
    to a Pod YAML manifest. Please note that it has nothing to do with the `Service`
    object itself. By adding some configuration to the container `spec` in the Pod
    object, you can basically tell Kubernetes to wait for the Pod to be fully ready
    before it can receive traffic from Services.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReadinessProbe`的实现通过向Pod的YAML清单添加一些配置数据来完成。请注意，它与`Service`对象本身无关。通过向Pod对象中的容器`spec`添加一些配置，你基本上可以告诉Kubernetes，在Pod完全准备好之前，它不能接收来自Services的流量。'
- en: '`ReadinessProbe` can be of three different types, as outlined here:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReadinessProbe`可以有三种不同的类型，如下所述：'
- en: '`Command`: Issue a command inside the pod that should complete with exit code
    `0`, indicating the Pod is ready.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Command`：在Pod内部发出一个命令，该命令应以退出代码`0`完成，表示Pod已准备好。'
- en: '`HTTP`: An HTTP request that should complete with a response code >= `200`
    and < `400`, which indicates the Pod is ready.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HTTP`：一个HTTP请求，该请求应以响应码>=`200`且<`400`完成，表示Pod已准备好。'
- en: '`TCP`: Issue a TCP connection attempt. If the connection is established, the
    Pod is ready.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TCP`：发起TCP连接尝试。如果连接建立，Pod就准备好了。'
- en: 'Here is a YAML file configuring an nginx Pod with a readiness probe of type
    HTTP:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个YAML文件，配置了带有HTTP类型准备性探测的nginx Pod：
- en: '[PRE36]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As you can see, we have two important inputs under the `readinessProbe` key,
    as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在`readinessProbe`键下有两个重要的输入，如下所示：
- en: '`initialDelaySeconds`, which indicates the number of seconds the probe will
    wait before running the first health check'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialDelaySeconds`，表示探测在进行第一次健康检查之前将等待的秒数'
- en: '`periodSeconds`, which indicates the number of seconds the probe will wait
    between two consecutive health checks'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`periodSeconds`，表示探测在两个连续健康检查之间将等待的秒数'
- en: The readiness probe will be replayed regularly, and the interval between two
    checks will be defined by the `periodSeconds` parameter.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 准备性探测将定期重放，两个检查之间的间隔将由`periodSeconds`参数定义。
- en: In our case, our `ReadinessProbe` will run an HTTP call against the `/ready`
    path. If this request receives an HTTP response code >= `200` and < `400`, then
    the probe will be a success, and the Pod will be considered healthy.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，`ReadinessProbe`将针对`/ready`路径进行HTTP调用。如果此请求收到的HTTP响应代码>= `200` 且 < `400`，则探测将成功，Pod将被视为健康。
- en: '`ReadinessProbe` is important. In our example, the endpoint being called should
    test that the application is really in such a state that it can receive traffic.
    So, try to call an endpoint that is relevant to the state of the actual application.
    For example, you can try to call a page that will open a MySQL connection internally
    to make sure the application is capable of communicating with its database if
    it is using one, and so on. If you’re a developer, do not hesitate to create a
    dedicated endpoint that will just open connections to the different backends to
    be fully sure that the application is definitely ready.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReadinessProbe`是非常重要的。在我们的示例中，被调用的端点应该测试应用程序是否真的处于能够接收流量的状态。因此，尝试调用与实际应用程序状态相关的端点。例如，您可以尝试调用一个页面，该页面将内部打开一个MySQL连接，以确保应用程序能够与数据库通信（如果它使用的是数据库），等等。如果您是开发人员，不要犹豫，创建一个专用的端点，该端点仅打开与不同后端的连接，以确保应用程序确实已经准备好。'
- en: The Pod will then join the pool being served by the Service and will start receiving
    traffic. `ReadinessProbe` can also be configured as TCP and commands, but we will
    keep these examples for `LivenessProbe`. Let’s discover them now!
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Pod将加入由Service提供服务的池，并开始接收流量。`ReadinessProbe`也可以配置为TCP和命令，但我们将这些示例保留给`LivenessProbe`。现在，让我们来探索它们吧！
- en: What is LivenessProbe and why do you need it?
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是LivenessProbe，它为什么重要？
- en: '`LivenessProbe` resembles `ReadinessProbe` a lot. In fact, if you have used
    any cloud providers before, you might already have heard about something called
    health checks. `LivenessProbe` is basically a health check.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`LivenessProbe`与`ReadinessProbe`非常相似。事实上，如果您以前使用过任何云服务提供商，您可能已经听说过“健康检查”这一概念。`LivenessProbe`基本上就是健康检查。'
- en: 'Liveness probes are used to determine whether a Pod is in a broken state or
    not, and the usage of `LivenessProbe` is especially suited for long-running processes
    such as web services. Imagine a situation where you have a Service forwarding
    traffic to three Pods and one of them is broken. Services cannot detect that on
    their own, and they will just continue to serve traffic to the three Pods, including
    the broken one. In such situations, 33% of your requests will inevitably lead
    to an error response, resulting in a poor UX, as illustrated in the following
    screenshot:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Liveness探测用于确定Pod是否处于故障状态，`LivenessProbe`特别适用于像Web服务这样的长期运行的进程。假设有一个情况，您的Service正在将流量转发到三个Pod，其中一个Pod出现故障。服务无法自行检测到这一点，它们会继续将流量转发到这三个Pod，包括故障的那个Pod。在这种情况下，33%的请求必然会导致错误响应，导致用户体验差，如以下截图所示：
- en: '![](img/B22019_08_10.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_08_10.png)'
- en: 'Figure 8.10: One of the Pods is broken but the Service will still forward traffic
    to it'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：其中一个Pod出现故障，但Service仍然会将流量转发给它
- en: You want to avoid such situations, and to do so, you need a way to detect situations
    where Pods are broken, plus a way to kill such a container so that it goes out
    of the pool of Pods being targeted by the Service.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望避免这种情况， 为此，您需要一种方法来检测Pod故障的情况，并且需要一种方法来终止这种容器，使其不再处于由Service指向的Pod池中。
- en: '`LivenessProbe` is the solution to this problem and is implemented at the Pod
    level. Be careful because `LivenessProbe` cannot repair a Pod: it can only detect
    that a Pod is not healthy and command its termination. Let’s see how we can implement
    a Pod with `LivenessProbe`.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '`LivenessProbe`是解决这个问题的方案，它是在Pod级别实现的。需要小心的是，`LivenessProbe`无法修复Pod：它只能检测Pod不健康并命令其终止。让我们看看如何实现一个带有`LivenessProbe`的Pod。'
- en: Implementing LivenessProbe
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现LivenessProbe
- en: '`LivenessProbe` is a health check that will be executed on a regular schedule
    to keep track of the application state in the long run. These health checks are
    executed by the `kubelet` component and can be of different types, as outlined
    here:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '`LivenessProbe`是一个定期执行的健康检查，用于长期跟踪应用程序的状态。这些健康检查由`kubelet`组件执行，可以是不同类型的，正如这里所列：'
- en: '**Command**, where you issue a command in the container and its result will
    tell whether the Pod is healthy or not (exit code = `0` means healthy)'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Command**，你在容器中发出一个命令，命令的结果将告诉你Pod是否健康（退出码=`0`表示健康）'
- en: '**HTTP**, where you run an HTTP request against the Pod, and its result tells
    whether the Pod is healthy or not (HTTP response code >= `200` and < `400` means
    the Pod is healthy)'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP**，你对Pod发出一个HTTP请求，它的结果告诉你Pod是否健康（HTTP响应码>=`200`且<`400`意味着Pod健康）'
- en: '**TCP**, where you define a TCP call (a successful connection means the Pod
    is healthy)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TCP**，在这里你定义一个TCP调用（成功连接意味着Pod健康）'
- en: '**GRPC**, if the application supports and implements the gRPC Health Checking
    Protocol'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRPC**，如果应用程序支持并实现了gRPC健康检查协议'
- en: Each of these liveness probes will require you to input a parameter called `periodSeconds`,
    which must be an integer. This will tell the `kubelet` component the number of
    seconds to wait before performing a new health check. You can also use another
    parameter called `initialDelaySeconds`, which will indicate the number of seconds
    to wait before performing the very first health check. Indeed, in some common
    situations, a health check might lead to flagging an application as unhealthy
    just because it was performed too early. That’s why it might be a good idea to
    wait a little bit before performing the first health check, and that parameter
    is here to help.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 每个livenessProbe都需要你输入一个名为`periodSeconds`的参数，必须是一个整数。该参数告诉`kubelet`组件在执行新的健康检查之前等待的秒数。你还可以使用另一个名为`initialDelaySeconds`的参数，它表示执行第一次健康检查之前等待的秒数。实际上，在一些常见的情况下，健康检查可能会导致应用程序被标记为不健康，仅仅是因为检查太早执行了。这就是为什么在执行第一次健康检查之前等待一段时间可能是个好主意，这个参数就是为此提供帮助的。
- en: '`LivenessProbe` configuration is achieved at the Pod YAML configuration manifest,
    not at the Service one. Each container in the Pod can have its own `livenessProbe`.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '`LivenessProbe`配置是在Pod的YAML配置清单中实现的，而不是在Service中实现的。Pod中的每个容器都可以有自己的`livenessProbe`。'
- en: HTTP livenessProbe
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HTTP livenessProbe
- en: 'HTTP probes in Kubernetes offer additional customizable fields, such as host,
    scheme, path, headers, and port, to fine-tune how the health check requests are
    made to the application. Here is a configuration file that checks if a Pod is
    healthy by running an HTTP call against a `/healthcheck` endpoint in a nginx container:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的HTTP探针提供了额外的可定制字段，如主机、方案、路径、头部和端口，可以精细调整健康检查请求如何发送到应用程序。这里是一个配置文件，它通过对nginx容器中的`/healthcheck`端点进行HTTP调用来检查Pod是否健康：
- en: '[PRE37]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Please pay attention to all sections after the `livenessProbe` blocks. If you
    understand this well, you can see that we will wait 5 seconds before performing
    the first health check, and then, we will run one HTTP call against the `/healthcheck`
    path on port `80` every 5 seconds. One custom HTTP header was added. Adding such
    a header will be useful to identify our health checks in the access logs. Be careful
    because the `/healthcheck` path probably won’t exist in our nginx container, and
    so this container will never be considered healthy because the liveness probe
    will result in a `404` HTTP response. Keep in mind that for an HTTP health check
    to succeed, it must answer an HTTP >= `200` and < `400`. With `404` being out
    of this range, the answer Pod won’t be healthy.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`livenessProbe`块之后的所有部分。如果你理解这一点，你会看到我们将在执行第一次健康检查前等待5秒钟，然后每5秒钟对端口`80`上的`/healthcheck`路径进行一次HTTP调用。一个自定义HTTP头部已经被添加。添加这样的头部将有助于在访问日志中识别我们的健康检查。需要小心的是，`/healthcheck`路径可能在我们的nginx容器中不存在，因此该容器将永远不会被视为健康，因为活跃探针将返回`404`
    HTTP响应。请记住，要使HTTP健康检查成功，必须返回一个HTTP响应代码>=`200`且<`400`。`404`不在此范围内，因此答复的Pod不会被视为健康。
- en: Command livenessProbe
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命令型 livenessProbe
- en: 'You can also use a command to check if a Pod is healthy or not. Let’s grab
    the same YAML configuration, but now, we will use a command instead of an HTTP
    call in the liveness probe, as follows:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用命令来检查 Pod 是否健康。让我们获取相同的 YAML 配置，但这次我们将在存活探针中使用命令而不是 HTTP 调用，如下所示：
- en: '[PRE38]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If you check this example, you can see that it is much simpler than the HTTP
    one. Here, we are basically running a `cat /hello/world` command every 5 seconds.
    If the file exists and the `cat` command completes with an exit code equal to
    `0`, then the health check will succeed. Otherwise, if the file is not present,
    the health check will fail, and the Pod will never be considered healthy and will
    be terminated.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看这个示例，你会发现它比 HTTP 健康检查要简单得多。这里，我们基本上每 5 秒运行一次 `cat /hello/world` 命令。如果文件存在且
    `cat` 命令以退出代码 `0` 完成，则健康检查成功。否则，如果文件不存在，健康检查将失败，Pod 将永远不会被视为健康，并且会被终止。
- en: TCP livenessProbe
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TCP 型 livenessProbe
- en: 'In this situation, we will attempt a connection to a TCP socket on port `80`.
    If the connection is successfully established, then the health check will pass,
    and the container will be considered ready. Otherwise, the health check will fail,
    and the Pod will be terminated eventually. The code is illustrated in the following
    snippet:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将尝试连接到 `80` 端口的 TCP 套接字。如果连接成功建立，健康检查将通过，容器将被认为已准备好。否则，健康检查将失败，Pod
    最终会被终止。代码如下所示：
- en: '[PRE39]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Using TCP health checks greatly resembles using HTTP ones since HTTP is based
    on TCP. But having TCP as a liveness probe is especially nice if you want to keep
    track of an application that is not based on using HTTP as protocol and if using
    that command is irrelevant to you, as when health-checking an LDAP connection,
    for example.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TCP 健康检查与使用 HTTP 健康检查非常相似，因为 HTTP 是基于 TCP 的。但如果你要监控一个不基于 HTTP 协议的应用，且使用该命令对你来说无关紧要（例如在健康检查
    LDAP 连接时），那么使用 TCP 作为存活探针尤其有用。
- en: Using named Port with TCP and HTTP livenessProbe
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用命名端口配置 TCP 和 HTTP 型 livenessProbe
- en: 'You can use the named port to configure the `livenessProbe` for HTTP and TCP
    types (but not with gRPC) as follows:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用命名端口来配置 `livenessProbe`，适用于 HTTP 和 TCP 类型（但不适用于 gRPC），如下所示：
- en: '[PRE40]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the preceding example, the `liveness-port` has been defined under the `ports`
    section and used under the `httpGet` for `livenessProbe`.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，`liveness-port` 已在 `ports` 部分中定义，并在 `livenessProbe` 的 `httpGet` 中使用。
- en: As we have explored multiple liveness probes, let us learn about startupProbe
    in the next section.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经探讨了多种存活探针，让我们在下一节学习 `startupProbe`。
- en: Using startupProbe
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `startupProbe`
- en: Legacy applications sometimes demand extra time on their first startup. This
    can create a dilemma when setting up liveness probes, as fast response times are
    crucial for detecting deadlocks.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 传统应用程序有时在首次启动时需要更多时间。这在设置存活探针时可能会造成困境，因为快速的响应时间对于检测死锁至关重要。
- en: The answer lies in using either initialDelaySeconds or a dedicated `startupProbe`.
    The `initialDelaySeconds` parameter allows you to postpone the first readiness
    probe, giving the application breathing room to initialize.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案在于使用 `initialDelaySeconds` 或专门的 `startupProbe`。`initialDelaySeconds` 参数允许你推迟首次就绪探针的执行，为应用程序提供初始化的时间。
- en: 'However, for more granular control, consider using a `startupProbe`. This probe
    mirrors your liveness probe (command, HTTP, or TCP check) but with a longer `failureThreshold
    * periodSeconds` duration. This extended waiting period ensures the application
    has ample time to initialize before being deemed ready for traffic, while still
    enabling the liveness probe to swiftly detect issues afterwards, as explained
    in the following YAML snippet:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，若需要更精细的控制，可以考虑使用 `startupProbe`。这个探针与存活探针（命令、HTTP 或 TCP 检查）类似，但具有更长的 `failureThreshold
    * periodSeconds` 时长。这个扩展的等待时间确保应用程序有足够的时间进行初始化，才会被认为已准备好接收流量，同时仍然能让存活探针在之后迅速检测到问题，正如以下的
    YAML 片段所示：
- en: '[PRE41]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see in the example code above, it is possible to combine multiple
    probes to ensure the application is ready to serve. In the following section,
    we will also learn how to use `ReadinessProbe` and `LivenessProbe` together.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上面的示例代码中看到的，确实可以结合使用多个探针来确保应用程序准备好提供服务。在接下来的部分中，我们还将学习如何同时使用 `ReadinessProbe`
    和 `LivenessProbe`。
- en: Using ReadinessProbe and LivenessProbe together
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同时使用 ReadinessProbe 和 LivenessProbe
- en: You can use `ReadinessProbe` and `LivenessProbe` together in the same Pod.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在同一个 Pod 中同时使用 `ReadinessProbe` 和 `LivenessProbe`。
- en: 'They are configured almost the same way—they don’t have exactly the same purpose,
    and it is fine to use them together. Please note that both the probes share these
    parameters:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的配置方式几乎相同——它们的目的并不完全相同，且可以一起使用。请注意，这两个探针共享以下参数：
- en: '`initialDelaySeconds`: The number of seconds to wait before the first probe
    execution.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialDelaySeconds`：在执行第一次探针前等待的秒数。'
- en: '`periodSeocnds`: The number of seconds between two probes.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`periodSeocnds`：两个探针之间的秒数。'
- en: '`timeoutSeconds`: The number of seconds to wait before timeout.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timeoutSeconds`：在超时之前等待的秒数。'
- en: '`successThreshold`: The number of successful attempts to consider a Pod is
    ready (for `ReadinessProbe`) or healthy (for `LivenessProbe`).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`successThreshold`：将 Pod 视为已准备好（对于 `ReadinessProbe`）或健康（对于 `LivenessProbe`）的成功尝试次数。'
- en: '`failureThreshold`: The number of failed attempts to consider a Pod is not
    ready (for `ReadinessProbe`) or ready to be killed (for `LivenessProbe`).'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`failureThreshold`：将 Pod 视为未准备好（对于 `ReadinessProbe`）或准备好被终止（对于 `LivenessProbe`）的失败尝试次数。'
- en: '`TerminationGracePeriodSeconds`: Give containers a grace period to shut down
    gracefully before being forcefully stopped (default inherits Pod-level value).'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TerminationGracePeriodSeconds`：在强制停止容器之前，给予容器优雅关闭的宽限期（默认继承 Pod 层级的值）。'
- en: We now have discovered `ReadinessProbe` and `LivenessProbe`, and we have reached
    the end of this chapter about Kubernetes Services and implementation methods.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了`ReadinessProbe`和`LivenessProbe`，并且本章关于 Kubernetes 服务和实现方法的内容已经结束。
- en: Summary
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter was dense and contained a huge amount of information on networking
    in general when applied to Kubernetes. Services are just like Pods: they are the
    foundation of Kubernetes and mastering them is crucial to being successful with
    the orchestrator.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容较为密集，涵盖了在 Kubernetes 中应用的网络知识。服务就像 Pods 一样：它们是 Kubernetes 的基础，掌握它们对使用这个编排工具至关重要。
- en: Overall, in this chapter, we discovered that Pods have dynamic IP assignments,
    and they get a unique IP address when they’re created. To establish a reliable
    way to connect to your Pods, you need a proxy called a `Service` in Kubernetes.
    We’ve also discovered that Kubernetes Services can be of multiple types and that
    each type of Service is designed to address a specific need. We’ve also discovered
    what `ReadinessProbe` and `LivenessProbe` are and how they can help you in designing
    health checks to ensure your pods get traffic when they are ready and live.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，本章我们了解到，Pods 拥有动态 IP 分配，在创建时会获得一个唯一的 IP 地址。为了建立一个可靠的连接方式来连接到你的 Pods，你需要一个被称为
    `Service` 的代理。我们还了解到 Kubernetes 服务可以有多种类型，每种类型的服务都旨在解决特定的需求。我们还发现了 `ReadinessProbe`
    和 `LivenessProbe` 是什么，以及它们如何帮助你设计健康检查，确保 Pods 在准备好和存活时能接收到流量。
- en: In the next chapter, we’ll continue to discover the basics of Kubernetes by
    discovering the concepts of `PersistentVolume` and `PersistentVolumeClaims`, which
    are the methods Kubernetes uses to deal with persistent data. It is going to be
    a very interesting chapter if you want to build and provision stateful applications
    on your Kubernetes clusters, such as database or file storage solutions.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续探索 Kubernetes 的基础知识，了解 `PersistentVolume` 和 `PersistentVolumeClaims`
    的概念，它们是 Kubernetes 处理持久化数据的方法。如果你希望在 Kubernetes 集群上构建和提供有状态应用程序，如数据库或文件存储解决方案，这将是一个非常有趣的章节。
- en: Further reading
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Services, Load Balancing, and Networking: [https://kubernetes.io/docs/concepts/services-networking/](https://kubernetes.io/docs/concepts/services-networking/)'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '服务、负载均衡与网络: [https://kubernetes.io/docs/concepts/services-networking/](https://kubernetes.io/docs/concepts/services-networking/)'
- en: 'Kubernetes Headless Services: [https://kubernetes.io/docs/concepts/services-networking/service/#headless-services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes 无头服务: [https://kubernetes.io/docs/concepts/services-networking/service/#headless-services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)'
- en: 'Configure Liveness, Readiness and Startup Probes: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '配置 Liveness、Readiness 和 Startup 探针: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)'
- en: 'Network Policies: https://kubernetes.io/docs/concepts/services-networking/network-policies/'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '网络策略: [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)'
- en: 'Declare Network Policy: https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明网络策略：https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/
- en: 'Default NetworkPolicy: [https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies)'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认网络策略：[https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies)
- en: 'EndpointSlices: [https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/)'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EndpointSlices：[https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/)
- en: 'Network Plugins: [https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络插件：[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)
- en: 'Cluster Networking: [https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/)'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群网络： [https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/)
- en: Join our community on Discord
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者一起讨论：
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
- en: '![](img/QR_Code1190011064790816562.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1190011064790816562.png)'
