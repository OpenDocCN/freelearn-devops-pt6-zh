- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Scaling GenAI Applications on Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上扩展 GenAI 应用
- en: In this chapter, we will cover **application scaling** strategies and best practices
    for Kubernetes. Application scaling is a process where K8s can dynamically scale
    the resources to match the application demand, ensuring efficient and cost-effective
    resource utilization and optimal application performance. Kubernetes provides
    different scaling mechanisms to scale applications based on metrics such as CPU
    usage, memory, or custom metrics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 Kubernetes 的**应用扩展**策略和最佳实践。应用扩展是一个过程，K8s 可以动态扩展资源以匹配应用需求，从而确保高效且具成本效益的资源利用，并优化应用性能。Kubernetes
    提供了不同的扩展机制，基于如 CPU 使用率、内存或自定义指标等度量来扩展应用。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要内容：
- en: '**Scaling metrics**'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展指标**'
- en: '**HorizontalPodAutoscaler** (**HPA**)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**水平 Pod 自动扩展器** (**HPA**)'
- en: '**VerticalPodAutoscaler** (**VPA**)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垂直 Pod 自动扩展器** (**VPA**)'
- en: '**Kubernetes Event-Driven** **Autoscaler** (**KEDA**)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 事件驱动** **自动扩展器** (**KEDA**)'
- en: '**Cluster** **Autoscaler** (**CA**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群** **自动扩展器** (**CA**)'
- en: '**Karpenter**'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Karpenter**'
- en: Scaling metrics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展指标
- en: To scale applications correctly, it is essential to choose the right metrics
    to ensure efficient resource utilization and a seamless end user experience. These
    metrics can be divided into conventional and custom metrics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确扩展应用，选择合适的指标至关重要，以确保高效的资源利用和无缝的终端用户体验。这些指标可以分为常规指标和自定义指标。
- en: Conventional metrics
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常规指标
- en: 'These are common metrics in Kubernetes used for horizontal or vertical scaling:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在 Kubernetes 中用于水平或垂直扩展的常见指标：
- en: '**CPU usage**: This measures the percentage of CPU utilization. High CPU utilization
    might indicate that the application is under heavy load, requiring more instances
    (Pods) to handle the demand, whereas a constantly low CPU usage might indicate
    the overprovisioning of resources, and the number of Pods can be scaled down.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU 使用率**：这衡量 CPU 利用率的百分比。高 CPU 利用率可能表示应用负载过重，需要更多实例（Pod）来处理需求，而持续较低的 CPU
    使用率则可能表示资源的过度配置，此时 Pod 数量可以缩减。'
- en: '**Memory usage**: This measures the amount of memory consumed by a Pod. Like
    CPU, high memory usage can signal that the application is handling a large amount
    of data, meaning more resources are needed to prevent memory shortages. When a
    process inside a container exceeds the memory limit, the container will be terminated
    by the container runtime (CRI), which is different from the CPU limit. If a CPU
    limit is set and a container exceeds that limit, the container will not get terminated
    but rather throttled or slowed down. Generally, it is not a best practice to use
    memory usage as a scaling metric because applications are often poor at memory
    management.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存使用率**：这衡量 Pod 消耗的内存量。与 CPU 相似，高内存使用率可能表明应用正在处理大量数据，意味着需要更多资源以防止内存不足。当容器中的进程超出内存限制时，容器将被容器运行时（CRI）终止，这与
    CPU 限制不同。如果设置了 CPU 限制，且容器超出该限制，容器不会被终止，而是会被限制或降低速度。通常，将内存使用率作为扩展指标并不是最佳实践，因为应用程序通常在内存管理方面表现不佳。'
- en: Custom metrics
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义指标
- en: 'Custom metrics allow K8s scaling based on more application or use case-specific
    metrics, allowing better granular control over the application’s performance.
    Some examples of custom metrics are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义指标允许 K8s 根据更多应用或特定用例的指标进行扩展，从而更精细地控制应用性能。一些自定义指标的示例如下：
- en: '**HTTP requests rate**: This measures the number of HTTP/HTTPS requests or
    API calls the application receives per second. We could use monitoring tools,
    such as **Prometheus**, to track the request rates and scale the application when
    requests spike and exceed a certain threshold.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP 请求率**：这衡量应用每秒接收到的 HTTP/HTTPS 请求或 API 调用的数量。我们可以使用监控工具，如 **Prometheus**，来追踪请求率，并在请求激增并超过某个阈值时扩展应用。'
- en: '**Queue length**: This measures the number of unprocessed jobs or messages
    in a queue, such as **Amazon SQS** or **RabbitMQ**. *Queue backlogs* indicate
    that the application is not able to keep up with the load and needs more resources
    to process jobs in a timely manner. It is a critical metric, especially in **event-driven
    architecture** (**EDA**). K8s scaling mechanisms, such as KEDA, support scaling
    based on queue metrics.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**队列长度**：这衡量了队列中未处理的任务或消息数量，如**Amazon SQS**或**RabbitMQ**。*队列积压*表明应用程序无法跟上负载，需要更多资源以及时处理任务。这是一个关键指标，特别是在**事件驱动架构**（**EDA**）中。K8s的扩展机制，如KEDA，支持基于队列指标进行扩展。'
- en: '**Latency/response time**: This measures the time it takes for the application
    to respond to requests. High latency often signals that the application is struggling
    under the current load, and scaling out additional instances can help maintain
    low response times.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟/响应时间**：这衡量了应用程序响应请求所需的时间。高延迟通常表示应用程序在当前负载下难以应对，扩展更多实例可以帮助保持较低的响应时间。'
- en: '**Error rate**: This measures the number of failed requests. An increase in
    the error rate could indicate that the current number of resources is insufficient
    to handle the load, leading to failures; scaling up the resources might be required.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误率**：这衡量了请求失败的数量。错误率的增加可能表示当前资源数量不足以处理负载，导致失败；可能需要扩展资源。'
- en: '**Concurrency/active sessions**: This measures the number of active connections,
    users, or sessions interacting with the application. For applications such as
    online games or video streaming platforms, the number of active users could be
    a critical indicator of the application load.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发/活跃会话**：这衡量了与应用程序交互的活跃连接、用户或会话的数量。对于在线游戏或视频流平台等应用程序，活跃用户数量可能是应用程序负载的关键指标。'
- en: '**GPU utilization**: This measures the percentage of GPU capacity consumed
    by an application. When scaling GenAI applications on K8s, using GPU utilization
    as a scaling metric is effective because of heavy reliance and the indication
    of the load on the application. With **NVIDIA GPUs**, metrics are exported using
    the **DCGM-Exporter** addon ([https://github.com/NVIDIA/dcgm-exporter](https://github.com/NVIDIA/dcgm-exporter)),
    which can be installed via Helm, allowing an observability agent (such as **Prometheus**)
    to scrape these metrics. We will configure this in our EKS cluster as part of
    [*Chapter 12*](B31108_12.xhtml#_idTextAnchor160).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU利用率**：这衡量了应用程序消耗的GPU容量的百分比。在K8s上扩展GenAI应用程序时，使用GPU利用率作为扩展指标是有效的，因为它反映了应用程序的负载和资源的重度依赖。对于**NVIDIA
    GPUs**，可以使用**DCGM-Exporter**插件（[https://github.com/NVIDIA/dcgm-exporter](https://github.com/NVIDIA/dcgm-exporter)）导出指标，该插件可以通过Helm安装，使得观察性代理（如**Prometheus**）能够抓取这些指标。我们将在我们的EKS集群中配置此项，作为[*第12章*](B31108_12.xhtml#_idTextAnchor160)的一部分。'
- en: These are some of the commonly used metrics used in K8s to scale the resources.
    Some applications may require a mix of metrics. For example, a web application
    might use network I/O and request rate together to ensure optimal resource utilization
    and performance. Custom metrics are not available by default for K8s autoscaling;
    a *custom metrics adapter* needs to be installed to make them available from respective
    metric sources. This adapter acts as a bridge between the metrics system and K8s,
    exposing the metrics via K8s custom metrics API. Some examples are **prometheus-adapter**
    ([https://github.com/kubernetes-sigs/prometheus-adapter](https://github.com/kubernetes-sigs/prometheus-adapter))
    and **Datadog Cluster** **Agent** ([https://docs.datadoghq.com/containers/guide/cluster_agent_autoscaling_metrics](https://docs.datadoghq.com/containers/guide/cluster_agent_autoscaling_metrics)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是K8s中常用的一些指标，用于扩展资源。有些应用程序可能需要混合使用多个指标。例如，一个web应用程序可能会同时使用网络I/O和请求速率，以确保资源利用率和性能的最优化。K8s的自动扩展默认不提供自定义指标；需要安装*自定义指标适配器*，以使它们从相应的指标源中可用。该适配器充当指标系统与K8s之间的桥梁，通过K8s自定义指标API暴露这些指标。一些例子包括**prometheus-adapter**（[https://github.com/kubernetes-sigs/prometheus-adapter](https://github.com/kubernetes-sigs/prometheus-adapter)）和**Datadog
    Cluster** **Agent**（[https://docs.datadoghq.com/containers/guide/cluster_agent_autoscaling_metrics](https://docs.datadoghq.com/containers/guide/cluster_agent_autoscaling_metrics)）。
- en: In this section, we discussed different types of scaling metrics, such as conventional
    and custom metrics, and had a look at some examples. We also discussed the custom
    GPU utilization metric and saw that it is an effective measure to scale the GenAI
    workloads. Next, let’s explore horizontal Pod autoscaling and see how these metrics
    can be used to autoscale K8s Pods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了不同类型的扩展度量，如常规度量和自定义度量，并查看了一些示例。我们还讨论了自定义GPU利用率度量，并发现它是扩展GenAI工作负载的有效指标。接下来，让我们探索水平Pod自动扩展，并了解如何使用这些度量来自动扩展K8s
    Pods。
- en: HorizonalPodAutoscaler (HPA)
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平Pod自动扩展器（HPA）
- en: HPA ([https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/))
    is a K8s feature that adjusts the number of Pods in a deployment based on user-defined
    metrics, such as CPU or memory utilization. The primary goal of HPA is to ensure
    that applications can handle varying loads by dynamically scaling in or out the
    number of Pods. HPA does not apply to objects that can’t be scaled, such as *DaemonSets*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: HPA（[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)）是K8s的一项功能，基于用户定义的度量（如CPU或内存利用率）调整部署中Pod的数量。HPA的主要目标是通过动态地扩展或收缩Pod的数量，确保应用能够处理不同的负载。HPA不适用于不能扩展的对象，如*DaemonSets*。
- en: HPA uses a metrics server or monitoring system, such as Prometheus, to collect
    real-time data on the defined metrics. HPA has a **controller** component that
    runs in the Kubernetes control plane. It periodically checks the current metrics
    of the target application, such as deployment, and compares it to the desired
    thresholds specified in the HPA resource configuration.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: HPA使用度量服务器或监控系统（如Prometheus）收集定义度量的实时数据。HPA有一个**控制器**组件，在Kubernetes控制平面中运行。它定期检查目标应用（如部署）的当前度量，并将其与HPA资源配置中指定的期望阈值进行比较。
- en: Based on the metrics, the controller adjusts the desired number of Pods. If
    resource usage, such as CPU utilization, exceeds the threshold, HPA increases
    the number of Pods, whereas if the usage drops below the threshold, HPA decreases
    the number of Pods, as shown in *Figure 6**.1*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些度量，控制器调整所需的Pod数量。如果资源使用率（如CPU利用率）超过阈值，HPA会增加Pod的数量，而如果使用率低于阈值，HPA会减少Pod的数量，如*图6.1*所示。
- en: '![Figure 6.1 – HPA overview](img/B31108_06_1.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – HPA概述](img/B31108_06_1.jpg)'
- en: Figure 6.1 – HPA overview
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – HPA概述
- en: 'The following YAML file indicates how HPA can be implemented for our e-commerce
    chatbot UI deployment in K8s:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下YAML文件显示了如何在K8s中为我们的电子商务聊天机器人UI部署实现HPA：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we are setting `kind` to `HorizontalPodAutoscaler`, which specifies
    that this manifest is for an HPA, and setting its `name` to `chabot-ui-hpa`. In
    the `spec` section, we are setting the scaling target for this HPA as `chatbot-ui-deployment`,
    which is a deployment, and `apps/v1` is the API version for the target resource.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将`kind`设置为`HorizontalPodAutoscaler`，这表示这个清单是针对HPA的，并将`name`设置为`chabot-ui-hpa`。在`spec`部分，我们将此HPA的扩展目标设置为`chatbot-ui-deployment`，这是一个部署，而`apps/v1`是目标资源的API版本。
- en: 'Next, we set the minimum number of replicas to `1` (`minReplicas: 1`) and the
    maximum number of replicas to `5` (`maxReplicas: 5`). Finally, we set the metrics
    that HPA can monitor and use to make scaling decisions. In this example, we are
    using average CPU utilization across all the *chatbot-ui-deployment* deployment
    Pods as the metric.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们将副本的最小数量设置为`1`（`minReplicas: 1`），最大副本数量设置为`5`（`maxReplicas: 5`）。最后，我们设置HPA可以监控并用来做扩展决策的度量。在这个例子中，我们使用所有*chatbot-ui-deployment*部署Pod的平均CPU利用率作为度量。'
- en: 'In the `target` specification, `averageUtilization: 70` sets the target CPU
    utilization to `70`. If the average CPU utilization exceeds 70%, HPA will start
    scaling up the number of replicas to meet this target; however, it will not exceed
    five replicas due to the maximum limit we defined. Once the CPU utilization drops
    below 70%, it will start scaling down but will ensure that there is still one
    replica running all the time (`minReplicas`). You can also use K8s imperative
    commands to create and manage HPA resources. For example commands, refer to official
    K8s documentation at [https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '在 `target` 规范中，`averageUtilization: 70` 将目标 CPU 利用率设置为 `70`。如果平均 CPU 利用率超过
    70%，HPA 将开始扩展副本数以满足此目标；然而，由于我们定义了最大副本数限制，它不会超过五个副本。当 CPU 利用率降到 70% 以下时，它将开始缩减副本数，但会确保始终有一个副本在运行（`minReplicas`）。你还可以使用
    K8s 强制命令来创建和管理 HPA 资源。有关命令的示例，请参考官方 K8s 文档 [https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/)。'
- en: You can download the HPA manifest from the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-hpa.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-hpa.yaml)
    and execute the following command to apply the HPA policy in our EKS cluster.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 仓库下载 HPA 清单，链接地址为 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-hpa.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-hpa.yaml)，并执行以下命令在我们的
    EKS 集群中应用 HPA 策略。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To prevent frequent scaling up and down, the following HPA behavior configuration
    can be used:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止频繁的扩缩，以下 HPA 行为配置可供使用：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `scaleDown` section controls how fast the HPA can scale down or remove the
    replicas. The maximum allowed scale-down has been set to 50% in this example,
    which means that HPA can remove up to 50% of the current replicas during each
    scale-down event. `periodSeconds` defines the time window in seconds over which
    the scaling rule is evaluated. `stabilizationWindowSeconds` specifies the amount
    of time (in seconds) that HPA will wait before scaling down after detecting lower
    resource utilization. This helps prevent frequent and aggressive scaling down
    that might occur due to temporary drops in usage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`scaleDown` 部分控制 HPA 缩减或移除副本的速度。在此示例中，最大允许的缩减比例已设置为 50%，这意味着 HPA 在每次缩减事件中可以移除最多
    50% 的当前副本。`periodSeconds` 定义了在此时间窗口内评估扩缩规则的秒数。`stabilizationWindowSeconds` 指定了
    HPA 在检测到较低资源利用率后，缩减副本之前要等待的时间（秒）。这有助于防止由于暂时的使用下降而导致频繁且过于激进的缩减。'
- en: In this case, the stabilization window is set to 180 seconds, meaning that HPA
    will wait for 3 minutes before it reduces the number of replicas after a drop
    in load.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，稳定窗口设置为 180 秒，这意味着 HPA 在负载下降后，会等待 3 分钟才会减少副本数量。
- en: The `scaleUp` section of behavior classification controls how fast the HPA can
    scale up or add new Pods. In this case, we are defining scale-up policies using
    a fixed number of Pods instead of percentage based approach. A percentage based
    policy would increase the number of Pods by a certain percentage of the current
    replica count. For example, if we have 5 Pods and the policy allows a 60% increase,
    HPA could scale up by 3 Pods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`scaleUp` 部分的行为分类控制 HPA 扩展或添加新 Pod 的速度。在此情况下，我们定义了一个固定数量的 Pod 来作为扩展策略，而不是基于百分比的方法。基于百分比的策略会根据当前副本数的百分比来增加
    Pod 数量。例如，如果我们有 5 个 Pod，并且策略允许 60% 的增加，HPA 就会扩展 3 个 Pod。'
- en: In this example, scaling is done in fixed increments of two Pods and HPA will
    allow adding up to two Pods within every 15-second window, if more replicas are
    required. We have set `stabilizationWindowSeconds` to `0`, meaning there’s no
    delay before scaling up.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，扩展是按每次增加两个 Pod 的固定增量进行的，如果需要更多副本，HPA 每 15 秒内最多可以增加两个 Pod。我们将 `stabilizationWindowSeconds`
    设置为 `0`，这意味着扩展之前没有延迟。
- en: 'In this walkthrough, we created an HPA policy based on the Pod-level metrics,
    which aggregate the resource usage of all containers within a Pod. However, in
    multi-container Pods, this metric might not accurately represent the performance
    of an individual container. To address this, K8s introduced container resource
    metrics that allow HPA to track the resource usage of specific containers across
    Pods when scaling the target resource. This approach enables you to set scaling
    thresholds for the containers that are most critical to your application. For
    example, if your Pod includes a web application and a sidecar container that provides
    logging, you can configure HPA to scale based solely on the web application’s
    CPU utilization while ignoring the sidecar’s resource use. The following code
    snippet demonstrates how to use the CPU utilization of the `web-app` container
    to scale the overall deployment:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们基于Pod级别的指标创建了一个HPA策略，该指标汇总了Pod内所有容器的资源使用情况。然而，在多容器Pod中，这个指标可能无法准确反映单个容器的性能。为了解决这个问题，K8s引入了容器资源指标，允许HPA在扩展目标资源时，跟踪跨Pod的特定容器资源使用情况。这种方法使您能够为最关键的容器设置扩展阈值。例如，如果您的Pod包含一个Web应用程序和一个提供日志记录的sidecar容器，您可以将HPA配置为仅根据Web应用程序的CPU利用率进行扩展，而忽略sidecar的资源使用。以下代码片段演示了如何使用`web-app`容器的CPU利用率来扩展整体部署：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this section, we explored HPA, a K8s feature designed to automatically adjust
    the number of Pods in a K8s Deployment, StatefulSet, or ReplicaSet based on user-defined
    metrics such as CPU, memory, or GPU utilization. Its primary goal is to maintain
    an adequate number of K8s Pods to handle dynamic workloads effectively. We also
    covered how to configure HPA policies and customize the scaling behavior. Next,
    let’s dive into VPA.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了HPA，这是一个K8s功能，旨在根据用户定义的指标（如CPU、内存或GPU利用率）自动调整K8s部署、StatefulSet或ReplicaSet中的Pod数量。其主要目标是保持足够数量的K8s
    Pods，以有效处理动态工作负载。我们还介绍了如何配置HPA策略并自定义扩展行为。接下来，让我们深入了解VPA。
- en: VerticalPodAutoscaler (VPA)
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VerticalPodAutoscaler（VPA）
- en: 'VPA can adjust the resource requests and limits for the CPU and memory of the
    Pods based on actual usage and configuration. This differs from HPA, which adjusts
    the number of Pods based on the metrics defined. VPA has four operating modes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: VPA可以根据实际使用情况和配置调整Pod的CPU和内存资源请求和限制。这与HPA不同，HPA根据定义的指标调整Pod的数量。VPA有四种操作模式：
- en: '**Off**: In this mode, VPA only provides resource recommendations, but does
    not apply any changes.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关闭**：在此模式下，VPA仅提供资源建议，但不应用任何更改。'
- en: '**Auto**: VPA applies changes to resource requests and restarts the Pods, if
    needed.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动**：VPA会对资源请求进行修改并重启Pod（如果需要的话）。'
- en: '**Recreate**: VPA applies changes on Pod creation and updates them on existing
    Pods by evicting them.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重建**：VPA在Pod创建时应用更改，并通过驱逐现有Pod来更新它们。'
- en: '**Initial**: VPA applies resource recommendations only when new Pods are created
    or existing Pods are restarted, without interfering with the running Pods.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始**：VPA仅在创建新Pod或重启现有Pod时应用资源建议，而不会干扰正在运行的Pod。'
- en: VPA collects resource usage data via the K8s Metrics API and suggests optimal
    CPU and memory values for resource requests and limits. In *Auto* mode, it can
    automatically evict Pods so that they are rescheduled with updated resource requests,
    as shown in *Figure 6**.2*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: VPA通过K8s Metrics API收集资源使用数据，并建议适当的CPU和内存值，用于资源请求和限制。在*自动*模式下，它可以自动驱逐Pod，从而使它们重新调度并应用更新的资源请求，如*图6.2*所示。
- en: '![Figure 6.2 – VPA overview](img/B31108_06_2.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – VPA概述](img/B31108_06_2.jpg)'
- en: Figure 6.2 – VPA overview
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – VPA概述
- en: Unlike HPA, VPA is not included with K8s by default; it is a separate project
    available on GitHub at [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler).
    Install the VPA add-on by following the instructions provided at [https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/installation.md](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/installation.md).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与HPA不同，VPA并不默认包含在K8s中；它是一个独立的项目，可以在GitHub上找到，网址为[https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)。按照[https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/installation.md](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/installation.md)上的说明安装VPA插件。
- en: 'The following YAML file implements VPA in our e-commerce chatbot UI application
    deployed in K8s:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的YAML文件在我们部署在K8s中的电子商务聊天机器人UI应用程序中实现了VPA：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can download the VPA manifest from the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-vpa.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-vpa.yaml)
    and execute the following command to apply the VPA policy in our EKS cluster:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从GitHub存储库下载VPA清单，网址为[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-vpa.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/chatbot-ui-vpa.yaml)，并执行以下命令将VPA策略应用于我们的EKS集群：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we are setting `kind` to `VerticalPodAutoscaler` and `updateMode`
    to `Auto`, which means that VPA will automatically adjust resource requests and
    limits. The `containerName *` wildcard indicates that the policy should be applied
    to all containers in the Pod.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将`kind`设置为`VerticalPodAutoscaler`，将`updateMode`设置为`Auto`，这意味着VPA将自动调整资源请求和限制。`containerName
    *`通配符表示该策略应适用于Pod中的所有容器。
- en: '`maxAllowed` limits ensure that VPA does not set resource values beyond the
    specified range. In this configuration, the `minAllowed` limits are defined as
    `1000m` (equivalent to one vCPU) and 2 GB of memory, while the maximum allowed
    CPU is `2000m` (or two vCPUs) with 4 GB of memory. Defining `controlledValues`
    as `RequestsAndLimits` means that VPA should manage both resource requests and
    limits. Resource requests are the amount of CPU or memory that a Pod requests
    from the Kubernetes scheduler when it starts. We can also set this to `RequestsOnly`
    if we want VPA to adjust the resource requests only, but not the limits.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxAllowed`限制确保VPA不会将资源值设置超出指定范围。在此配置中，`minAllowed`限制定义为`1000m`（相当于一个vCPU）和2GB内存，而最大允许的CPU为`2000m`（或两个vCPUs）和4GB内存。将`controlledValues`定义为`RequestsAndLimits`表示VPA应同时管理资源请求和限制。资源请求是Pod在启动时向Kubernetes调度程序请求的CPU或内存量。如果我们只希望VPA调整资源请求而不是限制，可以将其设置为`RequestsOnly`。'
- en: Combining HPA and VPA
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合HPA和VPA
- en: It is recommended not to combine HPA and VPA in the same cluster (unless VPA
    is set to “off”), as it can result in potential conflicts. For example, VPA adjusts
    the CPU/memory resource requests and limits for a Pod, while HPA scales the number
    of Pods based on current utilization. If HPA and VPA are used simultaneously,
    VPA changes might confuse HPA, as the resource usage of a single Pod fluctuates
    frequently, affecting the metrics used by HPA to scale.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 建议不要在同一集群中结合使用HPA和VPA（除非将VPA设置为“off”），因为这可能导致潜在的冲突。例如，VPA调整Pod的CPU/内存资源请求和限制，而HPA根据当前利用率扩展Pod的数量。如果同时使用HPA和VPA，VPA的更改可能会使HPA感到困惑，因为单个Pod的资源使用频繁波动，影响HPA用于扩展的指标。
- en: In this section, we explored VPA and its operating modes, along with an example
    manifest. VPA monitors resource usage of workloads and automatically adjusts the
    resource requests to optimize resource allocation in a K8s cluster. We also discussed
    potential conflicts that arise by combining HPA and VPA in auto-operating mode
    simultaneously.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了VPA及其操作模式，以及一个示例清单。VPA监视工作负载的资源使用情况，并自动调整资源请求以优化K8s集群中的资源分配。我们还讨论了同时在自动操作模式下结合使用HPA和VPA可能引发的潜在冲突。
- en: KEDA
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KEDA
- en: With cloud adoption, there is a growing trend of microservice and EDA adoption.
    In this implementation, different building blocks are divided into microservice-based
    implementations, which are self-contained and talk to each other only through
    API calls. This implementation allows easier updates and the flexibility to add
    new features.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 随着云采用的普及，微服务和EDA采用趋势日益增长。在这种实施中，不同的构建模块被划分为基于微服务的实现，这些实现是自包含的，仅通过API调用彼此通信。这种实现允许更容易进行更新，并具备添加新功能的灵活性。
- en: '**KEDA** ([https://keda.sh/](https://keda.sh/)) is an open source project that
    brings event-driven autoscaling to Kubernetes. It extends Kubernetes’ built-in
    HPA by allowing applications to scale based on external event sources, such as
    message queue depth, event stream size, or any custom metrics.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**KEDA** ([https://keda.sh/](https://keda.sh/)) 是一个开源项目，将事件驱动的自动缩放引入Kubernetes。它通过允许应用程序根据外部事件源（如消息队列深度、事件流大小或任何自定义指标）进行缩放，扩展了Kubernetes内置的HPA功能。'
- en: A KEDA **ScaledObject** is a custom resource that defines how a target (e.g.,
    a Deployment) should be autoscaled based on event-driven or external metrics.
    When we define a ScaledObject in KEDA, it automatically creates an HPA resource
    behind the scenes. The HPA resource then scales the deployment based on the metrics
    provided by KEDA. The ScaledObject defines the scaling logic (e.g., which external
    metric to use, scaling thresholds, and min/max replicas), and KEDA takes care
    of managing the HPA based on this configuration.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA的**ScaledObject**是一个自定义资源，定义了如何根据事件驱动或外部指标自动扩展目标（例如部署）。当我们在KEDA中定义一个ScaledObject时，它会在后台自动创建一个HPA资源。然后，HPA资源根据KEDA提供的指标扩展部署。ScaledObject定义了扩展逻辑（例如，使用哪个外部指标、扩展阈值以及最小/最大副本数），而KEDA则根据此配置管理HPA。
- en: KEDA is particularly useful for event-driven architectures where workloads may
    be sporadic and need scaling only when there’s an event, such as a new customer
    signing in or a new item being added to the cart.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA特别适用于事件驱动架构，在这种架构中，工作负载可能是偶发的，只有在发生事件时才需要扩展，例如新用户登录或新物品加入购物车。
- en: KEDA supports various **scalers**, which are integrations with external services,
    such as Amazon SQS, Apache Kafka, and Prometheus. These scalers watch for changes
    in metrics or events, such as the number of messages in a queue or the rate of
    HTTP requests.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA支持各种**扩展器**，这些扩展器与外部服务集成，如Amazon SQS、Apache Kafka和Prometheus。这些扩展器监控指标或事件的变化，例如队列中的消息数量或HTTP请求的速率。
- en: Note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to the KEDA documentation at [https://keda.sh/docs/latest/scalers/](https://keda.sh/docs/latest/scalers/)
    for a list of available scalers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考KEDA文档：[https://keda.sh/docs/latest/scalers/](https://keda.sh/docs/latest/scalers/)，以获取可用扩展器的列表。
- en: One unique feature of KEDA is that it can scale to zero when there are no events
    to process, which is beneficial in serverless and event-driven applications.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA的一个独特特点是，当没有事件需要处理时，它可以缩放到零，这在无服务器和事件驱动的应用程序中非常有用。
- en: To illustrate this behavior, let’s look at a sample **ScaledObject** configuration
    that enables KEDA to scale a deployment based on number of messages in an Amazon
    SQS queue
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种行为，让我们看一个示例**ScaledObject**配置，该配置使KEDA能够根据Amazon SQS队列中的消息数量扩展部署。
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are defining a ScaledObject, `amazonsqs-scaler`, for Amazon SQS-based scaling.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个用于基于Amazon SQS进行扩展的ScaledObject，`amazonsqs-scaler`。
- en: '`minReplicaCount` set to `0` defines the minimum number of replicas for the
    deployment. In this example, KEDA can scale the deployment down to zero, when
    there are no messages in the Amazon SQS queue. This helps conserve resources when
    there is no workload to process. `maxReplicaCount` set to `10` specifies the maximum
    number of replicas that KEDA can scale the deployment up to. This ensures that
    the deployment does not scale beyond 10 Pods, even if the queue size increases.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将`minReplicaCount`设置为`0`定义了部署的最小副本数。在此示例中，当Amazon SQS队列中没有消息时，KEDA可以将部署缩减为零。这有助于在没有工作负载处理时节省资源。将`maxReplicaCount`设置为`10`指定了KEDA可以将部署扩展到的最大副本数。这确保了即使队列大小增加，部署也不会扩展超过10个Pod。
- en: '`pollingInterval` set to `15` makes KEDA check the queue length every 15 seconds.
    In this case, KEDA will query the Amazon SQS API every 15 seconds to check the
    size of the queue. `cooldownPeriod` set to 180 seconds states that after scaling
    up, KEDA will wait for 3 minutes before scaling down the deployment, even if the
    workload drops. This prevents rapid scaling down after a temporary traffic spike
    and allows a more stable scaling.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将`pollingInterval`设置为`15`使得KEDA每15秒检查一次队列长度。在这种情况下，KEDA将每15秒查询一次Amazon SQS API，以检查队列的大小。将`cooldownPeriod`设置为180秒表示，在扩展后，KEDA会等待3分钟再缩放部署，即使工作负载减少。这可以防止在暂时流量激增后快速缩放，并实现更稳定的扩展。
- en: We are using `aws-sqs-queue` as the type of scaler, which is compatible with
    the Amazon SQS service. `queueURL` defines the management endpoint for the Amazon
    SQS service, where KEDA can query the queue depth. `queueLength` set to `10` defines
    the threshold for scaling. KEDA will trigger scaling when the number of messages
    in the queue exceeds 10\. Lastly, `keda-service-account` refers to the service
    account that KEDA will use to authenticate with the Amazon SQS service.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`aws-sqs-queue`作为扩展器类型，它与Amazon SQS服务兼容。`queueURL`定义了Amazon SQS服务的管理端点，KEDA可以在此查询队列深度。`queueLength`设置为`10`定义了扩展的阈值。当队列中的消息数量超过10时，KEDA将触发扩展。最后，`keda-service-account`指的是KEDA将用于与Amazon
    SQS服务进行身份验证的服务账户。
- en: 'Following is an example ScaledObject that automatically scales GenAI model
    deployments in K8s. It scales based on two triggers: the first one is based on
    the average number of incoming requests, and the second is based on the GPU utilization
    of the inference Pods. Both metrics are sourced from the local Prometheus setup,
    which collects these metrics from the NVIDIA DCGM exporter and the application
    metrics endpoint:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例 ScaledObject，它会自动扩展 K8s 中的 GenAI 模型部署。它基于两个触发器进行扩展：第一个触发器是基于传入请求的平均数量，第二个触发器是基于推理
    Pods 的 GPU 利用率。这两个指标来自本地 Prometheus 设置，Prometheus 从 NVIDIA DCGM 导出程序和应用程序指标端点收集这些指标：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this section, we explored the KEDA project, which brings event-driven autoscaling
    to K8s. KEDA extends the built-in HPA by enabling applications to scale based
    on custom metrics from various sources, including event-driven metrics such as
    message queue depth. By integrating KEDA into K8s, we can dynamically scale K8s
    workloads in response to external events, making resource allocation more responsive
    and efficient.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了 KEDA 项目，它为 K8s 带来了基于事件的自动扩展。KEDA 通过使应用能够基于来自多个来源的自定义指标进行扩展，扩展了内置的
    HPA，包括基于事件的指标，如消息队列深度。通过将 KEDA 集成到 K8s 中，我们可以根据外部事件动态扩展 K8s 工作负载，使资源分配更加响应和高效。
- en: Cluster Autoscaler (CA)
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群自动扩展器 (CA)
- en: The Kubernetes CA is a tool that adjusts the number of nodes in a Kubernetes
    cluster based on the needs of the workloads running in the cluster. It scales
    the cluster up or down by adding or removing nodes to meet the resource demands
    of pending or underutilized Pods.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes CA 是一个根据集群中运行的工作负载需求调整 Kubernetes 集群中节点数量的工具。它通过增加或删除节点来扩大或缩小集群，以满足待调度或低利用率
    Pods 的资源需求。
- en: When HPA detects that the resource usage exceeds the configured threshold, it
    increases the number of Pod replicas. However, if the existing nodes in the cluster
    don’t have enough capacity to schedule the new Pods, these Pods will remain unschedulable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当 HPA 检测到资源使用超过配置的阈值时，它会增加 Pod 副本的数量。然而，如果集群中现有的节点没有足够的容量来调度新的 Pods，这些 Pods
    将保持未调度状态。
- en: That’s where CA comes into the picture. CA detects that there are unschedulable
    Pods and provisions more nodes to accommodate these newly created Pods, which
    are unscheduled. Once the nodes are ready, the pending Pods are scheduled and
    start running on the new nodes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 CA 发挥作用的地方。CA 检测到存在无法调度的 Pods，并配置更多的节点来容纳这些新创建的、未调度的 Pods。一旦节点准备好，待处理的 Pods
    会被调度并开始在新节点上运行。
- en: CA supports both *scale-up*, when Pods are unschedulable due to insufficient
    resources, and *scale-down*; when the nodes are underutilized, the CA can remove
    them to optimize resource utilization and reduce costs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: CA 支持 *扩展*，即当 Pods 因资源不足无法调度时；也支持 *缩减*，即当节点未被充分利用时，CA 可以移除它们以优化资源利用率并降低成本。
- en: CA continuously monitors the Kubernetes cluster and interacts with the cloud
    provider’s API to add or remove nodes based on the current resource usage. It
    focuses on pending Pods that cannot be scheduled due to a lack of resources and
    underutilized nodes that have spare capacity.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: CA 持续监控 Kubernetes 集群，并与云提供商的 API 交互，依据当前的资源使用情况添加或删除节点。它主要关注因资源不足而无法调度的待处理
    Pods 和具有剩余容量的低利用率节点。
- en: 'The CA identifies a node group, such as an AWS Auto Scaling Group, that is
    capable of provisioning additional nodes with the necessary resources. Once the
    new node becomes available, the pending Pods are scheduled to run on it:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: CA 确定一个节点组，例如 AWS 自动扩展组，该组能够提供具有必要资源的额外节点。一旦新节点可用，待调度的 Pods 会被安排在该节点上运行：
- en: '**Scale-down**: When nodes are underutilized, meaning they are running with
    low resource usage or without any significant workloads, the CA checks whether
    the Pods on the underutilized nodes can be safely rescheduled on other nodes in
    the cluster.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩减**：当节点未被充分利用，即它们运行时资源使用率低或没有显著的工作负载时，CA 会检查是否可以将这些未充分利用节点上的 Pods 安全地重新调度到集群中的其他节点上。'
- en: '**Scale-up**: When the application experiences high resource demand and existing
    nodes cannot accommodate new or pending workloads, the CA triggers scale-up. It
    provisions additional nodes to ensure workload scheduling. Once new nodes join
    the cluster, pending Pods are scheduled on them to maintain the desired performance.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展**：当应用程序遇到高资源需求，而现有节点无法容纳新的或待处理的工作负载时，CA 会触发扩展。它会配置额外的节点以确保工作负载调度。一旦新节点加入集群，待调度的
    Pods 会被调度到这些节点上，以保持所需的性能。'
- en: While CA has been the traditional choice for scaling Kubernetes clusters, it
    has certain limitations, such as relying on predefined node groups and a polling-based
    mechanism for scaling decisions. This is where **Karpenter** can help. Karpenter
    is designed to address the inefficiencies of traditional autoscaling methods and
    will be covered in the next section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CA 一直是扩展 Kubernetes 集群的传统选择，但它存在一些局限性，例如依赖于预定义的节点组和基于轮询的扩展决策机制。这就是**Karpenter**可以提供帮助的地方。Karpenter
    旨在解决传统自动扩展方法的低效问题，下一节将详细介绍其功能。
- en: Karpenter
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Karpenter
- en: '**Karpenter** is an open source, flexible, high-performance Kubernetes CA built
    by AWS. It was first introduced in 2021 during the AWS re:Invent ([https://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/](https://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/))
    conference, with its primary purpose to improve and simplify K8s autoscaling experience.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**Karpenter** 是由 AWS 开发的开源、灵活且高性能的 Kubernetes 集群自动扩展器。它首次亮相于 2021 年的 AWS re:Invent
    ([https://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/](https://aws.amazon.com/blogs/aws/introducing-karpenter-an-open-source-high-performance-kubernetes-cluster-autoscaler/))
    大会，主要目的是改善和简化 K8s 的自动扩展体验。'
- en: 'Unlike the native Kubernetes CA, which primarily focuses on scaling nodes in
    response to pending Pods, Karpenter dynamically provisions right-sized compute
    resources based on the specific needs of workloads. Karpenter optimizes for both
    efficiency and performance in the following ways:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与原生 Kubernetes CA 主要侧重于响应待处理 Pods 来扩展节点不同，Karpenter 会根据工作负载的具体需求动态地提供合适的计算资源。Karpenter
    在以下几个方面优化了效率和性能：
- en: '**Faster and more efficient scaling**: Karpenter can directly communicate with
    the Kubernetes API server to understand the pending Pod requirements and can launch
    new nodes faster, reducing scheduling delays. Karpenter makes these scaling decisions
    in near-real-time by analyzing the specific needs of pending Pods. This means
    that nodes are provisioned based on the immediate requirements, which reduces
    latency and increases responsiveness to workload changes.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快速和更高效的扩展**：Karpenter 可以直接与 Kubernetes API 服务器通信，以了解待处理 Pod 的需求，并能够更快地启动新节点，从而减少调度延迟。Karpenter
    通过分析待处理 Pod 的具体需求，几乎实时地做出扩展决策。这意味着节点是根据即时需求进行配置的，从而减少了延迟并提高了对工作负载变化的响应能力。'
- en: '**Better utilization of nodes**: Unlike the CA, which typically provisions
    nodes from pre-configured instance groups or node pools, Karpenter can dynamically
    select the best instance types. It can pick instance sizes and types that match
    the resource requirements of the pending Pods, reducing wasted capacity and optimizing
    resource allocation.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好的节点利用率**：与 CA 通常从预配置的实例组或节点池中配置节点不同，Karpenter 可以动态选择最佳的实例类型。它可以选择与待处理 Pods
    的资源需求匹配的实例大小和类型，从而减少浪费的容量并优化资源分配。'
- en: '**Consolidation capabilities**: Karpenter continuously monitors the cluster
    and consolidates workloads by re-packing them onto fewer nodes when possible,
    terminating underutilized nodes. This consolidation helps to reduce costs by making
    better use of available node resources, whereas the CA generally scales down nodes
    based on pre-configured thresholds without aggressively consolidating workloads.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整合能力**：Karpenter 会持续监控集群，并在可能的情况下通过重新打包工作负载到更少的节点上来整合它们，从而终止那些使用不足的节点。这种整合有助于通过更有效地利用现有的节点资源来降低成本，而
    CA 通常根据预设的阈值进行缩减节点，而不积极整合工作负载。'
- en: '**Support for multiple instance types**: Karpenter can select from a wide range
    of instance types, including different generations and sizes. It does this based
    on current availability and pricing, ensuring that Pods are scheduled on the most
    cost-effective and resource-appropriate nodes.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持多种实例类型**：Karpenter 可以从多种实例类型中选择，包括不同的代际和大小。它根据当前的可用性和定价进行选择，确保 Pods 被调度到最具成本效益且资源合适的节点上。'
- en: '**Drift**: Karpenter automatically detects the nodes that have drifted from
    the desired state and replaces them in a rolling manner. This functionality can
    be used to perform patch upgrades or K8s version upgrades of the Karpenter-managed
    nodes.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**漂移**：Karpenter 会自动检测已漂移的节点，并以滚动方式替换它们。此功能可用于执行修补程序升级或 Karpenter 管理节点的 K8s
    版本升级。'
- en: Karpenter looks for pending Pods in the cluster that are marked *unschedulable*
    by the kube-scheduler and aggregates the resource and scheduling requirements
    of those Pods to make decisions to launch new worker nodes. It performs *bin-packing*
    to ensure the correct size and number of nodes are provisioned. It also actively
    looks for opportunities to reduce the overall cluster costs by terminating worker
    nodes that are empty, under-utilized, or can be replaced with cheaper nodes, as
    shown in *Figure 6**.3*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter 会在集群中查找被 kube-scheduler 标记为*不可调度*的待处理 Pods，并汇总这些 Pods 的资源和调度需求，从而做出决策启动新的工作节点。它执行*箱形打包*以确保正确的节点大小和数量被配置。它还会主动寻找机会，通过终止空闲、使用不足或可以被更便宜节点替换的工作节点来减少集群的整体成本，如*图
    6.3*所示。
- en: '![Figure 6.3 – Karpenter overview](img/B31108_06_3.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – Karpenter 概述](img/B31108_06_3.jpg)'
- en: Figure 6.3 – Karpenter overview
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – Karpenter 概述
- en: Karpenter provides two custom resources, **NodePools** ([https://karpenter.sh/v1.4/concepts/nodepools/](https://karpenter.sh/v1.4/concepts/nodepools/))
    and **NodeClasses** ([https://karpenter.sh/v1.4/concepts/nodeclasses/](https://karpenter.sh/v1.4/concepts/nodeclasses/)),
    for configuration. NodePools define a set of provisioning constraints for the
    nodes created by Karpenter, such as instance types, availability zones, CPU architecture,
    capacity type, taints, and labels. NodeClasses configure cloud-provider-specific
    settings such as VPC subnets, IAM role, AMI ID, and EC2 Security groups. Multiple
    NodePools can be created to cater to different workload requirements, such as
    a GPU-specific NodePool to launch GPU worker nodes for the GenAI training and
    inference applications, and generic NodePools to accommodate webservers and microservices.
    Always ensure that NodePools are created with distinct configurations that are
    mutually exclusive; if multiple NodePools are matched, Karpenter will randomly
    pick one to launch the worker nodes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter 提供了两个自定义资源，**NodePools** ([https://karpenter.sh/v1.4/concepts/nodepools/](https://karpenter.sh/v1.4/concepts/nodepools/))
    和 **NodeClasses** ([https://karpenter.sh/v1.4/concepts/nodeclasses/](https://karpenter.sh/v1.4/concepts/nodeclasses/))，用于配置。NodePools
    定义了 Karpenter 创建的节点的配置约束集，例如实例类型、可用区、CPU 架构、容量类型、污点和标签。NodeClasses 配置了云提供商特定的设置，例如
    VPC 子网、IAM 角色、AMI ID 和 EC2 安全组。可以创建多个 NodePools，以满足不同的工作负载需求，例如用于生成 AI 训练和推理应用程序的
    GPU 专用 NodePool，以及用于容纳 Web 服务器和微服务的通用 NodePools。始终确保 NodePools 具有互不重叠的配置；如果匹配多个
    NodePools，Karpenter 会随机选择一个来启动工作节点。
- en: Karpenter can be installed in the EKS cluster as a Helm chart, refer to the
    Getting Started guide at [https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/](https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/)
    for the instructions. In our setup, we installed the Karpenter using Terraform
    helm provider in Chapter 3 as part of the EKS cluster setup.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter 可以作为 Helm 图表安装到 EKS 集群中，具体安装步骤请参考 [https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/](https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/)
    中的入门指南。在我们的设置中，我们使用 Terraform Helm 提供程序在第 3 章中作为 EKS 集群设置的一部分安装了 Karpenter。
- en: 'You can verify the installation using the following command, which displays
    the status, version, and other details of the deployment:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令验证安装情况，该命令将显示部署的状态、版本和其他详细信息：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let’s use the power of Karpenter to automatically launch GPU instances
    for our GenAI workloads. In [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062), we
    created a dedicated EKS Managed Node group of G6 EC2 instances to deploy the Llama
    3 fine-tuning job and inference applications. A disadvantage of that approach
    is that the GPU worker node always remains attached to the cluster, regardless
    of whether GenAI applications are running, which is an inefficient use of the
    most expensive resources. Let’s go ahead and delete the node group and configure
    the Karpenter to manage the GPU instance provisioning.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们利用 Karpenter 的功能为我们的 GenAI 工作负载自动启动 GPU 实例。在 [*第 5 章*](B31108_05.xhtml#_idTextAnchor062)
    中，我们创建了一个专用的 EKS 管理节点组，使用 G6 EC2 实例来部署 Llama 3 微调任务和推理应用程序。这种方法的一个缺点是，无论 GenAI
    应用程序是否运行，GPU 工作节点始终附加在集群上，这对最昂贵的资源来说是一种低效的利用方式。现在，让我们删除该节点组，并配置 Karpenter 来管理
    GPU 实例的配置。
- en: 'Comment the following code in the `eks.tf` file and run the Terraform commands
    to delete the `eks-gpu-mng` node group. The complete file is available at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks.tf):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `eks.tf` 文件中注释以下代码，并运行 Terraform 命令来删除 `eks-gpu-mng` 节点组。完整文件可见于 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks.tf)：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can confirm that the node group has been deleted by checking the **Compute**
    tab in EKS console or by using the following command:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查 EKS 控制台中的**计算**标签页，或者使用以下命令来确认节点组已被删除：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we have deleted the GPU worker nodes in the cluster, let’s go ahead
    and configure the Karpenter by creating NodePool and EC2NodeClass resources to
    launch the GPU instances. Create a GPU NodePool called `eks-gpu-np` with a set
    of following requirements to pick G6 instance generation instances, from on-demand
    or spot capacity. The complete file is available at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-np.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-np.yaml):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经删除了集群中的 GPU 工作节点，接下来让我们通过创建 NodePool 和 EC2NodeClass 资源来配置 Karpenter，以启动
    GPU 实例。创建一个名为 `eks-gpu-np` 的 GPU NodePool，并设置以下要求，选择 G6 实例系列，来自按需或竞价容量。完整文件可见于
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-np.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-np.yaml)：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, create the `default-gpu` EC2NodeClass to configure the AMI, IAM role,
    VPC subnets, EC2 security groups, and so on. The complete file is available at
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-nc.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-nc.yaml):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建 `default-gpu` EC2NodeClass 来配置 AMI、IAM 角色、VPC 子网、EC2 安全组等。完整文件可见于 [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-nc.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch6/eks-gpu-nc.yaml)：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we have configured Karpenter, let’s rerun the fine-tuning job and
    see the compute autoscaling live in action. Execute the following commands to
    initiate the fine-tuning job. You can download the K8s manifest is available at
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml).
    Replace the container image, Hugging Face token, and model assets S3 bucket name
    values before running the commands:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了 Karpenter，接下来让我们重新运行微调任务，看看计算自动扩展的实时效果。执行以下命令来启动微调任务。你可以下载 K8s 清单文件，文件位于
    [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml)。在运行命令之前，请替换容器镜像、Hugging
    Face 令牌和模型资源 S3 桶名称值：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Given that the cluster doesn’t have any GPU worker nodes, kube-scheduler will
    mark the fine-tuning Pod as unschedulable. We can verify that by using the following
    commands. The first one will output the *Pending* status, and the second one shows
    the reason for this status.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于集群中没有 GPU 工作节点，kube-scheduler 会将微调 Pod 标记为不可调度。我们可以通过使用以下命令来验证这一点。第一个命令会输出*待处理*状态，第二个命令显示该状态的原因。
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Karpenter is actively looking for pending Pods that are unschedulable and launches
    the G6 family EC2 instance in response to our fine-tuning job. Let’s verify that
    by looking at the Karpenter logs:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter 正在积极寻找待处理的无法调度的 Pods，并启动 G6 系列的 EC2 实例来响应我们的微调任务。让我们通过查看 Karpenter
    日志来验证这一点：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the node has been initiated and joined the cluster, the fine-tuning job
    will be scheduled on it. You can verify by using the following commands:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦节点启动并加入集群，微调任务将会在该节点上调度。你可以通过使用以下命令来验证：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A few minutes after the job is complete, Karpenter will automatically detect
    the empty node, apply a taint to prevent new workloads from being scheduled, evict
    the existing Pods, and then terminate the node. You can verify that by checking
    the Karpenter logs:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 任务完成几分钟后，Karpenter 会自动检测到空闲节点，应用一个污点以防止新的工作负载被调度，逐出现有的 Pods，然后终止该节点。你可以通过查看
    Karpenter 日志来验证这一点：
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this section, we looked at the advantages of using Karpenter and installed
    it in the EKS cluster using Terraform and Helm. Then, we configured Karpenter
    to launch G6 instances for our Llama 3 fine-tuning job and inference workloads.
    Karpenter launched a G6 instance in response to the fine-tuning job and terminated
    it automatically after the job completion.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了使用 Karpenter 的优势，并通过 Terraform 和 Helm 在 EKS 集群中安装了它。然后，我们配置了 Karpenter
    启动 G6 实例，以支持我们的 Llama 3 微调任务和推理工作负载。Karpenter 在微调任务期间启动了一台 G6 实例，并在任务完成后自动终止该实例。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed scaling strategies and best practices for K8s
    applications to ensure efficient resource utilization and optimal performance.
    The chapter covers key scaling topics, including metrics, HPA, KEDA, VPA, CA,
    and Karpenter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了 K8s 应用程序的扩缩容策略和最佳实践，旨在确保资源的高效利用和最佳性能。本章涵盖了扩缩容的关键主题，包括指标、HPA、KEDA、VPA、CA
    和 Karpenter。
- en: Scaling in Kubernetes involves selecting the right scaling metrics. Conventional
    metrics help determine the need for adding or removing Pods. Custom metrics are
    used for more granular control in scaling decisions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中进行扩缩容涉及选择合适的扩缩容指标。常规指标帮助确定是否需要添加或删除 Pod，而自定义指标则用于在扩缩容决策中实现更精细的控制。
- en: HPA can automatically adjust the number of Pods in a deployment based on metrics
    such as CPU or memory usage, whereas VPA can adjust the resource requests and
    limits for individual Pods. VPA ensures optimal resource allocation but may conflict
    with HPA if both are used simultaneously.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 可以根据 CPU 或内存使用率等指标自动调整部署中 Pod 的数量，而 VPA 可以调整单个 Pod 的资源请求和限制。VPA 确保资源的最佳分配，但如果同时使用
    HPA 和 VPA，可能会产生冲突。
- en: KEDA brings event-driven autoscaling to K8s, enabling scaling based on external
    events such as message queue depths. It creates an HPA resource that manages scaling
    in response to event triggers such as spikes in API calls. KEDA can scale applications
    to zero, making it highly suitable for serverless and event-driven use cases.
    CA can adjust the number of nodes in a cluster based on Pod requirements. CA works
    closely with cloud provider APIs to manage nodes dynamically.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA 将事件驱动的自动扩缩容引入 K8s，支持基于外部事件（如消息队列深度）进行扩缩容。它创建了一个 HPA 资源，用于在事件触发（如 API 调用激增）时管理扩缩容。KEDA
    可以将应用程序扩缩容到零，因此非常适合无服务器和事件驱动的使用场景。CA 可以根据 Pod 的需求调整集群中节点的数量。CA 与云提供商的 API 紧密合作，动态管理节点。
- en: Lastly, we covered Karpenter, an alternative to CA. Karpenter can dynamically
    provision the right-sized compute resources to handle pending Pods. It optimizes
    for both performance and cost efficiency by selecting suitable instance types
    and terminating underutilized nodes to reduce costs. To demonstrate its functionality,
    we reran the Llama 3 fine-tuning job, during which Karpenter launched a node with
    GPU capabilities in response to the resource requirements, and automatically terminated
    the node once the job was complete. In the next chapter, we will discuss different
    strategies to optimize the overall cost of running GenAI applications on K8s.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了 Karpenter，作为 CA 的替代方案。Karpenter 可以动态提供适当大小的计算资源，以处理待处理的 Pod。它通过选择合适的实例类型并终止未充分利用的节点来优化性能和成本效率，以降低成本。为了演示其功能，我们重新运行了
    Llama 3 微调任务，在此过程中，Karpenter 根据资源需求启动了一台带 GPU 能力的节点，并在任务完成后自动终止该节点。在下一章中，我们将讨论优化在
    K8s 上运行 GenAI 应用程序的整体成本的不同策略。
