- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Managing Storage Replication with OpenEBS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenEBS 管理存储复制
- en: In the previous chapter, we looked at two serverless frameworks that are available
    with MicroK8s, both of which are Kubernetes-based platforms for designing, deploying,
    and managing modern serverless workloads. We also noticed that the ease with which
    serverless frameworks can be implemented appears to be tied to the ease with which
    MicroK8s can be deployed. Some guiding principles to remember when creating and
    deploying serverless apps were also highlighted. We also realized that we needed
    to follow best practices to safeguard our resources, apps, and infrastructure
    service provider accounts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 MicroK8s 提供的两种无服务器框架，这两种框架都是基于 Kubernetes 的平台，用于设计、部署和管理现代无服务器工作负载。我们还注意到，无服务器框架的实施易用性似乎与
    MicroK8s 的部署易用性密切相关。创建和部署无服务器应用时需要记住的一些指导原则也已被强调。我们还意识到，为了保护我们的资源、应用程序和基础设施服务提供商账户，我们需要遵循最佳实践。
- en: In this chapter, we will look into the next use case for supporting cloud-native
    storage solutions, such as OpenEBS, to provide persistent storage for our container
    applications. Cloud-native storage solutions enable comprehensive storage mechanisms.
    These solutions mimic the properties of cloud environments, such as scalability,
    reliability, container architecture, and high availability. These features make
    it simple to interface with the container management platform and provide persistent
    storage for container-based applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究支持云原生存储解决方案（如 OpenEBS）的下一个用例，以为我们的容器应用程序提供持久存储。云原生存储解决方案提供了全面的存储机制。这些解决方案模拟了云环境的特性，如可扩展性、可靠性、容器架构和高可用性。这些特性使得与容器管理平台的接口变得简单，并为基于容器的应用程序提供持久存储。
- en: First, we will look at the Kubernetes storage basics before diving into OpenEBS
    concepts. Containers are ephemeral, which means they are established for a specific
    reason and then shut down after that task is completed. Containers do not maintain
    state data on their own, and a new container instance has no memory/state of prior
    ones. Although a container provides storage, it is only ephemeral storage, so
    it is wiped when the container is turned off. Developers will need to manage persistent
    storage as part of containerized applications as they adopt containers for new
    use cases. A developer, for example, may want to operate a database in a container
    and store the data in a volume that survives the container’s shutdown process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将先了解 Kubernetes 存储基础知识，再深入了解 OpenEBS 的概念。容器是短暂的，这意味着它们是为特定目的建立的，在完成任务后会被关闭。容器本身不维护状态数据，新创建的容器实例无法记住之前容器的状态/数据。虽然容器提供存储，但它只是短暂存储，因此当容器关闭时，存储也会被清除。开发人员需要管理持久存储，作为容器化应用程序的一部分，因为他们在为新用例采用容器时，可能需要让容器持续存储数据。例如，开发人员可能希望在容器中运行数据库，并将数据存储在一个在容器关闭时仍然存在的卷中。
- en: Kubernetes provides a variety of management options for clusters of containers.
    The ability to manage persistent storage is one of these capabilities. Administrators
    can use Kubernetes persistent storage to keep track of both persistent and non-persistent
    data in a Kubernetes cluster. Multiple applications that operate on the cluster
    can then utilize storage resources dynamically.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 为容器集群提供了多种管理选项。管理持久存储的能力是其中之一。管理员可以使用 Kubernetes 持久存储来跟踪 Kubernetes
    集群中的持久数据和非持久数据。集群中的多个应用程序可以动态地利用存储资源。
- en: 'To help manage persistent storage, Kubernetes supports two primary mechanisms:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助管理持久存储，Kubernetes 支持两种主要机制：
- en: A **PersistentVolume** (**PV**) is a storage element that can be created manually
    or dynamically, depending on the storage class. It has a life cycle that is unaffected
    by the life cycle of Kubernetes pods. A pod can mount a PV, but the PV remains
    after the pod has shut down, and its data can still be accessed. Each PV can have
    its own set of parameters, such as disc type, storage tier, and performance.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PersistentVolume**（**PV**）是一个存储元素，可以根据存储类别手动或动态创建。它的生命周期不受 Kubernetes Pod
    生命周期的影响。Pod 可以挂载一个 PV，但 PV 在 Pod 关闭后仍然存在，其数据仍然可以访问。每个 PV 可以有自己的一组参数，例如磁盘类型、存储层次和性能。'
- en: A **PersistentVolumeClaim** (**PVC**) is a storage request that’s made by a
    Kubernetes user. Based on the custom parameters, any application operating on
    a container can request storage and define the size and other properties of the
    storage it requires (for example, the specific type of storage, such as SSD storage).
    Based on the available storage resources, the Kubernetes cluster can provision
    a PV.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PersistentVolumeClaim** (**PVC**) 是 Kubernetes 用户提出的存储请求。根据自定义参数，任何在容器上运行的应用程序都可以请求存储，并定义其所需的大小和其他属性（例如，特定类型的存储，如
    SSD 存储）。根据可用的存储资源，Kubernetes 集群可以供给一个 PV。'
- en: '`StorageClass` is a Kubernetes API object for configuring storage parameters.
    It’s a way of configuring a dynamic setup that generates new volumes based on
    demand. `StorageClass` defines the name of the volume plugin, as well as any external
    providers and a **Container Storage Interface** (**CSI**) driver, which allows
    containers to communicate with storage devices. CSI is a standard that allows
    containerized workloads to access any block and file storage systems.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`StorageClass` 是一个 Kubernetes API 对象，用于配置存储参数。它是一种动态配置方法，根据需求生成新卷。`StorageClass`
    定义了卷插件的名称，以及任何外部提供者和 **容器存储接口** (**CSI**) 驱动程序，允许容器与存储设备进行通信。CSI 是一种标准，允许容器化工作负载访问任何块存储和文件存储系统。'
- en: '`StorageClass` can be defined and PVs assigned by Kubernetes administrators.
    Each `StorageClass` denotes a different form of storage, such as fast SSD storage
    versus traditional magnetic drives or remote cloud storage. This enables a Kubernetes
    cluster to supply different types of storage based on the workload’s changing
    requirements.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`StorageClass` 可以由 Kubernetes 管理员定义并分配 PV。每个 `StorageClass` 表示不同类型的存储，例如快速的
    SSD 存储与传统的磁盘驱动器或远程云存储。这使得 Kubernetes 集群能够根据工作负载的变化需求提供不同类型的存储。'
- en: Dynamic volume provisioning is a feature of Kubernetes that allows storage volumes
    to be created on-demand. Administrators no longer need to manually build new storage
    volumes in their cloud or storage provider, then create PV objects to make them
    available in the cluster. When users request a specific storage type, the entire
    process is automated and provisioned. `StorageClass` objects are defined by the
    cluster administrator as needed. A volume plugin such as OpenEBS, also known as
    a provisioner, is referenced by each `StorageClass`. When a storage volume is
    automatically provisioned, the volume plugin provides a set of parameters and
    passes them to the provisioner.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 动态卷供给是 Kubernetes 的一项功能，允许根据需求创建存储卷。管理员不再需要手动在云端或存储提供商中构建新存储卷，然后创建 PV 对象使其在集群中可用。当用户请求特定的存储类型时，整个过程会自动化并进行供给。`StorageClass`
    对象由集群管理员根据需要定义。像 OpenEBS 这样的卷插件，也称为供给器，被每个 `StorageClass` 引用。当存储卷被自动供给时，卷插件提供一组参数并将其传递给供给器。
- en: The administrator can define many `StorageClass`, each of which can represent
    a distinct type of storage or the same storage with different specifications.
    This allows users to choose from a variety of storage solutions without having
    to worry about the implementation details.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员可以定义多个 `StorageClass`，每个 `StorageClass` 可以代表不同类型的存储或具有不同规格的相同存储。这使得用户可以选择多种存储解决方案，而无需担心实施细节。
- en: '**Container Attached Storage** (**CAS**) is quickly gaining traction as a viable
    option for managing stateful workloads and is becoming the favored method for
    executing durable, fault-tolerant stateful applications. CAS was brought to the
    Kubernetes platform via the OpenEBS project. It can be readily deployed in on-premises
    clusters, managed clusters in the public cloud, and even isolated air-gapped clusters.
    MicroK8s offers in-built support for OpenEBS via an add-on, making it the best
    solution for running Kubernetes clusters in air-gapped Edge/IoT scenarios.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器附加存储** (**CAS**) 正迅速成为管理有状态工作负载的可行选项，并且正成为执行耐久性和容错有状态应用程序的首选方法。CAS 是通过
    OpenEBS 项目引入 Kubernetes 平台的。它可以在本地集群、公共云中的托管集群，甚至在隔离的气隙集群中轻松部署。MicroK8s 通过附加组件内置支持
    OpenEBS，使其成为在气隙 Edge/IoT 场景中运行 Kubernetes 集群的最佳解决方案。'
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Overview of OpenEBS
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenEBS 概述
- en: Configuring and implementing a PostgreSQL stateful workload
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和实现 PostgreSQL 有状态工作负载
- en: Kubernetes storage best practices
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 存储最佳实践
- en: Overview of OpenEBS
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenEBS 概述
- en: 'In Kubernetes, storage is often integrated as an OS kernel module with individual
    nodes. Even the PVs are monolithic and legacy resources since they are strongly
    tied to the underlying components. CAS allows Kubernetes users to treat storage
    entities as microservices. CAS is made up of two parts: the control plane and
    the data plane. The control plane is implemented as a set of **Custom Resource
    Definitions** (**CRDs**) that deal with low-level storage entities. The data plane
    runs as a collection of pods close to the workload. It is in charge of the I/O
    transactions, which translate into read and write operations.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，存储通常作为操作系统内核模块与各个节点集成。即便是 PV 也属于单体化和传统资源，因为它们与底层组件紧密绑定。CAS 允许
    Kubernetes 用户将存储实体视为微服务。CAS 由两部分组成：控制平面和数据平面。控制平面作为一组 **自定义资源定义**（**CRDs**）实现，处理低级存储实体。数据平面作为靠近工作负载的一组
    pod 运行，负责 I/O 事务，即读写操作。
- en: The clean separation of the control plane and data plane provides the same benefits
    as running microservices on Kubernetes. This architecture decouples persistence
    from the underlying storage entities, allowing workloads to be more portable.
    It also adds scale-out capabilities to storage, allowing administrators and operators
    to dynamically expand volumes in response to the workload. Finally, CAS ensures
    that the data (PV) and compute (pod) are always co-located in a hyper-converged
    mode to maximize throughput and fault tolerance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面和数据平面的清晰分离提供了与在 Kubernetes 上运行微服务相同的好处。该架构将持久化与底层存储实体解耦，使得工作负载更加可移植。它还为存储增加了扩展能力，允许管理员和运维人员根据工作负载动态扩展卷。最后，CAS
    确保数据（PV）和计算（pod）始终以超融合模式共置，以最大化吞吐量和容错能力。
- en: 'Data is copied across many nodes using the synchronous replication feature
    of OpenEBS. The failure of a node would only affect the volume replicas on that
    node. The data on other nodes would remain available at the same performance levels,
    allowing applications to be more resilient to failures:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通过 OpenEBS 的同步复制功能跨多个节点进行复制。节点故障只会影响该节点上的卷副本。其他节点上的数据将保持可用，并且性能不受影响，从而使应用程序对故障具有更高的容错性：
- en: '![Figure 11.1 – Synchronous replication  ](img/Figure_11.01_B18115.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 同步复制](img/Figure_11.01_B18115.jpg)'
- en: Figure 11.1 – Synchronous replication
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 同步复制
- en: Creating instantaneous snapshots are also possible with the OpenEBS CAS architecture.
    These can be made and managed with the regular `kubectl` command. This extensive
    integration with Kubernetes allows for job portability and easier data backup
    and migration.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OpenEBS CAS 架构还可以创建即时快照。这些快照可以通过常规的 `kubectl` 命令进行创建和管理。与 Kubernetes 的深度集成使得作业具有更好的可移植性，同时也简化了数据备份和迁移。
- en: 'The following diagram shows the typical components of OpenEBS:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了 OpenEBS 的典型组件：
- en: '![Figure 11.2 – OpenEBS control plane and data plane ](img/Figure_11.02_B18115.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – OpenEBS 控制平面和数据平面](img/Figure_11.02_B18115.jpg)'
- en: Figure 11.2 – OpenEBS control plane and data plane
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – OpenEBS 控制平面和数据平面
- en: OpenEBS is a well-designed system built on CAS concepts. We’ll look at the architecture
    in more detail in the following sections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: OpenEBS 是一个基于 CAS 概念精心设计的系统。我们将在以下章节中更详细地了解其架构。
- en: Control plane
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制平面
- en: The control plane, disk manager, and data plane are assigned to each storage
    volume that’s been installed. The control plane is closer to the storage infrastructure;
    it keeps track of the storage volumes that are joined to each cluster node through
    SAN or block storage. Provisioning volumes, initiating snapshots, creating clones,
    creating storage policies, enforcing storage policies, and exporting volume metrics
    to other systems such as Prometheus are all handled directly by the control plane.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面、磁盘管理器和数据平面被分配给每个已安装的存储卷。控制平面更接近存储基础设施；它跟踪通过 SAN 或块存储连接到每个集群节点的存储卷。卷的配置、快照的启动、克隆的创建、存储策略的制定、存储策略的执行以及将卷指标导出到其他系统（如
    Prometheus）等操作，都是由控制平面直接处理的。
- en: An OpenEBS storage administrator interacts with the control plane to manage
    cluster-wide storage activities. Through an API server, the OpenEBS control plane
    is accessible to the outside world. A pod exposes the REST API for controlling
    resources such as volumes and policies. The declaration is initially submitted
    as a YAML file to the API server, which then starts the workflow. The API server
    communicates with the Kubernetes master’s API server to schedule volume pods in
    the data plane.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: OpenEBS 存储管理员通过控制平面与集群范围的存储活动进行交互。通过 API 服务器，OpenEBS 控制平面可以对外访问。Pod 暴露 REST
    API 用于控制资源，如卷和策略。声明最初作为 YAML 文件提交到 API 服务器，然后启动工作流。API 服务器与 Kubernetes 主控的 API
    服务器进行通信，以便在数据平面中调度卷 Pod。
- en: Dynamic provisioning is implemented via the control plane’s provisioner component
    using the standard Kubernetes external storage plugin. When an application builds
    a PVC from an existing storage class, the OpenEBS provisioner constructs a PV
    from the primitives in the storage class and binds it to the PVC.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 动态供应通过控制平面的供应器组件使用标准 Kubernetes 外部存储插件来实现。当应用程序从现有存储类创建 PVC 时，OpenEBS 供应器会从存储类中的原语构建一个
    PV，并将其绑定到 PVC。
- en: The OpenEBS control plane relies heavily on the `etcd` database, which serves
    as the cluster’s single source of truth.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: OpenEBS 控制平面在很大程度上依赖于 `etcd` 数据库，它作为集群的单一数据源。
- en: Now that we’ve seen what the control plane it’s, let’s learn more about the
    data plane.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了控制平面，接下来让我们更深入地了解数据平面。
- en: Data plane
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据平面
- en: The data plane is close to the workload, which remains in the volume’s I/O path.
    It manages the life cycle of the PV and PVCs while running in the user space.
    A variety of storage engines with varied capabilities are available on the data
    plane. **Jiva**, **cStor**, and **Local PV** are the three storage engines that
    are available at the time of writing. Jiva provides standard storage capabilities
    (block storage) and is typically used for smaller-scale workloads compared to
    cStor, which offers enterprise-grade functionality and extensive snapshot features.
    Local PV, on the other hand, provides performance through advanced features such
    as replication and snapshots.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面接近工作负载，并且仍然位于卷的 I/O 路径中。它在用户空间中运行的同时管理 PV 和 PVC 的生命周期。数据平面上提供了多种具有不同功能的存储引擎。在写作时，**Jiva**、**cStor**
    和 **Local PV** 是可用的三种存储引擎。Jiva 提供标准存储能力（块存储），通常用于较小规模的工作负载，而 cStor 提供企业级功能和广泛的快照功能。另一方面，Local
    PV 通过如复制和快照等高级功能提供更好的性能。
- en: Let’s take a closer look at each storage engine.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解每个存储引擎。
- en: Storage engines
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储引擎
- en: OpenEBS’s preferred storage engine is cStor. It’s a feature-rich and lightweight
    storage engine designed for high-availability workloads such as databases. It
    includes enterprise-level capabilities such as synchronous data replication, snapshots,
    clones, thin data provisioning, high data resiliency, data consistency, and on-demand
    capacity or performance increases. With just a single replica, cStor’s synchronous
    replication ensures excellent availability for stateful Kubernetes deployments.
    When a stateful application requires high data availability, cStor is set up with
    three replicas, with data written synchronously to each of the three replicas.
    Terminating and scheduling a new pod in a different node does not result in data
    loss because data is written to multiple replicas.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: OpenEBS 的首选存储引擎是 cStor。它是一个功能丰富且轻量级的存储引擎，旨在满足高可用性工作负载的需求，例如数据库。它包括企业级功能，如同步数据复制、快照、克隆、精简数据供应、高数据弹性、数据一致性和按需的容量或性能增加。通过仅一个副本，cStor
    的同步复制确保了有状态 Kubernetes 部署的优秀可用性。当有状态应用需要高数据可用性时，cStor 会设置为三个副本，数据会同步写入每个副本。终止和调度新的
    Pod 到不同节点时不会导致数据丢失，因为数据已经写入多个副本。
- en: Jiva was the first storage engine to be included in early OpenEBS versions.
    Jiva is the simplest of the options, as it runs entirely in user space and has
    conventional block storage features such as synchronous replication. Smaller applications
    running on nodes without the ability to install extra block storage devices benefit
    from Jiva. As a result, it is not appropriate for mission-critical tasks that
    require high performance or advanced storage capacities.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Jiva 是首个被包含在早期 OpenEBS 版本中的存储引擎。Jiva 是所有选项中最简单的，因为它完全运行在用户空间，并具有常规的块存储功能，如同步复制。运行在没有额外块存储设备的节点上的小型应用程序将受益于
    Jiva。因此，它不适用于需要高性能或高级存储能力的关键任务。
- en: '**Local persistent volume** (**Local PV**) is OpenEBS’s third and simplest
    storage engine. Local PV is a local disc that’s attached directly to a single
    Kubernetes mode. Kubernetes applications can now consume high-performance local
    storage using the traditional volume APIs. OpenEBS’s Local PV is a storage engine
    that may build PVs on worker nodes using local discs or host paths. Local PV can
    be used by cloud-native apps that do not require advanced storage features such
    as replication, snapshots, or clones. A StatefulSet that manages replication and
    HA on its own, for example, can set up a Local PV based on OpenEBS.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**本地持久卷**（**Local PV**）是 OpenEBS 的第三个也是最简单的存储引擎。Local PV 是直接附加到单个 Kubernetes
    节点的本地磁盘。Kubernetes 应用程序现在可以使用传统的卷 API 消耗高性能本地存储。OpenEBS 的 Local PV 是一种存储引擎，可以在工作节点上使用本地磁盘或主机路径构建
    PV。Local PV 可用于不需要高级存储功能（如复制、快照或克隆）的云原生应用程序。例如，管理复制和高可用性的 StatefulSet 可以基于 OpenEBS
    设置 Local PV。'
- en: 'In addition to the storage engines mentioned previously, the **Mayastor data
    engine**, a low latency engine that is currently in development, has a declarative
    data plane, which provides flexible, persistent storage for stateful applications.
    It is Kubernetes-native and provides fast, redundant storage that works in any
    Kubernetes cluster. The Mayastor add-on will become available with MicroK8s 1.24:
    [https://microk8s.io/docs/addon-mayastor](https://microk8s.io/docs/addon-mayastor).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的存储引擎之外，**Mayastor 数据引擎**是一种低延迟引擎，目前正在开发中，具有声明式的数据平面，为有状态应用提供灵活、持久的存储。它是
    Kubernetes 原生的，提供快速、冗余的存储，可在任何 Kubernetes 集群中使用。Mayastor 插件将在 MicroK8s 1.24 中提供：[https://microk8s.io/docs/addon-mayastor](https://microk8s.io/docs/addon-mayastor)。
- en: Another optional and popular feature of OpenEBS is copy-on-write snapshots.
    Snapshots are created instantly, and there is no limit to the number of snapshots
    that can be created. The incremental snapshot feature improves data migration
    and portability across Kubernetes clusters, as well as between cloud providers
    or data centers. Common application scenarios include efficient replication for
    backups and the use of clones for troubleshooting or development against a read-only
    copy of data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: OpenEBS 的另一个可选且受欢迎的功能是写时复制快照。快照可以即时创建，并且没有创建快照数量的限制。增量快照功能改善了 Kubernetes 集群之间以及不同云服务商或数据中心之间的数据迁移和可移植性。常见的应用场景包括高效的备份复制和利用克隆进行故障排除或开发，针对数据的只读副本。
- en: OpenEBS volumes also support backup and restore facilities that are compatible
    with Kubernetes backup and restore solutions such as Velero ([https://velero.io/](https://velero.io/)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: OpenEBS 卷还支持与 Kubernetes 备份和恢复解决方案（如 Velero）兼容的备份和恢复功能 ([https://velero.io/](https://velero.io/))。
- en: 'To learn more, you can check out my blog post on how to back up and restore
    Kubernetes cluster resources, including PVs: [https://www.upnxtblog.com/index.php/2019/12/16/how-to-back-up-and-restore-your-kubernetes-cluster-resources-and-persistent-volumes/](https://www.upnxtblog.com/index.php/2019/12/16/how-to-back-up-and-restore-your-kubernetes-cluster-resources-and-persistent-volumes/).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，您可以查看我关于如何备份和恢复 Kubernetes 集群资源（包括 PVs）的博客文章：[https://www.upnxtblog.com/index.php/2019/12/16/how-to-back-up-and-restore-your-kubernetes-cluster-resources-and-persistent-volumes/](https://www.upnxtblog.com/index.php/2019/12/16/how-to-back-up-and-restore-your-kubernetes-cluster-resources-and-persistent-volumes/)。
- en: Through the container attached storage technique, OpenEBS extends the benefits
    of software-defined storage to cloud-native applications. For a thorough comparison
    and preferred use cases for each of the storage engines, see the OpenEBS documentation
    at [https://openebs.io/docs/](https://openebs.io/docs/).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过容器附加存储技术，OpenEBS 将软件定义存储的优势扩展到云原生应用程序。有关各存储引擎的详细比较和推荐使用场景，请参阅 OpenEBS 文档：[https://openebs.io/docs/](https://openebs.io/docs/)。
- en: To recap, OpenEBS creates local or distributed Kubernetes PVs from any storage
    available to Kubernetes worker nodes. This makes it simple for application and
    platform teams to implement Kubernetes stateful workloads that require fast, reliable,
    and scalable CAS. It also ensures that each storage volume has a separate pod
    and a set of replica pods, which are managed and deployed in Kubernetes like any
    other container or microservice. OpenEBS is also installed as a container, allowing
    for convenient storage service allocation on a per-application, cluster, or container
    basis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 综述一下，OpenEBS可以从Kubernetes工作节点可用的任何存储中创建本地或分布式的Kubernetes PV。这使得应用和平台团队可以轻松实现需要快速、可靠和可扩展CAS的Kubernetes有状态工作负载。它还确保每个存储卷都有一个独立的Pod以及一组副本Pod，这些Pod像其他容器或微服务一样，在Kubernetes中进行管理和部署。OpenEBS也作为容器安装，允许按应用、集群或容器分配存储服务。
- en: Now, let’s learn how to configure and implement a PostgreSQL stateful application
    while utilizing the OpenEBS Jiva storage engine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何在利用OpenEBS Jiva存储引擎的同时，配置并实现一个PostgreSQL有状态应用。
- en: Configuring and implementing a PostgreSQL stateful workload
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置并实现PostgreSQL有状态工作负载
- en: In this section, we’ll configure and implement a PostgreSQL stateful workload
    while utilizing the OpenEBS storage engine. We’ll be using the Jiva storage engine
    for PostgreSQL persistence, creating test data, simulating node failure to see
    if the data is still intact, and confirming that OpenEBS replication is functioning
    as expected.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将配置并实现一个PostgreSQL有状态工作负载，同时使用OpenEBS存储引擎。我们将使用Jiva存储引擎来确保PostgreSQL的持久性，创建测试数据，模拟节点故障以查看数据是否仍然完整，并确认OpenEBS的复制功能是否正常工作。
- en: 'Now that understand OpenEBS, we will delve into the steps of configuring and
    deploying OpenEBS on the cluster. The following diagram depicts our Raspberry
    Pi cluster setup:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了OpenEBS，我们将深入探讨在集群上配置和部署OpenEBS的步骤。下图展示了我们的树莓派集群设置：
- en: '![Figure 11.3 – MicroK8s Raspberry Pi cluster ](img/Figure_11.03_B18115.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – MicroK8s 树莓派集群](img/Figure_11.03_B18115.jpg)'
- en: Figure 11.3 – MicroK8s Raspberry Pi cluster
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – MicroK8s 树莓派集群
- en: Now that we know what we want to do, let’s look at the requirements.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了想要做什么，让我们来看看需求。
- en: Requirements
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需求
- en: 'Before you begin, you will need the following prerequisites to build a Raspberry
    Pi Kubernetes cluster and configure OpenEBS:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，您需要以下前提条件来构建一个树莓派Kubernetes集群并配置OpenEBS：
- en: A microSD card (4 GB minimum, 8 GB recommended)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张microSD卡（最小4GB，推荐8GB）
- en: A computer with a microSD card drive
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台带有microSD卡驱动器的计算机
- en: A Raspberry Pi 2, 3, or 4 (1 or more)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台树莓派2、3或4（1台或更多）
- en: A micro-USB power cable (USB-C for the Pi 4)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一根micro-USB电源线（Pi 4需要USB-C）
- en: A Wi-Fi network or an ethernet cable with an internet connection
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Wi-Fi网络或一根带有互联网连接的以太网线
- en: (Optional) A monitor with an HDMI interface
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）一台带有HDMI接口的显示器
- en: (Optional) An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi
    4
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）Pi 2和3使用HDMI电缆，Pi 4使用micro-HDMI电缆
- en: (Optional) A USB keyboard
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）一只USB键盘
- en: Now that we’ve established what the requirements are for testing a PostgresSQL
    stateful workload backed by OpenEBS, let’s get started.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了测试由OpenEBS支持的PostgreSQL有状态工作负载的需求，接下来我们开始实施。
- en: Step 1 – Creating the MicroK8s Raspberry Pi cluster
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1 – 创建MicroK8s树莓派集群
- en: 'Please follow the steps that we covered in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    to create the MicroK8s Raspberry Pi cluster; here’s a quick refresher:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照我们在[*第5章*](B18115_05.xhtml#_idTextAnchor070)中介绍的步骤，*在多节点树莓派Kubernetes集群上创建并实施更新*，来创建MicroK8s树莓派集群；下面是简短的回顾：
- en: 'Install the OS image on the SD card:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SD卡上安装操作系统镜像：
- en: Configure the Wi-Fi access settings.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置Wi-Fi访问设置。
- en: Configure the remote access settings.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置远程访问设置。
- en: Configure the control group settings.
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置控制组设置。
- en: Configure the hostname.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置主机名。
- en: Install and configure MicroK8s.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并配置MicroK8s。
- en: Add a worker node.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个工作节点。
- en: 'A fully functional multi-node Kubernetes cluster would look as follows. To
    summarize, we have installed MicroK8s on the Raspberry Pi boards and joined multiple
    deployments to form the cluster. We’ve also added nodes to the cluster:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全功能的多节点Kubernetes集群如下所示。总结一下，我们在树莓派板上安装了MicroK8s，并将多个部署加入到集群中。我们还向集群中添加了节点：
- en: '![Figure 11.4 – Fully functional MicroK8s Raspberry Pi cluster ](img/Figure_11.04_B18115.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – 完全功能的MicroK8s树莓派集群](img/Figure_11.04_B18115.jpg)'
- en: Figure 11.4 – Fully functional MicroK8s Raspberry Pi cluster
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 完全功能的 MicroK8s 树莓派集群
- en: Now, let’s enable the OpenEBS add-on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们启用 OpenEBS 插件。
- en: Step 2 – Enabling the OpenEBS add-on
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2 – 启用 OpenEBS 插件
- en: 'The OpenEBS add-on is available with MicroK8s by default. Use the following
    command to enable OpenEBS:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: OpenEBS 插件在 MicroK8s 中默认可用。使用以下命令启用 OpenEBS：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the following command indicates that the `iscsid` controller
    must be enabled as a prerequisite. For storage management, OpenEBS uses the **Internet
    Small Computer System Interface** (**iSCSI**) technology. The iSCSI protocol is
    a TCP/IP-based protocol for creating storage area networks and establishing and
    managing interconnections between IP storage devices, hosts, and clients (SANs).
    These SANs allow the SCSI protocol to be used in high-speed data transmission
    networks with block-level data transfer between different data storage networks:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令的输出表明，必须启用 `iscsid` 控制器作为前提条件。对于存储管理，OpenEBS 使用 **互联网小型计算机系统接口** (**iSCSI**)
    技术。iSCSI 协议是一种基于 TCP/IP 的协议，用于创建存储区域网络并在 IP 存储设备、主机和客户端之间建立和管理互联（SAN）。这些 SAN 允许在高速数据传输网络中使用
    SCSI 协议，进行不同数据存储网络之间的块级数据传输：
- en: '![Figure 11.5 – Enabling the OpenEBS add-on ](img/Figure_11.05_B18115.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 启用 OpenEBS 插件](img/Figure_11.05_B18115.jpg)'
- en: Figure 11.5 – Enabling the OpenEBS add-on
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 启用 OpenEBS 插件
- en: 'Use the following command to enable the `iscsid` controller:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启用 `iscsid` 控制器：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following output indicates that `iscsid` has been installed successfully.
    Now, we can enable the OpenEBS add-on:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明 `iscsid` 已成功安装。现在，我们可以启用 OpenEBS 插件：
- en: '![Figure 11.6 – Enabling the iSCSI controller ](img/Figure_11.06_B18115.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 启用 iSCSI 控制器](img/Figure_11.06_B18115.jpg)'
- en: Figure 11.6 – Enabling the iSCSI controller
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 启用 iSCSI 控制器
- en: 'The following output indicates that the OpenEBS add-on has been enabled successfully:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明 OpenEBS 插件已成功启用：
- en: '![Figure 11.7 – Enabling the OpenEBS add-on ](img/Figure_11.07_B18115.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – 启用 OpenEBS 插件](img/Figure_11.07_B18115.jpg)'
- en: Figure 11.7 – Enabling the OpenEBS add-on
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 启用 OpenEBS 插件
- en: 'The Helm3 add-on is also enabled by default. Before we move on, let’s make
    sure that all of the OpenEBS components are up and running using the following
    command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Helm3 插件默认也已启用。在继续之前，让我们通过以下命令确保所有 OpenEBS 组件都已启动并运行：
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following output indicates that all the components are `Running`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明所有组件都在 `运行`：
- en: '![Figure 11.8 – The OpenEBS components are up and running ](img/Figure_11.08_B18115.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – OpenEBS 组件已启动并运行](img/Figure_11.08_B18115.jpg)'
- en: Figure 11.8 – The OpenEBS components are up and running
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – OpenEBS 组件已启动并运行
- en: Now that the OpenEBS add-on has been enabled, let’s deploy a PostgreSQL stateful
    workload.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 OpenEBS 插件已启用，让我们部署 PostgreSQL 有状态工作负载。
- en: Step 3 – Deploying the PostgreSQL stateful workload
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3 – 部署 PostgreSQL 有状态工作负载
- en: To recap from [*Chapter 1*](B18115_01.xhtml#_idTextAnchor014), *Getting Started
    with Kubernetes*, a StatefulSet is a Kubernetes workload API object for managing
    stateful applications. In a typical deployment, the user is not concerned about
    how the pods are scheduled, so long as it has no negative impact on the deployed
    application. However, to preserve the state in stateful applications with persistent
    storage, pods must be identified. This functionality is provided by StatefulSet,
    which creates pods with a persistent identifier that corresponds to its value
    across rescheduling. This way, even if a pod is recreated, it will be correctly
    mapped to the storage volumes, and the application’s state will be preserved.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[*第 1 章*](B18115_01.xhtml#_idTextAnchor014)，*Kubernetes 入门*，StatefulSet 是
    Kubernetes 工作负载 API 对象，用于管理有状态应用程序。在典型的部署中，用户并不关心 Pod 是如何调度的，只要它不会对部署的应用程序产生负面影响。然而，为了在具有持久存储的有状态应用中保持状态，Pod
    必须被标识。这个功能由 StatefulSet 提供，它创建具有持久标识符的 Pod，这些标识符在重新调度时保持一致。这样，即使 Pod 被重新创建，它也会正确地映射到存储卷，应用程序的状态将被保留。
- en: With the popularity of deploying database clusters in Kubernetes, managing states
    in a containerized environment have become even more important.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在 Kubernetes 中部署数据库集群的普及，在容器化环境中管理状态变得更加重要。
- en: 'We’ll need to set up the following resources to get the PostgreSQL configuration
    up and running:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置以下资源来使 PostgreSQL 配置正常运行：
- en: Storage class
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储类
- en: PersistentVolume
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PersistentVolume
- en: PersistentVolumeClaim
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PersistentVolumeClaim
- en: StatefulSet
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSet
- en: ConfigMap
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ConfigMap
- en: Service
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务
- en: 'To manage persistent storage, Kubernetes provides the `PersistentVolume` and
    `PersistentVolumeClaim` storage mechanisms, which we briefly discussed in the
    introduction. Here’s a quick rundown of what they are:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理持久存储，Kubernetes 提供了 `PersistentVolume` 和 `PersistentVolumeClaim` 存储机制，我们在简介中简要讨论过。这是它们的快速概述：
- en: '**PersistentVolume** (**PV**) is stored in a cluster that has been provisioned
    by a cluster administrator or dynamically provisioned using storage classes.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PersistentVolume** (**PV**) 存储在由集群管理员预配置的集群中，或通过存储类动态配置。'
- en: '**PersistentVolumeClaim** (**PVC**) is a user’s (developer’s) request for storage.
    It is comparable to a pod. PVCs consume PV resources, while pods consume node
    resources:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PersistentVolumeClaim** (**PVC**) 是用户（开发者）对存储的请求。它类似于 pod。PVC 使用 PV 资源，而
    pod 使用节点资源：'
- en: '![Figure 11.9 – PV and PVC storage basics ](img/Figure_11.09_B18115.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – PV 和 PVC 存储基础](img/Figure_11.09_B18115.jpg)'
- en: Figure 11.9 – PV and PVC storage basics
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – PV 和 PVC 存储基础
- en: Before we create a PV and PVC, let’s look at the storage class that OpenEBS
    has created for us.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 PV 和 PVC 之前，先查看一下 OpenEBS 为我们创建的存储类。
- en: '`StorageClass` allows administrators to describe the *classes* of storage that
    they provide. Different classes may correspond to different **Quality-of-Service**
    (**QoS**) levels, backup policies, or arbitrary policies that are determined by
    the cluster administrators.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`StorageClass` 允许管理员描述他们提供的 *存储类*。不同的类可能对应不同的 **服务质量** (**QoS**) 等级、备份策略或由集群管理员确定的任意策略。'
- en: 'Use the following command to retrieve the storage class that has been created
    by OpenEBS:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令检索 OpenEBS 创建的存储类：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following output shows that three `StorageClass` are available:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示有三个可用的 `StorageClass`：
- en: '![Figure 11.10 – OpenEBS storage classes ](img/Figure_11.10_B18115.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – OpenEBS 存储类](img/Figure_11.10_B18115.jpg)'
- en: Figure 11.10 – OpenEBS storage classes
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – OpenEBS 存储类
- en: '`openebs-hostpath` and `openebs-device` are recommended for single-node clusters.
    For multi-node clusters, `openebs-jiva-csi-default` is recommended.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`openebs-hostpath` 和 `openebs-device` 推荐用于单节点集群，对于多节点集群，推荐使用 `openebs-jiva-csi-default`。'
- en: 'Now, we must define `PersistentVolume`, which will use the storage class, as
    well as `PersistentVolumeClaim`, which will be used to claim this volume:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须定义 `PersistentVolume`，它将使用存储类，以及 `PersistentVolumeClaim`，它将用于声明此卷：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because we’re utilizing an OpenEBS disc provisioner, we’ll need to specify where
    our data will be saved on the host node. We’ll use `/var/data/` in this case.
    The `accessMode` option is also crucial. We’ll use `ReadWriteOnce` in this case.
    This ensures that only one pod can write at any given moment. As a result, no
    two pods end up with the same writing volume. We can also specify the size of
    this volume, which we chose to be 5 GB.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用的是 OpenEBS 磁盘供应程序，所以我们需要指定数据在主机节点上的存储位置。在此我们使用`/var/data/`。`accessMode`
    选项也至关重要。此处我们使用`ReadWriteOnce`。这确保每次只有一个 pod 可以写入。因此，不会有两个 pod 同时写入同一个卷。我们还可以指定该卷的大小，这里选择了
    5 GB。
- en: Note on Access Modes
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 访问模式说明
- en: 'Even though a volume supports several access modes, they can only be mounted
    one at a time:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一个卷支持多种访问模式，它们也只能一次挂载一个模式：
- en: '**ReadOnlyMany** (**ROX**): Can be mounted by multiple nodes in read-only mode'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReadOnlyMany** (**ROX**)：可以由多个节点以只读模式挂载'
- en: '**ReadWriteOnce** (**RWO**): Can be mounted by a single node in read-write
    mode'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReadWriteOnce** (**RWO**)：可以由单个节点以读写模式挂载'
- en: '**ReadWriteMany** (**RWX**): Multiple nodes can be mounted in read-write mode'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReadWriteMany** (**RWX**)：多个节点可以以读写模式挂载'
- en: 'Use the following command to create the PV and PVC:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建 PV 和 PVC：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following output indicates that `PersistentVolume` and `PersistentVolumeClaim`
    have been created successfully:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表示 `PersistentVolume` 和 `PersistentVolumeClaim` 已成功创建：
- en: '![Figure 11.11 – PV and PVC created successfully ](img/Figure_11.11_B18115.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.11 – PV 和 PVC 创建成功](img/Figure_11.11_B18115.jpg)'
- en: Figure 11.11 – PV and PVC created successfully
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 – PV 和 PVC 创建成功
- en: 'Before moving on, let’s check if PV and PVC are `Bound`. A `Bound` state indicates
    that the application has access to the necessary storage:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，先检查 PV 和 PVC 是否处于 `Bound` 状态。`Bound` 状态表示应用程序已访问所需的存储：
- en: '![Figure 11.12 – PV and PVC are bound ](img/Figure_11.12_B18115.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.12 – PV 和 PVC 已绑定](img/Figure_11.12_B18115.jpg)'
- en: Figure 11.12 – PV and PVC are bound
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 – PV 和 PVC 已绑定
- en: If a PVC becomes stuck waiting, `StatefulSet` will get stuck as well, as it
    will be unable to access its storage. As a result, double-check that both `StorageClass`
    and `PersistentVolume` have been set up correctly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果PVC变得卡住等待，`StatefulSet`也会被卡住，因为它将无法访问其存储。因此，请仔细检查`StorageClass`和`PersistentVolume`是否已正确设置。
- en: 'Now that we’ve set up the PV and PVC, we’ll set up `ConfigMap` with configurations
    such as the username and password required for our setup. To keep things simple
    in this example, we’ve hardcoded the values inside `ConfigMap`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了PV和PVC，我们将设置`ConfigMap`，并包含一些如用户名和密码等配置，以便我们的设置能正常工作。为了简化，本示例中我们将这些值硬编码到`ConfigMap`中：
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Use the following command to create `ConfigMap`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建`ConfigMap`：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following output indicates that the `postgres-configuration.yaml` file’s
    `ConfigMap` has been created successfully:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明`postgres-configuration.yaml`文件的`ConfigMap`已成功创建：
- en: '![Figure 11.13 – PostgreSQL ConfigMap created ](img/Figure_11.13_B18115.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13 – PostgreSQL ConfigMap已创建](img/Figure_11.13_B18115.jpg)'
- en: Figure 11.13 – PostgreSQL ConfigMap created
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 – PostgreSQL ConfigMap已创建
- en: 'Let’s use the `describe` command to fetch the details of the `ConfigMap` object
    that we have created. The following output shows that the configuration required
    for our PostgreSQL setup is ready:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`describe`命令获取我们创建的`ConfigMap`对象的详细信息。以下输出显示我们的PostgreSQL设置所需的配置已就绪：
- en: '![Figure 11.14 – PostgreSQL ConfigMap ](img/Figure_11.14_B18115.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14 – PostgreSQL ConfigMap](img/Figure_11.14_B18115.jpg)'
- en: Figure 11.14 – PostgreSQL ConfigMap
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – PostgreSQL ConfigMap
- en: 'Now that we’ve defined our `ConfigMap` and storage volume, we can define a
    `StatefulSet` that will make use of them:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了`ConfigMap`和存储卷，我们可以定义一个`StatefulSet`来使用它们：
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The definition of a `StatefulSet` is similar to that of deployments. We’ve
    added two more things:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`StatefulSet`的定义与部署类似。我们增加了两项内容：'
- en: We’ve loaded the environment variables from `ConfigMap` into the pod.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已将`ConfigMap`中的环境变量加载到Pod中。
- en: We’ve defined our volume, which will map to `/var/lib/PostgreSQL/data` within
    our pod. This volume is defined using the PVC that we discussed earlier.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义了卷，它将在Pod内映射到`/var/lib/PostgreSQL/data`。该卷是通过我们之前讨论的PVC来定义的。
- en: Finally, we have also created a `Service` resource that will expose our database.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还创建了一个`Service`资源，将暴露我们的数据库。
- en: 'Use the following command to create the `StatefulSet` and `Service` resources
    to expose the database:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建`StatefulSet`和`Service`资源以暴露数据库：
- en: '[PRE9]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following output indicates that both `StatefulSet` and `Service` have been
    created successfully:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出表明`StatefulSet`和`Service`已成功创建：
- en: '![Figure 11.15 – Postgres deployment succeeded ](img/Figure_11.15_B18115.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15 – Postgres部署成功](img/Figure_11.15_B18115.jpg)'
- en: Figure 11.15 – Postgres deployment succeeded
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – Postgres部署成功
- en: 'Before moving on, let’s verify that the pods and service have been created.
    The following output shows that the pods are `Running`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们验证一下Pod和Service是否已创建。以下输出显示Pod正在`Running`：
- en: '![Figure 11.16 – The Postgres pods are Running  ](img/Figure_11.16_B18115.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图11.16 – Postgres Pod正在运行](img/Figure_11.16_B18115.jpg)'
- en: Figure 11.16 – The Postgres pods are Running
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16 – Postgres Pod正在运行
- en: 'The following output shows that the service has been exposed on port `5432`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示服务已通过端口`5432`暴露：
- en: '![Figure 11.17 – The Postgres service has been exposed ](img/Figure_11.17_B18115.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图11.17 – Postgres服务已暴露](img/Figure_11.17_B18115.jpg)'
- en: Figure 11.17 – The Postgres service has been exposed
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17 – Postgres服务已暴露
- en: 'Let’s also look at where the `StatefulSet` pods are distributed across the
    cluster using the following command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令查看`StatefulSet`的Pod是如何分布在集群中的：
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following output shows that the PostgreSQL database pods are running on
    two nodes (`1` in `controlplane` and `1` in `worker1`):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示PostgreSQL数据库Pod正在两个节点上运行（`1`在`controlplane`，`1`在`worker1`）：
- en: '![Figure 11.18 – The database pods are running on two nodes ](img/Figure_11.18_B18115.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图11.18 – 数据库Pod运行在两个节点上](img/Figure_11.18_B18115.jpg)'
- en: Figure 11.18 – The database pods are running on two nodes
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18 – 数据库Pod运行在两个节点上
- en: With that, we have successfully configured PostgreSQL and it’s up and running.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已经成功配置了PostgreSQL并使其正常运行。
- en: Now, let’s create a test database and a table, and add a few records.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个测试数据库和表，并添加一些记录。
- en: Step 4 – Creating the test data
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 创建测试数据
- en: To create test data, use the `PgSQL` client or log into one of the pods so that
    we can create a test database and table.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建测试数据，使用 `PgSQL` 客户端或登录到其中一个 Pod，这样我们可以创建测试数据库和表。
- en: 'Use the following command to log into the PostgreSQL pod:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令登录到 PostgreSQL Pod：
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following output shows that we can log into the PostgreSQL pod:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示我们可以登录到 PostgreSQL Pod：
- en: '![Figure 11.19 – Logging into one of the PostgreSQL pods ](img/Figure_11.19_B18115.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.19 – 登录到其中一个 PostgreSQL Pod](img/Figure_11.19_B18115.jpg)'
- en: Figure 11.19 – Logging into one of the PostgreSQL pods
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 – 登录到其中一个 PostgreSQL Pod
- en: 'Now that we have logged into the pod, we have access to the `psql` PostgreSQL
    client. Use the following command to create the test database:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经登录到 Pod，我们可以使用 `psql` PostgreSQL 客户端。使用以下命令创建测试数据库：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following output shows that our test database, `inventory_mgmt`, has been
    created:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示我们的测试数据库`inventory_mgmt`已成功创建：
- en: '![Figure 11.20 – Creating the test database ](img/Figure_11.20_B18115.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.20 – 创建测试数据库](img/Figure_11.20_B18115.jpg)'
- en: Figure 11.20 – Creating the test database
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.20 – 创建测试数据库
- en: 'Let’s switch our connection to the new database we have created using `\c inventory_mgmt`.
    The following output indicates that we have successfully switched to a new database.
    Now, we can create a table:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用 `\c inventory_mgmt` 切换到我们创建的新数据库。以下输出显示我们已成功切换到新数据库。现在，我们可以创建表了：
- en: '![Figure 11.21 – Switching the connection to the new database ](img/Figure_11.21_B18115.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.21 – 切换到新数据库连接](img/Figure_11.21_B18115.jpg)'
- en: Figure 11.21 – Switching the connection to the new database
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.21 – 切换到新数据库连接
- en: 'In the new database, use the `CREATE TABLE` command to create a test table.
    The following output indicates that a new table called `products_master` has been
    created successfully:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在新数据库中，使用 `CREATE TABLE` 命令创建测试表。以下输出表示新表 `products_master` 已成功创建：
- en: '![Figure 11.22 – Creating the test table ](img/Figure_11.22_B18115.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.22 – 创建测试表](img/Figure_11.22_B18115.jpg)'
- en: Figure 11.22 – Creating the test table
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.22 – 创建测试表
- en: 'Now that the test table has been created, use the `INSERT` command to add a
    few records, as shown here:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在测试表已经创建，使用 `INSERT` 命令添加一些记录，如下所示：
- en: '![Figure 11.23 – Adding a few records to the test table ](img/Figure_11.23_B18115.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.23 – 向测试表中添加一些记录](img/Figure_11.23_B18115.jpg)'
- en: Figure 11.23 – Adding a few records to the test table
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.23 – 向测试表中添加一些记录
- en: 'Here, we have added records to our test table. Before we move on, let’s use
    the `SELECT` command to list the records, as shown in the following screenshot:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经向测试表中添加了记录。在继续之前，让我们使用`SELECT`命令列出记录，如下图所示：
- en: '![Figure 11.24 – Records from the test table ](img/Figure_11.24_B18115.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.24 – 测试表中的记录](img/Figure_11.24_B18115.jpg)'
- en: Figure 11.24 – Records from the test table
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.24 – 测试表中的记录
- en: To recap, in this section, we have created a test database, created a new table,
    and added a few records to the table. Now, let’s simulate node failure.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在这一部分中，我们已经创建了一个测试数据库，创建了一个新表，并向表中添加了一些记录。现在，让我们模拟节点故障。
- en: Step 5 – Simulating node failure
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五步 – 模拟节点故障
- en: To simulate node failure, we will use the `cordon` command to mark the node
    as `unschedulable`. If the node is `unschedulable`, the Kubernetes controller
    will not schedule new pods on this node.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟节点故障，我们将使用 `cordon` 命令将节点标记为 `不可调度`。如果节点是 `不可调度` 的，Kubernetes 控制器将不会在该节点上调度新的
    Pod。
- en: Let’s locate the PostgreSQL database pod’s node and cordon it off, preventing
    new pods from being scheduled on it.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定位 PostgreSQL 数据库 Pod 所在的节点，并将其封锁，防止新的 Pod 在该节点上调度。
- en: 'The following output shows that the database pods are running on `2` nodes
    (`1` in `controlplane` and `1` in `worker1`):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示数据库 Pod 正在 `2` 个节点上运行（`1` 在 `controlplane`，`1` 在 `worker1`）：
- en: '![Figure 11.25 – PostgreSQL database pods ](img/Figure_11.18_B18115.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.25 – PostgreSQL 数据库 Pods](img/Figure_11.18_B18115.jpg)'
- en: Figure 11.25 – PostgreSQL database pods
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.25 – PostgreSQL 数据库 Pods
- en: 'Let’s use `cordon` on the `worker1` node so that new pods are prevented from
    being scheduled on it:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 `worker1` 节点上使用 `cordon`，以防止新的 Pod 在该节点上被调度：
- en: '[PRE13]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following output shows that `worker1` has been cordoned:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示 `worker1` 已被封锁：
- en: '![Figure 11.26 – Cordoned Worker1 node ](img/Figure_11.26_B18115.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.26 – 已被封锁的 Worker1 节点](img/Figure_11.26_B18115.jpg)'
- en: Figure 11.26 – Cordoned Worker1 node
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.26 – 已被封锁的 Worker1 节点
- en: 'Even though the `worker1` node has been cordoned, existing pods will still
    run, so we can use the `drain` command to delete all the pods:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 即使`worker1`节点已经被隔离，现有的 pod 仍然会运行，因此我们可以使用 `drain` 命令删除所有的 pod：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following output shows that `worker1` can’t be drained due to pods with
    local storage provisioned:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示，由于存在具有本地存储配置的 pod，`worker1` 无法被清空：
- en: '![Figure 11.27 – Draining the Worker1 node ](img/Figure_11.27_B18115.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.27 – 清空 Worker1 节点](img/Figure_11.27_B18115.jpg)'
- en: Figure 11.27 – Draining the Worker1 node
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.27 – 清空 Worker1 节点
- en: Finally, we will use the `kubectl delete` command to delete the pod that is
    currently running on the cordoned node.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`kubectl delete`命令删除当前在已隔离节点上运行的 pod。
- en: 'The following output shows that pods running on `worker1` have been deleted
    successfully:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示，运行在 `worker1` 上的 pod 已成功删除：
- en: '![Figure 11.28 – Deleting the pods running on the Worker1 node ](img/Figure_11.28_B18115.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.28 – 删除在 Worker1 节点上运行的 pod](img/Figure_11.28_B18115.jpg)'
- en: Figure 11.28 – Deleting the pods running on the Worker1 node
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.28 – 删除在 Worker1 节点上运行的 pod
- en: The Kubernetes controller will now recreate a new pod and schedule it in a different
    node as soon as the pod is deleted. It cannot be placed on the same node since
    scheduling has been disabled; this is because we cordoned the `worker1` node.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 pod 被删除，Kubernetes 控制器将重新创建一个新 pod，并将其调度到不同的节点上。由于调度被禁用，它不能被放置到同一节点上；这是因为我们已经隔离了`worker1`节点。
- en: 'Let’s inspect where the pods are running using the `kubectl get pods` command.
    The following output shows that the new pod has been rescheduled to the `controlplane`
    node:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `kubectl get pods` 命令查看 pod 的运行位置。以下输出显示，新 pod 已被重新调度到 `controlplane`
    节点：
- en: '![Figure 11.29 – PostgreSQL database pods ](img/Figure_11.29_B18115.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.29 – PostgreSQL 数据库 pod](img/Figure_11.29_B18115.jpg)'
- en: Figure 11.29 – PostgreSQL database pods
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.29 – PostgreSQL 数据库 pod
- en: Even though the PVC has a `ReadWriteOnce` access mode and is mounted by a specific
    node for read-write access, the new pod that has been recreated can use the same
    PVC that has been abstracted by the underlying `OpenEBS` volumes into a single
    storage layer.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 PVC 的访问模式为`ReadWriteOnce`，并且由特定节点进行读写挂载，重新创建的新 pod 仍然可以使用由底层 `OpenEBS` 卷抽象出的
    PVC，这些卷已合并为单一的存储层。
- en: 'To verify if the new pod is using the same PVC, let’s connect to the new pod
    and see if the data is still intact by using the `kubectl exec` command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证新 pod 是否正在使用相同的 PVC，我们可以连接到新 pod，并使用 `kubectl exec` 命令查看数据是否仍然完好无损：
- en: '![Figure 11.30 – Logging into the PostgreSQL pod ](img/Figure_11.30_B18115.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.30 – 登录 PostgreSQL pod](img/Figure_11.30_B18115.jpg)'
- en: Figure 11.30 – Logging into the PostgreSQL pod
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.30 – 登录 PostgreSQL pod
- en: 'The following output shows that the data is intact even after deleting the
    pod and rescheduling it on a different node. This confirms that the replication
    `OpenEBS` is working properly:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示，即使删除了 pod 并将其重新调度到不同的节点上，数据仍然完好无损。这证明了复制的`OpenEBS`正在正常工作：
- en: '![Figure 11.31 – The data is intact ](img/Figure_11.31_B18115.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.31 – 数据完好无损](img/Figure_11.31_B18115.jpg)'
- en: Figure 11.31 – The data is intact
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.31 – 数据完好无损
- en: To summarize, data engines are responsible for maintaining the actual state
    generated by stateful applications, as well as providing sufficient storage capacity
    to retain the information and ensure that it remains intact over time. For example,
    the state can be created once, accessed over the next few minutes or days, updated,
    or simply left to be retrieved months or years later. You can use **Local PV**,
    **Jiva**, **cStor**, or **Mayastor**, depending on the type of storage associated
    with your Kubernetes worker nodes and your application performance needs.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据引擎负责维护由有状态应用生成的实际状态，并提供足够的存储容量来保留这些信息，确保它随时间保持完好无损。例如，状态可以创建一次，在接下来的几分钟或几天内访问、更新，或干脆在几个月或几年后再取回。根据
    Kubernetes 工作节点关联的存储类型和应用的性能需求，你可以使用**本地 PV**、**Jiva**、**cStor**或**Mayastor**。
- en: Choosing an engine is entirely dependent on your platform (resources and storage
    type), the application workload, and the application’s current and future capacity
    and/or performance growth. In the next section, we’ll look at some Kubernetes
    storage best practices, as well as some recommendations for data engines.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 选择引擎完全取决于你的平台（资源和存储类型）、应用工作负载以及应用当前和未来的容量和/或性能增长。在下一节中，我们将探讨一些 Kubernetes 存储最佳实践，并提供一些数据引擎的推荐。
- en: Kubernetes storage best practices
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 存储最佳实践
- en: 'For modern containerized applications deployed on Kubernetes, storage is a
    crucial concern. Kubernetes has progressed from local node filesystems mounted
    in containers to NFS, and finally to native storage, as described by the CSI specification,
    which allows for data durability and sharing. In this section, we’ll look at some
    of the best practices to take into consideration when configuring a PV:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在 Kubernetes 上部署的现代容器化应用程序，存储是一个至关重要的问题。Kubernetes 从容器中挂载的本地节点文件系统，逐步发展到 NFS，最终到本地存储，如
    CSI 规范所描述的那样，它支持数据持久性和共享。在本节中，我们将探讨在配置 PV 时需要考虑的一些最佳实践：
- en: Avoid statically creating and allocating PVs to decrease management costs and
    facilitate scaling. Use dynamic provisioning instead. Define an appropriate reclaim
    policy in your storage class to reduce storage costs when pods are deleted.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免静态创建和分配 PV，以减少管理成本并促进扩展。改为使用动态供应。为您的存储类定义适当的回收策略，以便在删除 Pod 时减少存储成本。
- en: Each node can only support a certain number of sizes, so different node sizes
    provide varying amounts of local storage and capacity. To install the optimum
    node sizes, plan accordingly for your application’s demands.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点只能支持一定数量的大小，因此不同的节点大小提供不同的本地存储和容量。为了安装最佳的节点大小，请根据应用程序的需求进行规划。
- en: 'The life cycle of a PV is independent of any individual container in the cluster.
    A PVC is a request for a specific type of storage made by a container user or
    application. Kubernetes documentation suggests the following for building a PV:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PV 的生命周期与集群中任何单个容器无关。PVC 是由容器用户或应用程序发出的对特定类型存储的请求。Kubernetes 文档建议构建 PV 时执行以下操作：
- en: PVCs should always be included in the container setup.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PVC 应始终包含在容器设置中。
- en: PVs should never be used in container configuration since they will bind a container
    to a specific volume.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PV 永远不应在容器配置中使用，因为它会将容器绑定到特定的卷。
- en: PVCs that don’t specify a specific class will fail if they don’t have a default
    `Storage Class`.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 PVC 未指定特定的类，并且没有默认的 `Storage Class`，则会失败。
- en: Give Storage Classes names that are meaningful.
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给存储类起具有意义的名称。
- en: At the namespace level, resource quotas are also provided, giving you another
    level of control over cluster resource utilization. The total amount of CPU, memory,
    and storage resources that all the containers executing in the namespace can utilize
    is limited by resource limits. It can also set storage resource limits based on
    service levels or backup requirements.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在命名空间级别，也提供资源配额，您可以通过这种方式对集群资源的使用进行更细粒度的控制。命名空间中所有容器可以使用的总 CPU、内存和存储资源受资源限制的限制。还可以根据服务级别或备份需求设置存储资源限制。
- en: Persistent storage hardware comes in a variety of shapes and sizes. SSDs, for
    example, outperform HDDs in terms of read/write performance, and NVMe SSDs are
    especially well-suited to high workloads. QoS criteria are added to the description
    of a PVC by some of the Kubernetes providers. This means that it prioritizes read/write
    volumes for specific installations, allowing for higher performance if the application
    requires it.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久化存储硬件有多种形状和尺寸。例如，SSD 在读写性能方面优于 HDD，而 NVMe SSD 尤其适合高负载工作。某些 Kubernetes 提供商会在
    PVC 描述中添加 QoS 标准。这意味着它优先处理特定安装的读写卷，从而在应用程序需要时提供更高的性能。
- en: Now, let’s look at some of the guidelines for selecting OpenEBS data engines.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看选择 OpenEBS 数据引擎的一些指南。
- en: Guidelines on choosing OpenEBS data engines
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择 OpenEBS 数据引擎的指南
- en: 'Each storage engine has its advantages, as shown in the following table. Choosing
    an engine is entirely dependent on your platform (resources and storage type),
    the application workload, and the application’s current and future capacity and/or
    performance growth. The following guidelines will assist you in selecting an engine:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 每个存储引擎都有其优点，如下表所示。选择引擎完全取决于您的平台（资源和存储类型）、应用程序工作负载以及应用程序当前和未来的容量和/或性能增长。以下指南将帮助您选择合适的引擎：
- en: '![Table 11.1 – Choosing OpenEBS data engines ](img/Table_11.01_B18115.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![表 11.1 – 选择 OpenEBS 数据引擎](img/Table_11.01_B18115.jpg)'
- en: Table 11.1 – Choosing OpenEBS data engines
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.1 – 选择 OpenEBS 数据引擎
- en: In conclusion, OpenEBS offers a set of data engines, each of which is designed
    and optimized for executing stateful workloads with varied capabilities on Kubernetes
    nodes with varying resource levels. In a Kubernetes cluster, platform SREs or
    administrators often choose one or more data engines. These data engines are chosen
    based on node capabilities or stateful application capabilities.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，OpenEBS提供了一套数据引擎，每个引擎都针对在Kubernetes节点上执行有状态工作负载而设计并优化，且节点的资源水平各异。在Kubernetes集群中，平台SRE或管理员通常会选择一个或多个数据引擎。这些数据引擎的选择依据节点能力或有状态应用的需求。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how Kubernetes persistent storage provides a convenient
    way for Kubernetes applications to request and consume storage resources. The
    PVC is declared by the user’s pod, and Kubernetes will find a PV to pair it with.
    If there is no PV to pair with, then it will go to the corresponding `StorageClass`
    and assist it in creating a PV before binding it to the PVC. The newly created
    PV must use the attached master node to create a remote disc for the host and
    then mount the attached remote disc to the host directory using the `kubelet`
    component of each node.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了Kubernetes持久化存储如何为Kubernetes应用程序提供便捷的存储资源请求和消耗方式。PVC由用户的pod声明，Kubernetes会找到一个PV来与之匹配。如果没有可匹配的PV，它将进入相应的`StorageClass`，帮助创建一个PV，并将其绑定到PVC。新创建的PV必须使用附加的主节点为主机创建远程磁盘，然后通过每个节点的`kubelet`组件将附加的远程磁盘挂载到主机目录。
- en: Kubernetes has made significant improvements to facilitate running stateful
    workloads by giving platform (or cluster administrators) and application developers
    the necessary abstractions. These abstractions ensure that different types of
    file and block storage (whether ephemeral or persistent, local or remote) are
    available wherever a container is scheduled (including provisioning/creating,
    attaching, mounting, unmounting, detaching, and deleting volumes), storage capacity
    management (container ephemeral storage usage, volume resizing, and generic operations),
    and influencing container scheduling based on storage (data gravity, availability,
    and so on).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在促进运行有状态工作负载方面取得了显著进展，通过为平台（或集群管理员）和应用程序开发者提供必要的抽象。这些抽象确保了不同类型的文件和块存储（无论是临时的还是持久的，本地的还是远程的）能够在容器调度的任何地方使用（包括配备/创建、附加、挂载、卸载、分离和删除卷），存储容量管理（容器临时存储使用、卷大小调整和常规操作），以及基于存储影响容器调度（数据重力、可用性等）。
- en: In the next chapter, you will learn how to deploy the Istio and Linkerd service
    mesh. You will also learn how to deploy and run a sample application, as well
    as how to configure and access dashboards.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何部署Istio和Linkerd服务网格。你还将学习如何部署和运行一个示例应用程序，以及如何配置和访问仪表盘。
