- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Monitoring the Health of Infrastructure and Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控基础设施和应用程序的健康状况
- en: In the last chapter, we looked at how to expose Services outside of the cluster,
    and load balancers were used to expose applications to the outside network. The
    single `LoadBalancer` Kubernetes service is implemented by `MetalLB`. `MetalLB`
    assigns a client an IP address from a predefined range when a `LoadBalancer` service
    is requested and informs the network that the IP address is in the cluster. `MetalLB`,
    which may be deployed alongside Ingress in the same Kubernetes cluster, can also
    be utilized as a load balancer. Another technique to expose the Ingress controller
    to the outside world is through `NodePort`. Both options were explored in detail
    in the previous chapter, with various examples.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何将服务暴露到集群外部，并使用负载均衡器将应用程序暴露到外部网络。单一的`LoadBalancer` Kubernetes 服务是通过`MetalLB`实现的。当请求`LoadBalancer`服务时，`MetalLB`会从预定义的范围中分配一个IP地址给客户端，并通知网络该IP地址属于集群内。`MetalLB`可以与
    Ingress 一起在同一 Kubernetes 集群中部署，也可以作为负载均衡器使用。另一种将 Ingress 控制器暴露到外部世界的方式是通过`NodePort`。这两种选项在上一章中已经详细探讨，并给出了各种示例。
- en: 'In this chapter, we will look at various options for monitoring, logging, and
    alerting for your infrastructure and applications. In a traditional, host-centric
    infrastructure, there used to be only two levels of monitoring: applications and
    the hosts that run them. Then, container abstraction came in, between the host
    and your applications, after which Kubernetes came in to orchestrate your containers.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨监控、日志记录和警报的各种选项，适用于您的基础设施和应用程序。在传统的以主机为中心的基础设施中，曾经只有两种监控层级：应用程序和运行它们的主机。随后，容器抽象技术出现了，位于主机和应用程序之间，之后
    Kubernetes 用于协调这些容器。
- en: 'To manage infrastructure thoroughly, Kubernetes must now be observed as well.
    As a result, four distinct components must now be monitored, each with its own
    set of challenges:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面管理基础设施，Kubernetes 本身现在也必须进行监控。因此，必须监控四个不同的组件，每个组件都有自己的一组挑战：
- en: Hosts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机
- en: Containers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器
- en: Applications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序
- en: And finally, the Kubernetes cluster itself
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，Kubernetes 集群本身
- en: To keep track of the health of the Kubernetes infrastructure, there is a need
    to collect metrics and events from all containers and Pods. However, to fully
    comprehend what clients or users are going through, there is now a need to keep
    track of the applications that are operating in these Pods. Note that you normally
    have very little influence over where workloads run when using Kubernetes, which
    automatically schedules them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪 Kubernetes 基础设施的健康状况，需要从所有容器和 Pod 中收集指标和事件。然而，为了完全理解客户或用户的经历，现在还需要跟踪在这些
    Pod 中运行的应用程序。需要注意的是，当使用 Kubernetes 时，您通常对工作负载的运行位置几乎没有控制权，因为 Kubernetes 会自动安排它们。
- en: When it comes to monitoring, Kubernetes forces you to reconsider your strategy.
    But if you know what to look for, where to look for it, and how to aggregate and
    analyze it, you can make sure your applications are running smoothly and Kubernetes
    is doing its job effectively.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控方面，Kubernetes 迫使你重新考虑策略。但如果你知道该关注什么，在哪里查找以及如何聚合和分析数据，你可以确保你的应用程序运行顺利，同时 Kubernetes
    能有效地执行其任务。
- en: For aggregating and reporting monitoring data from your cluster, the Kubernetes
    ecosystem currently offers two in-built add-ons, as detailed next.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从集群中聚合和报告监控数据，Kubernetes 生态系统目前提供了两个内置的附加组件，具体细节如下。
- en: Metrics Server collects resource consumption statistics from each `kubelet`
    on each node and returns aggregated metrics via the `Metrics` `kube-state-metrics`
    add-on service makes cluster state data public. Unlike Metrics Server, which provides
    metrics on Pod and node resource utilization, `kube-state-metrics` polls the control
    plane API server for information on the overall status of Kubernetes objects (nodes,
    Pods, Deployments, and so on), as well as resource restrictions and allocations.
    The information is then utilized to generate metrics, which may be accessed through
    the Metrics API. In short, `kube-state-metrics` focuses on creating whole new
    metrics from Kubernetes' object state, whereas `metrics-server` merely saves the
    most recent data and is not responsible for transmitting metrics to third-party
    destinations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Metrics Server 从每个节点上的每个 `kubelet` 收集资源消耗统计信息，并通过 `Metrics` 返回汇总的指标；`kube-state-metrics`
    插件使集群状态数据公开。与提供 Pod 和节点资源利用率指标的 Metrics Server 不同，`kube-state-metrics` 会查询控制平面
    API 服务器，获取有关 Kubernetes 对象（节点、Pod、部署等）整体状态的信息，以及资源限制和分配。然后，利用这些信息生成指标，用户可以通过 Metrics
    API 访问。这意味着，`kube-state-metrics` 侧重于从 Kubernetes 的对象状态生成全新的指标，而 `metrics-server`
    只是保存最新数据，并不负责将指标传输到第三方目的地。
- en: In the following sections, we'll go over in detail the various options for retrieving
    metrics using Metrics Server and `kube-state-metrics`. The advantage of MicroK8s
    is that the monitoring tools can be enabled in under a minute with only a few
    commands. It is small enough to fit on a Raspberry Pi and it can be used to develop
    a monitoring stack that can be deployed anywhere, even at the edge. Furthermore,
    this is built using some of the most popular open source components that come
    preinstalled with MicroK8s.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细介绍如何使用 Metrics Server 和 `kube-state-metrics` 获取指标的各种选项。MicroK8s
    的优点是监控工具只需几条命令即可在一分钟内启用。它足够小，可以运行在 Raspberry Pi 上，并且可以用于开发一个可以部署到任何地方的监控堆栈，甚至是边缘环境。此外，它是使用一些最流行的开源组件构建的，这些组件在
    MicroK8s 中已经预装。
- en: 'We''ll look at how to easily deploy monitoring tools at the edge in this chapter.
    Such a deployment provides privacy, low latency, and minimal bandwidth costs in
    **internet of things** (**IoT**)/edge applications. In this chapter, we''re going
    to cover the following main topics:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何轻松地在边缘部署监控工具。这样的部署提供了隐私保护、低延迟和最小带宽成本，适用于 **物联网**（**IoT**）/边缘应用程序。在本章中，我们将讨论以下几个主要主题：
- en: Overview of monitoring, logging, and alerting options
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控、日志记录和警报选项概述
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和警报堆栈
- en: Configuring a monitoring, logging, and alerting stack using the **Elasticsearch,
    Fluentd, and Kibana** (**EFK**) toolset
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **Elasticsearch、Fluentd 和 Kibana**（**EFK**）工具集配置监控、日志记录和警报堆栈
- en: Key metrics that need to be monitored
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要监控的关键指标
- en: Overview of monitoring, logging, and alerting options
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控、日志记录和警报选项概述
- en: Kubernetes has a lot of advantages, but it also adds a lot of complexity. Its
    capacity to distribute containerized applications across several nodes and even
    different data centers (cloud providers, for example) necessitates a comprehensive
    monitoring solution that can collect and aggregate metrics from a variety of sources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有很多优点，但它也增加了很多复杂性。它能够将容器化应用分布在多个节点甚至不同的数据中心（例如云提供商），这就需要一个全面的监控解决方案，能够从多个来源收集和汇总指标。
- en: 'Many free and paid solutions provide real-time monitoring of Kubernetes clusters
    and the applications they host, and continuous monitoring of system and application
    health is critical. Here, we list some prominent open source Kubernetes monitoring
    tools:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 许多免费和付费的解决方案提供 Kubernetes 集群及其托管的应用程序的实时监控，持续监控系统和应用程序健康至关重要。这里列出了一些著名的开源 Kubernetes
    监控工具：
- en: '`kubelets` and exposes them in the Kubernetes API server through the following
    Metrics API endpoints:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubelets` 并通过以下 Metrics API 端点在 Kubernetes API 服务器中公开它们：'
- en: '![Table 8.1 – Metrics API endpoints ](img/01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.1 – 指标 API 端点](img/01.jpg)'
- en: Table 8.1 – Metrics API endpoints
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 – 指标 API 端点
- en: '**Kubernetes Dashboard** (in-built) is a web **user interface** (**UI**) add-on
    for Kubernetes clusters that allows you to keep track of workload health. Using
    a simple web interface, Kubernetes Dashboard allows you to manage cluster resources
    and troubleshoot containerized applications. It provides a concise overview of
    cluster-wide and individual node resources. It also lists all clusters'' namespaces
    as well as all storage classes that have been declared.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kubernetes Dashboard**（内置）是一个为 Kubernetes 集群提供的网页**用户界面**（**UI**）插件，允许您跟踪工作负载的健康状况。通过简单的网页界面，Kubernetes
    Dashboard 使您能够管理集群资源并排查容器化应用程序的故障。它提供了集群范围和单个节点资源的简明概览。它还列出了所有集群的命名空间以及所有已声明的存储类。'
- en: '**Prometheus** is an open source system for collecting metrics on Kubernetes
    health. It deploys node exporter Pods on each cluster node, and its server collects
    data from nodes, Pods, and jobs. Final time-series metrics data is saved in a
    database, and alerts can be generated automatically based on predefined conditions.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Prometheus** 是一个开源系统，用于收集 Kubernetes 健康状况的度量数据。它在每个集群节点上部署节点出口（node exporter）Pod，并且其服务器收集来自节点、Pod
    和任务的数据。最终的时间序列度量数据保存在数据库中，并且可以根据预定义条件自动生成警报。'
- en: Prometheus has its own dashboard with limited capabilities that have been extended
    by the usage of other visualization tools such as Grafana, which uses the Prometheus
    database to provide advanced inquiries, debugging, and reporting designed for
    development, test, and production teams.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 具有自己的仪表板，功能有限，但通过使用其他可视化工具（如 Grafana）得到了扩展，Grafana 使用 Prometheus
    数据库提供针对开发、测试和生产团队设计的高级查询、调试和报告功能。
- en: 'It was built with the objective of monitoring applications and microservices
    in containers at scale, and it can connect to a wide range of third-party databases
    and supports the bridging of data from other tools. It is made up of three components
    at its core, as outlined here:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它的设计目标是大规模监控容器中的应用程序和微服务，能够连接各种第三方数据库，并支持桥接来自其他工具的数据。它的核心由三个组件组成，如下所示：
- en: All metrics data will be stored in an in-built time-series database.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有度量数据将存储在内置的时间序列数据库中。
- en: A data retrieval worker is in charge of obtaining metrics from outside sources
    and entering them into the database.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据检索工作者负责从外部来源获取度量指标，并将其输入数据库。
- en: A web server with a simple web interface for configuring and querying the stored
    data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配备简单网页界面的 web 服务器，用于配置和查询存储的数据。
- en: 'Some of the key features of Prometheus are presented here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 Prometheus 的一些关键特性：
- en: Time-series data classified by metric name and key/value pairs in a multidimensional
    data model.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列数据按度量名称和键/值对进行分类，采用多维数据模型。
- en: Using **Prometheus Query Language** (**PromQL**), a flexible query language
    that allows us to make use of this dimensionality without relying on distributed
    storage.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **Prometheus 查询语言**（**PromQL**），这是一种灵活的查询语言，允许我们在不依赖分布式存储的情况下利用这种维度。
- en: Single-server nodes are self-contained, and time series are collected using
    a pull model over **HyperText Transfer Protocol** (**HTTP**).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单服务器节点是自包含的，时间序列通过 **超文本传输协议**（**HTTP**）的拉取模型进行收集。
- en: Alternatively, an intermediary gateway can be used to push time series to destinations
    that are discovered using service discovery or static configuration.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，可以使用中介网关将时间序列推送到通过服务发现或静态配置发现的目标。
- en: Multiple graphing and dashboarding options are supported.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多种图表和仪表板选项。
- en: '**Grafana**, an open source analytics and metric visualization platform, includes
    four dashboards: **Cluster**, **Node**, **Pod/Container**, and **Deployment**.
    Grafana and the Prometheus data source are frequently used by Kubernetes administrators
    to create information-rich dashboards.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Grafana** 是一个开源分析和度量可视化平台，包含四个仪表板：**集群**、**节点**、**Pod/容器** 和 **部署**。Kubernetes
    管理员常常使用 Grafana 和 Prometheus 数据源来创建信息丰富的仪表板。'
- en: '**Elasticsearch, Fluentd, and Kibana** make up the EFK stack, which is a combination
    of three tools that function well together. Fluentd is a data collector for Kubernetes
    cluster nodes that collects logs from Pods. It sends these logs to the Elasticsearch
    search engine, which ingests and stores the data in a central location. The EFK
    stack''s UI is Kibana, a data visualization plugin for Elasticsearch that allows
    users to visualize collected logs and metrics and construct custom dashboards.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Elasticsearch、Fluentd和Kibana**组成了EFK堆栈，这是三个协同工作的工具的组合。Fluentd是Kubernetes集群节点的日志收集器，它从Pods收集日志，并将这些日志发送到Elasticsearch搜索引擎，后者将数据摄取并存储在一个集中位置。EFK堆栈的UI是Kibana，它是一个Elasticsearch的可视化插件，允许用户可视化收集的日志和度量指标，并构建自定义仪表板。'
- en: Now that we've seen a variety of choices for monitoring, logging, and alerting,
    we'll go over how to configure them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到各种监控、日志记录和告警的选择，我们将讨论如何配置它们。
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus、Grafana和Alertmanager工具配置监控和告警堆栈
- en: MicroK8s ships pre-integrated add-ons with Prometheus Operator for Kubernetes,
    which handles simplified monitoring definitions for Kubernetes services, as well
    as Prometheus instance deployment and management.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MicroK8s随Prometheus Operator一起提供预集成的插件，处理Kubernetes服务的简化监控定义，以及Prometheus实例的部署和管理。
- en: Note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Operators are Kubernetes-specific applications (Pods) that automate the configuration,
    management, and optimization of other Kubernetes deployments. Operators typically
    take care of the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Operators是Kubernetes特定的应用程序（Pods），它们自动化配置、管理和优化其他Kubernetes部署。Operators通常负责以下任务：
- en: a. Installing your Kubernetes cluster's specifications and offering initial
    setup and sizing for your deployment.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: a. 安装Kubernetes集群的规格，并为您的部署提供初始设置和大小调整。
- en: b. Reloading Deployments and Pods in real time to accommodate any user-requested
    parameter changes (hot config reloading).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: b. 实时重新加载部署和Pod，以适应任何用户请求的参数变化（热配置重载）。
- en: c. Scaling up or down automatically based on performance data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: c. 根据性能数据自动进行上下扩展。
- en: d. Backups, integrity checks, and other maintenance tasks should all be performed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: d. 应执行备份、完整性检查和其他维护任务。
- en: 'Once the Prometheus add-on is enabled, Prometheus Operator takes care of the
    installation and configuration of the following items:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 启用Prometheus插件后，Prometheus Operator负责安装和配置以下项目：
- en: '**Kubernetes-Prometheus stack**:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes-Prometheus堆栈**：'
- en: Prometheus servers
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prometheus服务器
- en: Alertmanager
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alertmanager
- en: Grafana
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Grafana
- en: Host-node exporter
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主机节点出口器
- en: '`kube-state-metrics`'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kube-state-metrics`'
- en: '**ServiceMonitor** entities that define metric endpoint autoconfiguration.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ServiceMonitor**实体定义了度量端点的自动配置。'
- en: '**Operator Custom Resource Definitions (CRDs) and ConfigMaps** that can be
    used to customize and scale the services, thus making our configuration entirely
    portable and declarative.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Operator自定义资源定义（CRDs）和ConfigMaps**，可以用来定制和扩展服务，从而使我们的配置完全可移植和声明式。'
- en: 'The following CRDs are managed by Prometheus Operator:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下CRD由Prometheus Operator管理：
- en: '**PrometheusDeployment**—The Operator ensures that a deployment that matches
    the resource definition is operating at all times.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PrometheusDeployment**—Operator确保与资源定义匹配的部署始终处于运行状态。'
- en: '**ServiceMonitor**—Declaratively specifies how to monitor groups of services.
    Based on the definition, the Operator produces a Prometheus scrape setup automatically.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ServiceMonitor**—声明性地指定如何监控一组服务。根据定义，Operator会自动生成Prometheus抓取设置。'
- en: '**PrometheusRule**—Specifies a Prometheus rule file to be loaded by a Prometheus
    instance with Prometheus alerting rules.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PrometheusRule**—指定一个Prometheus规则文件，供Prometheus实例加载并执行Prometheus告警规则。'
- en: '**AlertManager**—Specifies the Alertmanager deployment that is desired. The
    Operator ensures that a deployment that matches the resource definition is operating
    at all times.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlertManager**—指定所需的Alertmanager部署。Operator确保与资源定义匹配的部署始终处于运行状态。'
- en: 'For more information on Prometheus Operator, please refer to the following
    link:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解有关Prometheus Operator的更多信息，请参阅以下链接：
- en: '[https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)'
- en: 'The following diagram shows the components that we discussed previously:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了我们之前讨论的组件：
- en: '![Figure 8.1 – Prometheus Operator components ](img/Figure_8.01_B18115.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – Prometheus Operator 组件](img/Figure_8.01_B18115.jpg)'
- en: Figure 8.1 – Prometheus Operator components
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – Prometheus Operator 组件
- en: 'To summarize, we are going to use the following tools to collect, aggregate,
    and visualize metrics:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们将使用以下工具来收集、汇总和可视化度量：
- en: Kubernetes metrics pulled from Metrics Endpoints.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从度量端点拉取的 Kubernetes 度量。
- en: Host metrics using Prometheus Node Exporter.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prometheus Node Exporter 监控主机度量。
- en: Alerting using Prometheus Alertmanager.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prometheus Alertmanager 进行告警。
- en: Prometheus gathers data from configured targets (from the Kubernetes endpoints
    discussed in the previous section) at predetermined intervals, analyses rule expressions,
    displays the results, and can also send out alerts when certain criteria are matched.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus 从配置的目标（即前一节讨论的 Kubernetes 端点）以预定的时间间隔收集数据，分析规则表达式，展示结果，还能在匹配特定标准时发送告警。
- en: Visualization using Grafana pre-built dashboards.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Grafana 预构建的仪表板进行可视化。
- en: 'Now that we are clear on the tools, we will dive into the steps of configuring
    a monitoring and alerting stack. The following diagram depicts our Raspberry Pi
    cluster setup:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了所需工具，接下来我们将深入配置监控和告警堆栈的步骤。以下图示展示了我们的 Raspberry Pi 集群设置：
- en: '![Figure 8.2 – Raspberry Pi cluster setup ](img/Figure_8.02_B18115.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – Raspberry Pi 集群设置](img/Figure_8.02_B18115.jpg)'
- en: Figure 8.2 – Raspberry Pi cluster setup
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – Raspberry Pi 集群设置
- en: Now that we know what we want to do, let's look at the requirements.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了要做什么，接下来看看需求。
- en: Requirements for setting up a MicroK8s Raspberry Pi cluster
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 MicroK8s Raspberry Pi 集群的要求
- en: 'Before you begin, here are the prerequisites for building a Raspberry Pi Kubernetes
    cluster:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，以下是构建 Raspberry Pi Kubernetes 集群的前提条件：
- en: A microSD card (4 **gigabytes** (**GB**) minimum; 8 GB recommended)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张 microSD 卡（最低 4 **千兆字节**（**GB**）；推荐 8 GB）
- en: A computer with a microSD card drive
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台带 microSD 卡驱动的计算机
- en: A Raspberry Pi 2, 3, or 4 (one or more)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台 Raspberry Pi 2、3 或 4（一个或多个）
- en: A micro-USB power cable (USB-C for the Pi 4)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条 micro-USB 电源线（Pi 4 使用 USB-C）
- en: A Wi-Fi network or an Ethernet cable with an internet connection
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条带有互联网连接的 Wi-Fi 网络或以太网电缆
- en: (Optional) A monitor with a **High-Definition Multimedia Interface** (**HDMI**)
    interface
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）带有**高清多媒体接口**（**HDMI**）接口的显示器
- en: (Optional) An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi
    4
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）Pi 2 和 3 的 HDMI 电缆，Pi 4 的 micro-HDMI 电缆
- en: (Optional) A **Universal Serial Bus** (**USB**) keyboard
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）一款**通用串行总线**（**USB**）键盘
- en: Now that we've established the requirements, we'll go on to the step-by-step
    instructions on how to complete the process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经明确了需求，接下来是一步一步的操作指导，教你如何完成整个过程。
- en: Step 1 – Creating a MicroK8s Raspberry Pi cluster
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 – 创建一个 MicroK8s Raspberry Pi 集群
- en: 'Please follow the steps that we covered in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,
    Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters,*
    to create a MicroK8s Raspberry Pi cluster. Here''s a quick refresher:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照我们在[*第五章*](B18115_05.xhtml#_idTextAnchor070)《在多节点 Raspberry Pi Kubernetes
    集群上创建和实施更新》一章中讲解的步骤，来创建一个 MicroK8s Raspberry Pi 集群。以下是快速回顾：
- en: '*Step 1*: Installing the **operating system** (**OS**) image to a **Secure
    Digital** (**SD**) card'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1*：将**操作系统**（**OS**）镜像安装到**安全数字**（**SD**）卡上'
- en: '*Step 1a*: Configuring Wi-Fi access settings'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1a*：配置 Wi-Fi 访问设置'
- en: '*Step 1b*: Configuring remote access settings'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1b*：配置远程访问设置'
- en: '*Step 1c*: Configuring control group settings'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1c*：配置控制组设置'
- en: '*Step 1d*: Configuring hostname'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1d*：配置主机名'
- en: '*Step 2*: Installing and configuring MicroK8s'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 2*：安装和配置 MicroK8s'
- en: '*Step 3*: Adding worker node'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 3*：添加工作节点'
- en: 'A fully functional multi-node Kubernetes cluster should look like the one shown
    in the following screenshot. To summarize, we have installed MicroK8s on the Raspberry
    Pi boards and joined multiple Deployments to form the cluster. We have also added
    nodes to the cluster:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 完全可用的多节点 Kubernetes 集群应该像以下截图所示。总结一下，我们已经在 Raspberry Pi 板上安装了 MicroK8s，并将多个部署加入到集群中。我们还添加了节点到集群中：
- en: '![Figure 8.3 – Fully functional MicroK8s Kubernetes cluster ](img/Figure_8.03_B18115.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 完全可用的 MicroK8s Kubernetes 集群](img/Figure_8.03_B18115.jpg)'
- en: Figure 8.3 – Fully functional MicroK8s Kubernetes cluster
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 完全可用的 MicroK8s Kubernetes 集群
- en: We can now go to the next step of deploying monitoring tools, as we have a fully
    functional cluster.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以进入下一步，部署监控工具，因为我们已经有了一个完全可用的集群。
- en: By default, none of the MicroK8s add-ons are turned on. As a result, Grafana
    and Prometheus must be activated post-installation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，MicroK8s 的所有附加组件都是关闭的。因此，必须在安装后激活 Grafana 和 Prometheus。
- en: Step 2 – Configuring Prometheus, Grafana, and Alertmanager
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2 – 配置 Prometheus、Grafana 和 Alertmanager
- en: 'In this section, we''ll enable the Prometheus add-on and access the Prometheus
    and Grafana dashboards so that we can monitor the Kubernetes cluster and can view
    alerts if something goes wrong. Use the following command to enable the Dashboard
    and the Prometheus add-on:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将启用 Prometheus 附加组件并访问 Prometheus 和 Grafana 仪表盘，以便我们可以监控 Kubernetes 集群，并在出现问题时查看警报。使用以下命令启用仪表盘和
    Prometheus 附加组件：
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following command execution output indicates the Dashboard and the Prometheus
    add-on have been enabled successfully:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出表明仪表盘和 Prometheus 附加组件已成功启用：
- en: '![Figure 8.4 – Enabling Dashboard and the Prometheus add-on ](img/Figure_8.04_B18115.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 启用仪表盘和 Prometheus 附加组件](img/Figure_8.04_B18115.jpg)'
- en: Figure 8.4 – Enabling Dashboard and the Prometheus add-on
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 启用仪表盘和 Prometheus 附加组件
- en: 'It will take some time to finish activating the add-on, but the following command
    execution output shows that Prometheus has been successfully enabled:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 启用附加组件需要一些时间，但以下命令执行输出表明 Prometheus 已成功启用：
- en: '![Figure 8.5 – Add-ons activated ](img/Figure_8.05_B18115.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 已激活的附加组件](img/Figure_8.05_B18115.jpg)'
- en: Figure 8.5 – Add-ons activated
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 已激活的附加组件
- en: Grafana cannot be enabled with a command. When the Kubernetes Dashboard is enabled,
    it starts automatically.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 无法通过命令启用。当启用 Kubernetes 仪表盘时，它会自动启动。
- en: 'To access the Kubernetes Dashboard, we need to create a user and admin role
    binding. In the next steps, we will create a deployment for it:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 Kubernetes 仪表盘，我们需要创建一个用户和管理员角色绑定。在接下来的步骤中，我们将为此创建一个部署：
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a `dashboard-adminuser.yaml` file with the preceding content and use
    the following command to create a user and admin role binding:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 `dashboard-adminuser.yaml` 的文件，包含前述内容，并使用以下命令创建用户和管理员角色绑定：
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following command execution output confirms that there is no error in the
    deployment:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认部署没有错误：
- en: '![Figure 8.6 – Creating a user and admin role binding ](img/Figure_8.06_B18115.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 创建用户和管理员角色绑定](img/Figure_8.06_B18115.jpg)'
- en: Figure 8.6 – Creating a user and admin role binding
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 创建用户和管理员角色绑定
- en: 'To access the dashboard, we need an access token, which can be obtained by
    invoking the `kubectl` command, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问仪表盘，我们需要一个访问令牌，可以通过调用 `kubectl` 命令获取，方法如下：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Copy the token from the command's output and use it in the following step.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令的输出中复制令牌，并在下一步中使用它。
- en: 'It will be necessary to build a secure channel to the cluster with the following
    command in order to access the Kubernetes Dashboard:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 需要使用以下命令构建与集群的安全通道，以便访问 Kubernetes 仪表盘：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following command execution output confirms that a secure channel has been
    created and we can access the dashboard in the next step:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认已创建安全通道，我们可以在下一步中访问仪表盘：
- en: '![Figure 8.7 – Creating a secure channel for the dashboard ](img/Figure_8.07_B18115.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 为仪表盘创建安全通道](img/Figure_8.07_B18115.jpg)'
- en: Figure 8.7 – Creating a secure channel for the dashboard
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 为仪表盘创建安全通道
- en: 'After that, you''ll be able to access the dashboard at the following address:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您将能够通过以下地址访问仪表盘：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'By copying and pasting the token generated in the previous step, you will have
    access to the cluster''s web-based **command-line interface** (**CLI**), as illustrated
    in the following screenshot:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过复制和粘贴上一步生成的令牌，您将能够访问集群的基于 Web 的 **命令行界面** (**CLI**)，如下图所示：
- en: '![Figure 8.8 – Kubernetes Dashboard ](img/Figure_8.08_B18115.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – Kubernetes 仪表盘](img/Figure_8.08_B18115.jpg)'
- en: Figure 8.8 – Kubernetes Dashboard
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – Kubernetes 仪表盘
- en: 'As discussed earlier, Dashboard is a Kubernetes UI that can be accessed through
    the web. It can be used to deploy containerized applications to a Kubernetes cluster,
    troubleshoot them, and control the cluster''s resources. The dashboard can be
    used for a variety of purposes, including the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，仪表盘是一个可以通过网页访问的 Kubernetes UI。它可以用于将容器化应用部署到 Kubernetes 集群、进行故障排除并控制集群的资源。仪表盘可以用于多种用途，包括：
- en: All nodes and persistent storage volumes are listed in **Admin overview**, along
    with aggregated metrics for each node.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有节点和持久存储卷都列在 **管理员概览** 中，并显示每个节点的汇总指标。
- en: '**Workloads view** displays a list of all running applications by namespace,
    as well as current Pod memory utilization and the number of Pods in a Deployment
    that are currently ready.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作负载视图**显示按命名空间分类的所有运行应用程序的列表，以及当前 Pod 的内存利用率和 Deployment 中当前就绪的 Pod 数量。'
- en: '**Discover view** displays a list of services that have been made public and
    have enabled cluster discovery.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发现视图**显示已公开并启用集群发现的服务列表。'
- en: Drilling down logs from containers belonging to a single Pod is possible using
    the **Logs viewer** functionality.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用**日志查看器**功能，可以深入查看属于单个 Pod 的容器日志。
- en: For each clustered application and all Kubernetes resources running in the cluster,
    **Storage view** identifies persistent volume claims.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于集群中的每个应用程序和所有正在运行的 Kubernetes 资源，**存储视图**会标识持久卷声明。
- en: We will go to the next step of accessing Prometheus, Grafana, and Alertmanager
    now that we've enabled all of the required add-ons.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 启用了所有必需的附加组件后，我们将进入下一步，访问 Prometheus、Grafana 和 Alertmanager。
- en: Step 3 – Accessing Prometheus, Grafana, and Alertmanager
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步 – 访问 Prometheus、Grafana 和 Alertmanager
- en: We can validate whether Grafana, Prometheus, and Alertmanager are running on
    the cluster before moving on to other steps.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续执行其他步骤之前，我们可以验证 Grafana、Prometheus 和 Alertmanager 是否在集群上运行。
- en: 'Navigate to **Monitoring** under **Namespaces** on the Kubernetes Dashboard,
    and then click **Services**. A list of monitoring services running on the cluster,
    as well as cluster IP addresses, internal endpoints, and ports, will be displayed,
    as illustrated in the following screenshot:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 仪表盘中，导航到 **命名空间** 下的 **监控**，然后点击 **服务**。将显示运行在集群上的监控服务列表、集群 IP
    地址、内部端点和端口，如下图所示：
- en: '![Figure 8.9 – Validating Grafana, Prometheus, and Alertmanager are running
    on the cluster ](img/Figure_8.09_B18115.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 验证 Grafana、Prometheus 和 Alertmanager 是否在集群上运行](img/Figure_8.09_B18115.jpg)'
- en: Figure 8.9 – Validating Grafana, Prometheus, and Alertmanager are running on
    the cluster
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 验证 Grafana、Prometheus 和 Alertmanager 是否在集群上运行
- en: 'From Kubernetes Dashboard, shown in *Figure 8.9*, we can ensure the following
    components are operational:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Kubernetes 仪表盘中（如*图 8.9*所示），我们可以确保以下组件处于正常运行状态：
- en: '`prometheus-operator` Pod—The core of the stack, in charge of managing other
    Deployments such as Prometheus servers or Alertmanager servers'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus-operator` Pod——堆栈的核心，负责管理其他部署，例如 Prometheus 服务器或 Alertmanager 服务器'
- en: '`node-exporter` pod—Per physical host (one in this example)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node-exporter` Pod——每个物理主机（本示例中为一个）'
- en: '`kube-state-metrics` exporter'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-state-metrics` 导出器'
- en: '`prometheus-k8s` (replicas: 1)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prometheus-k8s`（副本数：1）'
- en: '`alertmanager-main` (replicas: 1)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alertmanager-main`（副本数：1）'
- en: '`grafana` (replicas: 1)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grafana`（副本数：1）'
- en: The Grafana and Prometheus UIs can then be accessible simply by putting the
    service IP and ports into the browser in the format `<IP address>:<port>`. The
    login and password for Grafana will be `admin/admin`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需将服务 IP 和端口输入浏览器，格式为 `<IP 地址>:<端口>`，即可访问 Grafana 和 Prometheus 的 UI。Grafana
    的登录用户名和密码为 `admin/admin`。
- en: 'By default, Grafana uses port `3000`; so, navigate to `http://localhost:3000`
    in your web browser, and you''ll be able to visit the Grafana interface, which
    is already populated with some interesting dashboards, as we can see here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Grafana 使用端口 `3000`，因此，请在网页浏览器中导航到 `http://localhost:3000`，你将能够访问 Grafana
    界面，其中已经填充了一些有趣的仪表板，如下所示：
- en: '![Figure 8.10 – Grafana pre-built dashboards ](img/Figure_8.10_B18115.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – Grafana 预构建仪表板](img/Figure_8.10_B18115.jpg)'
- en: Figure 8.10 – Grafana pre-built dashboards
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – Grafana 预构建仪表板
- en: 'Grafana comes with Prometheus preinstalled as a data source, as shown in the
    following screenshot:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 自带预安装的 Prometheus 作为数据源，如下图所示：
- en: '![Figure 8.11 – Grafana/Prometheus data source ](img/Figure_8.11_B18115.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – Grafana/Prometheus 数据源](img/Figure_8.11_B18115.jpg)'
- en: Figure 8.11 – Grafana/Prometheus data source
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – Grafana/Prometheus 数据源
- en: 'In a similar way, the Prometheus UI can be accessed. There will be no need
    for a username or password. By default, Prometheus uses port `9090` and exposes
    its internal metrics and performance. The Node Exporter Prometheus process runs
    on port `9100`. This exposes the details about the node, including storage space,
    `http://<IP address:9090/metrics` path. You can see an overview of the Prometheus
    UI here:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Prometheus UI 也可以访问。无需用户名或密码。默认情况下，Prometheus 使用端口 `9090`，并公开其内部指标和性能。Node
    Exporter Prometheus 进程运行在端口 `9100` 上。它公开有关节点的详细信息，包括存储空间，路径为`http://<IP 地址:9090/metrics`。你可以在此查看
    Prometheus UI 概述：
- en: '![Figure 8.12 – Prometheus UI ](img/Figure_8.12_B18115.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图8.12 – Prometheus UI](img/Figure_8.12_B18115.jpg)'
- en: Figure 8.12 – Prometheus UI
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – Prometheus UI
- en: Prometheus will scrape and store data based on the predefined configuration.
    Go to the dashboard to see whether Prometheus has information about the time series
    that this endpoint exposes on the node.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus将根据预定义的配置抓取和存储数据。请访问仪表盘查看Prometheus是否获取了此端点在节点上暴露的时间序列信息。
- en: 'To see a list of metrics this server is collecting, use the dropdown next to
    the `node_` that have been collected by Node Exporter can be found in the list.
    The `cpu metric` node, for example, displays the node''s CPU utilization, as illustrated
    in the following screenshot:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此服务器收集的度量指标列表，请使用下拉菜单，`node_`旁边的指标可在列表中找到。以`cpu metric`节点为例，它显示了节点的CPU利用率，如下截图所示：
- en: '![Figure 8.13 – Prometheus metrics visualization ](img/Figure_8.13_B18115.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – Prometheus度量指标可视化](img/Figure_8.13_B18115.jpg)'
- en: Figure 8.13 – Prometheus metrics visualization
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – Prometheus度量指标可视化
- en: 'ServiceMonitor automatically detects and registers each target in your Prometheus
    configuration, as illustrated here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ServiceMonitor会自动检测并注册Prometheus配置中的每个目标，如此处所示：
- en: '![Figure 8.14 – Prometheus scrape targets ](img/Figure_8.14_B18115.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图8.14 – Prometheus抓取目标](img/Figure_8.14_B18115.jpg)'
- en: Figure 8.14 – Prometheus scrape targets
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 – Prometheus抓取目标
- en: 'From the Prometheus server interface, the **Alerts** tab displays alerts that
    are created, as illustrated in the following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在Prometheus服务器界面中，**告警**标签页显示已创建的告警，如下截图所示：
- en: '![Figure 8.15 – Prometheus Alertmanager ](img/Figure_8.15_B18115.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图8.15 – Prometheus Alertmanager](img/Figure_8.15_B18115.jpg)'
- en: Figure 8.15 – Prometheus Alertmanager
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 – Prometheus Alertmanager
- en: 'Please refer to the Prometheus community GitHub repository for predefined alert
    rules, at the following link:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考Prometheus社区的GitHub仓库，查看预定义的告警规则，链接如下：
- en: '[https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14)'
- en: To summarize, we looked at how to quickly deploy a Kubernetes monitoring and
    alerting stack using the Prometheus add-on, which is also easy to expand, alter,
    or migrate to a new set of servers based on the needs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们了解了如何通过Prometheus插件快速部署Kubernetes监控和告警堆栈，同时，这个堆栈也很容易根据需求进行扩展、修改或迁移到新的服务器集群。
- en: 'The following things should be noted for production deployments:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境部署时需要注意以下事项：
- en: '**Long-term storage**—The Prometheus database stores metrics for the previous
    15 days by default. Prometheus doesn''t offer long-term storage of metrics. There
    is no option for backup, data redundancy, trend analysis, data mining, and so
    on.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长期存储**——Prometheus数据库默认存储过去15天的度量指标。Prometheus不提供长期存储功能。没有备份、数据冗余、趋势分析、数据挖掘等选项。'
- en: '**Authorization and authentication**—There is no server-side authentication,
    authorization, or encryption provided by Prometheus or its components.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**授权和认证**——Prometheus及其组件不提供服务器端的认证、授权或加密功能。'
- en: There is no support for vertical/horizontal scalability.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持垂直/水平可扩展性。
- en: We've looked at how to use the Prometheus add-on to enable Kubernetes monitoring
    and alerting, and now, we'll look at how to use the EFK toolset to configure a
    logging, monitoring, and alerting stack.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何使用Prometheus插件启用Kubernetes监控和告警，接下来，我们将探讨如何使用EFK工具集配置日志记录、监控和告警堆栈。
- en: Configuring a logging, monitoring, and alerting stack using the EFK toolset
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EFK工具集配置日志记录、监控和告警堆栈
- en: 'In cases where we need to analyze massive volumes of log data collected by
    Pods running many services and applications on a Kubernetes cluster, a centralized,
    cluster-level logging stack could be useful. EFK is the most popular centralized
    logging solution. Elasticsearch is a real-time search engine that supports full-text
    and structured searches, as well as analytics, and is distributed and scalable.
    It''s most commonly used for indexing and searching large amounts of log data.
    Elasticsearch is widely used in conjunction with Kibana, a powerful data visualization
    frontend and dashboard for Elasticsearch. Kibana is a web-based tool that allows
    you to quickly query and get insight into Kubernetes applications by viewing Elasticsearch
    log data and creating dashboards and queries. To gather, transform, and transfer
    log data to the Elasticsearch backend, we''ll use Fluentd, a popular open source
    data collector, to tail container log files, filter and change data, and feed
    it to an Elasticsearch cluster for indexing and storage on our Kubernetes nodes.
    The following diagram depicts what we want to achieve using the EFK toolset:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要分析由多个服务和应用程序在 Kubernetes 集群中运行的 Pods 收集的大量日志数据时，集中式集群级别的日志堆栈可能非常有用。EFK 是最流行的集中式日志解决方案。Elasticsearch
    是一个实时搜索引擎，支持全文和结构化搜索，以及分析功能，且是分布式和可扩展的。它最常用于索引和搜索大量的日志数据。Elasticsearch 通常与 Kibana
    一起使用，后者是一个强大的数据可视化前端和 Elasticsearch 的仪表盘。Kibana 是一个基于 Web 的工具，可以通过查看 Elasticsearch
    日志数据、创建仪表盘和查询来快速查询并深入了解 Kubernetes 应用程序。为了收集、转换并将日志数据传输到 Elasticsearch 后端，我们将使用
    Fluentd，这是一个流行的开源数据收集器，它可以跟踪容器日志文件、过滤和更改数据，并将其传输到 Elasticsearch 集群进行索引和存储。下图展示了我们希望通过
    EFK 工具集实现的目标：
- en: '![Figure 8.16 – Centralized logging solution: EFK toolset ](img/Figure_8.16_B18115.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16 – 集中式日志解决方案：EFK 工具集](img/Figure_8.16_B18115.jpg)'
- en: 'Figure 8.16 – Centralized logging solution: EFK toolset'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 集中式日志解决方案：EFK 工具集
- en: Since EFK isn't available for `arm64` architecture, I'll be using an Ubuntu
    **virtual machine** (**VM**) for this section. The instructions for setting up
    a MicroK8s cluster are the same as in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,
    Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 EFK 不支持 `arm64` 架构，我将在本节使用 Ubuntu **虚拟机**（**VM**）。设置 MicroK8s 集群的步骤与 [*第
    5 章*](B18115_05.xhtml#_idTextAnchor070)*，创建和实施多节点树莓派 Kubernetes 集群更新* 中的指示相同。
- en: Now that we are clear on what we want to achieve, we will dive into the steps
    in detail.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清楚了要实现的目标，接下来将详细介绍步骤。
- en: Step 1 – Enabling the Fluentd add-on
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 – 启用 Fluentd 插件
- en: 'We''ll enable the Fluentd add-on in this section, which allows the EFK toolset
    to gather log data, pass it to Elasticsearch for indexing, and then view aggregated
    logs using the Kibana dashboard. Use the following command to enable the Fluentd
    add-on:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节启用 Fluentd 插件，它允许 EFK 工具集收集日志数据，传递给 Elasticsearch 进行索引，然后通过 Kibana 仪表盘查看聚合日志。使用以下命令启用
    Fluentd 插件：
- en: '[PRE6]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When you enable this add-on, Elasticsearch, Fluentd, and Kibana (the EFK stack)
    will be added to MicroK8s.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 启用此插件后，Elasticsearch、Fluentd 和 Kibana（EFK 堆栈）将会添加到 MicroK8s 中。
- en: 'The following command execution output confirms that the EFK add-on has been
    enabled:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认已启用 EFK 插件：
- en: '![Figure 8.17 – Enabling Fluentd add-on ](img/Figure_8.17_B18115.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.17 – 启用 Fluentd 插件](img/Figure_8.17_B18115.jpg)'
- en: Figure 8.17 – Enabling Fluentd add-on
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 – 启用 Fluentd 插件
- en: Before moving to the next step, let's verify the add-on has been activated.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一步之前，让我们确认插件是否已激活。
- en: 'To do this, use the `microk8s status` command. The following command execution
    output indicates that the Fluentd add-on has been enabled:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，请使用 `microk8s status` 命令。以下命令执行输出表示 Fluentd 插件已启用：
- en: '![Figure 8.18 – Validating whether the add-on is activated ](img/Figure_8.18_B18115.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.18 – 验证插件是否已激活](img/Figure_8.18_B18115.jpg)'
- en: Figure 8.18 – Validating whether the add-on is activated
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18 – 验证插件是否已激活
- en: 'All of the services for EFK are active, as shown in the output of the command
    shown next:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 EFK 服务均已激活，如下列命令输出所示：
- en: '![Figure 8.19 – Verifying EFK Pods are running ](img/Figure_8.19_B18115.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.19 – 验证 EFK Pods 是否正在运行](img/Figure_8.19_B18115.jpg)'
- en: Figure 8.19 – Verifying EFK Pods are running
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.19 – 验证 EFK Pods 是否正在运行
- en: 'We now have all the services of EFK up and running. To access the Kibana dashboard,
    we will need to build a secure channel (as we did for Kubernetes Dashboard) to
    the cluster with the command shown next:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启动了 EFK 的所有服务。要访问 Kibana 仪表板，我们需要构建一个安全通道（就像我们为 Kubernetes 仪表板做的那样）到集群，使用下列命令：
- en: '[PRE7]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following command execution output confirms that port forwarding is successful:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认端口转发成功：
- en: '![Figure 8.20 – Creating a secure channel for the Kibana dashboard ](img/Figure_8.20_B18115.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.20 – 为 Kibana 仪表板创建安全通道](img/Figure_8.20_B18115.jpg)'
- en: Figure 8.20 – Creating a secure channel for the Kibana dashboard
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.20 – 为 Kibana 仪表板创建安全通道
- en: 'The Kibana dashboard should be now available at the following address:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 仪表板现在应该可以通过以下地址访问：
- en: '[PRE8]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To summarize, we now have a completely functional EFK stack that can be configured.
    The next step is to start defining an index pattern in the Kibana dashboard.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们现在已经拥有一个完全功能性的 EFK 堆栈，可以进行配置。下一步是开始在 Kibana 仪表板中定义索引模式。
- en: Step 2 – Defining an index pattern
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步 – 定义索引模式
- en: We are going to analyze whether the EFK container can start up logs itself.
    To do that, we'll need to establish an index pattern. A collection of documents
    with similar characteristics is referred to as an index. An index is given a name,
    which is used to refer to it while conducting indexing, searching, updating, and
    deleting operations on the documents it contains.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分析 EFK 容器是否能自行启动日志。为此，我们需要建立一个索引模式。具有相似特征的文档集合称为一个索引。索引会被赋予一个名称，在执行索引、搜索、更新和删除其包含的文档时使用该名称来引用它。
- en: 'Launch the Kibana dashboard, and you should see Kibana welcome page, as shown
    in the following screenshot:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Kibana 仪表板，你应该看到 Kibana 欢迎页面，如下图所示：
- en: '![Figure 8.21 – Kibana welcome page ](img/Figure_8.21_B18115.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.21 – Kibana 欢迎页面](img/Figure_8.21_B18115.jpg)'
- en: Figure 8.21 – Kibana welcome page
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21 – Kibana 欢迎页面
- en: 'Click `logstash-*`), as shown in the following screenshot:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 单击`logstash-*`，如下面的截图所示：
- en: '![Figure 8.22 – Creating an index pattern ](img/Figure_8.22_B18115.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.22 – 创建索引模式](img/Figure_8.22_B18115.jpg)'
- en: Figure 8.22 – Creating an index pattern
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.22 – 创建索引模式
- en: 'Kibana will then request a field with a time/timestamp that it can use to visualize
    time-series data. This is the `@timestamp` field in our case, as shown in the
    following screenshot:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 将请求一个带时间戳的字段，以便它可以用来可视化时间序列数据。对我们来说，这是`@timestamp`字段，如下图所示：
- en: '![Figure 8.23 – Creating an index pattern with a timestamp field ](img/Figure_8.23_B18115.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.23 – 创建带时间戳字段的索引模式](img/Figure_8.23_B18115.jpg)'
- en: Figure 8.23 – Creating an index pattern with a timestamp field
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.23 – 创建带时间戳字段的索引模式
- en: 'Click **Create index pattern**, and it should just take a few minutes now that
    we''ve built the index pattern. You can see the output here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 单击**创建索引模式**，现在只需几分钟，既然我们已经构建了索引模式。你可以在这里看到输出：
- en: '![Figure 8.24 – Index pattern created ](img/Figure_8.24_B18115.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.24 – 索引模式创建](img/Figure_8.24_B18115.jpg)'
- en: Figure 8.24 – Index pattern created
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.24 – 索引模式创建
- en: 'Select the **Discover** option from the left-hand drop-down menu. You should
    see container log events displayed, as shown in the following screenshot:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 从左侧下拉菜单中选择**发现**选项，你应该看到容器日志事件，如下图所示：
- en: '![Figure 8.25 – Discovering data using the index pattern ](img/Figure_8.25_B18115.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.25 – 使用索引模式发现数据](img/Figure_8.25_B18115.jpg)'
- en: Figure 8.25 – Discovering data using the index pattern
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.25 – 使用索引模式发现数据
- en: The next step is to filter and examine the container startup log events now
    that we've created an index pattern and organized the data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是过滤并查看容器启动日志事件，现在我们已经创建了索引模式并整理了数据。
- en: Step 3 – Filtering and viewing the data
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步 – 过滤并查看数据
- en: 'There will be a listing of all log events with fields available for filtering
    on the left-hand side, as illustrated in the following screenshot. You may either
    create a new filter or utilize the `kubernetes.podname` parameter in **Kibana
    Query Language** (**KQL**) to filter events:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧将列出所有日志事件，并提供可用于过滤的字段，如下图所示。你可以创建新的过滤器，或者使用**Kibana 查询语言**（**KQL**）中的`kubernetes.podname`参数来过滤事件：
- en: '![Figure 8.26 – Filtering log events for a particular Pod ](img/Figure_8.26_B18115.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.26 – 为特定 Pod 过滤日志事件](img/Figure_8.26_B18115.jpg)'
- en: Figure 8.26 – Filtering log events for a particular Pod
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.26 – 为特定 Pod 过滤日志事件
- en: The log list is now filtered to show only log events from that particular Pod.
    You can explore any event or filter to see more information.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 日志列表现在已过滤，只显示来自特定 Pod 的日志事件。你可以查看任何事件或过滤以查看更多信息。
- en: 'When the Fluent Bit log processor is enabled, it will read, parse, and filter
    the logs of every Pod on the Kubernetes cluster, enriching each entry with the
    following data:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 Fluent Bit 日志处理器后，它将读取、解析并过滤 Kubernetes 集群中每个 Pod 的日志，为每条日志条目添加以下数据：
- en: Pod name
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 名称
- en: Pod **identifier** (**ID**)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod **标识符** (**ID**)
- en: Container name
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器名称
- en: Container ID
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器 ID
- en: Labels
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签
- en: Annotations
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注释
- en: Once all the events are indexed, the alerting configuration of Kibana could
    be used to create rules that detect failure scenarios and then act when those
    criteria are fulfilled.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有事件被索引，Kibana 的告警配置可以用来创建规则，检测失败场景，并在满足这些条件时执行操作。
- en: 'More details on alerting can be found here:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有关告警的更多细节，请参见这里：
- en: '[https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.xhtml](https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.xhtml)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.xhtml](https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.xhtml)'
- en: '`Fluentd` has a lighter version, Fluent Bit, which was created by the same
    team for situations with more limited resources. Functionality-wise, `Fluentd`
    is a log aggregator, while Fluent Bit is just a forwarder. `Fluentd` offers a
    more robust ecosystem, whereas Fluent Bit is more prevalent in IoT devices.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`Fluentd` 有一个更轻量的版本，Fluent Bit，这是同一团队为资源有限的情况而创建的。从功能上讲，`Fluentd` 是一个日志聚合器，而
    Fluent Bit 只是一个转发器。`Fluentd` 提供了一个更强大的生态系统，而 Fluent Bit 更常见于物联网设备中。'
- en: 'More details on Fluent Bit can be found here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Fluent Bit 的更多细节，请参见这里：
- en: '[https://fluentbit.io/](https://fluentbit.io/)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://fluentbit.io/](https://fluentbit.io/)'
- en: Congratulations! Using the EFK stack, we have learned how to aggregate all Kubernetes
    container logs and analyze them centrally.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！通过使用 EFK 栈，我们已经学会了如何聚合所有 Kubernetes 容器日志并集中分析它们。
- en: To recap, we looked at some of the most popular monitoring, logging, and alerting
    stack options. The next step is to determine which critical metrics should be
    monitored in order to manage your infrastructure and applications effectively.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们查看了一些最流行的监控、日志记录和告警栈选项。下一步是确定哪些关键指标需要被监控，以便有效管理你的基础设施和应用程序。
- en: Key metrics that need to be monitored
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要监控的关键指标
- en: The rapid adoption of containers in enterprise organizations has provided numerous
    benefits to developers. However, the flexibility and scalability that Kubernetes
    provides in deploying containerized applications have also introduced new complications.
    Keeping track of the health of applications abstracted by containers and then
    abstracted again by Kubernetes can be difficult without the right tools because
    there is no longer a 1-to-1 correlation between an application and the server
    it runs on.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 容器在企业组织中的快速普及为开发人员带来了许多好处。然而，Kubernetes 在部署容器化应用程序时提供的灵活性和可扩展性，也带来了新的复杂性。跟踪由容器抽象出来的应用程序的健康状况，然后再由
    Kubernetes 抽象出来，可能没有合适的工具就会变得非常困难，因为应用程序和其运行的服务器之间不再是一对一的关系。
- en: Containerized applications can be spread across multiple environments, and Kubernetes
    is a complicated environment. Monitoring tools should have the capability to collect
    metrics from across a distributed environment and deal with the transient nature
    of containerized resources. Monitoring tools rely on services as their endpoint
    because Pods and their containers are in constant motion and dynamically scheduled.
    Services broadcast an IP address that can be accessed from outside Pods, allowing
    services to communicate in real time as Pods and containers are built and removed.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 容器化应用程序可以分布在多个环境中，而 Kubernetes 是一个复杂的环境。监控工具应该具备从分布式环境中收集指标的能力，并处理容器化资源的瞬态特性。监控工具依赖服务作为其端点，因为
    Pods 和它们的容器处于不断移动和动态调度状态。服务广播一个可以从 Pods 外部访问的 IP 地址，允许服务在 Pods 和容器创建和移除时实时通信。
- en: 'In Kubernetes, there are two levels of monitoring, as outlined here:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，有两个级别的监控，如下所述：
- en: '**Cluster monitoring**—Monitors the health of a Kubernetes cluster as a whole.
    Helps in checking whether nodes are up to date and running, how many applications
    are running on each node, and how the cluster as a whole is using resources.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群监控**—监控整个 Kubernetes 集群的健康状态。帮助检查节点是否是最新并且正在运行，每个节点上运行了多少个应用程序，以及集群整体如何使用资源。'
- en: '**Pod monitoring**—Keeps track of issues that affect individual Pods, including
    a Pod''s resource use, application metrics, and metrics linked to replication
    or autoscaling.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod 监控**—跟踪影响单个 Pod 的问题，包括 Pod 的资源使用情况、应用程序度量以及与复制或自动扩展相关的度量。'
- en: As we discussed in the preceding sections, Kubernetes-based architecture already
    provides a framework for analyzing and monitoring your applications. You can get
    a comprehensive view of application health and performance with a suitable monitoring
    solution that integrates with Kubernetes' built-in abstractions, even if the containers
    that execute those applications are continually shifting across hosts or scaling
    up and down.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面章节中讨论的，基于 Kubernetes 的架构已经提供了分析和监控应用程序的框架。即使执行这些应用程序的容器不断在主机之间移动或上下扩展，您仍然可以通过适当的监控解决方案，结合
    Kubernetes 内置的抽象，全面了解应用程序的健康状况和性能。
- en: Next, we will look at some of the key metrics that should be monitored. These
    are listed here.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看一些应该监控的关键度量。以下列出了这些度量。
- en: '**Cluster level**—The following cluster-state metrics can give you a high-level
    picture of your cluster''s current state. They can reveal problems with nodes
    or Pods, alerting you to the risk of a bottleneck or the need to expand up your
    cluster:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群级别**—以下集群状态度量可以为您提供集群当前状态的概览。它们能够揭示节点或 Pods 的问题，提醒您可能存在瓶颈或需要扩展集群的风险：'
- en: '![Table 8.2 – Cluster-state metrics ](img/02.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.2 – 集群状态度量](img/02.jpg)'
- en: Table 8.2 – Cluster-state metrics
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2 – 集群状态度量
- en: '**Node level**—The following measures give you a high-level picture of a node''s
    health and whether or not the scheduler can schedule Pods on it. When you compare
    resource utilization to resource requests and limits, you may get a better idea
    of whether your cluster has enough resources to handle its workloads and accommodate
    new ones. It''s critical to maintain and track resource utilization across your
    cluster''s levels, especially for your nodes and the Pods that run on them:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点级别**—以下措施为您提供节点健康状况的概览，并帮助判断调度程序是否可以在该节点上调度 Pods。当您将资源利用率与资源请求和限制进行比较时，您可以更好地了解集群是否有足够的资源来处理工作负载并容纳新的工作负载。维护并跟踪集群各个级别的资源利用率，尤其是节点及其上运行的
    Pods，非常关键：'
- en: '![Table 8.3 – Node-level metrics ](img/03.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.3 – 节点级别度量](img/03.jpg)'
- en: Table 8.3 – Node-level metrics
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.3 – 节点级别度量
- en: '**Pod level**—Although a Pod may be functioning, if it is not accessible, this
    means it is not ready to accept traffic. This is normal in some situations, such
    as when a Pod is first launched or when a change to the Pod''s specifications
    is made and deployed. However, if you notice a surge in the number of unavailable
    Pods or Pods that are constantly unavailable, it could suggest a setup issue.
    Keep track of the following metrics to gauge the health of Pods:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pod级别**—尽管一个 Pod 可能正在运行，但如果它不可访问，意味着它还不能接收流量。在某些情况下，这是正常现象，例如当 Pod 刚启动时，或者当
    Pod 的规格发生变化并进行部署时。然而，如果您注意到不可用的 Pods 数量激增，或者 Pods 一直不可用，这可能意味着存在配置问题。跟踪以下度量，以评估
    Pods 的健康状况：'
- en: '![Table 8.4 – Pod-level metrics ](img/04.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.4 – Pod级别度量](img/04.jpg)'
- en: Table 8.4 – Pod-level metrics
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.4 – Pod级别度量
- en: '**Container level**—Some of the container metrics that should be tracked to
    assess container health are listed next:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器级别**—以下是一些应跟踪的容器度量，用于评估容器健康状况：'
- en: '![Table 8.5 – Container metrics ](img/05.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.5 – 容器度量](img/05.jpg)'
- en: Table 8.5 – Container metrics
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.5 – 容器度量
- en: '**Storage**—Volumes serve as a crucial abstraction in the Kubernetes storage
    architecture. Containers can request storage resources dynamically via a mechanism
    called volume claims, and volumes can be persistent or non-persistent, as detailed
    here:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**存储**—卷在 Kubernetes 存储架构中作为关键的抽象层存在。容器可以通过称为卷声明的机制动态请求存储资源，且卷可以是持久的或非持久的，具体详情如下：'
- en: '![Table 8.6 – Storage metrics ](img/06.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.6 – 存储度量](img/06.jpg)'
- en: Table 8.6 – Storage metrics
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.6 – 存储度量
- en: '**Control plane**—The worker nodes and Pods in a cluster are managed by the
    control plane. Here are the control plane components that need to be monitored:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**控制平面**—集群中的工作节点和 Pods 由控制平面管理。以下是需要监控的控制平面组件：'
- en: '**etcd**—Stores configuration information that each node in the cluster can
    use'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd**—存储集群中每个节点可以使用的配置信息'
- en: '**API server**—Validates and configures data for API objects such as Pods,
    services, and replication controllers, among other things'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 服务器**——验证并配置如 Pods、服务、复制控制器等 API 对象的数据。'
- en: '**Scheduler**—Manages the use of workloads and the assignment of Pods to available
    nodes'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**——管理工作负载的使用和 Pod 分配到可用节点的任务。'
- en: '**Controller Manager**—A daemon in charge of gathering and sending data to
    the API server.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器管理器**——一个守护进程，负责收集并将数据发送到 API 服务器。'
- en: 'You can see more details on these components here:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里查看更多关于这些组件的详细信息：
- en: '![Table 8.7 – Control-plane metrics ](img/07.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.7 – 控制平面指标](img/07.jpg)'
- en: Table 8.7 – Control-plane metrics
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.7 – 控制平面指标
- en: Kubernetes events
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 事件
- en: 'Kubernetes events are a resource type in Kubernetes that are created automatically
    when other resources'' states change, errors occur, or other messages need to
    be broadcast to the system. Here are various types of events that need to be monitored:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 事件是 Kubernetes 中的一种资源类型，当其他资源的状态变化、发生错误或需要向系统广播其他消息时，事件会自动创建。以下是需要监控的各种事件类型：
- en: '**Failed events**—While containers are created on a regular basis, the operation
    can frequently fail; as a result, Kubernetes does not successfully create that
    container. Failed events are frequently associated with image retrieval errors.
    These failures could be caused by typos, insufficient permissions, or upstream
    build failures. Furthermore, nodes can also fail on their own. When these failures
    occur, applications should fall back to functional, remaining nodes, but some
    kind of alerting system is required to determine why the failure occurred. Because
    a failure is a showstopper—that is, your container will not run until it is resolved—you
    should pay close attention to this event type.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**失败事件**——容器经常被创建，但操作经常会失败；因此，Kubernetes 无法成功创建该容器。失败的事件通常与镜像拉取错误相关。这些失败可能是由于拼写错误、权限不足或上游构建失败造成的。此外，节点本身也可能会失败。当这些失败发生时，应用程序应回退到功能正常的剩余节点，但需要某种警报系统来确定故障原因。因为失败是一个决定性问题——即你的容器在问题解决之前无法运行——你应该特别关注此事件类型。'
- en: '**Evicted events**—Certain Pods can consume a disproportionate amount of computing
    and memory resources in comparison to their respective runtimes. Kubernetes addresses
    this issue by evicting Pods and reallocating disk, memory, or CPU space elsewhere.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**驱逐事件**——某些 Pods 相较于其运行时可能消耗过多的计算和内存资源。Kubernetes 通过驱逐这些 Pods，并将磁盘、内存或 CPU
    空间重新分配到其他地方来解决这个问题。'
- en: '**Storage specific events**—Storage within Pods is commonly used by applications
    and workloads. Volumes provided by respective providers store critical content
    that is required by application runtimes. Upon creation, Pods mount these volumes,
    paving the way for successful operation. These events can alert you when storage
    volumes are behaving strangely. Furthermore, a node may not be in good enough
    health to mount a volume. These errors, on the other hand, can make it appear
    as if a Pod is just getting started. However, discovering these events can assist
    you in resolving the underlying issues caused by faulty volume mounting.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**存储特定事件**——Pod 内的存储通常被应用程序和工作负载使用。各个提供商提供的卷存储着应用运行时所需的重要内容。Pod 在创建时会挂载这些卷，为成功运行铺平道路。这些事件可以在存储卷出现异常行为时发出警报。此外，节点的健康状况可能不足以挂载卷，这些错误可能让
    Pod 看起来像是刚刚启动。但发现这些事件可以帮助你解决由错误的卷挂载导致的潜在问题。'
- en: '`FailedScheduling` event occurring when the Kubernetes scheduler is unable
    to find a suitable node.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`FailedScheduling` 事件发生在 Kubernetes 调度器无法找到合适的节点时。'
- en: '`NodeNotReady` event denotes a node that is not ready for Pod scheduling.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`NodeNotReady` 事件表示某个节点尚未准备好进行 Pod 调度。'
- en: 'For Kubernetes events, metrics and alert criteria that must be monitored are
    listed here:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要监控的 Kubernetes 事件、指标和警报标准：
- en: '![Table 8.8 – Kubernetes events metrics ](img/08.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![表 8.8 – Kubernetes 事件指标](img/08.jpg)'
- en: Table 8.8 – Kubernetes events metrics
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.8 – Kubernetes 事件指标
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: To summarize, we've covered key Kubernetes components, as well as the metrics
    and events that can help you track their health and performance over time. We
    have also covered how to collect all of the metrics using built-in Kubernetes
    APIs and utilities, allowing you to gain comprehensive visibility into your container
    infrastructure and workloads.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经覆盖了关键的 Kubernetes 组件，以及可以帮助你跟踪它们健康状况和性能的指标和事件。我们还讨论了如何使用 Kubernetes
    内置的 API 和工具收集所有指标，从而让你能够全面了解容器基础设施和工作负载的状态。
- en: We looked at Prometheus, Grafana, and Alertmanager as tools for setting up a
    monitoring and alerting stack. We've also looked at how to set up a centralized,
    cluster-level logging stack with the EFK toolset, which can handle massive amounts
    of log data. Finally, we went over the essential indicators that should be watched
    in order to successfully manage your infrastructure and apps.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了 Prometheus、Grafana 和 Alertmanager 作为设置监控和告警堆栈的工具。我们还介绍了如何使用 EFK 工具集设置集中式的集群级别日志记录堆栈，该工具集能够处理大量日志数据。最后，我们回顾了应该监视的关键指标，以便成功管理基础设施和应用程序。
- en: We'll look at how to develop and deploy a machine learning model using the Kubeflow
    MLOps platform in the next chapter. Kubeflow and MicroK8s deliver reliable and
    efficient operations as well as infrastructure optimization.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，我们将探讨如何使用 Kubeflow MLOps 平台开发和部署机器学习模型。Kubeflow 和 MicroK8s 提供可靠高效的操作以及基础设施优化。
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和告警堆栈
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和告警堆栈
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和告警堆栈
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和告警堆栈
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和告警堆栈
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和告警堆栈
- en: Configuring a monitoring and alerting stack using the Prometheus, Grafana, and
    Alertmanager tools
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus、Grafana 和 Alertmanager 工具配置监控和告警堆栈
