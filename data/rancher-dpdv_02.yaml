- en: '*Chapter 1*: Introduction to Rancher and Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*：Rancher 和 Kubernetes 简介'
- en: This chapter will focus on the history of Rancher and Kubernetes. We will cover
    what products and solutions came before Rancher and Kubernetes and how they have
    evolved into what they are today. At the end of this chapter, you should have
    a good understanding of the origins of Rancher and Kubernetes and their core concepts.
    This knowledge is essential for you to understand why Rancher and Kubernetes are
    what they are.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍 Rancher 和 Kubernetes 的历史。我们将讲解 Rancher 和 Kubernetes 之前出现过哪些产品和解决方案，以及它们是如何演变成今天的样子的。在本章结束时，你应该能够清楚了解
    Rancher 和 Kubernetes 的起源及其核心概念。这个知识对你理解 Rancher 和 Kubernetes 为什么会发展成现在的样子至关重要。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: The history of Rancher Labs as a company
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher Labs 作为一家公司历史
- en: Rancher's earlier products
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 早期的产品
- en: What is Rancher's core philosophy?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 的核心理念是什么？
- en: Where did Kubernetes come from?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 是从哪里来的？
- en: What problem is Kubernetes trying to solve?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 试图解决什么问题？
- en: Comparing Kubernetes with Docker Swarm and OpenShift
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较 Kubernetes 与 Docker Swarm 和 OpenShift
- en: The history of Rancher Labs as a company
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher Labs 作为一家公司历史
- en: Rancher Labs was founded in 2014 in Cupertino, California, by Sheng Liang, Shannon
    Williams, Darren Shepherd, and Will Chanas. It was a container management platform
    before Kubernetes was a thing. From the beginning, Rancher was built on the idea
    that everything should be open source and community-driven. With Rancher being
    an open source company, all of the products they have released (including Rancher,
    RancherOS, RKE, K3s, Longhorn, and more) have been 100% open source. Rancher Lab's
    flagship product is Rancher. Primarily, Rancher is a management and orchestration
    platform for containerized workloads both on-premises and in the cloud. Rancher
    can do this because it has always been vendor-neutral; that is, Rancher can deploy
    a workload using physical hardware in your data center to cloud VMs in AWS to
    even a Raspberry Pi in a remote location.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher Labs 于 2014 年在加利福尼亚州库比蒂诺由 Sheng Liang、Shannon Williams、Darren Shepherd
    和 Will Chanas 创立。它在 Kubernetes 之前就是一个容器管理平台。从一开始，Rancher 就基于“所有内容都应该是开源且由社区驱动”的理念构建。作为一家开源公司，Rancher
    发布的所有产品（包括 Rancher、RancherOS、RKE、K3s、Longhorn 等）都是 100% 开源的。Rancher Labs 的旗舰产品是
    Rancher。Rancher 主要是一个容器化工作负载的管理和编排平台，适用于本地和云端。Rancher 能做到这一点是因为它始终保持供应商中立；也就是说，Rancher
    可以使用数据中心的物理硬件、AWS 云中的虚拟机，甚至远程位置的 Raspberry Pi 来部署工作负载。
- en: Rancher's earlier products
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher 早期的产品
- en: When Rancher v1.0 was released in March of 2016, it only supported Docker Swarm
    and Rancher Cattle clusters. Docker Swarm was the early cluster orchestration
    tool that created a number of the core concepts that we still use today; for instance,
    the idea that an application should be defined as a group of containers that can
    be created and destroyed at any time. Another concept is that containers should
    live on a virtual network that is accessible on all nodes in a cluster. You can
    expose your containers via a load balancer which, in the case of Docker Swarm,
    is just a basic TCP load balancer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Rancher v1.0 于 2016 年 3 月发布时，它仅支持 Docker Swarm 和 Rancher Cattle 集群。Docker
    Swarm 是早期的集群编排工具，它创造了许多今天我们仍在使用的核心概念；例如，应用程序应该被定义为一组可以随时创建和销毁的容器。另一个概念是，容器应该生活在一个虚拟网络中，集群中的所有节点都能访问。你可以通过负载均衡器公开你的容器，在
    Docker Swarm 中，这只是一个基本的 TCP 负载均衡器。
- en: While the Rancher server was being created, Rancher Labs was working on their
    own Docker cluster software, called Cattle, which is when Rancher went **General
    Availability (GA**) with the launch of Rancher v1.0\. Cattle was designed to address
    the limitations of Docker Swarm, which spanned several different areas.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Rancher 服务器创建的同时，Rancher Labs 还在开发他们自己的 Docker 集群软件，叫做 Cattle，这也是 Rancher
    在推出 v1.0 版本时实现**正式发布（GA）**的时刻。Cattle 的设计旨在解决 Docker Swarm 的局限性，这些局限性涉及多个领域。
- en: The first was networking. Originally, Docker Swarm's networking overlay was
    built on `node01` with an IP address of `192.168.11.22` wants to connect to another
    container hosted on `node02` with an IP address of `192.168.12.33`. The networking
    swarm uses basic Linux routing to route anything inside the `192.168.12.0/24`
    subnet to `node02` over the IPsec tunnel. This core concept is still in use today
    by the majority of Kubernetes's CNI providers. The main issue is in managing the
    health of these tunnels over time and dealing with compatibility issues between
    the nodes. Cattle addressed this issue by moving IPsec into a container and then
    wrapping a management layer to handle the creation, deletion, and monitoring of
    the tunnels.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是网络。最初，Docker Swarm 的网络覆盖是在 `node01` 上构建的，IP 地址为 `192.168.11.22`，想要连接到托管在
    `node02` 上的另一个容器，其 IP 地址为 `192.168.12.33`。网络 Swarm 使用基本的 Linux 路由，将 `192.168.12.0/24`
    子网内的任何数据包路由到 `node02`，通过 IPsec 隧道传输。这一核心概念至今仍被大多数 Kubernetes 的 CNI 提供者使用。主要问题在于如何随着时间的推移管理这些隧道的健康状况，并处理节点之间的兼容性问题。Cattle
    通过将 IPsec 移动到容器中，并为其包装一个管理层来处理隧道的创建、删除和监控，解决了这个问题。
- en: The second main issue was to do with load balancing. With Docker Swarm, we were
    limited to very basic TCP/layer4 load balancing. We didn't have sessions, SSL,
    or connection management. This is because load balancing was all done by iptable
    rules. Cattle addressed this issue by deploying HAProxy on all nodes in the cluster.
    Following this, Cattle used a custom container, called `rancher-metadata`, to
    dynamically build HAProxy's config every time a container was created or deleted.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个主要问题与负载均衡有关。在 Docker Swarm 中，我们只能使用非常基础的 TCP/层4 负载均衡。我们没有会话、SSL 或连接管理功能。这是因为负载均衡完全通过
    iptable 规则来完成的。Cattle 通过在集群中的所有节点上部署 HAProxy 解决了这个问题。此后，Cattle 使用一个名为 `rancher-metadata`
    的自定义容器，每次创建或删除容器时动态构建 HAProxy 的配置。
- en: The third issue was storage. With Docker Swarm, there weren't any storage options
    outside bind mounting to a host filesystem. This meant that you had to create
    a clustered filesystem or shared network and then manually map them to all of
    your Docker hosts. Cattle addressed this by creating `rancher-nfs`, which is a
    tool that can mount NFS shares inside a container and create a bind mount. As
    Rancher went on, other storage providers were added, such as AWS and VMware.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个问题是存储。在 Docker Swarm 中，除了绑定挂载到主机文件系统外，没有其他存储选项。这意味着你必须创建一个集群文件系统或共享网络，然后手动将它们映射到所有
    Docker 主机。Cattle 通过创建 `rancher-nfs` 来解决这个问题，这是一款可以在容器内挂载 NFS 共享并创建绑定挂载的工具。随着 Rancher
    的发展，其他存储提供商也被加入其中，如 AWS 和 VMware。
- en: Then, as time moved forward at Rancher, the next giant leap was when authentication
    providers were added, because Rancher provides access to the clusters that Rancher
    manages by integrating external authentication providers such as Active Directory,
    LDAP, and GitHub. This is unique to Rancher, as Kubernetes still doesn't integrate
    very well with external authentication providers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，Rancher 迈出了下一步的重要发展，当时增加了身份验证提供商，因为 Rancher 通过集成外部身份验证提供商（如 Active Directory、LDAP
    和 GitHub）提供对 Rancher 管理的集群的访问。这是 Rancher 独有的，因为 Kubernetes 至今仍与外部身份验证提供商集成得不好。
- en: What is Rancher's core philosophy?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher 的核心理念是什么？
- en: 'Rancher is built around several core design principles:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 的构建基于几个核心设计原则：
- en: '**Open source**: All code, components, and services that make up Rancher or
    come with Rancher must be open source. Because of this, Rancher has a large community
    built around it, with users providing feedback, documentation, and contributing
    code.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：构成 Rancher 或与 Rancher 一起提供的所有代码、组件和服务必须是开源的。因此，Rancher 拥有一个庞大的社区，用户提供反馈、文档，并贡献代码。'
- en: '**No lock-ins**: Rancher is designed with no vendor lock-in, including being
    locked inside Rancher. With containerization evolving so quickly, Rancher needs
    to enable users to change technologies with as little impact as possible. A core
    requirement of all products and solutions that Rancher provides is that they can
    be used with or without the Rancher server. An example of this is Longhorn; there
    are zero dependencies between Rancher and Longhorn. This means that at any time,
    a user can uninstall one without impacting the other. This includes the ability
    to uninstall Rancher without losing your clusters. Rancher does this by having
    a process in place for a user to take over the management of a cluster directly
    and kick Rancher out of the picture.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无锁定**：Rancher 设计时考虑了无供应商锁定，包括在 Rancher 内部也不被锁定。随着容器化技术快速演变，Rancher 需要使用户能够尽可能少地影响下改变技术。Rancher
    提供的所有产品和解决方案的核心要求是，它们可以与或无 Rancher 服务器一起使用。Longhorn 就是一个例子；Rancher 和 Longhorn
    之间没有任何依赖关系。这意味着用户可以随时卸载其中一个而不影响另一个。这包括卸载 Rancher 而不会丢失集群的能力。Rancher 通过实施一个过程，让用户直接接管集群的管理并将
    Rancher 排除在外来实现了这一点。'
- en: '`clusters.management.cattle.io` and the same with nodes as an object under
    `nodes.management.cattle.io`, which is scoped to a namespace with the cluster
    ID. Because of this, users and applications can directly query Rancher objects
    without needing to talk to Rancher''s API. The reason for this change was mainly
    to do with scalability. With Cattle and MySQL, all cluster-related tasks had to
    go back to the Rancher server. So, as you scaled up the size of your cluster and
    the number of clusters, you had to scale up the Rancher server, too. This resulted
    in customers hitting issues such as "task storms" where a single node rebooting
    in a cluster causes a flood of requests that are sent to the Rancher server, which,
    in turn, causes other tasks to timeout, which then causes more requests. In the
    end, the only thing you can do is to shut everything down and slowly bring it
    back up.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters.management.cattle.io` 和与 `nodes.management.cattle.io` 下的对象一样，它们仅限于具有集群
    ID 的命名空间。因此，用户和应用程序可以直接查询 Rancher 对象而无需与 Rancher 的 API 交互。这种改变的原因主要与可伸缩性有关。使用
    Cattle 和 MySQL 时，所有与集群相关的任务都必须返回到 Rancher 服务器。因此，随着集群规模和集群数量的增加，必须同时扩展 Rancher
    服务器。这导致客户遇到诸如“任务风暴”的问题，即集群中的单个节点重新启动会导致发送到 Rancher 服务器的请求洪水，进而导致其他任务超时，然后引起更多请求。最终，你只能关闭一切，然后慢慢重新启动。'
- en: '**Everything is stateless**: Because everything is a Kubernetes object, there
    is no need for a database for Rancher. All Rancher pods are stateless, meaning
    they can be destroyed at any time for any reason. Additionally, Rancher can rely
    on Kubernetes controllers to simply spin up new pods without needing Rancher to
    do anything.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一切都是无状态的**：因为一切都是 Kubernetes 对象，Rancher 不需要数据库。所有 Rancher pods 都是无状态的，意味着可以因任何原因随时销毁。此外，Rancher
    可依赖 Kubernetes 控制器简单地启动新的 pods 而无需 Rancher 做任何事情。'
- en: '**Controller model**: All Rancher services are designed around the Kubernetes
    controller model. A control loop is always running, watching the current state,
    and comparing it to the desired state. And if any differences are found, it applies
    the application logic to make the current state match the desired state. Alongside
    this, it uses the same leader election process with Kubernetes core components.
    This ensures there is only one source of truth and ensures certain controllers
    will handle failing over after a failure.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器模型**：所有 Rancher 服务都围绕 Kubernetes 控制器模型设计。控制循环始终在运行，监视当前状态并将其与期望状态进行比较。如果发现任何差异，它会应用应用程序逻辑使当前状态与期望状态匹配。同时，它与
    Kubernetes 核心组件使用相同的领导选举过程。这确保只有一个真实数据来源，并确保某些控制器在故障后能够进行故障切换。'
- en: Where did Kubernetes come from?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 是从哪里来的？
- en: The name **Kubernetes** originates from Greek, meaning **helmsman** or **pilot**.
    Kubernetes is abbreviated to **k8s** due to the number of letters between the
    **K** and **S**. Initially, engineers created Kubernetes at Google from an internal
    project called Borg. Google's Borg system is a cluster manager that was designed
    to run Google's internal applications. These applications are made up of tens
    of thousands of microservices hosted on clusters worldwide, with each cluster
    being made up of tens of thousands of machines. Borg provided three main benefits.
    The first benefit was the abstraction of resource and failure management, so application
    designers could focus on application development. The second benefit was its high
    reliability and availability by design. All parts of Borg were designed, from
    the beginning, to be highly available. This was done by making applications stateless.
    This was done so that any component could be destroyed at any time for any reason
    without impacting availability and, at the same time, could be scaled horizontally
    to hundreds of instances across clusters. The third benefit was an effective workload;
    Borg was designed to have a minimal overhead on the compute resources being managed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes** 这个名字来源于希腊语，意为 **舵手** 或 **驾驶员**。Kubernetes 被简称为 **k8s**，因为在 **K**
    和 **S** 之间有 8 个字母。最初，工程师们在 Google 基于一个内部项目 Borg 创建了 Kubernetes。Google 的 Borg 系统是一个集群管理器，旨在运行
    Google 内部的应用程序。这些应用程序由成千上万的微服务组成，这些微服务托管在全球的集群中，每个集群由成千上万台机器组成。Borg 提供了三个主要好处。第一个好处是资源和故障管理的抽象化，使得应用程序设计者可以专注于应用程序的开发。第二个好处是其设计的高可靠性和可用性。Borg
    的所有组件从一开始就设计为高可用性。这是通过使应用程序无状态来实现的。这样，任何组件都可以在任何时间因任何原因被销毁而不会影响可用性，同时还可以横向扩展到跨集群的数百个实例。第三个好处是有效的工作负载；Borg
    被设计为对计算资源的管理开销最小化。'
- en: 'Kubernetes can be traced directly back to Borg, as many of the developers at
    Google that worked on Kubernetes were formerly developers on the Borg project.
    Because of this, many of its core concepts were incorporated into Kubernetes,
    with the only real difference being that Borg was custom-made for Google, and
    its requirements for Kubernetes need to be more generalized and flexible. However,
    there are four main features that have been derived from Borg:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 可以直接追溯到 Borg，因为许多曾在 Google 从事 Kubernetes 开发的工程师，之前都参与过 Borg 项目的开发。由于这一点，许多
    Borg 的核心概念被融入到了 Kubernetes 中，唯一的实际区别在于 Borg 是为 Google 定制的，而 Kubernetes 需要更加通用和灵活。然而，从
    Borg 衍生出的四个主要特性如下：
- en: '**Pods**: A pod is the smallest unit of scheduling in Kubernetes. This object
    can include one or more containers, with each container in the pods sharing resources
    such as an IP address, volumes, and other local resources. One of the main design
    principles is that a pod should be disposable and shouldn''t change after creation.
    Another primary principle is that all application configurations should be handled
    at the pod level. For example, a database connection string should be defined
    as part of the pod''s definition instead of the application code. This is done
    so that any changes to the configuration of an application won''t require the
    code to be recompiled and redeployed. Additionally, the pod takes the concept
    of paired processes from Borg, with the classic example being a log collector.
    This is because, typically, your container should only have one primary process
    running inside it.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod**：Pod 是 Kubernetes 中最小的调度单元。这个对象可以包含一个或多个容器，Pod 中的每个容器共享一些资源，如 IP 地址、卷和其他本地资源。其主要设计原则之一是，Pod
    应该是可丢弃的，且在创建后不应发生变化。另一个主要原则是所有应用程序配置应该在 Pod 层面进行处理。例如，数据库连接字符串应作为 Pod 定义的一部分，而不是在应用程序代码中定义。这样，任何应用程序配置的更改都不需要重新编译和重新部署代码。此外，Pod
    从 Borg 中引入了配对进程的概念，经典示例是日志收集器。这是因为，通常情况下，容器内部应该只运行一个主进程。'
- en: 'An example of this is a web server: the server creates logs, but how do you
    ship those logs to a log server like Splunk? One option is to add a custom agent
    to your application pod which is easy. But now that you manage more than one process
    inside a container, you''ll have duplicate code in your environment, and most
    importantly, you now have to do error handling for both your main application
    and this additional logging agent. This is where sidecars come into play and allow
    you to bolt containers together inside a pod in a repeatable and consistent manner.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是Web服务器：服务器会生成日志，但如何将这些日志传输到像Splunk这样的日志服务器呢？一种选择是向您的应用程序Pod添加一个自定义代理，这很简单。但现在，您在容器中管理多个进程，这会导致环境中存在重复代码，最重要的是，您现在必须同时为主应用程序和这个附加的日志代理进行错误处理。这时，Sidecar的概念就派上用场，它允许您以可重复且一致的方式将容器组合在一起，形成一个Pod。
- en: '**Services**: One of Borg''s primary roles was the life cycle management of
    applications and pods. Because of this, the pod name and IP address are ephemeral
    and can change at any time for any reason. So, the concept of a service was created
    as an abstraction level wherein you can define a service object that references
    a pod or pods by using labels. Kubernetes will then handle the mapping of the
    service records to its pods. These benefits load balance the traffic for a service
    among the pods that make up that service. Service records allow Kubernetes to
    add and remove pods without disrupting the applications because the service to
    pod mapping can simply be changed without the requesting client being aware.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**：Borg的主要作用之一是应用程序和Pod的生命周期管理。因此，Pod的名称和IP地址是短暂的，可能随时因任何原因发生变化。于是，服务的概念应运而生，作为一种抽象层，您可以通过使用标签来定义一个服务对象，该对象引用一个或多个Pod。Kubernetes将处理服务记录与其Pod的映射。这些好处在于，能够在组成该服务的Pod之间负载均衡流量。服务记录允许Kubernetes添加或移除Pod而不干扰应用程序，因为服务与Pod的映射可以简单地改变，而请求的客户端并不察觉。'
- en: '`"application=web_frontend," "environment=production," "department=marketing"`.
    Note that each of these keys is a different label selector rule that can create
    a service record. This has the side benefit of making the reporting and tracking
    of usage much easier.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"application=web_frontend," "environment=production," "department=marketing"`。请注意，这些键是不同的标签选择规则，可以创建服务记录。这还有一个副作用，就是使得报告和使用跟踪变得更加容易。'
- en: '**Every pod has an IP**: When Borg was created, all of the containers on a
    host would share the host''s IP address and then use different ports for each
    container. This allowed Borg to use a standard IP network. However, this created
    a burden on infrastructure and application teams, as Borg needed to schedule ports
    for containers. This required applications to have a set of predefined ports that
    would be needed for their container.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个Pod都有一个IP**：当Borg创建时，宿主机上的所有容器共享宿主机的IP地址，并通过不同的端口来区分每个容器。这使得Borg能够使用标准的IP网络。然而，这也给基础设施和应用程序团队带来了负担，因为Borg需要为容器调度端口。这要求应用程序为其容器预定义一组端口。'
- en: What problem is Kubernetes trying to solve?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes试图解决什么问题？
- en: 'Kubernetes was designed to solve several problems. The primary areas are as
    follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的设计旨在解决几个问题，主要包括以下几个方面：
- en: '**Availability**: Everyone, from the application owner to the developers, to
    the end users, has come to expect 24x7x365 uptime, with work outages and downtime
    being a four-letter word in IT. With containerization and microservices, this
    bar has only gotten higher. Kubernetes addresses this issue by scheduling containers
    across nodes and using the desired state versus the actual state. The idea is
    that any failures are just a change in the actual state that triggers the controllers
    to schedule pods until the actual state matches the desired state.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：从应用程序所有者到开发人员，再到最终用户，每个人都期望系统实现24x7x365的正常运行时间，停机和故障在IT领域几乎是禁忌。随着容器化和微服务的普及，这一标准变得更高。Kubernetes通过在节点间调度容器，并使用期望状态与实际状态的对比来解决这一问题。其理念是，任何故障都只是实际状态发生变化，触发控制器调度Pods，直到实际状态与期望状态匹配。'
- en: '**CI/CD**: Traditional development was carried out using monolithic developments,
    with a few significant releases per year. This required a ton of developers working
    for months to test their releases and build a ton of manual processes to deploy
    their applications. Kubernetes addresses this issue by being driven by the desired
    state and config file. This means implementing a DevOps workflow that allows developers
    to automate steps and continuously integrate, test, and deploy code. All of this
    will enable teams to fail fast and fix fast.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CI/CD**：传统开发使用单片开发进行，每年发布几次重大版本。这需要大量开发人员几个月来测试其发布，并构建大量手动流程来部署其应用程序。Kubernetes通过由期望状态和配置文件驱动来解决此问题。这意味着实施DevOps工作流程，允许开发人员自动化步骤并持续集成、测试和部署代码。所有这些都将使团队能够快速失败和快速修复。'
- en: '**Efficiency**: Traditional IT was a black hole that companies threw money
    into. One of the reasons behind this was high availability. For one application,
    you would need at least two servers for each component of your production application.
    Also, you would require additional servers for each of your lower environments
    (such as DEV, QAS, Test, and more). Today, companies want to be as efficient with
    their IT spending as possible. Kubernetes addresses this need by making spinning
    up environments very easy. With CI/CD, you can simply create a new namespace,
    deploy your application, run whatever tests you want, and then tear down the namespace
    to reclaim its resources.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：传统IT是公司投入资金的黑洞之一。其背后的原因之一是高可用性。对于一个应用程序，您至少需要为生产应用程序的每个组件的两个服务器。此外，您还需要为每个较低的环境（如DEV、QAS、Test等）的每个服务器额外的服务器。如今，公司希望尽可能高效地利用其IT支出。Kubernetes通过简化启动环境来满足这一需求。通过CI/CD，您可以简单地创建一个新的命名空间，部署您的应用程序，运行您想要的任何测试，然后销毁该命名空间以回收其资源。'
- en: '**Automate scaling**: Traditionally, you would design and build your environment
    around your peak workload. For instance, let''s say your application is mainly
    busy during business hours and is idle during off-peak hours. You are wasting
    money because you pay the same amount for your compute resources at 100% and 1%.
    However, traditionally, it would take days or even weeks to spin up a new server,
    install your application, config it, and, finally, update the load balancer. This
    made it impossible to scale up and down rapidly. So, some companies just decided
    to scale up and stay there. Kubernetes addresses this issue by making it easy
    to scale up or down, as it just involves a simple change to the desired state.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化扩展**：传统上，您会根据最高工作负载来设计和构建环境。例如，假设您的应用主要在工作时间繁忙，在非高峰时段处于空闲状态。您会因为在100%和1%的计算资源上支付相同费用而浪费资金。然而，传统上，要花费几天甚至几周来启动新服务器，安装您的应用程序，配置它，最后更新负载均衡器。这使得无法快速扩展或收缩。因此，一些公司决定只进行扩展并保持状态。Kubernetes通过简单更改期望状态来解决此问题，从而轻松实现扩展或收缩。'
- en: Let's say that an application currently has two web servers, and you want to
    add a pod to handle the load. Just change the number of replicas to three because
    the current state doesn't match the desired state. The controllers kick up and
    start spinning up a new pod. This can be automated using Kubernetes' built-in
    **horizontal pod autoscaler** (**HPA**), which uses several metrics ranging from
    simple metrics such as CPU and memory to custom metrics such as overall application
    response times. Additionally, Kubernetes can use its **vertical pod autoscaler**
    (**VPA**) to automatically tune your CPU and memory limits over time. Following
    this, Kubernetes can use node scaling to dynamically add and remove nodes to your
    clusters as resources are required. This means your application might have 10
    pods with 10 worker nodes during the day, but it might drop to only 1 pod with
    1 worker node after hours. This means you can save the cost of 9 nodes for 16
    hours per day plus the weekends; all of this without your application having to
    do anything.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个应用程序当前有两个Web服务器，并且您希望添加一个Pod来处理负载。只需将副本数更改为三，因为当前状态与期望状态不匹配。控制器会启动并开始启动一个新的Pod。可以使用Kubernetes内置的**水平Pod自动缩放器（HPA）**自动化此过程，该缩放器使用多种度量标准，从简单的CPU和内存到整体应用程序响应时间的自定义度量标准。此外，Kubernetes还可以使用其**垂直Pod自动缩放器（VPA）**随时间自动调整您的CPU和内存限制。随后，Kubernetes可以使用节点扩展来根据需要动态添加和删除节点到您的集群中。这意味着白天您的应用程序可能有10个Pod和10个工作节点，但在下班后，它可能只有1个Pod和1个工作节点。这意味着您可以节省16小时每天加上周末的9个节点的成本，所有这些都无需您的应用程序做任何事情。
- en: Comparing Kubernetes with Docker Swarm and OpenShift
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 Kubernetes 与 Docker Swarm 和 OpenShift
- en: We will compare both of these in the following section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的部分进行比较。
- en: Kubernetes versus Docker Swarm
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 与 Docker Swarm
- en: Kubernetes and Docker Swarm are open source container orchestration platforms
    that have several identical core functions but significant differences.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 和 Docker Swarm 是两个开源容器编排平台，它们具有几个相同的核心功能，但也有显著差异。
- en: Scalability
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Kubernetes is a complex system with several components that all need to work
    together to make the cluster operate, making it more challenging to set up and
    administrate. Kubernetes requires you to manage a database (`etcd`), including
    taking backups and creating SSL certificates for all of the different components.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个复杂的系统，包含多个组件，这些组件需要协同工作才能使集群正常运行，因此它的设置和管理较为复杂。Kubernetes 需要你管理一个数据库（`etcd`），包括备份和为所有不同组件创建
    SSL 证书。
- en: Docker Swarm is far simpler, with everything just being included in Docker.
    All you need to do is create a manager and join nodes to the swarm. However, because
    everything is baked-in, you don't get the higher-level features such as autoscaling,
    node provisioning, and more.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 要简单得多，所有功能都包含在 Docker 中。你所需要做的就是创建一个管理节点并将其他节点加入 Swarm。但由于一切都已集成，你不能获得诸如自动伸缩、节点配置等高级功能。
- en: Networking
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络
- en: Kubernetes uses a flat network model with all pods sharing a large network subnet
    and another network for creating services. Additionally, Kubernetes allows you
    to customize and change network providers. For example, if you don't like a particular
    canal, or can't do network-level encryption, you can switch to another provider,
    such as Weave, which can do network encryption.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用平面网络模型，所有 Pod 共享一个大型网络子网，并且使用另一个网络来创建服务。此外，Kubernetes 允许你自定义和更改网络提供商。例如，如果你不喜欢某个特定的网络，或者无法进行网络级加密，你可以切换到另一个提供商，比如
    Weave，它可以进行网络加密。
- en: Docker Swarm networking is fundamental. By default, Docker Swarm creates IPsec
    tunnels between all nodes in the cluster using IPsec for the encryption. This
    speed can be good because modern CPUs provide hardware acceleration for AES; however,
    you can still take a performance hit depending on your hardware and workload.
    Additionally, with Docker Swarm, you can't switch network providers as you only
    get what is provided.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 网络是基础。默认情况下，Docker Swarm 在集群中的所有节点之间创建 IPsec 隧道，并使用 IPsec 进行加密。由于现代
    CPU 提供对 AES 的硬件加速，因此这种速度可能不错；然而，依赖于你的硬件和工作负载，你仍然可能会遇到性能下降的问题。此外，在 Docker Swarm
    中，你无法切换网络提供商，因为你只能使用默认提供的网络。
- en: Application deployment
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用部署
- en: Kubernetes uses YAML and its API to enable users to define applications and
    their resources. Because of this, there are tools such as Helm that allow application
    owners to define their application in a templatized format, making it very easy
    for applications to be published in a user-friendly format called **Helm charts**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用 YAML 和其 API 使用户能够定义应用程序及其资源。因为如此，像 Helm 这样的工具允许应用程序所有者以模板化格式定义他们的应用程序，这使得应用程序能够以用户友好的格式发布，称为
    **Helm charts**。
- en: Docker Swarm is built on the Docker CLI with a minimal API for management. The
    only package management tool is Docker Compose, which hasn't been widely adopted
    due to its limited customization and the high degree of manual work required to
    deploy it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 基于 Docker CLI，使用最小的 API 进行管理。唯一的包管理工具是 Docker Compose，但由于其定制性有限且部署过程需要大量手动工作，因此并未被广泛采用。
- en: High availability
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高可用性
- en: Kubernetes has been built from the ground up to be highly available and to have
    the ability to handle a range of failures, including pods detecting unhealthy
    pods using advanced features such as running commands inside the pods to verify
    their health. This includes all of the management components such as Kube-scheduler,
    Kube-apiserver, and more. Each of these components is designed to be stateless
    with built-in leader election and failover management.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 从一开始就被构建为高度可用，并具备处理各种故障的能力，包括通过运行命令来检查 Pod 的健康状况，进而检测不健康的 Pod。这包括所有管理组件，如
    Kube-scheduler、Kube-apiserver 等。这些组件设计为无状态，并内置领导选举和故障转移管理。
- en: Docker Swarm is highly available mainly by its ability to clone services between
    nodes, with the Swarm manager nodes being in an active-standby configuration in
    the case of a failure.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 主要通过其在节点之间克隆服务的能力来实现高可用性，在出现故障时，Swarm 管理节点会处于主备配置。
- en: Load balancing
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Kubernetes pods can be exposed using superficial layer 4 (TCP/UDP mode) load
    balancing services. Then, for external access, Kubernetes has two options. The
    first is node-port, which acts as a simple method of port-forwarding from the
    node's IP address to an internal service record. The second is for more complex
    applications, where Kubernetes can use an ingress controller to provide layer
    7 (HTTP/HTTPS mode) load balancing, routing, and SSL management.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Pod 可以通过浅层的 4 层（TCP/UDP 模式）负载均衡服务暴露出来。然后，对于外部访问，Kubernetes 有两种选择。第一种是
    node-port，它作为一种简单的端口转发方法，将节点的 IP 地址转发到内部服务记录。第二种适用于更复杂的应用，Kubernetes 可以使用 ingress
    控制器提供 7 层（HTTP/HTTPS 模式）负载均衡、路由和 SSL 管理。
- en: Docker Swarm load balancing is DNS-based, meaning Swarm uses round-robin DNS
    to distribute incoming requests between containers. Because of this, Docker Swarm
    is limited to layer 4 only, with no option to use any of the higher-level features
    such as SSL and host-based routing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 的负载均衡基于 DNS，这意味着 Swarm 使用轮询 DNS 将传入请求分配到容器之间。因此，Docker Swarm 仅限于
    4 层，并没有办法使用任何更高级的功能，如 SSL 和基于主机的路由。
- en: Management
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理
- en: Kubernetes provides several tools in which to manage the cluster and its applications,
    including `kubectl` for command-line access and even a web UI via the Kubernetes
    dashboard service. It even offers higher-level UIs such as Rancher and Lens. This
    is because Kubernetes is built around a REST API that is highly flexible. This
    means that applications and users can easily integrate their tools into Kubernetes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了多个工具来管理集群及其应用，包括用于命令行访问的 `kubectl`，以及通过 Kubernetes 仪表盘服务提供的 Web
    UI。它甚至还提供了更高层次的 UI，如 Rancher 和 Lens。这是因为 Kubernetes 是围绕一个高度灵活的 REST API 构建的。这意味着应用程序和用户可以轻松地将他们的工具集成到
    Kubernetes 中。
- en: Docker Swarm doesn't offer a built-in dashboard. There are some third-party
    dashboards such as **Swarmpit**, but there hasn't been very much adoption around
    these tools and very little standardization.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 不提供内建的仪表盘。虽然有一些第三方仪表盘，如 **Swarmpit**，但这些工具的采用率很低，且缺乏标准化。
- en: Security
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全性
- en: Kubernetes provides a built-in RBAC model allowing fine-grained control for
    Kubernetes resources. For example, you can grant pod permission to just one secret
    with another pod being given access to all secrets in a namespace. This is because
    Kubernetes authorization is built on SSL certifications and tokens for authentication.
    This allows Kubernetes to simply pass the certificate and token as a file mounted
    inside a pod. This makes it straightforward for applications to gain access to
    the Kubernetes API.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了内建的 RBAC 模型，允许对 Kubernetes 资源进行细粒度的控制。例如，你可以授予一个 Pod 仅访问一个密钥，而另一个
    Pod 则可以访问命名空间中的所有密钥。这是因为 Kubernetes 的授权是基于 SSL 证书和令牌进行身份验证的。这使得 Kubernetes 可以简单地将证书和令牌作为文件挂载到
    Pod 内。这使得应用程序可以轻松访问 Kubernetes API。
- en: The Docker Swarm security model is primarily network-based using TLS (mTLS)
    and is missing many fine-grained controls and integrations, with Docker Swarm
    only having the built-in roles of none, view only, restricted control, scheduler,
    and full control. This is because the access model for Docker Swarm was built
    for cluster administration and not application integration. In addition to this,
    originally, the Docker API only supported basic authentication.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 的安全模型主要基于网络，使用 TLS（mTLS），且缺少许多细粒度的控制和集成，Docker Swarm 只具有内建的角色，如无权限、只读、受限控制、调度器和完全控制。这是因为
    Docker Swarm 的访问模型是为集群管理而设计的，而不是应用集成。此外，最初 Docker API 仅支持基本身份验证。
- en: Kubernetes versus OpenShift
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 与 OpenShift
- en: Both Kubernetes and OpenShift share a lot of features and architectures. Both
    follow the same core design practices, but they differ in terms of how they are
    executed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 和 OpenShift 共享许多功能和架构。两者都遵循相同的核心设计原则，但它们在执行方式上有所不同。
- en: Networking
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络
- en: Kubernetes lacks a built-in networking solution and relies on third-party plug-ins
    such as canal, flannel, and Weave to provide networking for the cluster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 本身没有内建的网络解决方案，而是依赖第三方插件，如 canal、flannel 和 Weave，为集群提供网络服务。
- en: OpenShift provides a built-in network solution called **Open vSwitch**. This
    is a VXLAN--based software-defined network stack that can easily be integrated
    into RedHat's other products. There is some support for third-party network plugins,
    but they are limited and much harder to support.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 提供了一个内建的网络解决方案，称为 **Open vSwitch**。这是一个基于 VXLAN 的软件定义网络栈，可以轻松地与 RedHat
    的其他产品集成。虽然也支持第三方网络插件，但这些插件的支持有限且较难维护。
- en: Application deployment
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序部署
- en: Kubernetes takes the approach of being as flexible as possible when deploying
    applications to the cluster, allowing users to deploy any Linux distribution they
    choose, including supporting Windows-based images and nodes. This is because Kubernetes
    is vendor-agnostic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 采用尽可能灵活的方式来部署应用程序到集群，允许用户部署他们选择的任何 Linux 发行版，包括支持基于 Windows 的镜像和节点。这是因为
    Kubernetes 是与供应商无关的。
- en: OpenShift takes the approach of standardizing the whole stack on RedHat products
    such as RHEL for the node's operating system. Technically, there is little to
    nothing to stop OpenShift from running on other Linux distributions such as Ubuntu.
    Additionally, Openshift puts limits on the types of container images that are
    allowed to run inside the cluster. Again, technically, there isn't much preventing
    a user from deploying an Ubuntu image on the Openshift cluster, but they will
    most likely run into issues around supportably.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 采用了将整个技术栈标准化的方式，基于如 RHEL 这样的 RedHat 产品作为节点的操作系统。从技术上讲，OpenShift 运行在其他
    Linux 发行版上（如 Ubuntu）几乎没有任何障碍。此外，OpenShift 对允许在集群中运行的容器镜像类型设置了限制。同样，从技术角度来看，用户并没有太多障碍去在
    OpenShift 集群中部署 Ubuntu 镜像，但他们很可能会遇到与可支持性相关的问题。
- en: Security
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全性
- en: Kubernetes had a built-in tool for pod-level security called **Pod Security
    Policies** (**PSPs**). PSPs were used to enforce limits on pods such as blocking
    a pod from running as root or binding to a host's filesystem. PSPs were deprecated
    in v1.21 due to several limitations of the tool. Now, PSPs are being replaced
    by a third-party tool called **OPA Gatekeeper**, which allows all of the same
    security rules but with a different enforcement model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 拥有一个内建的工具用于 pod 级别的安全性，称为 **Pod 安全策略**（**PSPs**）。PSPs 用于执行对 pod 的限制，例如阻止
    pod 以 root 用户身份运行或绑定到主机的文件系统。由于该工具存在多个限制，PSPs 在 v1.21 版本中被弃用。现在，PSPs 被一个名为 **OPA
    Gatekeeper** 的第三方工具取代，它允许使用相同的安全规则，但采用不同的执行模型。
- en: OpenShift has a much stricter security mindset, with the option to be secure
    as a default, and it doesn't require cluster hardening like Kubernetes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 在安全性方面有着更加严格的理念，默认提供安全选项，并且不像 Kubernetes 那样需要进行集群硬化。
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about Rancher's history and how it got its start.
    Following this, we went over Rancher's core philosophy and how it was designed
    around Kubernetes. Then, we covered where Kubernetes got its start and its core
    philosophy. We then dived into what the core problems are that Kubernetes is trying
    to solve. Finally, we examined the pros and cons of Kubernetes, Docker Swarm,
    and OpenShift.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了 Rancher 的历史及其起步过程。接下来，我们讨论了 Rancher 的核心理念以及它如何围绕 Kubernetes 进行设计。然后，我们回顾了
    Kubernetes 的起源以及它的核心理念。接着，我们深入探讨了 Kubernetes 想要解决的核心问题。最后，我们分析了 Kubernetes、Docker
    Swarm 和 OpenShift 的优缺点。
- en: In the next chapter, we will cover the high-level architecture and processes
    of Rancher and its products, including RKE, K3s, and RancherD.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍 Rancher 及其产品（包括 RKE、K3s 和 RancherD）的高层架构和流程。
