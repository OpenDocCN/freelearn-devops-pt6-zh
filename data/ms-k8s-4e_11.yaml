- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Running Kubernetes on Multiple Clusters
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多个集群上运行 Kubernetes
- en: In this chapter, we’ll take it to the next level and consider options for running
    Kubernetes and deploying workloads on multiple clouds and multiple clusters. Since
    a single Kubernetes cluster has limits, once you exceed these limits you must
    run multiple clusters. A typical Kubernetes cluster is a closely-knit unit where
    all the components run in relative proximity and are connected by a fast network
    (typically, a physical data center or cloud provider availability zone). This
    is great for many use cases, but there are several important use cases where systems
    need to scale beyond a single cluster or a cluster needs to be stretched across
    multiple availability zones.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将进一步探讨在多个云平台和多个集群上运行 Kubernetes 和部署工作负载的选项。由于单个 Kubernetes 集群有其限制，一旦超过这些限制，你就必须运行多个集群。一个典型的
    Kubernetes 集群是一个紧密连接的单元，所有组件都在相对靠近的地方运行，并通过快速网络（通常是物理数据中心或云提供商的可用区）连接。这适用于许多用例，但也有一些重要的用例，系统需要超越单个集群的规模，或者集群需要跨多个可用区扩展。
- en: This is a very active area in Kubernetes these days. In the previous edition
    of the book, this chapter covered Kubernetes Federation and Gardener. Since then,
    the Kubernetes Federation project was abandoned. There are now many projects that
    provide different flavors of multi-cluster solutions, such as direct management,
    Virtual Kubelet solutions, and the gardener.cloud project, which is pretty unique.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Kubernetes 当前非常活跃的一个领域。在本书的前一版中，这一章介绍了 Kubernetes 联邦和 Gardener。自那时以来，Kubernetes
    联邦项目已被弃用。目前有许多项目提供不同类型的多集群解决方案，如直接管理、虚拟 Kubelet 解决方案，以及非常独特的 gardener.cloud 项目。
- en: 'The topics we will cover include the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖的主题包括：
- en: Stretched clusters vs multiple clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展集群与多个集群的对比
- en: The history of cluster federation in Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 集群联邦的历史
- en: Cluster API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cluster API
- en: Karmada
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karmada
- en: Clusternet
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clusternet
- en: Clusterpedia
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clusterpedia
- en: Open Cluster Management
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放集群管理
- en: Virtual Kubelet solutions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟 Kubelet 解决方案
- en: Introduction to the Gardener project
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gardener 项目简介
- en: Stretched Kubernetes clusters versus multi-cluster Kubernetes
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展 Kubernetes 集群与多集群 Kubernetes 的对比
- en: 'There are several reasons to run multiple Kubernetes clusters:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 运行多个 Kubernetes 集群的原因有几个：
- en: You want redundancy in case the geographical zone your cluster runs in has some
    issues
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望在集群运行所在的地理区域出现问题时有冗余
- en: You need more nodes or pods than a single Kubernetes cluster supports
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要更多的节点或 pod，而单个 Kubernetes 集群无法支持
- en: You want to isolate workloads across different clusters for security reasons
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望因安全原因在不同集群之间隔离工作负载
- en: For the first reason it is possible to use a stretched cluster; for the other
    reasons, you must run multiple clusters.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第一个原因，可以使用扩展集群；而出于其他原因，你必须运行多个集群。
- en: Understanding stretched Kubernetes clusters
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解扩展的 Kubernetes 集群
- en: A stretched cluster (AKA wide cluster) is a single Kubernetes cluster where
    the control plane nodes and the work nodes are provisioned across multiple geographical
    availability zones or regions. Cloud providers offer this model for HA-managed
    Kubernetes clusters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展集群（也叫广域集群）是一个单一的 Kubernetes 集群，其中控制平面节点和工作节点分布在多个地理可用区或区域。云提供商为 HA 管理的 Kubernetes
    集群提供这种模式。
- en: Pros of a stretched cluster
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展集群的优点
- en: 'There are several benefits to the stretched cluster model:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展集群模型有几个优点：
- en: Your cluster, with proper redundancy, is protected from data center failures
    as a **SPOF** (**single point of failure**)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的集群，通过适当的冗余，能够防止数据中心故障作为 **SPOF**（**单点故障**）
- en: The simplicity of operating against a single Kubernetes cluster is a huge win
    (logging, metrics, and upgrades)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作单个 Kubernetes 集群的简便性是一个巨大的优势（日志记录、度量和升级）
- en: When you run your own unmanaged stretched cluster you can extend it transparently
    to additional locations (on-prem, edge, and other cloud providers)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你运行自己的非托管扩展集群时，可以透明地将其扩展到其他位置（本地、边缘或其他云提供商）
- en: Cons of a stretched cluster
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展集群的缺点
- en: 'However, the stretched model has its downsides too:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，扩展模型也有其缺点：
- en: You can’t exceed the limits of a single Kubernetes cluster
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能超过单个 Kubernetes 集群的限制
- en: Degraded performance due to cross-zone networking
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于跨区网络连接导致的性能下降
- en: On the cloud cross-zone, networking costs might be substantial
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云端跨区网络的费用可能相当可观
- en: Cluster upgrades are an all-or-nothing affair
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群升级是一个全有或全无的事务
- en: In short, it’s good to have the option for stretched clusters, but be prepared
    to switch to the multi-cluster model if some of the downsides are unacceptable.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，拥有拉伸集群的选项是好事，但如果一些缺点是无法接受的，则要准备切换到多集群模型。
- en: Understanding multi-cluster Kubernetes
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解多集群 Kubernetes
- en: Multi-cluster Kubernetes means provisioning multiple Kubernetes clusters. Large-scale
    systems often can’t be deployed on a single cluster for various reasons mentioned
    earlier. That means you need to provision multiple Kubernetes clusters and then
    figure out how to deploy your workloads on all these clusters and how to handle
    various use cases, such as some clusters being unavailable or having degraded
    performance. There are many more degrees of freedom.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群 Kubernetes 意味着提供多个 Kubernetes 集群。基于前述的各种原因，大规模系统通常无法部署在单个集群上。这意味着你需要提供多个
    Kubernetes 集群，然后弄清如何在所有这些集群上部署你的工作负载，并处理各种使用案例，比如一些集群不可用或性能下降的情况。这带来了更多的自由度。
- en: Pros of multi-cluster Kubernetes
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多集群 Kubernetes 的优点
- en: 'Here are some of the benefits of the multi-cluster model:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是多集群模型的一些好处：
- en: Scale your system arbitrarily – there are no inherent limits on the number of
    clusters
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统可以任意扩展 —— 集群数量没有固有的限制
- en: Provide cluster-level isolation to sensitive workloads at the RBAC level
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 RBAC 级别为敏感工作负载提供集群级别的隔离
- en: Utilize multiple cloud providers without incurring excessive costs (as long
    as most traffic remains within the same cloud provider region)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不产生过多成本的情况下（只要大部分流量仍然保持在同一云服务提供商的区域内），利用多个云服务提供商
- en: Upgrade and perform incremental operations, even for cluster-wide operations
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级和执行增量操作，即使是针对整个集群的操作
- en: Cons of multi-cluster Kubernetes
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多集群 Kubernetes 的缺点
- en: 'However, there are some non-trivial downsides to the multi-cluster level:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多集群模型也存在一些非常不平凡的缺点：
- en: The very high complexity of provisioning and managing a fleet of clusters
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署和管理一系列集群的非常高复杂度
- en: Need to figure out how to connect all the clusters
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要弄清如何连接所有的集群
- en: Need to figure out how to store and provide access to data across all the clusters
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要弄清如何存储并提供对所有集群中数据的访问
- en: A lot of options to shoot yourself in the foot when designing multi-cluster
    deployments
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设计多集群部署时，有很多方式会让你自食其果
- en: Need to work hard to provide centralized observability across all clusters
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要努力在所有集群中提供集中观测能力
- en: There are solutions out there for some of these problems, but at this point
    in time, there is no clear winner you can just adopt and easily configure for
    your needs. Instead, you will need to adapt and solve problems depending on the
    specific issues raised with your organization’s multi-cluster structure.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有一些问题的解决方案，但目前还没有一个清晰的赢家可以轻松配置以适应你组织的多集群结构的需求。相反，你需要根据组织的具体问题来适应和解决问题。
- en: The history of cluster federation in Kubernetes
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 中集群联合的历史
- en: In the previous editions of the book, we discussed Kubernetes Cluster Federation
    as a solution to managing multiple Kubernetes clusters as a single conceptual
    cluster. Unfortunately, this project has been inactive since 2019, and the Kubernetes
    multi-cluster **Special Interest Group** (**SIG**) is considering archiving it.
    Before we describe more modern approaches let’s get some historical context. It’s
    funny to talk about the history of a project like Kubernetes that didn’t even
    exist before 2014, but the pace of development and the large number of contributors
    took Kubernetes through an accelerated evolution. This is especially relevant
    for the Kubernetes Federation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在书的早期版本中，我们讨论了 Kubernetes 集群联合作为管理多个 Kubernetes 集群的单一概念集群的解决方案。不幸的是，自 2019 年以来，该项目一直处于停滞状态，并且
    Kubernetes 多集群**特别兴趣小组（SIG）**正在考虑将其存档。在我们描述更现代化的方法之前，让我们先了解一些历史背景。谈论一个像 Kubernetes
    这样的项目的历史，甚至在 2014 年之前根本不存在，这是很有趣的，但是其发展速度和大量贡献者的参与加速了 Kubernetes 的进化。这对于 Kubernetes
    联合尤为重要。
- en: 'In March 2015, the first revision of the Kubernetes Cluster Federation proposal
    was published. It was fondly nicknamed “Ubernetes” back then. The basic idea was
    to reuse the existing Kubernetes APIs to manage multiple clusters. This proposal,
    now called Federation V1, went through several rounds of revision and implementation
    but never reached general availability, and the main repo has been retired: [https://github.com/kubernetes-retired/federation](https://github.com/kubernetes-retired/federation).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年3月，Kubernetes集群联合提案的第一个版本被发布。当时它被亲切地称为“Ubernetes”。基本想法是重用现有的Kubernetes
    API来管理多个集群。这个提案现在被称为Federation V1，经过了几轮修订和实施，但始终未达到广泛使用，且主仓库已被退休：[https://github.com/kubernetes-retired/federation](https://github.com/kubernetes-retired/federation)。
- en: 'The SIG multi-cluster workgroup realized that the multi-cluster problem is
    more complicated than initially perceived. There are many ways to skin this particular
    cat and there is no one-size-fits-all solution. The new direction for cluster
    federation was to use dedicated APIs for federation. A new project and a set of
    tools were created and implemented as Kubernetes Federation V2: [https://github.com/kubernetes-sigs/kubefed](https://github.com/kubernetes-sigs/kubefed).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: SIG多集群工作组意识到，多集群问题比最初认为的要复杂得多。解决这一特定问题有多种方法，而且没有一种适用于所有的解决方案。集群联合的新方向是使用专门的API来进行联合。为此，创建并实施了一个新项目和一组工具——Kubernetes
    Federation V2：[https://github.com/kubernetes-sigs/kubefed](https://github.com/kubernetes-sigs/kubefed)。
- en: Unfortunately, this didn’t take off either, and the consensus of the multi-cluster
    SIG is that since the project is not being maintained, it needs to be archived.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个项目也没有取得成功，关于多集群SIG的共识是，由于该项目未再维护，因此需要将其归档。
- en: 'See the notes for the meeting from 2022-08-09: [https://tinyurl.com/sig-multicluster-notes](https://tinyurl.com/sig-multicluster-notes).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 查看2022年8月9日的会议记录：[https://tinyurl.com/sig-multicluster-notes](https://tinyurl.com/sig-multicluster-notes)。
- en: There are a lot of projects out there moving fast to try to solve the multi-cluster
    problem, and they all operate at different levels. Let’s look at some of the prominent
    ones. The goal here is just to introduce these projects and what makes them unique.
    It is beyond the scope of this chapter to fully explore each one. However, we
    will dive deeper into one of the projects – the Cluster API – in *Chapter 17*,
    *Running Kubernetes in Production*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多项目正在快速推进，试图解决多集群问题，它们都在不同层面上进行操作。让我们来看看其中一些突出项目。这里的目标仅仅是介绍这些项目以及它们的独特之处。深入探讨每个项目超出了本章的范围。不过，我们将在*第17章*《在生产环境中运行Kubernetes》中深入探讨其中一个项目——Cluster
    API。
- en: Cluster API
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cluster API
- en: The Cluster API (AKA CAPI) is a project from the Cluster Lifecycle SIG. Its
    goal is to make provisioning, upgrading, and operating multiple Kubernetes clusters
    easy. It supports both kubeadm-based clusters as well as managed clusters via
    dedicated providers. It has a cool logo inspired by the famous “It’s turtles all
    the way down” story. The idea is that the Cluster API uses Kubernetes to manage
    Kubernetes clusters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster API（也叫CAPI）是Cluster Lifecycle SIG的一个项目。其目标是简化多个Kubernetes集群的配置、升级和操作。它支持基于kubeadm的集群以及通过专用提供者管理的集群。它有一个很酷的标志，灵感来自著名的“从头到尾全是乌龟”的故事。这个想法是，Cluster
    API使用Kubernetes来管理Kubernetes集群。
- en: '![](img/B18998_11_01.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_01.png)'
- en: 'Figure 11.1: The Cluster API logo'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：Cluster API标志
- en: Cluster API architecture
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cluster API架构
- en: 'The Cluster API has a very clean and extensible architecture. The primary components
    are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster API具有非常清晰和可扩展的架构。其主要组件包括：
- en: The management cluster
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理集群
- en: The work cluster
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作集群
- en: The bootstrap provider
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动提供者
- en: The infrastructure provider
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施提供者
- en: The control plane
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面
- en: Custom resources
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义资源
- en: '![](img/B18998_11_02.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_02.png)'
- en: 'Figure 11.2: Cluster API architecture'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：Cluster API架构
- en: Let’s understand the role of each one of these components and how they interact
    with each other.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解这些组件中的每一个的作用以及它们如何相互作用。
- en: Management cluster
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理集群
- en: The management cluster is a Kubernetes cluster that is responsible for managing
    other Kubernetes clusters (work clusters). It runs the Cluster API control plane
    and providers, and it hosts the Cluster API custom resources that represent the
    other clusters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 管理集群是一个负责管理其他Kubernetes集群（工作集群）的Kubernetes集群。它运行Cluster API控制平面和提供者，并托管表示其他集群的Cluster
    API自定义资源。
- en: The clusterctl CLI can be used to work with the management cluster. The clusterctl
    CLI is a command-line tool with a lot of commands and options, if you want to
    experiment with the Cluster API through its CLI, visit [https://cluster-api.sigs.k8s.io/clusterctl/overview.html](https://cluster-api.sigs.k8s.io/clusterctl/overview.html).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: clusterctl CLI可以用来操作管理集群。clusterctl CLI是一个命令行工具，提供了大量的命令和选项。如果你想通过其CLI来试验Cluster
    API，请访问[https://cluster-api.sigs.k8s.io/clusterctl/overview.html](https://cluster-api.sigs.k8s.io/clusterctl/overview.html)。
- en: Work cluster
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作集群
- en: A work cluster is just a regular Kubernetes cluster. These are the clusters
    that developers use to deploy their workloads. The work clusters don’t need to
    be aware that they are managed by the Cluster API.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 工作集群只是一个普通的Kubernetes集群。这些集群是开发人员用来部署其工作负载的。工作集群无需意识到它们是由Cluster API管理的。
- en: Bootstrap provider
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动提供者
- en: When CAPI creates a new Kubernetes cluster, it needs certificates before it
    can create the work cluster’s control plane and, finally, the worker nodes. This
    is the job of the bootstrap provider. It ensures all the requirements are met
    and eventually joins the worker nodes to the control plane.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当CAPI创建一个新的Kubernetes集群时，在创建工作集群的控制平面和最终的工作节点之前，它需要证书。这是启动提供者的工作。它确保满足所有要求，并最终将工作节点加入控制平面。
- en: Infrastructure provider
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础设施提供者
- en: The infrastructure provider is a pluggable component that allows CAPI to work
    in different infrastructure environments, such as cloud providers or bare-metal
    infrastructure providers. The infrastructure provider implements a set of interfaces
    as defined by CAPI to provide access to compute and network resources.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施提供者是一个可插拔组件，使CAPI能够在不同的基础设施环境中工作，例如云提供商或裸金属基础设施提供商。基础设施提供者实现了CAPI定义的一组接口，以提供对计算和网络资源的访问。
- en: 'Check out the current providers’ list here: [https://cluster-api.sigs.k8s.io/reference/providers.html](https://cluster-api.sigs.k8s.io/reference/providers.html).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看当前提供者列表：[https://cluster-api.sigs.k8s.io/reference/providers.html](https://cluster-api.sigs.k8s.io/reference/providers.html)。
- en: Control plane
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制平面
- en: 'The control plane of a Kubernetes cluster consists of the API server, the etcd
    stat store, the scheduler, and the controllers that run the control loops to reconcile
    the resources in the cluster. The control plane of the work clusters can be provisioned
    in various ways. CAPI supports the following modes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群的控制平面由API服务器、etcd状态存储、调度器以及运行控制循环以调和集群中资源的控制器组成。工作集群的控制平面可以通过不同的方式来提供。CAPI支持以下模式：
- en: Machine-based – the control plane components are deployed as static pods on
    dedicated machines
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于机器的——控制平面组件作为静态Pod部署在专用机器上
- en: Pod-based – the control plane components are deployed via `Deployments` and
    `StatefulSet`, and the API server is exposed as a `Service`
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Pod的——控制平面组件通过`Deployments`和`StatefulSet`部署，API服务器作为`Service`暴露
- en: External – the control plane is provisioned and managed by an external provider
    (typically, a cloud provider)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部——控制平面由外部提供者（通常是云提供商）提供和管理
- en: Custom resources
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义资源
- en: 'The custom resources represent the Kubernetes clusters and machines managed
    by CAPI as well as additional auxiliary resources. There are a lot of custom resources,
    and some of them are still considered experimental. The primary CRDs are:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源代表由CAPI管理的Kubernetes集群和机器以及其他附加资源。自定义资源数量众多，其中一些仍被视为实验性。主要的CRD有：
- en: '`Cluster`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cluster`'
- en: '`ControlPlane` (represents control plane machines)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ControlPlane`（代表控制平面机器）'
- en: '`MachineSet` (represents worker machines)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineSet`（代表工作机器）'
- en: '`MachineDeployment`'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineDeployment`'
- en: '`Machine`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Machine`'
- en: '`MachineHealthCheck`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineHealthCheck`'
- en: Some of these generic resources have references to corresponding resources offered
    by the infrastructure provider.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些通用资源中的一些引用了基础设施提供者提供的相应资源。
- en: 'The following diagram illustrates the relationships between the control plane
    resources that represent the clusters and machine sets:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了代表集群和机器集的控制平面资源之间的关系：
- en: '![](img/B18998_11_03.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_03.png)'
- en: 'Figure 11.3: Cluster API control plane resources'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：Cluster API控制平面资源
- en: 'CAPI also has an additional set of experimental resources that represent a
    managed cloud provider environment:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: CAPI还具有一组额外的实验性资源，代表一个受管的云提供者环境：
- en: '`MachinePool`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachinePool`'
- en: '`ClusterResourceSet`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClusterResourceSet`'
- en: '`ClusterClass`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClusterClass`'
- en: See [https://github.com/kubernetes-sigs/cluster-api](https://github.com/kubernetes-sigs/cluster-api)
    for more details.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [https://github.com/kubernetes-sigs/cluster-api](https://github.com/kubernetes-sigs/cluster-api)
    获取更多细节。
- en: Karmada
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Karmada
- en: Karmada is a CNCF sandbox project that focuses on deploying and running workloads
    across multiple Kubernetes clusters. Its claim to fame is that you don’t need
    to make changes to your application configuration. While CAPI was focused on the
    lifecycle management of clusters, Karmada picks up when you already have a set
    of Kubernetes clusters and you want to deploy workloads across all of them. Conceptually,
    Karmada is a modern take on the abandoned Kubernetes Federation project.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Karmada 是一个 CNCF 沙盒项目，专注于跨多个 Kubernetes 集群部署和运行工作负载。它的特点是你无需修改应用程序配置。虽然 CAPI
    专注于集群生命周期管理，但 Karmada 在你已经拥有一组 Kubernetes 集群，并希望在所有集群中部署工作负载时发挥作用。从概念上讲，Karmada
    是对已废弃的 Kubernetes 联邦项目的现代化改进。
- en: It can work with Kubernetes in the cloud, on-prem, and on the edge.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以与 Kubernetes 在云端、本地和边缘环境中一起工作。
- en: See [https://github.com/karmada-io/karmada](https://github.com/karmada-io/karmada).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [https://github.com/karmada-io/karmada](https://github.com/karmada-io/karmada)。
- en: Let’s look at Karmada’s architecture.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 Karmada 的架构。
- en: Karmada architecture
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Karmada 架构
- en: 'Karmada is heavily inspired by Kubernetes. It provides a multi-cluster control
    plane with similar components to the Kubernetes control plane:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Karmada 深受 Kubernetes 的启发。它提供了一个多集群控制平面，包含与 Kubernetes 控制平面类似的组件：
- en: Karmada API server
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karmada API 服务器
- en: Karmada controller manager
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karmada 控制器管理器
- en: Karmada scheduler
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karmada 调度器
- en: If you understand how Kubernetes works, then it is pretty easy to understand
    how Karmada extends it 1:1 to multiple clusters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你理解 Kubernetes 的工作原理，那么理解 Karmada 如何将其 1:1 扩展到多个集群就非常简单。
- en: 'The following diagram illustrates the Karmada architecture:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Karmada 的架构：
- en: '![](img/B18998_11_04.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_04.png)'
- en: 'Figure 11.4: Karmada architecture'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：Karmada 架构
- en: Karmada concepts
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Karmada 概念
- en: Karmada is centered around several concepts implemented as Kubernetes CRDs.
    You define and update your applications and services using these concepts and
    Karmada ensures that your workloads are deployed and run in the right place across
    your multi-cluster system.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Karmada 以多个作为 Kubernetes CRD 实现的概念为中心。你使用这些概念定义和更新你的应用程序和服务，Karmada 会确保你的工作负载在多集群系统中正确部署和运行。
- en: Let’s look at these concepts.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些概念。
- en: ResourceTemplate
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ResourceTemplate
- en: The resource template looks just like a regular Kubernetes resource such as
    `Deployment` or `StatefulSet`, but it doesn’t actually get deployed to the Karmada
    control plane. It only serves as a blueprint that will eventually be deployed
    to member clusters.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 资源模板看起来就像常规的 Kubernetes 资源，例如 `Deployment` 或 `StatefulSet`，但它并不会实际部署到 Karmada
    控制平面。它仅作为一个蓝图，最终将被部署到成员集群。
- en: PropagationPolicy
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PropagationPolicy
- en: 'The propagation policy determines where a resource template should be deployed.
    Here is a simple propagation policy that will place the `nginx` Deployment into
    two clusters, called `member1` and `member2`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 传播策略决定了资源模板应该部署到哪里。这里有一个简单的传播策略，它将 `nginx` 部署到两个集群，分别叫做 `member1` 和 `member2`：
- en: '[PRE0]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: OverridePolicy
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OverridePolicy
- en: 'Propagation policies operate across multiple clusters, but sometimes, there
    are exceptions. The override policy lets you apply fine-grained rules to override
    existing propagation policies. There are several types of rules:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: PropagationPolicy 跨多个集群操作，但有时会有例外。OverridePolicy 允许你应用细粒度规则，以覆盖现有的传播策略。规则有几种类型：
- en: '`ImageOverrider`: Dedicated to overriding images for workloads'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImageOverrider`：专门用于覆盖工作负载的镜像'
- en: '`CommandOverrider`: Dedicated to overriding commands for workloads'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CommandOverrider`：专门用于覆盖工作负载的命令'
- en: '`ArgsOverrider`: Dedicated to overriding args for workloads'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ArgsOverrider`：专门用于覆盖工作负载的参数'
- en: '`PlaintextOverrider`: A general-purpose tool to override any kind of resources'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PlaintextOverrider`：一个通用工具，用于覆盖任何类型的资源'
- en: Additional capabilities
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外功能
- en: 'There is much more to Karmada, such as:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Karmada 还有更多功能，例如：
- en: Multi-cluster de-scheduling
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多集群重新调度
- en: Re-scheduling
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新调度
- en: Multi-cluster failover
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多集群故障切换
- en: Multi-cluster service discovery
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多集群服务发现
- en: 'Check the Karmada documentation for more details: [https://karmada.io/docs/](https://karmada.io/docs/).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 Karmada 文档了解更多细节：[https://karmada.io/docs/](https://karmada.io/docs/)。
- en: Clusternet
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Clusternet
- en: 'Clusternet is an interesting project. It is centered around the idea of managing
    multiple Kubernetes clusters as “visiting the internet” (hence the name “Clusternet”).
    It supports cloud-based, on-prem, edge, and hybrid clusters. The core features
    of Clusternet are:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Clusternet 是一个有趣的项目。它围绕管理多个 Kubernetes 集群的理念展开，目标是像“访问互联网”一样管理集群（因此取名为“Clusternet”）。它支持云端、本地、边缘和混合集群。Clusternet
    的核心功能包括：
- en: Kubernetes multi-cluster management and governance
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 多集群管理与治理
- en: Application coordination
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用协调
- en: A CLI via the kubectl plugin
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 kubectl 插件提供的 CLI
- en: Programmatic access via a wrapper to the Kubernetes Client-Go library
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对 Kubernetes Client-Go 库的包装进行编程访问
- en: Clusternet architecture
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clusternet 架构
- en: 'The Clusternet architecture is similar to Karmada but simpler. There is a parent
    cluster that runs the Clusternet hub and Clusternet scheduler. On each child cluster,
    there is a Clusternet agent. The following diagram illustrates the structure and
    interactions between the components:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Clusternet 架构类似于 Karmada，但更为简化。存在一个父集群，它运行 Clusternet hub 和 Clusternet scheduler。在每个子集群上，运行一个
    Clusternet agent。下图展示了各个组件之间的结构和交互：
- en: '![](img/B18998_11_05.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_05.png)'
- en: 'Figure 11.5: Clusternet architecture'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：Clusternet 架构
- en: Clusternet hub
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Clusternet hub
- en: The hub has multiple roles. It is responsible for approving cluster registration
    requests and creating namespaces, service accounts, and RBAC resources for all
    child clusters. It also serves as an aggregated API server that maintains WebSocket
    connections to the agent on child clusters. The hub also provides a Kubernetes-like
    API to proxy requests to each child cluster. Last but not least, the hub coordinates
    the deployment of applications and their dependencies to multiple clusters from
    a single set of resources.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Hub 有多个角色。它负责批准集群注册请求，并为所有子集群创建命名空间、服务账户和 RBAC 资源。它还充当一个聚合的 API 服务器，维护与子集群上代理的
    WebSocket 连接。Hub 还提供类似 Kubernetes 的 API，将请求代理到每个子集群。最后但同样重要的是，Hub 协调从单一资源集群部署应用程序及其依赖项到多个集群。
- en: Clusternet scheduler
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Clusternet scheduler
- en: The Clusternet scheduler is the component that is responsible for ensuring that
    resources (called feeds in Clusternet terminology) are deployed and balanced across
    all the child clusters according to policies called `SchedulingStrategy`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Clusternet scheduler 是负责确保资源（在 Clusternet 术语中称为 feeds）根据名为 `SchedulingStrategy`
    的策略在所有子集群间部署和均衡的组件。
- en: Clusternet agent
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Clusternet agent
- en: The Clusternet agent runs on every child cluster and communicates with the hub.
    The agent on a child cluster is the equivalent of the kubelet on a node. It has
    several roles. The agent registers its child cluster with the parent cluster.
    The agent provides a heartbeat to the hub that includes a lot of information,
    such as the Kubernetes version, running platform, health, readiness, and liveness
    of workloads. The agent also sets up the WebSocket connection to the hub on the
    parent cluster to allow full-duplex communication channels over a single TCP connection.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Clusternet agent 在每个子集群上运行，并与 Hub 通信。子集群上的 agent 相当于节点上的 kubelet。它有多个角色。该 agent
    将其子集群注册到父集群。该 agent 向 Hub 提供心跳信息，包含许多内容，如 Kubernetes 版本、运行平台、健康状况、就绪状态和工作负载的存活状态。该
    agent 还设置与父集群 Hub 的 WebSocket 连接，以便通过单一 TCP 连接实现全双工通信通道。
- en: Multi-cluster deployment
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多集群部署
- en: 'Clusternet models multi-cluster deployment as subscriptions and feeds. It provides
    a `Subscription` custom resource that can be used to deploy a set of resources
    (called feeds) to multiple clusters (called subscribers) based on different criteria.
    Here is an example of a `Subscription` that deploys a `Namespace`, a `Service`,
    and a `Deployment` to multiple clusters with a label of `clusters.clusternet.io/cluster-id`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Clusternet 将多集群部署建模为订阅和 feeds。它提供一个 `Subscription` 自定义资源，可以根据不同的标准将一组资源（称为 feeds）部署到多个集群（称为订阅者）。以下是一个
    `Subscription` 示例，它将一个 `Namespace`、一个 `Service` 和一个 `Deployment` 部署到多个集群，并带有 `clusters.clusternet.io/cluster-id`
    标签：
- en: '[PRE1]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: See [https://clusternet.io](https://clusternet.io) for more details.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详情请参见 [https://clusternet.io](https://clusternet.io)。
- en: Clusterpedia
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Clusterpedia
- en: Clusterpedia is a CNCF sandbox project. Its central metaphor is Wikipedia for
    Kubernetes clusters. It has a lot of capabilities around multi-cluster search,
    filtering, field selection, and sorting. This is unusual because it is a read-only
    project. It doesn’t offer to help with managing the clusters or deploying workloads.
    It is focused on observing your clusters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Clusterpedia 是一个 CNCF 沙箱项目。它的中心隐喻是 Kubernetes 集群的维基百科。它具有多集群搜索、过滤、字段选择和排序等多种能力。这是不同寻常的，因为它是一个只读项目。它不提供帮助管理集群或部署工作负载。它专注于观察您的集群。
- en: Clusterpedia architecture
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clusterpedia 架构
- en: The architecture is similar to other multi-cluster projects. There is a control
    plane element that runs the Clusterpedia API server and ClusterSynchro manager
    components. For each observed cluster, there is a dedicated component called cluster
    syncro that synchronizes the state of the clusters into the storage layer of Clusterpedia.
    One of the most interesting aspects of the architecture is the Clusterpedia aggregated
    API server, which makes all your clusters seem like a single huge logical cluster.
    Note that the Clusterpedia API server and the ClusterSynchro manager are loosely
    coupled and don’t interact directly with each other. They just read and write
    from a shared storage layer.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 架构类似于其他多集群项目。有一个控制平面元素，运行 Clusterpedia API 服务器和 ClusterSynchro 管理器组件。对于每个观察到的集群，都有一个专用组件称为集群同步器，将集群的状态同步到
    Clusterpedia 的存储层中。架构的一个最有趣的方面是 Clusterpedia 聚合 API 服务器，它使所有集群看起来像一个巨大的逻辑集群。请注意，Clusterpedia
    API 服务器和 ClusterSynchro 管理器松散耦合，不直接互动。它们只是从共享的存储层读取和写入。
- en: '![](img/B18998_11_06.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_06.png)'
- en: 'Figure 11.6: Clusterpedia architecture'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6：Clusterpedia 架构
- en: Let’s look at each of the components and understand what their purpose is.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个组件，并理解它们的目的是什么。
- en: Clusterpedia API server
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Clusterpedia API 服务器
- en: The Clusterpedia API server is an aggregated API server. That means that it
    registers itself with the Kubernetes API server and, in practice, extends the
    standard Kubernetes API server via custom endpoints. When requests come to the
    Kubernetes API server, it forwards them to the Clusterpedia API server, which
    accesses the storage layer to satisfy them. The Kubernetes API server serves as
    a forwarding layer for the requests that Clusterpedia handles.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Clusterpedia API 服务器是一个聚合 API 服务器。这意味着它会向 Kubernetes API 服务器注册自己，并通过自定义端点扩展标准的
    Kubernetes API 服务器。当请求发送到 Kubernetes API 服务器时，它会将其转发到 Clusterpedia API 服务器，后者访问存储层来满足这些请求。Kubernetes
    API 服务器充当 Clusterpedia 处理的请求的转发层。
- en: This is an advanced aspect of Kubernetes. We will discuss API server aggregation
    in *Chapter 15*, *Extending Kubernetes*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Kubernetes 的一个高级方面。我们将在 *第 15 章* *扩展 Kubernetes* 中讨论 API 服务器聚合。
- en: ClusterSynchro manager
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClusterSynchro 管理器
- en: Clusterpedia observes multiple clusters to provide its search, filter, and aggregation
    features. One way to implement it is that whenever a request comes in, Clusterpedia
    would query all the observed clusters, collect the results, and return them. This
    approach is very problematic, as some clusters might be slow to respond and similar
    requests will require returning the same information, which is wasteful and costly.
    Instead, the ClusterSynchro manager collectively synchronizes the state of each
    observed cluster into Clusterpedia storage, where the Clusterpedia API server
    can respond quickly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Clusterpedia 观察多个集群以提供其搜索、过滤和聚合功能。一种实现方式是，每当有请求进来时，Clusterpedia 将查询所有观察到的集群，收集结果并返回。这种方法非常问题，因为某些集群可能响应缓慢，并且类似的请求需要返回相同的信息，这是浪费和昂贵的。相反，ClusterSynchro
    管理器会集体将每个观察到的集群的状态同步到 Clusterpedia 存储中，Clusterpedia API 服务器可以快速响应。
- en: Storage layer
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储层
- en: The storage layer is an abstraction layer that stores the state of all observed
    clusters. It provides a uniform interface that can be implemented by different
    storage components. The Clusterpedia API server and the ClusterSynchro manager
    interact with the storage layer interface and never talk to each other directly.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 存储层是一个抽象层，用于存储所有观察到的集群的状态。它提供了一个可以由不同存储组件实现的统一接口。Clusterpedia API 服务器和 ClusterSynchro
    管理器与存储层接口进行交互，从不直接互相通信。
- en: Storage component
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储组件
- en: The storage component is an actual data store that implements the storage layer
    interface and stores the state of observed clusters. Clusterpedia was designed
    to support different storage components to provide flexibility for their users.
    Currently, supported storage components include MySQL, Postgres, and Redis.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 存储组件是实际的数据存储，执行存储层接口并存储观察到的集群状态。Clusterpedia 旨在支持不同的存储组件，以便为用户提供灵活性。目前，支持的存储组件包括
    MySQL、Postgres 和 Redis。
- en: Importing clusters
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入集群
- en: 'To onboard clusters into Clusterpedia, you define a PediaCluster custom resource.
    It is pretty straightforward:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要将集群导入到 Clusterpedia，你需要定义一个 PediaCluster 自定义资源。这非常简单：
- en: '[PRE2]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You need to provide credentials to access the cluster, and then Clusterpedia
    will take over and sync its state.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要提供访问集群的凭证，然后 Clusterpedia 会接管并同步其状态。
- en: Advanced multi-cluster search
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级多集群搜索
- en: 'This is where Clusterpedia shines. You can access the Clusterpedia cluster
    via an API or through kubectl. When accessing it through a URL it looks like you
    hit the aggregated API server endpoint:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Clusterpedia 的亮点。你可以通过 API 或 kubectl 访问 Clusterpedia 集群。通过 URL 访问时，它看起来像是你访问了聚合的
    API 服务器端点：
- en: '[PRE3]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can specify the target cluster as a query parameter (in this case, `cluster-1`
    and `cluster-2`).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将目标集群指定为查询参数（在此案例中，`cluster-1` 和 `cluster-2`）。
- en: 'When accessing through kubectl, you specify the target clusters as a label
    (in this case, `"search.clusterpedia.io/clusters in (cluster-1,cluster-2)"`):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 kubectl 访问时，你指定目标集群为标签（在此案例中，`"search.clusterpedia.io/clusters in (cluster-1,cluster-2)"`）：
- en: '[PRE4]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Other search labels and queries exist for namespaces and resource names:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他用于命名空间和资源名称的搜索标签和查询：
- en: '`search.clusterpedia.io/namespaces` (query parameter is `namespaces`)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`search.clusterpedia.io/namespaces`（查询参数是 `namespaces`）'
- en: '`search.clusterpedia.io/names` (query parameter is `names`)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`search.clusterpedia.io/names`（查询参数是 `names`）'
- en: There is also an experimental fuzzy search label, `internalstorage.clusterpedia.io/fuzzy-name`,
    for resource names, but no query parameter. This is useful as often, resources
    have generated names with random suffixes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个实验性的模糊搜索标签 `internalstorage.clusterpedia.io/fuzzy-name` 用于资源名称，但没有查询参数。这很有用，因为资源通常会生成带有随机后缀的名称。
- en: 'You can also search by creation time:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以按创建时间进行搜索：
- en: '`search.clusterpedia.io/before` (query parameter is `before`)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`search.clusterpedia.io/before`（查询参数是 `before`）'
- en: '`search.clusterpedia.io/since` (query parameter is `since`)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`search.clusterpedia.io/since`（查询参数是 `since`）'
- en: Other capabilities include filtering by resource labels or field selectors as
    well as organizing the results using `OrderBy` and `Paging`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 其他功能包括按资源标签或字段选择器进行过滤，以及使用 `OrderBy` 和 `Paging` 来组织结果。
- en: Resource collections
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源集合
- en: Another important concept is resource collections. The standard Kubernetes API
    offers a straightforward REST API where you can list or get one kind of resource
    at a time. However, often, users would like to get multiple types of resources
    at the same time. For example, the `Deployment`, `Service`, and `HorizontalPodAutoscaler`
    with a specific label. This requires multiple calls via the standard Kubernetes
    API, even if all these resources are available on one cluster.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要概念是资源集合。标准的 Kubernetes API 提供了一个简单的 REST API，你可以一次列出或获取一种资源。然而，用户通常希望同时获取多种类型的资源。例如，带有特定标签的
    `Deployment`、`Service` 和 `HorizontalPodAutoscaler`。这需要通过标准的 Kubernetes API 进行多次调用，即使这些资源都在一个集群中。
- en: 'Clusterpedia defines a `CollectionResource` that groups together resources
    that belong to the following categories:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Clusterpedia 定义了一个 `CollectionResource`，将以下类别的资源组合在一起：
- en: '`any` (all resources)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`any`（所有资源）'
- en: '`workloads` (`Deployments`, `StatefulSets`, and `DaemonSets`)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workloads`（`Deployments`、`StatefulSets` 和 `DaemonSets`）'
- en: '`kuberesources` (all resources other than workloads)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kuberesources`（除工作负载外的所有资源）'
- en: 'You can search for any combination of resources in one API call by passing
    API groups and resource kinds:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过传递 API 组和资源类型，在一次 API 调用中搜索任意组合的资源：
- en: '[PRE5]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: See [https://github.com/clusterpedia-io/clusterpedia](https://github.com/clusterpedia-io/clusterpedia)
    for more details.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请见 [https://github.com/clusterpedia-io/clusterpedia](https://github.com/clusterpedia-io/clusterpedia)。
- en: Open Cluster Management
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放集群管理
- en: '**Open Cluster Management** (**OCM**) is a CNCF sandbox project for multi-cluster
    management, as well as multi-cluster scheduling and workload placement. Its claim
    to fame is closely following many Kubernetes concepts, extensibility via addons,
    and strong integration with other open source projects, such as:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放集群管理** (**OCM**) 是一个 CNCF 沙箱项目，旨在进行多集群管理，以及多集群调度和工作负载放置。它的特点是紧密遵循许多 Kubernetes
    概念，通过插件实现可扩展性，并与其他开源项目（如）进行强集成：'
- en: Submariner
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Submariner
- en: Clusternet (that we covered earlier)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clusternet（我们之前介绍过的）
- en: KubeVela
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KubeVela
- en: The scope of OCM covers cluster lifecycle, application lifecycle, and governance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: OCM 的范围涵盖集群生命周期、应用生命周期和治理。
- en: Let’s look at OCM’s architecture.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 OCM 的架构。
- en: OCM architecture
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OCM 架构
- en: OCM’s architecture follows the hub and spokes model. It has a hub cluster, which
    is the OCM control plane that manages multiple other clusters (the spokes).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: OCM 的架构遵循中心和辐射模型。它有一个中心集群，即 OCM 控制平面，用于管理多个其他集群（辐射集群）。
- en: 'The control plane’s hub cluster runs two controllers: the registration controller
    and the placement controller. In addition, the control plane runs multiple management
    addons, which are the foundation for OCM’s extensibility. On each managed cluster,
    there is a so-called Klusterlet that has a registration-agent and work-agent that
    interact with the registration controller and placement controller on the hub
    cluster. Then, there are also addon agents that interact with the addons on the
    hub cluster.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面的中心集群运行两个控制器：注册控制器和放置控制器。此外，控制平面还运行多个管理插件，它们是 OCM 可扩展性的基础。在每个托管集群上，都有一个所谓的
    Klusterlet，它包含一个注册代理和一个工作代理，与中心集群上的注册控制器和放置控制器进行交互。然后，还有插件代理与中心集群上的插件进行交互。
- en: 'The following diagram illustrates how the different components of OCM communicate:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了 OCM 各个组件如何进行通信：
- en: '![](img/B18998_11_07.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_07.png)'
- en: 'Figure 11.7: OCM architecture'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7：OCM 架构
- en: Let’s look at the different aspects of OCM.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 OCM 的不同方面。
- en: OCM cluster lifecycle
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OCM 集群生命周期
- en: Cluster registration is a big part of OCM’s secure multi-cluster story. OCM
    prides itself on the secure double opt-in handshake registration. Since a hub-and-spoke
    cluster may have different administrators, this model provides protection for
    each side from undesired requests. Each side can terminate the relationship at
    any time.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 集群注册是 OCM 安全多集群故事中的重要部分。OCM 以其安全的双重确认握手注册为傲。由于中心-辐射式集群可能有不同的管理员，这种模型为每一方提供了对不希望的请求的保护。每一方都可以随时终止关系。
- en: 'The following diagram demonstrates the registration process (CSR means certificate
    signing request):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了注册过程（CSR 表示证书签名请求）：
- en: '![](img/B18998_11_08.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_08.png)'
- en: 'Figure 11.8: OCM registration process'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：OCM 注册过程
- en: OCM application lifecycle
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OCM 应用生命周期
- en: The OCM application lifecycle supports creating, updating, and deleting resources
    across multiple clusters.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: OCM 应用生命周期支持在多个集群之间创建、更新和删除资源。
- en: 'The primary building block is the `ManifestWork` custom resource that can define
    multiple resources. Here is an example that contains only a single `Deployment`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的构建块是 `ManifestWork` 自定义资源，它可以定义多个资源。下面是一个仅包含单个 `Deployment` 的示例：
- en: '[PRE6]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `ManifestWork` is created on the hub cluster and is deployed to the target
    cluster according to the namespace mapping. Each target cluster has a namespace
    representing it in the hub cluster. A work agent running on the target cluster
    will monitor all `ManifestWork` resources on the hub cluster in their namespace
    and sync changes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`ManifestWork` 在中心集群上创建，并根据命名空间映射部署到目标集群。每个目标集群在中心集群中都有一个表示它的命名空间。运行在目标集群上的工作代理将监控它们命名空间中所有来自中心集群的
    `ManifestWork` 资源，并同步更改。'
- en: OCM governance, risk, and compliance
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OCM 治理、风险与合规性
- en: OCM provides a governance model based on policies, policy templates, and policy
    controllers. The policies can be bound to a specific set of clusters for fine-grained
    control.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: OCM 提供了一种基于策略、策略模板和策略控制器的治理模型。这些策略可以绑定到特定的集群集合，从而实现精细的控制。
- en: 'Here is a sample policy that requires the existence of a namespace called `Prod`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例策略，要求存在一个名为 `Prod` 的命名空间：
- en: '[PRE7]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: See [https://open-cluster-management.io/](https://open-cluster-management.io/)
    for more details.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [https://open-cluster-management.io/](https://open-cluster-management.io/)
    获取更多详情。
- en: Virtual Kubelet
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟 Kubelet
- en: 'Virtual Kubelet is a fascinating project. It impersonates a kubelet to connect
    Kubernetes to other APIs such as AWS Fargate or Azure ACI. The Virtual Kubelet
    looks like a node to the Kubernetes cluster, but the compute resources backing
    it up are abstracted away. The Virtual Kubelet looks like just another node to
    the Kubernetes cluster:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟 Kubelet 是一个令人着迷的项目。它伪装成一个 kubelet，将 Kubernetes 连接到其他 API，例如 AWS Fargate 或
    Azure ACI。虚拟 Kubelet 对 Kubernetes 集群看起来就像一个节点，但它背后的计算资源是抽象的。虚拟 Kubelet 对 Kubernetes
    集群来说看起来就像另一个节点：
- en: '![](img/B18998_11_09.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_09.png)'
- en: 'Figure 11.9: Virtual Kubelet, which looks like a regular node to the Kubernetes
    cluster'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9：虚拟 Kubelet，对 Kubernetes 集群看起来像一个常规节点
- en: 'The features of the Virtual Kubelet are:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟 Kubelet 的特性包括：
- en: Creating, updating, and deleting pods
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建、更新和删除 pod
- en: Accessing container logs and metrics
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问容器日志和指标
- en: Getting a pod, pods, and pod status
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取 pod、pods 和 pod 状态
- en: Managing capacity
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理容量
- en: Accessing node addresses, node capacity, and node daemon endpoints
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问节点地址、节点容量和节点守护进程端点
- en: Choosing the operating system
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择操作系统
- en: Supporting your own virtual network
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持你自己的虚拟网络
- en: See [https://github.com/virtual-kubelet/virtual-kubelet](https://github.com/virtual-kubelet/virtual-kubelet)
    for more details.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请见 [https://github.com/virtual-kubelet/virtual-kubelet](https://github.com/virtual-kubelet/virtual-kubelet)。
- en: This concept can be used to connect multiple Kubernetes clusters too, and several
    projects follow this approach. Let’s look briefly at some projects that use Virtual
    Kubelet for multi-cluster management such as tensile-kube, Admiralty, and Liqo.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念也可以用来连接多个 Kubernetes 集群，许多项目采用了这种方式。让我们简要了解一些使用虚拟 Kubelet 进行多集群管理的项目，例如
    tensile-kube、Admiralty 和 Liqo。
- en: Tensile-kube
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tensile-kube
- en: Tensile-kube is a sub-project of the Virtual Kubelet organization on GitHub.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Tensile-kube 是 Virtual Kubelet 组织在 GitHub 上的一个子项目。
- en: 'Tensile-kube brings the following to the table:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Tensile-kube 提供以下功能：
- en: Automatic discovery of cluster resources
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群资源的自动发现
- en: Async notification of pod modifications
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步通知 pod 修改
- en: Full access to pod logs and kubectl exec
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全访问 pod 日志和 kubectl exec
- en: Global scheduling of pods
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pod 的全局调度
- en: Re-scheduling of pods using descheduler
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 descheduler 重新调度 pod
- en: PV/PVC
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PV/PVC
- en: Service
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务
- en: Tensile-kube uses the terminology of the upper cluster for the cluster that
    contains the Virtual Kubelets, and the lower clusters for the clusters that are
    exposed as virtual nodes in the upper cluster.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Tensile-kube 使用上层集群（包含虚拟 Kubelet 的集群）和下层集群（作为虚拟节点在上层集群中暴露的集群）这一术语。
- en: 'Here is the tensile-kube architecture:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 tensile-kube 架构：
- en: '![](img/B18998_11_10.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_10.png)'
- en: 'Figure 11.10: Tensile-kube architecture'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10：Tensile-kube 架构
- en: See [https://github.com/virtual-kubelet/tensile-kube](https://github.com/virtual-kubelet/tensile-kube)
    for more details.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请见 [https://github.com/virtual-kubelet/tensile-kube](https://github.com/virtual-kubelet/tensile-kube)。
- en: Admiralty
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Admiralty
- en: Admiralty is an open source project backed by a commercial company. Admiralty
    takes the Virtual Kubelet concept and builds a sophisticated solution for multi-cluster
    orchestration and scheduling. Target clusters are represented as virtual nodes
    in the source cluster. It has a pretty complicated architecture that involves
    three levels of scheduling. Whenever a pod is created on a proxy, pods are created
    on the source cluster, candidate pods are created on each target cluster, and
    eventually, one of the candidate pods is selected and becomes a delegate pod,
    which is a real pod that actually runs its containers. This is all supported by
    custom multi-cluster schedulers built on top of the Kubernetes scheduling framework.
    To schedule workloads on Admiralty, you need to annotate any pod template with
    `multicluster.admiralty.io/elect=""` and Admiralty will take it from there.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Admiralty 是一个由商业公司支持的开源项目。Admiralty 采用了虚拟 Kubelet 概念，并构建了一个复杂的多集群调度和编排解决方案。目标集群作为源集群中的虚拟节点来表示。它有一个相当复杂的架构，涉及三层调度。每当在代理上创建
    pod 时，会在源集群上创建 pods，在每个目标集群上创建候选 pod，最终选定一个候选 pod 作为委托 pod，后者是一个真实的 pod，实际运行其容器。所有这一切都由基于
    Kubernetes 调度框架构建的自定义多集群调度器支持。要在 Admiralty 上调度工作负载，你需要为任何 pod 模板添加注解 `multicluster.admiralty.io/elect=""`，然后
    Admiralty 将接管后续操作。
- en: 'Here is a diagram that demonstrates the interplay between different components:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个展示不同组件之间相互作用的图示：
- en: '![](img/B18998_11_11.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_11.png)'
- en: 'Figure 11.11: Admiralty architecture'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：Admiralty 架构
- en: 'Admiralty provides the following features:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Admiralty 提供以下功能：
- en: Highly available
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用
- en: Live disaster recovery
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时灾难恢复
- en: Dynamic **CDN** (**content delivery network**)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态**CDN**（**内容分发网络**）
- en: Multi-cluster workflows
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多集群工作流
- en: Support for edge computing, IoT, and 5G
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持边缘计算、物联网（IoT）和5G
- en: Governance
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 治理
- en: Cluster upgrades
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群升级
- en: Clusters as cattle abstraction
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群作为牲畜的抽象
- en: Global resource federation
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全球资源联合
- en: Cloud bursting and arbitrage
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云爆发与套利
- en: See [https://admiralty.io](https://admiralty.io) for more details.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://admiralty.io](https://admiralty.io)了解更多详情。
- en: Liqo
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Liqo
- en: Liqo is an open source project based on the liquid computing concept. Let your
    tasks and data float around and find the best place to run. Its scope is very
    impressive, as it targets not only the compute aspect of running pods across multiple
    clusters but also provides network fabric and storage fabric. These aspects of
    connecting clusters and managing data across clusters are often harder problems
    to solve than just running workloads.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Liqo是一个基于液态计算概念的开源项目。让你的任务和数据漂浮并找到最佳运行位置。它的范围非常广泛，不仅针对在多个集群中运行Pod的计算方面，还提供了网络架构和存储架构。这些连接集群并跨集群管理数据的方面，通常比单纯运行工作负载更难解决。
- en: In Liqo’s terminology, the management cluster is called the home cluster and
    the target clusters are called foreign clusters. The virtual nodes in the home
    cluster are called “Big” nodes, and they represent the foreign clusters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在Liqo的术语中，管理集群称为本地集群，目标集群称为外部集群。本地集群中的虚拟节点称为“大”节点，它们代表外部集群。
- en: Liqo utilizes IP address mapping to achieve a flat IP address space across all
    foreign clusters that may have internal IP conflicts.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Liqo利用IP地址映射实现了跨所有可能存在内部IP冲突的外部集群的扁平IP地址空间。
- en: Liqo filters and batches events from the foreign clusters to reduce pressure
    on the home cluster.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Liqo过滤并批处理来自外部集群的事件，以减轻本地集群的压力。
- en: 'Here is a diagram of the Liqo architecture:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是Liqo架构的图示：
- en: '![](img/B18998_11_12.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_12.png)'
- en: 'Figure 11.12: Liqo architecture'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：Liqo架构
- en: See [https://liqo.io](https://liqo.io) for more details.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://liqo.io](https://liqo.io)了解更多详情。
- en: Let’s move on and take an in-depth look at the Gardener project, which takes
    a different approach.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解Gardener项目，它采用了不同的方法。
- en: Introducing the Gardener project
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Gardener项目
- en: The Gardener project is an open source project developed by SAP. It lets you
    manage thousands (yes, thousands!) of Kubernetes clusters efficiently and economically.
    Gardener solves a very complex problem, and the solution is elegant but not simple.
    Gardener is the only project that addresses both the cluster lifecycle and application
    lifecycle.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Gardener项目是由SAP开发的开源项目。它使你能够高效、经济地管理成千上万个（没错，是成千上万个！）Kubernetes集群。Gardener解决了一个非常复杂的问题，而且解决方案优雅但并不简单。Gardener是唯一一个同时处理集群生命周期和应用生命周期的项目。
- en: In this section, we will cover the terminology of Gardener and its conceptual
    model, dive deep into its architecture, and learn about its extensibility features.
    The primary theme of Gardener is to use Kubernetes to manage Kubernetes clusters.
    A good way to think about Gardener is Kubernetes-control-plane-as-a-service.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖Gardener的术语及其概念模型，深入探讨其架构，并了解其可扩展性功能。Gardener的主要主题是使用Kubernetes来管理Kubernetes集群。理解Gardener的一个好方法是把它看作是Kubernetes控制平面即服务。
- en: See [https://gardener.cloud](https://gardener.cloud) for more details.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://gardener.cloud](https://gardener.cloud)了解更多详情。
- en: Understanding the terminology of Gardener
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Gardener的术语
- en: The Gardener project, as you may have guessed, uses botanical terminology to
    describe the world. There is a garden, which is a Kubernetes cluster responsible
    for managing seed clusters. A seed is a Kubernetes cluster responsible for managing
    a set of shoot clusters. A shoot cluster is a Kubernetes cluster that runs actual
    workloads.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，Gardener项目采用了植物学术语来描述其世界。园区是一个Kubernetes集群，负责管理种子集群。种子是一个Kubernetes集群，负责管理一组shoot集群。shoot集群是运行实际工作负载的Kubernetes集群。
- en: The cool idea behind Gardener is that the shoot clusters contain only the worker
    nodes. The control planes of all the shoot clusters run as Kubernetes pods and
    services in the seed cluster.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Gardener背后的酷点子是，shoot集群仅包含工作节点。所有shoot集群的控制平面作为Kubernetes Pods和服务运行在seed集群中。
- en: 'The following diagram describes in detail the structure of Gardener and the
    relationships between its components:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图描述了Gardener的结构及其各个组件之间的关系：
- en: '![](img/B18998_11_13.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_13.png)'
- en: 'Figure 11.13: The Gardener project structure'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13：Gardener 项目结构
- en: Don’t panic! Underlying all this complexity is a crystal clear conceptual model.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 不要慌张！在所有这些复杂性背后，是一个清晰明了的概念模型。
- en: Understanding the conceptual model of Gardener
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Gardener 的概念模型
- en: The architecture diagram of Gardener can be overwhelming. Let’s unpack it slowly
    and surface the underlying principles. Gardener really embraces the spirit of
    Kubernetes and offloads a lot of the complexity of managing a large set of Kubernetes
    clusters to Kubernetes itself. At its heart, Gardener is an aggregated API server
    that manages a set of custom resources using various controllers. It embraces
    and takes full advantage of Kubernetes’ extensibility. This approach is common
    in the Kubernetes community. Define a set of custom resources and let Kubernetes
    manage them for you. The novelty of Gardener is that it takes this approach to
    the extreme and abstracts away parts of Kubernetes infrastructure itself.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: Gardener 的架构图可能会让人感到不知所措。让我们慢慢解开它，揭示其背后的基本原则。Gardener 真正拥抱了 Kubernetes 的精神，将管理大量
    Kubernetes 集群的复杂性交给 Kubernetes 自身。Gardener 的核心是一个聚合的 API 服务器，通过各种控制器管理一组自定义资源。它充分利用了
    Kubernetes 的可扩展性，这种做法在 Kubernetes 社区中很常见。定义一组自定义资源，让 Kubernetes 来为你管理它们。Gardener
    的创新之处在于，它将这一方法推向极致，甚至抽象化了 Kubernetes 基础设施的某些部分。
- en: In a “normal” Kubernetes cluster, the control plane runs in the same cluster
    as the worker nodes. Typically, in large clusters, control plane components like
    the Kubernetes API server and etcd run on dedicated nodes and don’t mix up with
    the worker nodes. Gardener thinks in terms of many clusters and it takes all the
    control planes of all the shoot clusters and has a seed cluster to manage them.
    So the Kubernetes control plane of the shoot clusters is managed in the seed cluster
    as regular Kubernetes `Deployments`, which automatically provides replication,
    monitoring, self-healing, and rolling updates by Kubernetes.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在“正常”的 Kubernetes 集群中，控制平面与工作节点运行在同一个集群中。通常，在大型集群中，Kubernetes API 服务器和 etcd
    等控制平面组件会运行在专用节点上，不与工作节点混合。Gardener 从多个集群的角度进行思考，它将所有 shoot 集群的控制平面集中管理，并且有一个种子集群来管理它们。因此，shoot
    集群的 Kubernetes 控制平面作为常规的 Kubernetes `Deployments` 在种子集群中进行管理，这样 Kubernetes 会自动提供复制、监控、自愈和滚动更新等功能。
- en: So, the control plane of a Kubernetes shoot cluster is analogous to a `Deployment`.
    The seed cluster, on the other hand, maps to a Kubernetes node. It manages multiple
    shoot clusters. It is recommended to have a seed cluster per cloud provider. The
    Gardener developers actually work on a gardenlet controller for seed clusters
    that is similar to the kubelet on nodes.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Kubernetes shoot 集群的控制平面类似于一个 `Deployment`。而种子集群则映射到一个 Kubernetes 节点，它管理多个
    shoot 集群。建议每个云提供商使用一个种子集群。Gardener 的开发人员实际上在为种子集群开发一个 gardenlet 控制器，它类似于节点上的 kubelet。
- en: If the seed clusters are like Kubernetes nodes, then the Garden cluster that
    manages those seed clusters is like a Kubernetes cluster that manages its worker
    nodes.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果种子集群像 Kubernetes 节点，那么管理这些种子集群的 Garden 集群就像是管理其工作节点的 Kubernetes 集群。
- en: By pushing the Kubernetes model this far, the Gardener project leverages the
    strengths of Kubernetes to achieve robustness and performance that would be very
    difficult to build from scratch.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 通过大力推进 Kubernetes 模型，Gardener 项目利用了 Kubernetes 的优势，达到了构建一个从零开始非常难实现的鲁棒性和性能。
- en: Let’s dive into the architecture.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨架构。
- en: Diving into the Gardener architecture
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解 Gardener 架构
- en: Gardener creates a Kubernetes namespace in the seed cluster for each shoot cluster.
    It manages the certificates of the shoot clusters as Kubernetes secrets in the
    seed cluster.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Gardener 会在种子集群中为每个 shoot 集群创建一个 Kubernetes 命名空间。它将 shoot 集群的证书作为 Kubernetes
    密钥存储在种子集群中。
- en: Managing the cluster state
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理集群状态
- en: The etcd data store for each cluster is deployed as a StatefulSet with one replica.
    In addition, events are stored in a separate etcd instance. The etcd data is periodically
    snapshotted and stored in remote storage for backup and restore purposes. This
    enables very fast recovery of clusters that lost their control plane (e.g., when
    an entire seed cluster becomes unreachable). Note that when a seed cluster goes
    down, the shoot cluster continues to run as usual.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 每个集群的 etcd 数据存储作为一个带有单个副本的 StatefulSet 部署。此外，事件会存储在一个独立的 etcd 实例中。etcd 数据会定期快照，并存储在远程存储中，以备份和恢复使用。这使得当集群丧失控制平面时（例如，整个种子集群变得无法访问时），能够非常快速地恢复。请注意，当种子集群出现故障时，shoot
    集群仍会照常运行。
- en: Managing the control plane
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理控制平面
- en: As mentioned before, the control plane of a shoot cluster X runs in a separate
    seed cluster, while the worker nodes run in a shoot cluster. This means that pods
    in the shoot cluster can use internal DNS to locate each other, but communication
    to the Kubernetes API server running in the seed cluster must be done through
    an external DNS. This means the Kubernetes API server runs as a `Service` of the
    `LoadBalancer` type.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，射出集群 X 的控制平面运行在一个独立的种子集群中，而工作节点运行在射出集群中。这意味着射出集群中的 Pods 可以使用内部 DNS 进行相互定位，但与运行在种子集群中的
    Kubernetes API 服务器的通信必须通过外部 DNS 来完成。这意味着 Kubernetes API 服务器作为 `LoadBalancer` 类型的
    `Service` 运行。
- en: Preparing the infrastructure
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备基础设施
- en: When creating a new shoot cluster, it’s important to provide the necessary infrastructure.
    Gardener uses Terraform for this task. A Terraform script is dynamically generated
    based on the shoot cluster specification and stored as a ConfigMap within the
    seed cluster. To facilitate this process, a dedicated component (Terraformer)
    runs as a job, performs all the provisioning, and then writes the state into a
    separate ConfigMap.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建新的射出集群时，提供必要的基础设施非常重要。Gardener 使用 Terraform 来完成这项任务。根据射出集群的规格，动态生成一个 Terraform
    脚本，并将其作为 ConfigMap 存储在种子集群中。为了简化这一过程，一个专用组件（Terraformer）作为一个作业运行，执行所有的资源配置，然后将状态写入一个单独的
    ConfigMap 中。
- en: Using the Machine controller manager
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用机器控制器管理器
- en: To provision nodes in a provider-agnostic manner that can work for private clouds
    too, Gardener has several custom resources such as `MachineDeployment`, `MachineClass`,
    `MachineSet`, and `Machine`. They work with the Kubernetes Cluster Lifecycle group
    to unify their abstractions because there is a lot of overlap. In addition, Gardener
    takes advantage of the cluster auto-scaler to offload the complexity of scaling
    node pools up and down.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以与提供商无关的方式配置节点，Gardener 提供了多个自定义资源，例如 `MachineDeployment`、`MachineClass`、`MachineSet`
    和 `Machine`。它们与 Kubernetes 集群生命周期组协作，以统一其抽象，因为它们之间有很多重叠。此外，Gardener 还利用了集群自动扩缩器，以减轻节点池扩展和收缩的复杂性。
- en: Networking across clusters
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨集群网络
- en: The seed cluster and shoot clusters can run on different cloud providers. The
    worker nodes in the shoot clusters are often deployed in private networks. Since
    the control plane needs to interact closely with the worker nodes (mostly the
    kubelet), Gardener creates a VPN for direct communication.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 种子集群和射出集群可以运行在不同的云提供商上。射出集群中的工作节点通常部署在私有网络中。由于控制平面需要与工作节点（主要是 kubelet）紧密交互，Gardener
    创建了一个 VPN 以便直接通信。
- en: Monitoring clusters
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控集群
- en: Observability is a big part of operating complex distributed systems. Gardener
    provides a lot of monitoring out of the box using best-of-class open source projects
    like a central Prometheus server, deployed in the garden cluster that collects
    information about all seed clusters. In addition, each shoot cluster gets its
    own Prometheus instance in the seed cluster. To collect metrics, Gardener deploys
    two `kube-state-metrics` instances for each cluster (one for the control plane
    in the seed and one for the worker nodes in the shoot). The node-exporter is deployed
    too to provide additional information on the nodes. The Prometheus `AlertManager`
    is used to notify the operator when something goes wrong. Grafana is used to display
    dashboards with relevant data on the state of the system.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性是运营复杂分布式系统的重要组成部分。Gardener 提供了丰富的监控功能，内置了顶级的开源项目，如部署在花园集群中的中央 Prometheus
    服务器，收集所有种子集群的信息。此外，每个射出集群都会在种子集群中获得一个独立的 Prometheus 实例。为了收集指标，Gardener 为每个集群部署了两个
    `kube-state-metrics` 实例（一个用于种子集群中的控制平面，一个用于射出集群中的工作节点）。节点导出器也被部署，用于提供有关节点的额外信息。Prometheus
    `AlertManager` 用于在出现问题时通知操作员。Grafana 用于显示包含系统状态相关数据的仪表板。
- en: The gardenctl CLI
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gardenctl CLI
- en: 'You can manage Gardener using only kubectl, but you will have to switch profiles
    and contexts a lot as you explore different clusters. Gardener provides the `gardenctl`
    command-line tool that offers higher-level abstractions and can operate on multiple
    clusters at the same time. Here is an example:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以仅使用 kubectl 来管理 Gardener，但在探索不同集群时，你需要频繁切换配置文件和上下文。Gardener 提供了 `gardenctl`
    命令行工具，它提供了更高层次的抽象，并且可以同时操作多个集群。以下是一个示例：
- en: '[PRE8]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: One of the most prominent features of Gardener is its extensibility. It has
    a large surface area and it supports many environments. Let’s see how extensibility
    is built into its design.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Gardener的一个突出特点是其可扩展性。它有着广泛的适用范围，支持许多环境。让我们来看一下它是如何在设计中实现可扩展性的。
- en: Extending Gardener
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展Gardener
- en: 'Gardener supports the following environments:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Gardener支持以下环境：
- en: AliCloud
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿里云
- en: AWS
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS
- en: Azure
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure
- en: Equinix Metal
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Equinix Metal
- en: GCP
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP
- en: OpenStack
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack
- en: vSphere
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere
- en: 'It started, like Kubernetes itself, with a lot of provider-specific support
    in the primary Gardener repository. Over time, it followed the Kubernetes example
    that externalized cloud providers and migrated the providers to separate Gardener
    extensions. Providers can be specified using a CloudProfile CRD such as:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 它像Kubernetes一样，从主Gardener仓库中的大量特定于提供商的支持开始。随着时间的推移，它借鉴了Kubernetes的做法，将云提供商外部化，并将这些提供商迁移到独立的Gardener扩展中。可以使用CloudProfile
    CRD来指定提供商，例如：
- en: '[PRE9]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, a shoot cluster will choose a provider and configure it with the necessary
    information:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一个shoot集群将选择一个提供商，并使用必要的信息进行配置：
- en: '[PRE10]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'But, the extensibility goals of Gardener go far beyond just being provider
    agnostic. The overall process of standing up a Kubernetes cluster involves many
    steps. The Gardener project aims to let the operator customize each and every
    step by defining custom resources and webhooks. Here is the general flow diagram
    with the CRDs, mutating/validating admission controllers, and webhooks associated
    with each step:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，Gardener的可扩展性目标远不止于仅仅做到与提供商无关。搭建Kubernetes集群的整个过程涉及许多步骤。Gardener项目的目标是让运维人员通过定义自定义资源和Webhook，来定制每一个步骤。以下是与每个步骤相关的CRD、变更/验证准入控制器和Webhook的总体流程图：
- en: '![](img/B18998_11_14.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_11_14.png)'
- en: 'Figure 11.14: Flow diagram of CRDs mutating and validating admission controllers'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14：CRD变更和验证准入控制器的流程图
- en: 'Here are the CRD categories that comprise the extensibility space of Gardener:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是构成Gardener可扩展性空间的CRD类别：
- en: Providers for DNS management, such as Route53 and CloudDNS
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于DNS管理的提供商，如Route53和CloudDNS
- en: Providers for blob storage, including S3, GCS, and ABS
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于Blob存储的提供商，包括S3、GCS和ABS
- en: Infrastructure providers like AWS, GCP, and Azure
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施提供商，如AWS、GCP和Azure
- en: Support for various operating systems such as CoreOS Container Linux, Ubuntu,
    and FlatCar Linux
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持CoreOS Container Linux、Ubuntu和FlatCar Linux等各种操作系统
- en: Network plugins like Calico, Flannel, and Cilium
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络插件，如Calico、Flannel和Cilium
- en: Optional extensions, such as Let’s Encrypt’s certificate service
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选扩展，例如Let’s Encrypt的证书服务
- en: We have covered Gardener in depth, which brings us to the end of the chapter.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经深入探讨了Gardener，这也标志着本章的结束。
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we’ve covered the exciting area of multi-cluster management.
    There are many projects that tackle this problem from different angles. The Cluster
    API project has a lot of momentum for solving the sub-problem of managing the
    lifecycle of multiple clusters. Many other projects take on the resource management
    and application lifecycle. These projects can be divided into two categories:
    projects that explicitly manage multiple clusters using a management cluster and
    managed clusters, and projects that utilize the Virtual Kubelet where whole clusters
    appear as virtual nodes in the main cluster.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了多集群管理这一激动人心的领域。有许多项目从不同的角度来解决这个问题。Cluster API项目在解决多个集群生命周期管理的子问题上有着很大的动力。许多其他项目则着眼于资源管理和应用生命周期。这些项目可以分为两类：一类是通过管理集群和被管理集群来显式管理多个集群，另一类是利用虚拟Kubelet，将整个集群作为虚拟节点呈现在主集群中。
- en: The Gardener project has a very interesting approach and architecture. It tackles
    the problem of multiple clusters from a different perspective and focuses on the
    large-scale management of clusters. It is the only project that addresses both
    cluster lifecycle and application lifecycle.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Gardener项目有一种非常有趣的方式和架构。它从不同的角度解决了多个集群的问题，并专注于集群的大规模管理。它是唯一同时处理集群生命周期和应用生命周期的项目。
- en: At this point, you should have a clear understanding of the current state of
    multi-cluster management and what the different projects offer. You may decide
    that it’s still too early or that you want to take the plunge.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你应该已经清楚地了解了当前多集群管理的状况以及不同项目所提供的功能。你可以决定是觉得现在还为时过早，还是准备勇敢尝试。
- en: 'In the next chapter, we will explore the exciting world of serverless computing
    on Kubernetes. Serverless can mean two different things: you don’t have to manage
    servers for your long-running workloads, and also, running functions as a service.
    Both forms of serverless are available for Kubernetes, and both of them are extremely
    useful.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探索 Kubernetes 上激动人心的无服务器计算世界。无服务器可以有两种不同的含义：你不需要为长时间运行的工作负载管理服务器，同时，还可以将函数作为服务运行。Kubernetes
    提供了这两种形式的无服务器计算，它们都非常有用。
